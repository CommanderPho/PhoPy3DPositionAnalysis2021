{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e4f6b1-02ff-407f-a303-1946a2708a0e",
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import sys\n",
    "import traceback # for stack trace formatting\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "import numpy as np\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "# %gui qt\n",
    "# !env QT_API=\"pyqt5\"\n",
    "%gui qt5\n",
    "# %gui qt6\n",
    "# from PyQt5.Qt import QApplication\n",
    "# # start qt event loop\n",
    "# _instance = QApplication.instance()\n",
    "# if not _instance:\n",
    "#     _instance = QApplication([])\n",
    "# app = _instance\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "from pyphoplacecellanalysis.General.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "# global_data_root_parent_path = Path(r'W:\\Data') # Windows Apogee\n",
    "global_data_root_parent_path = Path(r'/media/MAX/Data') # Diba Lab Workstation Linux\n",
    "# global_data_root_parent_path = Path(r'/Volumes/MoverNew/data') # rMBP\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d9aad-f22c-4607-8bb7-e315312b325a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40fa1a4-56ac-49fe-aefc-c25669d862d4",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_session_names_list: ['2006-6-13_14-42-6', '2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_3-23-37', '2021-12-06 Code Backup', '2006-6-09_1-22-43', 'DibaCode', 'KamranMatlabCode', '2006-6-12_15-55-31']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-26-53'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2021-12-06 Code Backup'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/DibaCode'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/KamranMatlabCode'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " PosixPath('/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "## Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "### Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26506af-54f3-43f4-af6f-be88d37e2013",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Single basedir (non-batch) testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7768e9-3492-4312-abfc-c239adc7229c",
   "metadata": {
    "tags": [
     "load",
     "single_session"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.spikes.mat... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:140: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Computing linear positions for all active epochs for session... Saving updated position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.position.npy... 2006-6-13_14-42-6.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.interpolated_spike_positions.npy... 2006-6-13_14-42-6.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/2006-6-13_14-42-6.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/one/2006-6-13_14-42-6/ripple_df.pkl.\n",
      "Loading success: .mua.npy.\n",
      "Loading success: .pbe.npy.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 521.0076666191453)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 521.0076666191453, end: 854.7482555289753)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 854.7482555289753)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to whitelist, including only 7 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (15787,)\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 7 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (10112,)\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 7 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (25900,)\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "basedir = local_session_paths_list[0] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, force_reload=True, skip_extended_batch_computations=True)\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=True, skip_extended_batch_computations=True, debug_print=False)\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=True, skip_extended_batch_computations=True) # temp no-save\n",
    "## SAVE AFTERWARDS!\n",
    "\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=False, active_pickle_filename='20221214200324-loadedSessPickle.pkl', skip_extended_batch_computations=True)\n",
    "# curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=False, active_pickle_filename='loadedSessPickle - full-good.pkl', skip_extended_batch_computations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446496b-c286-4a76-b888-94be2f92aee3",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true tags=[] jp-MarkdownHeadingCollapsed=true",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "# Future: theta-dependent placefields: build separate placefields for each phase of theta (binned in theta). There should be one set (where the animal is representing the present) that nearly perfectly predicts the animal's location.\n",
    "    # the rest of the variability \n",
    "\n",
    "    1. Basic Hilbert transform\n",
    "    2. But Theta wave-shape (sawtooth) at higher running speeds.\n",
    "        - do peak-to-trough and trough-to-peak separate\n",
    "        ** Nat will send me something\n",
    "        \n",
    "- remember Eloy's theta-dependent placefields. I'm ashamed that I fucked up with Eloy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ff852-0f53-4eac-89d7-3a890d7fd743",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36565a6-97ac-4ed7-be87-87d972fa3057",
   "metadata": {},
   "source": [
    "https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "Nat's code for detecting the sawtooth theta is here (lines 271-393ish): https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "\n",
    "It's all based on this paper: https://www.jneurosci.org/content/32/2/423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79d7b015-d93e-488c-8575-4525546a93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratchpad for opto\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import Analysis.python.LFP.preprocess_data as pd\n",
    "\n",
    "import scipy.signal as signal\n",
    "import pickle\n",
    "import os\n",
    "# import Analysis.python.LFP.helpers as helpers\n",
    "\n",
    "## LFP analysis functions from https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/lfp_analysis.py\n",
    "\n",
    "# instead of `import Analysis.python.LFP.lfp_analysis as lfp`\n",
    "class lfp(object):\n",
    "    ## Create Butterworth filter - copied from scipy-cookbook webpage\n",
    "    @classmethod\n",
    "    def butter_bandpass(cls, lowcut, highcut, fs, order=2):\n",
    "        \"\"\"\n",
    "        Simplify inputs for creating a Butterworth filter. copied from scipy-cookbook webpage.\n",
    "        :param lowcut: Hz\n",
    "        :param highcut: Hz\n",
    "        :param fs: Sampling rate in Hz\n",
    "        :param order: (optional) 2 = default\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        b, a = signal.butter(order, [low, high], btype='band')\n",
    "\n",
    "        return b, a\n",
    "\n",
    "\n",
    "    ## filter data through butterworth filter\n",
    "    @classmethod\n",
    "    def butter_bandpass_filter(cls, data, lowcut, highcut, fs, type='filtfilt', order=2):\n",
    "        \"\"\"\n",
    "        Filter data through butterworth bandpass filter. Copied from scipy-cookbook webpage.\n",
    "        :param data: array of data sampled at fs\n",
    "        :param lowcut: 4 Hz\n",
    "        :param highcut: 10 Hz\n",
    "        :param type: 'filtfilt' (default) filters both ways, 'lfilt' filters forward only (and likely induces a phase offset).\n",
    "        :param fs: 30000 Hz\n",
    "        :param order: (optional) default = 2 to match Sieglie et al., eLife (2014)\n",
    "        :return: filt_data: filtered data\n",
    "        \"\"\"\n",
    "        b, a = cls.butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        if type == 'lfilt':\n",
    "            filt_data = signal.lfilter(b, a, data)\n",
    "        elif type == 'filtfilt':\n",
    "            filt_data = signal.filtfilt(b, a, data)\n",
    "\n",
    "        return filt_data\n",
    "\n",
    "\n",
    "    ## Peak-trough detection via Belluscio et al. (2012) J. Neuro\n",
    "    @classmethod\n",
    "    def get_local_extrema(cls, trace, type='max'):\n",
    "        \"\"\" Get local extrema, assuming it occurs near the middle of the trace. spits out an np.nan if there a relative min\n",
    "        or max occurs at the edge.\n",
    "        :param trace: lfp trace\n",
    "        :param type: 'max' (default) or 'min'\n",
    "        :return: index in trace where max/min is located. np.nan if there is a relative minima/maxima at edge of trace.\n",
    "        \"\"\"\n",
    "        if type == 'max':\n",
    "            temp = signal.argrelmax(trace, order=int(len(trace)/2))[0]\n",
    "        elif type == 'min':\n",
    "            temp = signal.argrelmin(trace, order=int(len(trace)/2))[0]\n",
    "\n",
    "        if temp.size == 1:\n",
    "            ind_rel_extreme = temp[0]\n",
    "        else:\n",
    "            ind_rel_extreme = np.nan\n",
    "\n",
    "        return ind_rel_extreme\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def peak_trough(cls, trace, SRlfp, order=2, lowcut_bell=1, highcut_bell=80, peak_trough_offset_sec=0.07):\n",
    "        \"\"\" Peak-trough method (Belluscio et al. 2012 J Neuro) - fold into lfp_analysis.peak_trough_detect eventually\n",
    "            from Nat's https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "            Detect the peaks and troughs in the wide-filtered trace using peak-trough method and return time points\n",
    "        Args:\n",
    "            trace: The trace from which to detect peaks and troughs.\n",
    "            SRlfp: Sampling rate (Hz)\n",
    "            order: The order of the filter used.\n",
    "            time_span: The length of the time window in which to detect peaks and troughs.\n",
    "            start_time: The start time of the time window.\n",
    "            peak_trough_offset_sec: seconds to look for trough of wide-filtered trace next to 4-10Hz filtered trace\n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            wide_peak_inds_good: The wide peaks\n",
    "            wide_trough_inds_good: The wide troughs\n",
    "\n",
    "        Usage:\n",
    "\n",
    "            lowcut = 4 # Hz\n",
    "            highcut = 10 # Hz\n",
    "            SRlfp = SRlfp = traces.sampling_rate\n",
    "            wave_phase_inds, wave_phases, wide_peak_inds_good, wide_trough_inds_good = lfp.peak_trough(trace, traces.sampling_rate, order=2, lowcut_bell=1, highcut_bell=80, peak_trough_offset_sec=0.07)\n",
    "\n",
    "        \"\"\"\n",
    "        trace_lfilt = lfp.butter_bandpass_filter(trace, lowcut, highcut, SRlfp, order=order, type='lfilt')\n",
    "        \n",
    "        wide_filt = lfp.butter_bandpass_filter(trace, lowcut_bell, highcut_bell, SRlfp, order=order)\n",
    "        offset_frames = np.round(peak_trough_offset_sec*SRlfp)\n",
    "\n",
    "        # First detect peak and trough off narrowband filtered signal - do hilbert transform\n",
    "        # trough = -pi->pi, peak = 0 (- -> +)\n",
    "        trace_analytic = signal.hilbert(trace_lfilt)  # get real and imaginary parts of signal\n",
    "        trig_trace_phase = np.angle(trace_analytic)\n",
    "        # ax.plot(time_plot, trig_trace_phase*v_range/8, 'r-')\n",
    "        peak_bool = np.bitwise_and(trig_trace_phase[0:-1] < 0, trig_trace_phase[1:] >= 0)\n",
    "        peak_bool = np.append(peak_bool, False)\n",
    "        trough_bool = np.bitwise_and(trig_trace_phase[0:-1] > 0, trig_trace_phase[1:] <= 0)\n",
    "        trough_bool = np.append(trough_bool, False)\n",
    "\n",
    "        # Indices to peak and trough of narrowband trace\n",
    "        peak_inds = np.where(peak_bool)[0]\n",
    "        trough_inds = np.where(trough_bool)[0]\n",
    "\n",
    "        ## now step through and find closest peak/trough in the wide-filtered trace when compared to the narrowband filtered trace.\n",
    "        # THIS IS ALL COMMENTED NOW SO THAT YOU DONT ACCIDENTALLY OVERWRITE EXISTING VALUES - NEED TO IMPLEMENT DOWNSAMPLING FIRST!!!\n",
    "        wide_peak_inds = []\n",
    "        wide_trough_inds = []\n",
    "\n",
    "        # Step through and look for each trough in the WIDE filtered signal between two peaks in the NARROW filtered signal\n",
    "        # how fast is this compared to just running it on all the trace and looking for closest inds? Bet it depends on if I\n",
    "        # downsample first...\n",
    "\n",
    "        n = 0\n",
    "        for idp, idp1 in zip(peak_inds[0:-1], peak_inds[1:]):\n",
    "            wide_trough_inds.append(lfp.get_local_extrema(wide_filt[idp:idp1], type='min') + idp)\n",
    "            n = n + 1\n",
    "            if int(n/100) == n/100:\n",
    "                print(n)\n",
    "\n",
    "        n = 0\n",
    "        # Ditto to above but for peaks\n",
    "        for idt, idt1 in zip(trough_inds[0:-1], trough_inds[1:]):\n",
    "            wide_peak_inds.append(lfp.get_local_extrema(wide_filt[idt:idt1], type='max') + idt)\n",
    "            n = n + 1\n",
    "            if int(n/100) == n/100:\n",
    "                print(n)\n",
    "\n",
    "        ## looks decent except when there is crappy theta. Filter out these epochs? Put on speed threshold?\n",
    "        wide_peak_inds_good = [idp for idp in wide_peak_inds if not np.isnan(idp)]\n",
    "        wide_trough_inds_good = [idt for idt in wide_trough_inds if not np.isnan(idt)]\n",
    "\n",
    "        ## Get rise and falling times of theta - trough = -pi/+pi, peak = 0\n",
    "\n",
    "        # if peak times generally lead trough times, chop off first peak value\n",
    "        if np.nanmean(np.array(wide_peak_inds) - np.array(wide_trough_inds)) < 0:\n",
    "            peak_inds_use = wide_peak_inds[1:]\n",
    "            trough_inds_use = wide_trough_inds[0:-1]\n",
    "            next_trough_inds = wide_trough_inds[1:]\n",
    "        else:\n",
    "            peak_inds_use = wide_peak_inds\n",
    "            trough_inds_use = wide_trough_inds\n",
    "            next_trough_inds = wide_trough_inds[1:]\n",
    "\n",
    "        wave_phase_inds = []\n",
    "        wave_phases = []\n",
    "        for idt, idp, idt1 in zip(trough_inds_use, peak_inds_use, next_trough_inds):\n",
    "            if not np.any(np.isnan([idt, idp, idt1])) and idt < idp < idt1:  # only run below if you have reliable peak/trough info\n",
    "                trace_snippet = wide_filt[idt:idt1]  # grab a snippet of the trace to use\n",
    "                if np.all(trace_snippet <= 0) or np.all(trace_snippet >= 0) or trace_snippet[0] > 0 or trace_snippet[-1] > 0\\\n",
    "                        or wide_filt[idp] < 0:  # Make sure trace is not all above or below zero and that peak/troughs are above/below zero\n",
    "                    wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "                else:\n",
    "                    rise_zero = np.max(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) < idp))[0])\n",
    "                    fall_zero = np.min(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) > idp))[0])\n",
    "                    wave_phase_inds.extend([idt, idt + rise_zero, idp, idt + fall_zero, idt1-1])\n",
    "            else:\n",
    "                wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "            wave_phases.extend([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\n",
    "\n",
    "            \n",
    "        return wave_phase_inds, wave_phases, wide_peak_inds_good, wide_trough_inds_good, peak_inds_use, trough_inds_use, next_trough_inds, trace_lfilt, wide_filt \n",
    "\n",
    "      \n",
    "    \n",
    "from neuropy.analyses import oscillations\n",
    "## Plot trace in a nice working window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b575b75-f436-47e7-a1aa-80c5c826d261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef3e504-3dcb-42c0-babe-fd2427958152",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfpFile = curr_active_pipeline.sess.eegfile # neuropy.io.binarysignalio.BinarysignalIO\n",
    "traces = lfpFile.get_signal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205a9330-ecfa-4c87-b252-7d26458320a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878.2899361022364"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa71c73-96ee-4923-bfd0-7889cca3f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected channels for ripples: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95]\n",
      "22810 epochs detected initially\n",
      "2 epochs reamining after merging close ones\n",
      "2 epochs reamining after deleting epochs with weaker power\n",
      "2 epochs reamining after deleting short epochs\n",
      "0 epochs reamining after deleting very long epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start, stop, peakpower, peaktime, duration, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ripple_epochs = oscillations.detect_ripple_epochs(traces, curr_active_pipeline.sess.probegroup)\n",
    "ripple_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c1f171-78c4-4dee-8639-150d43e77973",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = lfpFile.get_signal(channel_indx=19)\n",
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e47941-6e39-4deb-a2b5-405ae7d5a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfpFile.n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62af39-2df5-482f-80a7-712ac145137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfpFile.n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd3aed-55af-4ce7-92cd-6025a8fe9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f4c5d32-7e65-4d2e-bbda-8062b85af16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 1099619)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces_ds = traces.traces\n",
    "np.shape(traces_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "087854eb-80f1-4142-877c-f2bd11224962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 1099619)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.traces.shape # (96, 1099619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f02d414f-1c67-4e86-ab6c-2f6a85e4f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_plot = 19  # channel you triggered off of\n",
    "artifact_chan = 13  # this channel should have good stimulation artifact on it for reference...\n",
    "\n",
    "order = 2\n",
    "lowcut = 4 # Hz\n",
    "highcut = 10 # Hz\n",
    "SRlfp = SRlfp = traces.sampling_rate\n",
    "\n",
    "trace = traces.traces[chan_plot, :]\n",
    "trace_lfilt = lfp.butter_bandpass_filter(trace, lowcut, highcut, SRlfp, order=order, type='lfilt')\n",
    "trace_filtfilt = lfp.butter_bandpass_filter(trace, lowcut, highcut, SRlfp, order=order, type='filtfilt')\n",
    "\n",
    "wave_phase_inds, wave_phases, wide_peak_inds_good, wide_trough_inds_good, peak_inds_use, trough_inds_use, next_trough_inds, trace_lfilt, wide_filt = lfp.peak_trough(trace, traces.sampling_rate, order=2, lowcut_bell=1, highcut_bell=80, peak_trough_offset_sec=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2dcf9d89-db76-4de5-9e7d-c53ca89e78d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_range = 4793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'uV')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "start_time = traces.t_start\n",
    "time_span = 5.0 # traces.duration\n",
    "time_plot = traces.time\n",
    "wide_filt = trace_lfilt\n",
    "\n",
    "v_range = np.nanmax(trace)\n",
    "print(f'{v_range = }')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharex=True, sharey=True)\n",
    "fig.set_size_inches([26, 3])\n",
    "hraw = ax.plot(time_plot, trace)\n",
    "ax.plot(time_plot, wide_filt, 'm')\n",
    "ax.plot(time_plot, trace_lfilt, 'k--')\n",
    "ax.set_xlim([start_time*60, start_time*60 + time_span])\n",
    "# ax.set_ylim([-v_range, v_range])\n",
    "ax.set_xlabel(['Time (s)'])\n",
    "ax.set_ylabel('uV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f319565-b7f6-4fc1-b4c4-ece383283803",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6606,) (6607,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m fig35\u001b[38;5;241m.\u001b[39mset_size_inches([\u001b[38;5;241m13.5\u001b[39m, \u001b[38;5;241m4.8\u001b[39m])\n\u001b[1;32m      9\u001b[0m rise_times \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marray(peak_inds_use) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(trough_inds_use))\u001b[38;5;241m/\u001b[39mSRlfp\n\u001b[0;32m---> 10\u001b[0m fall_times \u001b[38;5;241m=\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_trough_inds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeak_inds_use\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m/\u001b[39mSRlfp\n\u001b[1;32m     11\u001b[0m ax35[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhist(rise_times, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.3\u001b[39m))\n\u001b[1;32m     12\u001b[0m ax35[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeak-Trough Method\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6606,) (6607,) "
     ]
    }
   ],
   "source": [
    "# now plot\n",
    "wave_phase_inds_good = [idph for idph in wave_phase_inds if not np.isnan(idph)]\n",
    "wave_phases_good = [ph for ph, idph in zip(wave_phases, wave_phase_inds) if not np.isnan(idph)]\n",
    "ax.plot(time_plot[wave_phase_inds_good], np.asarray(wave_phases_good)*v_range/8, 'r-')\n",
    "\n",
    "## histogram of rise times vs fall times overlaid to prove I'm doing things correctly\n",
    "fig35, ax35 = plt.subplots(1, 2)\n",
    "fig35.set_size_inches([13.5, 4.8])\n",
    "rise_times = (np.array(peak_inds_use) - np.array(trough_inds_use))/SRlfp\n",
    "fall_times = (np.array(next_trough_inds) - np.array(peak_inds_use))/SRlfp\n",
    "ax35[0].hist(rise_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[0].set_title('Peak-Trough Method')\n",
    "ax35[0].set_xlabel('Rising Phase Times (s)')\n",
    "ax35[0].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(rise_times)) + ' sec')\n",
    "ax35[1].hist(fall_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[1].set_title('Peak-Trough Method')\n",
    "ax35[1].set_xlabel('Falling Phase Times (s)')\n",
    "ax35[1].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(fall_times)) + ' sec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2907e-06a0-47ce-ac4b-028cdd2168b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Direct Import of Nat's Method (Pre 2023-02-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0117cd8-3177-4008-8972-5f4d8c34cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Peak-trough method (Belluscio et al. 2012 J Neuro) - fold into lfp_analysis.peak_trough_detect eventually\n",
    "# from Nat's https://github.com/diba-lab/ephys/blob/master/Analysis/python/LFP/scripts/theta_phase_stim_verify.py\n",
    "\n",
    "## Needs: trace, SRlfp, order\n",
    "order = 2\n",
    "\n",
    "lowcut_bell = 1  # Hz\n",
    "highcut_bell = 80  # Hz\n",
    "peak_trough_offset_sec = 0.07  # seconds to look for trough of wide-filtered trace next to 4-10Hz filtered trace\n",
    "\n",
    "wide_filt = lfp.butter_bandpass_filter(trace, lowcut_bell, highcut_bell, SRlfp, order=order)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharex=True, sharey=True)\n",
    "fig.set_size_inches([26, 3])\n",
    "hraw = ax.plot(time_plot, trace)\n",
    "ax.plot(time_plot, wide_filt, 'm')\n",
    "ax.plot(time_plot, trace_lfilt, 'k--')\n",
    "ax.set_xlim([start_time*60, start_time*60 + time_span])\n",
    "ax.set_ylim([-v_range, v_range])\n",
    "ax.set_xlabel(['Time (s)'])\n",
    "ax.set_ylabel('uV')\n",
    "\n",
    "offset_frames = np.round(peak_trough_offset_sec*SRlfp)\n",
    "\n",
    "# First detect peak and trough off narrowband filtered signal - do hilbert transform\n",
    "# trough = -pi->pi, peak = 0 (- -> +)\n",
    "trace_analytic = signal.hilbert(trace_lfilt)  # get real and imaginary parts of signal\n",
    "trig_trace_phase = np.angle(trace_analytic)\n",
    "# ax.plot(time_plot, trig_trace_phase*v_range/8, 'r-')\n",
    "peak_bool = np.bitwise_and(trig_trace_phase[0:-1] < 0, trig_trace_phase[1:] >= 0)\n",
    "peak_bool = np.append(peak_bool, False)\n",
    "trough_bool = np.bitwise_and(trig_trace_phase[0:-1] > 0, trig_trace_phase[1:] <= 0)\n",
    "trough_bool = np.append(trough_bool, False)\n",
    "\n",
    "# Indices to peak and trough of narrowband trace\n",
    "peak_inds = np.where(peak_bool)[0]\n",
    "trough_inds = np.where(trough_bool)[0]\n",
    "\n",
    "# Check that above code works...\n",
    "# ax.plot(time_plot[peak_bool], trace_lfilt[peak_bool], 'r*')\n",
    "# ax.plot(time_plot[trough_bool], trace_lfilt[trough_bool], 'g*')\n",
    "\n",
    "##  Plot times between peak and trough - seems likes looking 0.07 seconds to either side should be ok...\n",
    "fig2, ax2 = plt.subplots(1, 2)\n",
    "ax2[0].hist(np.diff(np.where(trough_bool))[0]/SRlfp)\n",
    "ax2[0].set_xlabel('Trough-to-trough times (s)')\n",
    "ax2[1].hist(np.diff(np.where(peak_bool))[0]/SRlfp)\n",
    "ax2[1].set_xlabel('Peak-to-peak times (s)')\n",
    "\n",
    "## now step through and find closest peak/trough in the wide-filtered trace when compared to the narrowband filtered trace.\n",
    "# THIS IS ALL COMMENTED NOW SO THAT YOU DONT ACCIDENTALLY OVERWRITE EXISTING VALUES - NEED TO IMPLEMENT DOWNSAMPLING FIRST!!!\n",
    "wide_peak_inds = []\n",
    "wide_trough_inds = []\n",
    "\n",
    "# Step through and look for each trough in the WIDE filtered signal between two peaks in the NARROW filtered signal\n",
    "# how fast is this compared to just running it on all the trace and looking for closest inds? Bet it depends on if I\n",
    "# downsample first...\n",
    "\n",
    "n = 0\n",
    "for idp, idp1 in zip(peak_inds[0:-1], peak_inds[1:]):\n",
    "    wide_trough_inds.append(lfp.get_local_extrema(wide_filt[idp:idp1], type='min') + idp)\n",
    "    n = n + 1\n",
    "    if int(n/100) == n/100:\n",
    "        print(n)\n",
    "\n",
    "n = 0\n",
    "# Ditto to above but for peaks\n",
    "for idt, idt1 in zip(trough_inds[0:-1], trough_inds[1:]):\n",
    "    wide_peak_inds.append(lfp.get_local_extrema(wide_filt[idt:idt1], type='max') + idt)\n",
    "    n = n + 1\n",
    "    if int(n/100) == n/100:\n",
    "        print(n)\n",
    "\n",
    "## looks decent except when there is crappy theta. Filter out these epochs? Put on speed threshold?\n",
    "wide_peak_inds_good = [idp for idp in wide_peak_inds if not np.isnan(idp)]\n",
    "wide_trough_inds_good = [idt for idt in wide_trough_inds if not np.isnan(idt)]\n",
    "\n",
    "ax.plot(time_plot[wide_peak_inds_good], wide_filt[wide_peak_inds_good], 'ro')\n",
    "ax.plot(time_plot[wide_trough_inds_good], wide_filt[wide_trough_inds_good], 'go')\n",
    "\n",
    "## Get rise and falling times of theta - trough = -pi/+pi, peak = 0\n",
    "\n",
    "# if peak times generally lead trough times, chop off first peak value\n",
    "if np.nanmean(np.array(wide_peak_inds) - np.array(wide_trough_inds)) < 0:\n",
    "    peak_inds_use = wide_peak_inds[1:]\n",
    "    trough_inds_use = wide_trough_inds[0:-1]\n",
    "    next_trough_inds = wide_trough_inds[1:]\n",
    "else:\n",
    "    peak_inds_use = wide_peak_inds\n",
    "    trough_inds_use = wide_trough_inds\n",
    "    next_trough_inds = wide_trough_inds[1:]\n",
    "\n",
    "\n",
    "wave_phase_inds = []\n",
    "wave_phases = []\n",
    "for idt, idp, idt1 in zip(trough_inds_use, peak_inds_use, next_trough_inds):\n",
    "    if not np.any(np.isnan([idt, idp, idt1])) and idt < idp < idt1:  # only run below if you have reliable peak/trough info\n",
    "        trace_snippet = wide_filt[idt:idt1]  # grab a snippet of the trace to use\n",
    "        if np.all(trace_snippet <= 0) or np.all(trace_snippet >= 0) or trace_snippet[0] > 0 or trace_snippet[-1] > 0\\\n",
    "                or wide_filt[idp] < 0:  # Make sure trace is not all above or below zero and that peak/troughs are above/below zero\n",
    "            wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "        else:\n",
    "            rise_zero = np.max(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) < idp))[0])\n",
    "            fall_zero = np.min(np.where(np.bitwise_and(trace_snippet <= 0, np.arange(idt, idt1) > idp))[0])\n",
    "            wave_phase_inds.extend([idt, idt + rise_zero, idp, idt + fall_zero, idt1-1])\n",
    "    else:\n",
    "        wave_phase_inds.extend([np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    wave_phases.extend([-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e10e5a-5d95-4a95-bed7-fe99b179a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot\n",
    "wave_phase_inds_good = [idph for idph in wave_phase_inds if not np.isnan(idph)]\n",
    "wave_phases_good = [ph for ph, idph in zip(wave_phases, wave_phase_inds) if not np.isnan(idph)]\n",
    "ax.plot(time_plot[wave_phase_inds_good], np.asarray(wave_phases_good)*v_range/8, 'r-')\n",
    "\n",
    "\n",
    "## histogram of rise times vs fall times overlaid to prove I'm doing things correctly\n",
    "fig35, ax35 = plt.subplots(1, 2)\n",
    "fig35.set_size_inches([13.5, 4.8])\n",
    "rise_times = (np.array(peak_inds_use) - np.array(trough_inds_use))/SRlfp\n",
    "fall_times = (np.array(next_trough_inds) - np.array(peak_inds_use))/SRlfp\n",
    "ax35[0].hist(rise_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[0].set_title('Peak-Trough Method')\n",
    "ax35[0].set_xlabel('Rising Phase Times (s)')\n",
    "ax35[0].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(rise_times)) + ' sec')\n",
    "ax35[1].hist(fall_times, bins=20, range=(-0.15, 0.3))\n",
    "ax35[1].set_title('Peak-Trough Method')\n",
    "ax35[1].set_xlabel('Falling Phase Times (s)')\n",
    "ax35[1].text(0.15, 1000, 'mean = ' + '{:.3f}'.format(np.nanmean(fall_times)) + ' sec')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-poetry",
   "language": "python",
   "name": "spike3d-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
