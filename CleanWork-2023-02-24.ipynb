{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb364483-af18-4721-a504-688311f0a4ab",
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import traceback # for stack trace formatting\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# # os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "# # os.environ[\"MODIN_ENGINE\"] = \"unidist\" # Modin will use Unidist\n",
    "# # os.environ[\"UNIDIST_BACKEND\"] = \"mpi\" # Unidist will use MPI backend\n",
    "# import modin.pandas as pd # alternative to pandas which is much faster\n",
    "# # Installed with `poetry add modin[all]`\n",
    "\n",
    "# from pandas_profiling import ProfileReport ## for dataframe viewing\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables, objsize, print_object_memory_usage, debug_dump_object_member_shapes, TypePrintMode\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_time_str, get_now_time_precise_str\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "from pyphoplacecellanalysis.General.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "global_data_root_parent_path = Path(r'W:\\Data') # Windows Apogee\n",
    "# global_data_root_parent_path = Path(r'/media/MAX/Data') # Diba Lab Workstation Linux\n",
    "# global_data_root_parent_path = Path(r'/Volumes/MoverNew/data') # rMBP\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538e2a-4e39-4d11-90b5-a9fef9258058",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f917bad9-8fe7-4882-b83b-71cf878fffd2",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_session_names_list: ['2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_1-22-43', '2006-6-09_3-23-37', '2006-6-12_15-55-31', '2006-6-13_14-42-6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-07_11-26-53'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-08_14-26-15'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_3-23-37'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-12_15-55-31'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-13_14-42-6'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "## Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "### Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ### Animal `pin01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='pin01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # no exper_name ('one' or 'two') folders for this animal.\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['redundant','showclus','sleep','tmaze'])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155124df-eddb-461c-804f-de8f7acd332e",
   "metadata": {
    "tags": [
     "load",
     "single_session"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.epochs_info.mat... done.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.position_info.mat... done.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.spikes.mat... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\IsolatedSpike3DEnv\\NeuroPy\\neuropy\\core\\session\\Formats\\SessionSpecifications.py:140: UserWarning: WARNING: Optional File: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Computing linear positions for all active epochs for session... Saving updated position results results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.position.npy... 2006-6-08_14-26-15.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.interpolated_spike_positions.npy... 2006-6-08_14-26-15.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\ripple_df.pkl.\n",
      "Loading success: .mua.npy.\n",
      "Loading success: .pbe.npy.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1211.5580800310709)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1211.5580800310709, end: 2093.8978568242164)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2093.8978568242164)\n",
      "computing neurons mua for session...\n",
      "\n",
      "using provided computation_functions_name_whitelist: ['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_two_step_position_decoding_computation']\n",
      "due to whitelist, including only 6 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36668,)\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 6 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26242,)\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 6 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (62911,)\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "# %%viztracer\n",
    "basedir = local_session_paths_list[1] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# epoch_name_whitelist = ['maze']\n",
    "epoch_name_whitelist = None\n",
    "active_computation_functions_name_whitelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        # '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_whitelist=epoch_name_whitelist,\n",
    "                                          computation_functions_name_whitelist=active_computation_functions_name_whitelist,\n",
    "                                          saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=True, skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d77d8-811b-4402-a37d-dcd82efa8fc9",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "# 2023-02-24 Decoders \n",
    "- [ ] where are cells chosen for inclusion in the input of the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c5cf00e-cd16-40ab-a8f9-7b0b2c4c36ed",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "long_epoch_name, short_epoch_name, global_epoch_name = long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "\n",
    "recalculate_anyway = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17793403-7842-4c6c-ade7-2a0c487f777d",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self will be re-binned to match target_pf1D...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36668,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26242,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "decoding_time_bin_size: 0.03333\n"
     ]
    }
   ],
   "source": [
    "# Make the 1D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf1D = long_results.pf1D\n",
    "short_pf1D = short_results.pf1D\n",
    "global_pf1D = global_results.pf1D\n",
    "\n",
    "# short_pf1D, did_update_bins = short_pf1D.conform_to_position_bins(long_pf1D, force_recompute=True) # not needed because it's done in one_step_decoder_1D.conform_to_position_bins(...)\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D  = [results_data.get('pf1D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_1D, did_recompute = short_one_step_decoder_1D.conform_to_position_bins(long_one_step_decoder_1D, force_recompute=True)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_1D is None) or (short_two_step_decoder_1D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_1D is not None and short_two_step_decoder_1D is not None)\n",
    "\n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "# decoding_time_bin_size = 0.03 # 0.03333333333333333\n",
    "print(f'decoding_time_bin_size: {decoding_time_bin_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fc6cd-84b9-405f-bf53-6b66a6c32525",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "#### Get 2D Decoders for validation and comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7b9806-923f-42fd-921f-1f75bdddc5cb",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self will be re-binned to match target_pf1D...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36668,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26242,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n"
     ]
    }
   ],
   "source": [
    "# Make the 2D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf2D = long_results.pf2D\n",
    "short_pf2D = short_results.pf2D\n",
    "global_pf2D = global_results.pf2D\n",
    "\n",
    "# long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "# long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "\n",
    "# short_pf2D, did_update_bins = short_pf2D.conform_to_position_bins(long_pf2D)\n",
    "long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_2D, did_recompute = short_one_step_decoder_2D.conform_to_position_bins(long_one_step_decoder_2D)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_2D is None) or (short_two_step_decoder_2D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_2D is not None and short_two_step_decoder_2D is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43499e87-0693-4d11-9d33-100a987737ac",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.sum(long_one_step_decoder_2D.marginal.x.p_x_given_n) =36668.0,\t np.sum(long_one_step_decoder_1D.p_x_given_n) = 36668.00000000002\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "1D Decoder should have an x-posterior equal to its own posterior",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m long_one_step_decoder_2D\u001b[38;5;241m.\u001b[39mmarginal\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mmost_likely_positions_1D\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m long_one_step_decoder_1D\u001b[38;5;241m.\u001b[39mmost_likely_positions\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust equal but: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlong_one_step_decoder_2D\u001b[38;5;241m.\u001b[39mmarginal\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mmost_likely_positions_1D\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlong_one_step_decoder_1D\u001b[38;5;241m.\u001b[39mmost_likely_positions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m## validate values:\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(long_one_step_decoder_2D\u001b[38;5;241m.\u001b[39mmarginal\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mp_x_given_n, long_one_step_decoder_1D\u001b[38;5;241m.\u001b[39mp_x_given_n), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1D Decoder should have an x-posterior equal to its own posterior\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(curr_epoch_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarginal_x\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost_likely_positions_1D\u001b[39m\u001b[38;5;124m'\u001b[39m], curr_epoch_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost_likely_positions\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1D Decoder should have an x-posterior with most_likely_positions_1D equal to its own most_likely_positions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 1D Decoder should have an x-posterior equal to its own posterior"
     ]
    }
   ],
   "source": [
    "# Sums are similar:\n",
    "print(f'{np.sum(long_one_step_decoder_2D.marginal.x.p_x_given_n) =},\\t {np.sum(long_one_step_decoder_1D.p_x_given_n) = }') # 31181.999999999996 vs 31181.99999999999\n",
    "\n",
    "## Validate:\n",
    "assert long_one_step_decoder_2D.marginal.x.p_x_given_n.shape == long_one_step_decoder_1D.p_x_given_n.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.p_x_given_n.shape =} and {long_one_step_decoder_1D.p_x_given_n.shape =}\"\n",
    "assert long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape == long_one_step_decoder_1D.most_likely_positions.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape =} and {long_one_step_decoder_1D.most_likely_positions.shape =}\"\n",
    "\n",
    "## validate values:\n",
    "assert np.allclose(long_one_step_decoder_2D.marginal.x.p_x_given_n, long_one_step_decoder_1D.p_x_given_n), f\"1D Decoder should have an x-posterior equal to its own posterior\"\n",
    "assert np.allclose(curr_epoch_result['marginal_x']['most_likely_positions_1D'], curr_epoch_result['most_likely_positions']), f\"1D Decoder should have an x-posterior with most_likely_positions_1D equal to its own most_likely_positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372d125-bca4-4dba-a4bc-ca1fd83d918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate the spikes that are coming in to build the decoder... those are from the placefields actually.\n",
    "    # so it'll be gating the placefield cells.\n",
    "    \n",
    "active_config_name = 'maze1'\n",
    "active_pf_1D = long_pf1D\n",
    "# curr_active_pipeline.computation_results['maze']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275c444-2439-4d16-bbf3-35d8b777d93c",
   "metadata": {},
   "source": [
    "# Testing Placefields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c086de63-62f8-4226-8d74-9930580b30ff",
   "metadata": {
    "tags": [
     "required_imports"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "from neuropy.utils.misc import split_list_of_dicts\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "\n",
    "def _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options: dict) -> dict:\n",
    "    \"\"\" Computes the PfNDs for all the swept parameters (combinations of grid_bin, smooth, etc)\n",
    "    \n",
    "    Usage:\n",
    "        smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "        grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "        all_param_sweep_options = cartesian_product(smooth_options, grid_bin_options)\n",
    "        param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "        output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "\n",
    "    \"\"\"\n",
    "    output_pfs = {} # empty dict\n",
    "\n",
    "    for a_sweep_dict in all_param_sweep_options:\n",
    "        a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
    "        output_pfs[a_sweep_tuple] = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), **a_sweep_dict) # grid_bin=, etc\n",
    "        \n",
    "    return output_pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab4d0e-0df1-42c4-9d4a-7f3596217965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - [ ] Test that changing the position bins post-hoc is equivalent to initially computing with those position bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7f8f-6f12-4539-be0a-9bc9f7aa07ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get testing variables from `curr_active_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec24bfe-191c-4244-890d-5183053a61f5",
   "metadata": {
    "tags": [
     "curr_active_pipeline"
    ]
   },
   "outputs": [],
   "source": [
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "pyramidal_only_spikes_df = deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal') ## get only the pyramidal spikes\n",
    "active_pos = curr_active_pipeline.sess.position\n",
    "active_pos_df = active_pos.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4583150b-eb31-4ebf-a5bb-b35444c450b3",
   "metadata": {
    "tags": [
     "curr_active_pipeline"
    ]
   },
   "outputs": [],
   "source": [
    "## Save for NeuroPy testing:\n",
    "finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "sess_identifier_key='sess'\n",
    "spikes_df.to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "active_pos.to_dataframe().to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9344-c10b-488a-855f-b5547a324393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load testing variables from file 'NeuroPy/tests/neuropy_pf_testing.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03923ef5-5e66-4ee8-84a0-eaee7622222f",
   "metadata": {
    "tags": [
     "load",
     "independent"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Corresponding load for Neuropy Testing file 'NeuroPy/tests/neuropy_pf_testing.h5': \n",
    "    ## Save for NeuroPy testing:\n",
    "    finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "    sess_identifier_key='sess'\n",
    "    spikes_df.to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "    active_pos.to_dataframe().to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df', format='table')\n",
    "\"\"\"\n",
    "finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "sess_identifier_key='sess'\n",
    "# Load the saved .h5 spikes_df and active_pos dataframes for testing:\n",
    "spikes_df = pd.read_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "pyramidal_only_spikes_df = deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal') ## get only the pyramidal spikes\n",
    "\n",
    "active_pos_df = pd.read_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df')\n",
    "active_pos = active_pos_df.position.to_Position_obj() # convert back to a full position object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d54730-06da-4290-8d62-c619dbc82ad8",
   "metadata": {},
   "source": [
    "## Conduct Parameter Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21053b39-214b-49be-905c-da33146f27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "# grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "# grid_bin_options = [(5,5)]\n",
    "# param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "\n",
    "smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0)]\n",
    "grid_bin_options = [(0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(smooth=smooth_options, grid_bin=grid_bin_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3cfbc-5fff-41da-ba7e-a6ca1ffc9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_thresh_options = [0.0, 1.0, 25.0, 50.0, 100.0, 200.0]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(speed_thresh=speed_thresh_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "print_aligned_columns(['speed_thresh', 'num_good_neurons', 'num_total_spikes'], [speed_thresh_options, num_good_placefield_neurons_list, num_total_spikes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf913cb9-3cc4-4289-ab0c-39ee28167ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "frate_thresh_options = [0.0, 0.1, 1.0, 5.0, 10.0, 100.0]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(frate_thresh=frate_thresh_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "print_aligned_columns(['frate_thresh', 'num_good_neurons', 'num_total_spikes'], [frate_thresh_options, num_good_placefield_neurons_list, num_total_spikes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999df69-49b9-49f1-9698-1c7ee62afe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337434e7-eb87-48e4-8de6-8245a51c9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(5, 3), smooth=(0.0, 0.0), frate_thresh=0.1) # TODO: FIXME: BUG: when frate_thresh=0.0, there are 0 good placefield_neuronIDs for all computations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a15e13-59c6-4224-acdf-f2d5fc5e22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PfND_TimeDependent Parameter Sweeps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb234b7-7215-4189-818a-2687a6977aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "\n",
    "# PfND_TimeDependent\n",
    "orginal_pf1D_dt = PfND_TimeDependent(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), frate_thresh=0.0)\n",
    "orginal_pf1D_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54e573-d047-4f17-a080-23e656fa26b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-03-03 - Decoder Testing\n",
    "Useful Decoder-related plotting and visuzliation classes:\n",
    "```python\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018a2cad-75e7-47ab-b90c-7783c851ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The viztracer extension is already loaded. To reload it, use:\n",
      "  %reload_ext viztracer\n"
     ]
    }
   ],
   "source": [
    "%load_ext viztracer\n",
    "from viztracer import VizTracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06ffeae-87ba-42ce-96a2-a127361da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e250e4a6-df77-4617-8a99-dd91618be259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "Total Entries: 2784                                                             \n",
      "Use the following command to open the report:\n",
      "\u001b[92mvizviewer C:\\Users\\pho\\repos\\IsolatedSpike3DEnv\\Spike3D\\viztracer_2023-03-07_08-03-build_new_PfND.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-build_new_PfND.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "    ## Build placefield for the decoder to use:\n",
    "    original_pf1D = PfND(deepcopy(pyramidal_only_spikes_df), deepcopy(active_pos.linear_pos_obj), frate_thresh=0.0) # all other settings default\n",
    "\n",
    "    new_decoder_pf_params = deepcopy(original_pf1D.config) # should be a PlacefieldComputationParameters\n",
    "    # override some settings before computation:\n",
    "    new_decoder_pf_params.time_bin_size = 0.1\n",
    "\n",
    "    ## 1D Decoder\n",
    "    new_decoder_pf1D = original_pf1D\n",
    "    new_1D_decoder_spikes_df = new_decoder_pf1D.filtered_spikes_df.copy()\n",
    "\n",
    "    # Why would it need both the pf1D and the spikes? Doesn't the pf1D include the spikes (and determine the placefields, which are all that are used)???\n",
    "    new_1D_decoder = BayesianPlacemapPositionDecoder(new_decoder_pf_params.time_bin_size, new_decoder_pf1D, new_1D_decoder_spikes_df, debug_print=False)\n",
    "    new_1D_decoder.compute_all()\n",
    "\n",
    "    print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca005c14-bd80-451f-b691-7000f72e764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_1D_decoder.time_bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84ece5-564f-4e0a-b806-ebdb1e94d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_decoder = new_1D_decoder # strangely this makes original_pf.included_neuron_IDs wrapped in an extra list!\n",
    "original_neuron_ids = np.array(original_decoder.pf.ratemap.neuron_ids) # original_pf.included_neuron_IDs\n",
    "subset_included_neuron_IDXs = np.arange(10) # only get the first 10 neuron_ids\n",
    "subset_included_neuron_ids = original_neuron_ids[subset_included_neuron_IDXs] # only get the first 10 neuron_ids\n",
    "print(f'{original_neuron_ids = }\\n{subset_included_neuron_ids = }')\n",
    "neuron_sliced_1D_decoder = original_decoder.get_by_id(subset_included_neuron_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d14c5-2b64-4f19-bae4-64910c68b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_1D_decoder.neuron_IDXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e48db7-800a-403a-bf3d-63c91d73647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PfND\n",
    "get_by_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae1ac0-b7f4-40df-957d-c42ad74ee704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc90d44-7231-40e4-851c-a276a810434b",
   "metadata": {},
   "source": [
    "## Specific Epoch Decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee4f5acd-f070-4242-a528-18cf5956e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e3a8c5-944e-402a-b1c2-177821d70019",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lap-Epochs Decoding:\n",
    "sess = curr_active_pipeline.sess\n",
    "active_decoder = new_1D_decoder\n",
    "decoding_time_bin_size = 0.02\n",
    "global_pos_df = sess.position.to_dataframe()\n",
    "\n",
    "laps_copy = deepcopy(sess.laps)\n",
    "laps_filter_epochs = laps_copy.filtered_by_lap_flat_index(np.arange(6)).as_epoch_obj() # epoch object\n",
    "laps_filter_epochs_decoder_result = active_decoder.decode_specific_epochs(sess.spikes_df, filter_epochs=laps_filter_epochs, decoding_time_bin_size=decoding_time_bin_size, debug_print=False)\n",
    "laps_filter_epochs_decoder_result.epoch_description_list = [f'lap[{epoch_tuple.lap_id}]' for epoch_tuple in laps_filter_epochs.to_dataframe()[['lap_id']].itertuples()] # Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1333e2be-bf03-4c34-8342-00dfaa0cd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_plot_tuple = plot_decoded_epoch_slices(laps_filter_epochs, laps_filter_epochs_decoder_result, global_pos_df=global_pos_df, variable_name='lin_pos', xbin=active_decoder.xbin,\n",
    "                                                        name='stacked_epoch_slices_matplotlib_subplots_LAPS', debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5cf84-156a-4fbe-8102-2ede8e114138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create legend object and add to figure\n",
    "last_axes = list(laps_plot_tuple[2].axs)[-1]\n",
    "last_axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51317ae-2c9c-4064-aa30-0f1fe2e0e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_plot_tuple[2].fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7d6a9-daa9-4cc6-a289-33a9f7adc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_legend = laps_plot_tuple[2].fig.legend(handles=list(laps_plot_tuple[2].axs), labels=[p.get_label() for p in list(laps_plot_tuple[2].axs)], loc='lower center', ncol=4) #\n",
    "out_legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6684d5-5162-4ce0-b563-bf9754c3191b",
   "metadata": {},
   "source": [
    "## \"Leave-one-out\" decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f43e6e3-4664-453b-976f-766115078839",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (58871,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57801,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57325,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57666,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56364,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56973,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57414,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57038,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57617,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57410,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57677,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57649,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57295,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57283,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57739,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57626,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57064,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56722,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57315,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57133,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57502,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57117,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57183,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56853,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56924,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57472,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57062,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57278,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57535,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56886,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57123,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57059,) should be less than time_window_edges: (84280,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57390,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56864,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56938,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56717,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (55993,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57116,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57170,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56995,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57448,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57394,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56544,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57598,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57748,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57326,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (55187,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57239,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (55870,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57483,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57345,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (55603,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56806,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56035,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57724,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56525,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57353,) should be less than time_window_edges: (84268,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (55844,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56923,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56993,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57298,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (56163,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57275,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57254,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (54936,) should be less than time_window_edges: (84282,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (57766,) should be less than time_window_edges: (84282,)!\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_leave_one_aclu_out_decoding_analysis\n",
    "\n",
    "## Lap-Epochs Decoding:\n",
    "sess = curr_active_pipeline.sess\n",
    "# decoding_time_bin_size = 0.02\n",
    "decoding_time_bin_size = 0.5\n",
    "\n",
    "# Common for all decoders:\n",
    "laps_copy = deepcopy(sess.laps)\n",
    "active_filter_epochs = laps_copy.filtered_by_lap_flat_index(np.arange(20)).as_epoch_obj() # epoch object\n",
    "filter_epoch_description_list = [f'lap[{epoch_tuple.lap_id}]' for epoch_tuple in active_filter_epochs.to_dataframe()[['lap_id']].itertuples()]\n",
    "\n",
    "original_1D_decoder, one_left_out_decoder_dict, one_left_out_filter_epochs_decoder_result_dict, one_left_out_omitted_aclu_distance_df, most_contributing_aclus = perform_leave_one_aclu_out_decoding_analysis(pyramidal_only_spikes_df, active_pos_df, active_filter_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "519b5bc3-aa60-4012-85dd-1b5bd172c200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_left_out_filter_epochs_decoder_result_dict[2].most_likely_positions_list) # len(...): 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f681f7e-6e8f-4046-b91c-7a116074d28f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# one_left_out_filter_epochs_decoder_result_dict[aclu].most_likely_positions_list[epoch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f42e83b9-1782-4431-88e3-ebb436e5871c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Plotting leave-one-out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38d64891-cd85-474a-8463-0d1681258f0e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e4ee8c7-be9e-45b1-b421-a3aca5f4c86d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Plotting leave-one-out:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m laps_plot_tuple \u001b[38;5;241m=\u001b[39m \u001b[43mplot_decoded_epoch_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_filter_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mone_left_out_filter_epochs_decoder_result_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_pos_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_pos_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlin_pos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxbin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_1D_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxbin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstacked_epoch_slices_matplotlib_subplots_LAPS_leave_one_out\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\repos\\IsolatedSpike3DEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\DisplayFunctions\\DecoderPredictionError.py:636\u001b[0m, in \u001b[0;36mplot_decoded_epoch_slices\u001b[1;34m(filter_epochs, filter_epochs_decoder_result, global_pos_df, variable_name, xbin, enable_flat_line_drawing, debug_test_max_num_slices, name, debug_print)\u001b[0m\n\u001b[0;32m    633\u001b[0m epoch_slices \u001b[38;5;241m=\u001b[39m epochs_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;66;03m# epoch_description_list = [f'ripple {epoch_tuple.label} (peakpower: {epoch_tuple.peakpower})' for epoch_tuple in epochs_df[['label', 'peakpower']].itertuples()]\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m epoch_labels \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_epochs_decoder_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_description_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_print:\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "laps_plot_tuple = plot_decoded_epoch_slices(active_filter_epochs,  one_left_out_filter_epochs_decoder_result_dict[2], global_pos_df=active_pos_df, variable_name='lin_pos', xbin=original_1D_decoder.xbin,\n",
    "                                                        name='stacked_epoch_slices_matplotlib_subplots_LAPS_leave_one_out', debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36951b80-958d-43d2-a7c7-0d93bae0bd85",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    " one_left_out_filter_epochs_decoder_result_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6f03895-8253-47a4-b992-1f790c5b7f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>omitted_aclu</th>\n",
       "      <th>distance</th>\n",
       "      <th>avg_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>108</td>\n",
       "      <td>[805384.5270954198, 818629.2393698376, 732670....</td>\n",
       "      <td>894526.202837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>85</td>\n",
       "      <td>[870599.6236279319, 632354.149506013, 756842.1...</td>\n",
       "      <td>878863.305982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>62</td>\n",
       "      <td>[930369.4441492383, 629780.6524278668, 762494....</td>\n",
       "      <td>872116.532157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>73</td>\n",
       "      <td>[901300.1622875874, 649189.9876117947, 826559....</td>\n",
       "      <td>871502.874103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>68</td>\n",
       "      <td>[838207.6336714462, 639315.0516196529, 775738....</td>\n",
       "      <td>869770.706560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>61</td>\n",
       "      <td>[801233.1701610248, 641216.3876390441, 743200....</td>\n",
       "      <td>813768.284732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>93</td>\n",
       "      <td>[791604.755090524, 624797.0773569717, 769031.1...</td>\n",
       "      <td>805926.869310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>88</td>\n",
       "      <td>[842353.8938342747, 538512.7402264528, 781333....</td>\n",
       "      <td>801622.269493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>75</td>\n",
       "      <td>[862106.3347104664, 600615.7498066148, 737344....</td>\n",
       "      <td>785910.762408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>86</td>\n",
       "      <td>[794993.2505276686, 563753.4114839033, 620134....</td>\n",
       "      <td>724775.196392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    omitted_aclu                                           distance  \\\n",
       "63           108  [805384.5270954198, 818629.2393698376, 732670....   \n",
       "51            85  [870599.6236279319, 632354.149506013, 756842.1...   \n",
       "35            62  [930369.4441492383, 629780.6524278668, 762494....   \n",
       "43            73  [901300.1622875874, 649189.9876117947, 826559....   \n",
       "40            68  [838207.6336714462, 639315.0516196529, 775738....   \n",
       "..           ...                                                ...   \n",
       "34            61  [801233.1701610248, 641216.3876390441, 743200....   \n",
       "58            93  [791604.755090524, 624797.0773569717, 769031.1...   \n",
       "54            88  [842353.8938342747, 538512.7402264528, 781333....   \n",
       "45            75  [862106.3347104664, 600615.7498066148, 737344....   \n",
       "52            86  [794993.2505276686, 563753.4114839033, 620134....   \n",
       "\n",
       "         avg_dist  \n",
       "63  894526.202837  \n",
       "51  878863.305982  \n",
       "35  872116.532157  \n",
       "43  871502.874103  \n",
       "40  869770.706560  \n",
       "..            ...  \n",
       "34  813768.284732  \n",
       "58  805926.869310  \n",
       "54  801622.269493  \n",
       "45  785910.762408  \n",
       "52  724775.196392  \n",
       "\n",
       "[65 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_left_out_omitted_aclu_distance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a706e595-b36e-4b16-b775-60712d9923e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Get time-binned spikes during each epoch event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c882a-187e-4ae9-8242-de2cef285b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Determine the probability of that observed neural sequence given the placefields.\n",
    "    # - [ ] Issue: this probability only accounts for the cells that are firing, not the ones that aren't firing, right?\n",
    "    Consider two cells (A and B) that highly correlated (effectively redundant) in a particular environment.\n",
    "        - IF we observe activity of A without activity B, that is highly suggestive that we are not representing this environment where they are correlated, right? But our likelihood doesn't measure this quanity.\n",
    "    # - [ ] Issue: The typical conditional probability assumes all cells are independent, which is specifically NOT the case, and especially not for replays.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a967f-500b-4261-b31d-9d8a34256ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a03b0e-9977-4af9-98a4-93021f804a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59608c2b-3b75-468c-989f-ae87449feb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_contributing_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff74922-f2d4-4998-84d5-2b99d3f1cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEBUG: Get just one of the decoders using a left-out cell:\n",
    "# left_out_aclu, curr_aclu_omitted_decoder = list(one_left_out_decoder_dict.items())[0]\n",
    "# left_out_aclu, curr_aclu_omitted_decoder = list(one_left_out_decoder_dict.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc051fc3-9af7-46d0-beae-eb1951970472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a simple plot for comparison:\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(curr_time_bins, window_center_measured_pos_x, label='measured')\n",
    "# ax.plot(curr_time_bins, curr_most_likely_positions, label=f'omit_{left_out_aclu}')\n",
    "ax.plot(curr_time_bins, curr_most_likely_valid_positions, label=f'valid only omit_{left_out_aclu}')\n",
    "plt.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06b78f-f51b-41d0-b6e6-c1431c2df912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(pos_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aae5aa-965e-4940-96a3-9f82b103105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_pos_df.resample(\n",
    "    time_binned_position_df\n",
    "    interpolate\n",
    "    \n",
    "time_binned_position_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3ef89-e5da-4712-b675-02404eee8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_out_aclu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f8ffa-0e1c-4e71-be0f-e35b54084c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_decoder.decode_specific_epochs("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8904a-c517-4d64-8528-c776c76819c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95390cc6-4b60-4a98-a479-8a6c06cdace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_filter_epochs_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c46e5-655a-4cae-a920-1c0b26405eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_1D_decoder.compute_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e0f93-237b-441c-8667-3977e6c2ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.print_helpers import print_object_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54771eca-b5dd-4250-a621-39a496c4ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(original_1D_decoder) # original_1D_decoder - object size: 90.199349 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5dd56-1eee-4434-ade5-1c6a187b2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(one_left_out_laps_filter_epochs_decoder_result_dict) # one_left_out_laps_filter_epochs_decoder_result_dict - object size: 403.150110 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e916ab-2baf-4a0d-91d2-33e51a33e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(one_left_out_decoder_dict) # 70\n",
    "print_object_memory_usage(one_left_out_decoder_dict) # object size: 12696.855616 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21ea59-269c-48df-a06c-30559680df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decoder = list(one_left_out_decoder_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b7111-5625-485e-8fa5-60b3ffb26042",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decoder.decode_specific_epochs(spikes_df="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad59ae-10a3-4405-9d67-353196a1227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decoder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e50c6-ed5e-43ca-b423-5214f462fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _perform_specific_epochs_decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fd8ca-655e-4395-8020-42b37ff97106",
   "metadata": {},
   "source": [
    "# 2023-02-27 - Test whether conform to active position works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8788d1-134b-4351-93bb-b142a4608b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Placefields with varying bin-sizes:\n",
    "### Here we use frate_thresh=0.0 which ensures that differently binned ratemaps don't have different numbers of spikes or cells.\n",
    "smooth_options = [(None, None)]\n",
    "grid_bin_options = [(0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(grid_bin=grid_bin_options, smooth=smooth_options, frate_thresh=[0.0])\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d501f7-0ac3-4cae-ad9e-9f655389bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_good_placefield_neurons_list, num_total_spikes_list, num_spikes_per_spiketrain_list = compare_placefields_info(output_pfs)\n",
    "print_aligned_columns(['grid_bin x smooth', 'num_good_neurons', 'num_total_spikes'], \n",
    "                     [all_param_sweep_options, num_good_placefield_neurons_list, num_total_spikes_list], enable_checking_all_values_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd76742-2059-41d3-8038-6cb4fcb5bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf605a2-24f2-4574-9520-81c0706f45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14fdbc-6a86-4de8-ad5c-0ce86fab500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sweep_option_n_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbea74-64cb-4df0-bd35-fe621395a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_binned_pf = list(output_pfs.values())[0]\n",
    "coarse_binned_pf = list(output_pfs.values())[-1]\n",
    "\n",
    "print(f'{coarse_binned_pf.bin_info = }\\n{fine_binned_pf.bin_info = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51689e35-11d0-40c0-97c4-56daa9120748",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_binned_pf.bin_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0b738-ad5f-46ac-914f-731a497c6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_binned_pf.bin_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5a9bc-3ec1-4d75-b257-3a039be88bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_fine_binned_pf = deepcopy(fine_binned_pf)\n",
    "rebinned_fine_binned_pf.conform_to_position_bins(target_pf1D=coarse_binned_pf, force_recompute=True)\n",
    "assert rebinned_fine_binned_pf.bin_info == coarse_binned_pf.bin_info # the bins must be equal after conforming\n",
    "\n",
    "num_good_placefield_neurons_list, num_total_spikes_list, num_spikes_per_spiketrain_list = compare_placefields_info(dict(zip(['coarse', 'original', 'rebinned'],[coarse_binned_pf, fine_binned_pf, rebinned_fine_binned_pf])))\n",
    "print_aligned_columns(['pf', 'num_good_neurons', 'num_total_spikes'], [['coarse', 'original', 'rebinned'], num_good_placefield_neurons_list, num_total_spikes_list], enable_checking_all_values_width=True)\n",
    "\n",
    "assert num_good_placefield_neurons_list[0] == num_good_placefield_neurons_list[-1] # require the rebinned pf to have the same number of good neurons as the one that it conformed to\n",
    "assert num_total_spikes_list[0] == num_total_spikes_list[-1] # require the rebinned pf to have the same number of total spikes as the one that it conformed to\n",
    "# assert num_spikes_per_spiketrain_list[0] == num_spikes_per_spiketrain_list[-1] # require the rebinned pf to have the same number of spikes in each spiketrain as the one that it conformed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c6b01-b0eb-4dd6-b820-f0019b85420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO 2023-03-02: plot the three placefields next to each other horizontally (as a single row for comparison):\n",
    "fig, axs = _plot_parameter_sweep(dict(zip([frozenset({'pf':'coarse'}), frozenset({'pf':'original'}), frozenset({'pf':'rebinned'})],[coarse_binned_pf, fine_binned_pf, rebinned_fine_binned_pf])), {'pf':3}, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06843c9-8c6e-4350-979e-fa24ad0550bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73811c-d158-45e6-ac59-9d566463aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ddf44-053c-43df-9855-ab5b2d8ee8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test excluding certain neurons from the placefield\n",
    "original_pf = fine_binned_pf\n",
    "original_pf_neuron_ids = original_pf.included_neuron_IDs.copy()\n",
    "subset_included_neuron_IDXs = np.arange(10) # only get the first 10 neuron_ids\n",
    "subset_included_neuron_ids = original_pf_neuron_ids[subset_included_neuron_IDXs] # only get the first 10 neuron_ids\n",
    "print(f'{original_pf_neuron_ids = }\\n{subset_included_neuron_ids = }')\n",
    "neuron_sliced_pf = deepcopy(fine_binned_pf)\n",
    "neuron_sliced_pf = neuron_sliced_pf.get_by_id(subset_included_neuron_ids)\n",
    "neuron_sliced_pf_neuron_ids = neuron_sliced_pf.included_neuron_IDs\n",
    "print(f'{neuron_sliced_pf_neuron_ids = }')\n",
    "\n",
    "assert np.all(neuron_sliced_pf_neuron_ids == subset_included_neuron_ids) # ensure that the returned neuron ids actually equal the desired subset\n",
    "assert np.all(np.array(neuron_sliced_pf.ratemap.neuron_ids) == subset_included_neuron_ids) # ensure that the ratemap neuron ids actually equal the desired subset\n",
    "assert len(neuron_sliced_pf.ratemap.tuning_curves) == len(subset_included_neuron_ids) # ensure one output tuning curve for each neuron_id\n",
    "np.all(np.isclose(neuron_sliced_pf.ratemap.tuning_curves, [original_pf.ratemap.tuning_curves[idx] for idx in subset_included_neuron_IDXs])) # ensure that the tuning curves built for the neuron_slided_pf are the same as those subset as retrieved from the  original_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3c6ea-8629-499b-8233-33107a246504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_sliced_pf.cell_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f175ff1-ca29-4ba2-865b-9e003d0d1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda42f06-de7d-4f16-9557-5fc596a7fff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf2c25-b755-4407-a65c-23175d6493f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_pf.ratemap.tuning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cf13f-521f-4daa-9f92-0cc330ccaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd846fe-a582-48b1-982b-b0df6cec3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5746da4-a6d7-4f84-90f6-db6802186e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1ce59-7b74-4b58-81c7-679c5df50a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test selecting non-existant neuron_ids for inclusion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9faa1-307c-46a4-90f8-0021dc72f400",
   "metadata": {},
   "source": [
    "# 2023-03-06 - Test Plotting Decoder leave-one-out paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064887c-1f93-4059-a857-41428e380ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation: `_perform_time_dependent_pf_sequential_surprise_computation`\n",
    "_perform_relative_entropy_analyses\n",
    "\"\"\" NOTE: 2022-12-14 - this mirrors the non-global version at `pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ExtendedStats.ExtendedStatsComputations._perform_time_dependent_pf_sequential_surprise_computation` that I just modified except it only uses the global epoch.\n",
    "class TimeDependentPlacefieldSurpriseMode(ExtendedEnum):\n",
    "    \"\"\"for _perform_relative_entropy_analyses \"\"\"\n",
    "    STATIC_METHOD_ONLY = \"static_method_only\"\n",
    "    USING_EXTANT = \"using_extant\"\n",
    "    BUILD_NEW = \"build_new\"\n",
    "    \n",
    "\n",
    "\n",
    "_perform_time_dependent_pf_sequential_surprise_computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57342a6b-80fc-4f24-ae35-2830736b51cd",
   "metadata": {},
   "source": [
    "# 2023-03-07 - Look at scISIs for simple expectation violation in replays/laps given decoders\n",
    "Would like to build a distribution (potentially poisson) that models the temporal relation between two spikes of each unit (e.g. cell_i and cell_j). This allows us to assess how likely it is that any two spikes (spike_i_t, spike_j_t+1) are to be sampled from this distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb0ad2a9-844a-4868-b6aa-1549c586c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_df.spikes.add_same_cell_ISI_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e24b00fd-02ab-4443-a152-f496de3b6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_timestamp_column_name=self.time_variable_name # 't_rel_seconds'\n",
    "self._obj['scISI'] = -1 # initialize the 'scISI' column (same-cell Intra-spike-interval) to -1\n",
    "\n",
    "for (i, a_cell_id) in enumerate(self._obj.spikes.neuron_ids):\n",
    "    # loop through the cell_ids\n",
    "    curr_df = safe_pandas_get_group(self._obj.groupby('aclu'), a_cell_id)\n",
    "    curr_series_differences = curr_df[spike_timestamp_column_name].diff() # These are the ISIs\n",
    "    #set the properties for the points in question:\n",
    "    self._obj.loc[curr_df.index,'scISI'] = curr_series_differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7a593-8ea0-454f-b2f0-dd76cd5782ff",
   "metadata": {},
   "source": [
    "# 2023-03-07 - Look at firing_rate_trends and previous surprise metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "326ffb34-1183-45e0-bb0d-591c18c3de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_result = curr_active_pipeline.computation_results['maze1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d547a8b-217c-422e-9255-07d4044634d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the time-binning from `firing_rate_trends`:\n",
    "active_firing_rate_trends = computation_result.computed_data['firing_rate_trends']\n",
    "time_bin_size_seconds, pf_included_spikes_only = active_firing_rate_trends['time_bin_size_seconds'], active_firing_rate_trends['pf_included_spikes_only']\n",
    "\n",
    "active_time_binning_container, active_time_binned_unit_specific_binned_spike_counts = pf_included_spikes_only['time_binning_container'], pf_included_spikes_only['time_binned_unit_specific_binned_spike_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e674097-476b-43aa-bbfd-7199c2d67b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    }
   ],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_time_dependent_pf_sequential_surprise_computation'], enabled_filter_names=['maze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5f6acab-cb75-49cf-bcc6-2e5a0eabac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "curr_active_pipeline.registered_computation_function_dict['_perform_time_dependent_pf_sequential_surprise_computation'].output_provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71c45c99-3eec-4dea-9482-86e2b076785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_extended_stats = curr_active_pipeline.computation_results['maze'].computed_data['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['relative_entropy_analyses']\n",
    "post_update_times = active_relative_entropy_results['post_update_times']\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals = active_relative_entropy_results['time_intervals']\n",
    "long_short_rel_entr_curves_frames = active_relative_entropy_results['long_short_rel_entr_curves_frames']\n",
    "short_long_rel_entr_curves_frames = active_relative_entropy_results['short_long_rel_entr_curves_frames']\n",
    "flat_relative_entropy_results = active_relative_entropy_results['flat_relative_entropy_results']\n",
    "flat_jensen_shannon_distance_results = active_relative_entropy_results['flat_jensen_shannon_distance_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548cac-954f-4d9f-9fd7-78e7680eff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import draw_epoch_regions\n",
    "\n",
    "# Show basic relative entropy vs. time plot:\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(post_update_times, flat_relative_entropy_results)\n",
    "ax.set_ylabel('Relative Entropy')\n",
    "ax.set_xlabel('t (seconds)')\n",
    "epochs_collection, epoch_labels = draw_epoch_regions(curr_active_pipeline.sess.epochs, ax, defer_render=False, debug_print=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7a991-30fe-4940-b5eb-a92db67ddd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# ax.plot(post_update_times, flat_relative_entropy_results)\n",
    "extents = (post_update_times[0], post_update_times[-1], active_pf_1D_dt.xbin[0], active_pf_1D_dt.xbin[-1]) # (left, right, bottom, top)\n",
    "ax.imshow(flat_relative_entropy_results.T, extent=extents)\n",
    "ax.set_ylabel('Relative Entropy')\n",
    "ax.set_xlabel('t (seconds)')\n",
    "fig.suptitle('flat_relative_entropy_results.T')\n",
    "epochs_collection, epoch_labels = draw_epoch_regions(curr_active_pipeline.sess.epochs, ax, defer_render=False, debug_print=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e6894-1916-4f53-b8d1-36bcdb121e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-03-07 - Similar to previous surprise metrics: build time-dependent placefield for each the long and short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f2974-922e-4cfa-b91d-7cc2194fa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can compute the surprise of a given epoch by taking the pf_dt computed up to epoch_start, computing the snapshot up to epoch_end, and then computing the surprise between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82d36a-8cce-42ba-8383-e1a0a2827b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute snapshots (historical_snapshots_dict) for each Epoch:\n",
    "\n",
    "active_pf_1D_dt = PfND_TimeDependent(deepcopy(active_session_spikes_df), deepcopy(active_pos.linear_pos_obj), epochs=included_epochs,\n",
    "                                                speed_thresh=computation_config.speed_thresh, frate_thresh=computation_config.frate_thresh,\n",
    "                                                grid_bin=computation_config.grid_bin, grid_bin_bounds=computation_config.grid_bin_bounds, smooth=computation_config.smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b026a-6f76-410c-9d8f-df1561caebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute surprise for each snapshot:\n",
    "pf_overlap_results = []\n",
    "flat_relative_entropy_results = []\n",
    "flat_jensen_shannon_distance_results = []\n",
    "\n",
    "n_snapshots = len(historical_snapshots_dict)\n",
    "snapshot_times = list(historical_snapshots_dict.keys())\n",
    "snapshots = list(historical_snapshots_dict.values())\n",
    "snapshot_indicies = np.arange(n_snapshots) # [0, 1, 2, 3, 4]\n",
    "\n",
    "post_update_times = snapshot_times[1:] # all but the first snapshot\n",
    "\n",
    "snapshot_pair_indicies = build_pairwise_indicies(snapshot_indicies) # [(0, 1), (1, 2), (2, 3), ... , (146, 147), (147, 148), (148, 149)]\n",
    "for earlier_snapshot_idx, later_snapshot_idx in snapshot_pair_indicies:\n",
    "    ## Extract the two sequential snapshots for this period:\n",
    "    earlier_snapshot, later_snapshot = snapshots[earlier_snapshot_idx], snapshots[later_snapshot_idx]\n",
    "    earlier_snapshot_t, later_snapshot_t = snapshot_times[earlier_snapshot_idx], snapshot_times[later_snapshot_idx]\n",
    "\n",
    "    ## Proof of concept, comute surprise between the two snapshots:\n",
    "    # relative_entropy_overlap_dict, relative_entropy_overlap_scalars_df = compute_relative_entropy_divergence_overlap(earlier_snapshot, later_snapshot, debug_print=False)\n",
    "    # print(earlier_snapshot['occupancy_weighted_tuning_maps_matrix'].shape) # (108, 63)\n",
    "    # print(later_snapshot['occupancy_weighted_tuning_maps_matrix'].shape) # (108, 63)\n",
    "    # relative_entropy_result_dict = compute_surprise_relative_entropy_divergence(earlier_snapshot['occupancy_weighted_tuning_maps_matrix'], later_snapshot['occupancy_weighted_tuning_maps_matrix'])\n",
    "    relative_entropy_result_dict = compute_surprise_relative_entropy_divergence(earlier_snapshot.occupancy_weighted_tuning_maps_matrix, later_snapshot.occupancy_weighted_tuning_maps_matrix)\n",
    "\n",
    "    # 'long_short_relative_entropy'\n",
    "\n",
    "    # aclu_keys = [k for k,v in relative_entropy_result_dict.items() if v is not None] # len(aclu_keys) # 101\n",
    "    # short_long_rel_entr_curves = np.vstack([v['short_long_rel_entr_curve'] for k,v in relative_entropy_result_dict.items() if v is not None])\n",
    "\n",
    "    # np.vstack(relative_entropy_result_dict['short_long_relative_entropy'])\n",
    "\n",
    "\n",
    "    # short_long_rel_entr_curves # .shape # (101, 63)\n",
    "    # print(f\"{relative_entropy_result_dict['short_long_rel_entr_curve'].shape}\") # (108, 63)\n",
    "    # print(f\"{relative_entropy_result_dict['short_long_relative_entropy'].shape}\") # (63,)\n",
    "\n",
    "    flat_relative_entropy_results.append(relative_entropy_result_dict['short_long_relative_entropy'])\n",
    "    flat_jensen_shannon_distance_results.append(relative_entropy_result_dict['jensen_shannon_distance'])\n",
    "\n",
    "    pf_overlap_results.append({'t': (earlier_snapshot_t, later_snapshot_t),\n",
    "                               'snapshots': (earlier_snapshot, later_snapshot),\n",
    "                               'relative_entropy_result_dict': relative_entropy_result_dict,\n",
    "        # 'short_long_rel_entr_curves': short_long_rel_entr_curves,\n",
    "        # 'relative_entropy_overlap_scalars_df': relative_entropy_overlap_scalars_df,        \n",
    "    })\n",
    "\n",
    "# flatten the relevent results:\n",
    "post_update_times = np.array(post_update_times)\n",
    "flat_jensen_shannon_distance_results = np.vstack(flat_jensen_shannon_distance_results) # flatten the list\n",
    "flat_relative_entropy_results = np.vstack(flat_relative_entropy_results) # flatten the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c7c69-5918-4924-b831-e24e3029405c",
   "metadata": {},
   "source": [
    "# Overflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb1473-77c2-4b0b-8334-44939abfbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all spikes:\n",
    "active_epoch_placefields1Da = PfND(deepcopy(spikes_df), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74209eda-e41f-46e2-95d6-869a225c3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyramidal spikes only:\n",
    "active_epoch_placefields1Db = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # grid_bin=, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef94996-c927-44c7-8826-d0e29b102bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlacefieldComputationParameters\n",
    "# Parameter sweeps:\n",
    "\n",
    "# grid_bin = [(1.0, 1.0)]\n",
    "\n",
    "# 10.0\n",
    "cls.compute_position_grid_bin_size(sess.position.x, sess.position.y, num_bins=(64, 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-poetry",
   "language": "python",
   "name": "spike3d-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
