{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb364483-af18-4721-a504-688311f0a4ab",
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import traceback # for stack trace formatting\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pandas_profiling import ProfileReport ## for dataframe viewing\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables, objsize, print_object_memory_usage, debug_dump_object_member_shapes, TypePrintMode\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_time_str, get_now_time_precise_str\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "from pyphoplacecellanalysis.General.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "global_data_root_parent_path = Path(r'W:\\Data') # Windows Apogee\n",
    "# global_data_root_parent_path = Path(r'/media/MAX/Data') # Diba Lab Workstation Linux\n",
    "# global_data_root_parent_path = Path(r'/Volumes/MoverNew/data') # rMBP\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538e2a-4e39-4d11-90b5-a9fef9258058",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917bad9-8fe7-4882-b83b-71cf878fffd2",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "## Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "### Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ### Animal `pin01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='pin01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # no exper_name ('one' or 'two') folders for this animal.\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['redundant','showclus','sleep','tmaze'])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155124df-eddb-461c-804f-de8f7acd332e",
   "metadata": {
    "tags": [
     "load",
     "single_session"
    ]
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "# %%viztracer\n",
    "basedir = local_session_paths_list[1] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# epoch_name_whitelist = ['maze']\n",
    "epoch_name_whitelist = None\n",
    "active_computation_functions_name_whitelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        # '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_whitelist=epoch_name_whitelist,\n",
    "                                          computation_functions_name_whitelist=active_computation_functions_name_whitelist,\n",
    "                                          saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=False, skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d77d8-811b-4402-a37d-dcd82efa8fc9",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "# 2023-02-24 Decoders \n",
    "- [ ] where are cells chosen for inclusion in the input of the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cf00e-cd16-40ab-a8f9-7b0b2c4c36ed",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "long_epoch_name, short_epoch_name, global_epoch_name = long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "\n",
    "recalculate_anyway = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17793403-7842-4c6c-ade7-2a0c487f777d",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "# Make the 1D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf1D = long_results.pf1D\n",
    "short_pf1D = short_results.pf1D\n",
    "global_pf1D = global_results.pf1D\n",
    "\n",
    "# short_pf1D, did_update_bins = short_pf1D.conform_to_position_bins(long_pf1D, force_recompute=True) # not needed because it's done in one_step_decoder_1D.conform_to_position_bins(...)\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D  = [results_data.get('pf1D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_1D, did_recompute = short_one_step_decoder_1D.conform_to_position_bins(long_one_step_decoder_1D, force_recompute=True)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_1D is None) or (short_two_step_decoder_1D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_1D is not None and short_two_step_decoder_1D is not None)\n",
    "\n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "# decoding_time_bin_size = 0.03 # 0.03333333333333333\n",
    "print(f'decoding_time_bin_size: {decoding_time_bin_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fc6cd-84b9-405f-bf53-6b66a6c32525",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "#### Get 2D Decoders for validation and comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b9806-923f-42fd-921f-1f75bdddc5cb",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "# Make the 2D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf2D = long_results.pf2D\n",
    "short_pf2D = short_results.pf2D\n",
    "global_pf2D = global_results.pf2D\n",
    "\n",
    "# long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "# long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "\n",
    "# short_pf2D, did_update_bins = short_pf2D.conform_to_position_bins(long_pf2D)\n",
    "long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_2D, did_recompute = short_one_step_decoder_2D.conform_to_position_bins(long_one_step_decoder_2D)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_2D is None) or (short_two_step_decoder_2D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_2D is not None and short_two_step_decoder_2D is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43499e87-0693-4d11-9d33-100a987737ac",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "# Sums are similar:\n",
    "print(f'{np.sum(long_one_step_decoder_2D.marginal.x.p_x_given_n) =},\\t {np.sum(long_one_step_decoder_1D.p_x_given_n) = }') # 31181.999999999996 vs 31181.99999999999\n",
    "\n",
    "## Validate:\n",
    "assert long_one_step_decoder_2D.marginal.x.p_x_given_n.shape == long_one_step_decoder_1D.p_x_given_n.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.p_x_given_n.shape =} and {long_one_step_decoder_1D.p_x_given_n.shape =}\"\n",
    "assert long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape == long_one_step_decoder_1D.most_likely_positions.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape =} and {long_one_step_decoder_1D.most_likely_positions.shape =}\"\n",
    "\n",
    "## validate values:\n",
    "assert np.allclose(long_one_step_decoder_2D.marginal.x.p_x_given_n, long_one_step_decoder_1D.p_x_given_n), f\"1D Decoder should have an x-posterior equal to its own posterior\"\n",
    "assert np.allclose(curr_epoch_result['marginal_x']['most_likely_positions_1D'], curr_epoch_result['most_likely_positions']), f\"1D Decoder should have an x-posterior with most_likely_positions_1D equal to its own most_likely_positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372d125-bca4-4dba-a4bc-ca1fd83d918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate the spikes that are coming in to build the decoder... those are from the placefields actually.\n",
    "    # so it'll be gating the placefield cells.\n",
    "    \n",
    "active_config_name = 'maze1'\n",
    "active_pf_1D = long_pf1D\n",
    "# curr_active_pipeline.computation_results['maze']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275c444-2439-4d16-bbf3-35d8b777d93c",
   "metadata": {},
   "source": [
    "# Testing Placefields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c086de63-62f8-4226-8d74-9930580b30ff",
   "metadata": {
    "tags": [
     "required_imports"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "from neuropy.utils.misc import split_list_of_dicts\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "\n",
    "def _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options: dict) -> dict:\n",
    "    \"\"\" Computes the PfNDs for all the swept parameters (combinations of grid_bin, smooth, etc)\n",
    "    \n",
    "    Usage:\n",
    "        smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "        grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "        all_param_sweep_options = cartesian_product(smooth_options, grid_bin_options)\n",
    "        param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "        output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "\n",
    "    \"\"\"\n",
    "    output_pfs = {} # empty dict\n",
    "\n",
    "    for a_sweep_dict in all_param_sweep_options:\n",
    "        a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
    "        output_pfs[a_sweep_tuple] = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), **a_sweep_dict) # grid_bin=, etc\n",
    "        \n",
    "    return output_pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab4d0e-0df1-42c4-9d4a-7f3596217965",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test that changing the position bins post-hoc is equivalent to initially computing with those position bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7f8f-6f12-4539-be0a-9bc9f7aa07ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get testing variables from `curr_active_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f12de3-87c2-423a-8e55-54ba662cfe57",
   "metadata": {
    "tags": [
     "curr_active_pipeline"
    ]
   },
   "outputs": [],
   "source": [
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "active_pos = curr_active_pipeline.sess.position\n",
    "\n",
    "## Save for NeuroPy testing:\n",
    "finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "sess_identifier_key='sess'\n",
    "spikes_df.to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "active_pos.to_dataframe().to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9344-c10b-488a-855f-b5547a324393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load testing variables from file 'NeuroPy/tests/neuropy_pf_testing.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03923ef5-5e66-4ee8-84a0-eaee7622222f",
   "metadata": {
    "tags": [
     "load",
     "independent"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Corresponding load for Neuropy Testing file 'NeuroPy/tests/neuropy_pf_testing.h5': \n",
    "    ## Save for NeuroPy testing:\n",
    "    finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "    sess_identifier_key='sess'\n",
    "    spikes_df.to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "    active_pos.to_dataframe().to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df', format='table')\n",
    "\"\"\"\n",
    "finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "sess_identifier_key='sess'\n",
    "# Load the saved .h5 spikes_df and active_pos dataframes for testing:\n",
    "spikes_df = pd.read_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "active_pos_df = pd.read_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df')\n",
    "active_pos = active_pos_df.position.to_Position_obj() # convert back to a full position object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d54730-06da-4290-8d62-c619dbc82ad8",
   "metadata": {},
   "source": [
    "## Conduct Parameter Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21053b39-214b-49be-905c-da33146f27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "# grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "# grid_bin_options = [(5,5)]\n",
    "# param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "\n",
    "smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0)]\n",
    "grid_bin_options = [(0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(smooth=smooth_options, grid_bin=grid_bin_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3cfbc-5fff-41da-ba7e-a6ca1ffc9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_thresh_options = [0.0, 1.0, 25.0, 50.0, 100.0, 200.0]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(speed_thresh=speed_thresh_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "print_aligned_columns(['speed_thresh', 'num_good_neurons', 'num_total_spikes'], [speed_thresh_options, num_good_placefield_neurons_list, num_total_spikes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf913cb9-3cc4-4289-ab0c-39ee28167ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "frate_thresh_options = [0.0, 0.1, 1.0, 5.0, 10.0, 100.0]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(frate_thresh=frate_thresh_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "print_aligned_columns(['frate_thresh', 'num_good_neurons', 'num_total_spikes'], [frate_thresh_options, num_good_placefield_neurons_list, num_total_spikes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999df69-49b9-49f1-9698-1c7ee62afe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e157e49-a8b7-4910-8c6d-a51caff360a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(5, 3), smooth=(0.0, 0.0), frate_thresh=0.1) # TODO: FIXME: BUG: when frate_thresh=0.0, there are 0 good placefield_neuronIDs for all computations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54e573-d047-4f17-a080-23e656fa26b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decoder Testing (not yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83935a90-cd90-4ec8-a34c-4c88142e7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "## Build placefield for the decoder to use:\n",
    "original_pf1D = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), frate_thresh=0.0) # all other settings default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e29a608-b084-4d40-ba7d-53827d2b4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the new decoder with custom params:\n",
    "# active_computation_config = curr_active_pipeline.computation_results[active_config_name].computation_config\n",
    "# new_decoder_pf_params = deepcopy(active_computation_config.pf_params) # should be a PlacefieldComputationParameters\n",
    "\n",
    "new_decoder_pf_params = deepcopy(original_pf1D.config) # should be a PlacefieldComputationParameters\n",
    "# override some settings before computation:\n",
    "new_decoder_pf_params.time_bin_size = 0.1\n",
    "\n",
    "## 1D Decoder\n",
    "new_decoder_pf1D = original_pf1D\n",
    "new_1D_decoder_spikes_df = new_decoder_pf1D.filtered_spikes_df.copy()\n",
    "\n",
    "# Why would it need both the pf1D and the spikes? Doesn't the pf1D include the spikes (and determine the placefields, which are all that are used)???\n",
    "new_1D_decoder = BayesianPlacemapPositionDecoder(new_decoder_pf_params.time_bin_size, new_decoder_pf1D, new_1D_decoder_spikes_df, debug_print=False)\n",
    "new_1D_decoder.compute_all()\n",
    "\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca005c14-bd80-451f-b691-7000f72e764e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_1D_decoder.time_bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c84ece5-564f-4e0a-b806-ebdb1e94d49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_neuron_ids = array([  2,   3,   4,   5,   8,  10,  11,  13,  14,  15,  16,  19,  21,\n",
      "        23,  24,  25,  26,  27,  28,  31,  32,  33,  34,  36,  37,  41,\n",
      "        49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n",
      "        62,  63,  64,  66,  67,  68,  69,  70,  73,  74,  75,  76,  78,\n",
      "        81,  82,  83,  85,  86,  87,  88,  89,  90,  92,  93,  96,  98,\n",
      "       100, 102, 105, 108, 109])\n",
      "subset_included_neuron_ids = array([ 2,  3,  4,  5,  8, 10, 11, 13, 14, 15])\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5728,) should be less than time_window_edges: (21050,)!\n"
     ]
    }
   ],
   "source": [
    "original_decoder = new_1D_decoder # strangely this makes original_pf.included_neuron_IDs wrapped in an extra list!\n",
    "original_neuron_ids = np.array(original_decoder.pf.ratemap.neuron_ids) # original_pf.included_neuron_IDs\n",
    "subset_included_neuron_IDXs = np.arange(10) # only get the first 10 neuron_ids\n",
    "subset_included_neuron_ids = original_neuron_ids[subset_included_neuron_IDXs] # only get the first 10 neuron_ids\n",
    "print(f'{original_neuron_ids = }\\n{subset_included_neuron_ids = }')\n",
    "neuron_sliced_1D_decoder = original_decoder.get_by_id(subset_included_neuron_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92f28edd-b192-46b6-802e-6c6a7006290f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_sliced_1D_decoder.neuron_IDXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec100156-7c13-42bc-9604-efc3c7dc82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Leave-one-out\" decoding:\n",
    "def _compute_leave_one_out_decoding(original_decoder):\n",
    "    \"\"\" \"Leave-one-out\" decoding\n",
    "    WARNING: this might suck up a ton of memory! \n",
    "    \"\"\"\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except ImportError:\n",
    "        tqdm = lambda x: x # NO-OP alternative to tqdm progress bar when not using tqdm\n",
    "\n",
    "    original_neuron_ids = np.array(original_decoder.pf.ratemap.neuron_ids) # original_pf.included_neuron_IDs\n",
    "    one_left_out_decoder_dict = {}\n",
    "    for i, aclu_to_omit in enumerate(original_neuron_ids):\n",
    "        subset_included_neuron_ids = np.array([aclu for aclu in original_neuron_ids if aclu != aclu_to_omit]) # get all but the omitted neuron\n",
    "        one_left_out_decoder_dict[aclu_to_omit] = original_decoder.get_by_id(subset_included_neuron_ids)\n",
    "        \n",
    "    return one_left_out_decoder_dict\n",
    "\n",
    "\n",
    "## Build placefield for the decoder to use:\n",
    "original_decoder_pf1D = PfND(deepcopy(self.spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(self.active_pos.linear_pos_obj), frate_thresh=0.0) # all other settings default\n",
    "## Build the new decoder with custom params:\n",
    "new_decoder_pf_params = deepcopy(original_decoder_pf1D.config) # should be a PlacefieldComputationParameters\n",
    "new_decoder_pf_params.time_bin_size = 0.1\n",
    "original_1D_decoder = BayesianPlacemapPositionDecoder(new_decoder_pf_params.time_bin_size, original_decoder_pf1D, original_decoder_pf1D.filtered_spikes_df.copy(), debug_print=False)\n",
    "original_1D_decoder.compute_all()\n",
    "\n",
    "one_left_out_decoder_dict = _compute_leave_one_out_decoding(original_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73fb7e68-99bf-403e-8ae4-65edde41e7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "156aa1a6-f2ed-418a-b7dd-5c264d7c5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.print_helpers import print_object_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0081f58-6b69-4231-a952-4217a5e1e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(one_left_out_decoder_dict) # 70\n",
    "print_object_memory_usage(one_left_out_decoder_dict) # object size: 12696.855616 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b358852-524a-4785-ad66-bb201ced2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object size: 12696.855616 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12696.855615615845"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e50c6-ed5e-43ca-b423-5214f462fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _perform_specific_epochs_decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fd8ca-655e-4395-8020-42b37ff97106",
   "metadata": {},
   "source": [
    "# 2023-02-27 - Test whether conform to active position works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8788d1-134b-4351-93bb-b142a4608b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Placefields with varying bin-sizes:\n",
    "### Here we use frate_thresh=0.0 which ensures that differently binned ratemaps don't have different numbers of spikes or cells.\n",
    "smooth_options = [(None, None)]\n",
    "grid_bin_options = [(0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(grid_bin=grid_bin_options, smooth=smooth_options, frate_thresh=[0.0])\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d501f7-0ac3-4cae-ad9e-9f655389bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_good_placefield_neurons_list, num_total_spikes_list, num_spikes_per_spiketrain_list = compare_placefields_info(output_pfs)\n",
    "print_aligned_columns(['grid_bin x smooth', 'num_good_neurons', 'num_total_spikes'], \n",
    "                     [all_param_sweep_options, num_good_placefield_neurons_list, num_total_spikes_list], enable_checking_all_values_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd76742-2059-41d3-8038-6cb4fcb5bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf605a2-24f2-4574-9520-81c0706f45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14fdbc-6a86-4de8-ad5c-0ce86fab500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sweep_option_n_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbea74-64cb-4df0-bd35-fe621395a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_binned_pf = list(output_pfs.values())[0]\n",
    "coarse_binned_pf = list(output_pfs.values())[-1]\n",
    "\n",
    "print(f'{coarse_binned_pf.bin_info = }\\n{fine_binned_pf.bin_info = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51689e35-11d0-40c0-97c4-56daa9120748",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_binned_pf.bin_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0b738-ad5f-46ac-914f-731a497c6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_binned_pf.bin_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5a9bc-3ec1-4d75-b257-3a039be88bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_fine_binned_pf = deepcopy(fine_binned_pf)\n",
    "rebinned_fine_binned_pf.conform_to_position_bins(target_pf1D=coarse_binned_pf, force_recompute=True)\n",
    "assert rebinned_fine_binned_pf.bin_info == coarse_binned_pf.bin_info # the bins must be equal after conforming\n",
    "\n",
    "num_good_placefield_neurons_list, num_total_spikes_list, num_spikes_per_spiketrain_list = compare_placefields_info(dict(zip(['coarse', 'original', 'rebinned'],[coarse_binned_pf, fine_binned_pf, rebinned_fine_binned_pf])))\n",
    "print_aligned_columns(['pf', 'num_good_neurons', 'num_total_spikes'], [['coarse', 'original', 'rebinned'], num_good_placefield_neurons_list, num_total_spikes_list], enable_checking_all_values_width=True)\n",
    "\n",
    "assert num_good_placefield_neurons_list[0] == num_good_placefield_neurons_list[-1] # require the rebinned pf to have the same number of good neurons as the one that it conformed to\n",
    "assert num_total_spikes_list[0] == num_total_spikes_list[-1] # require the rebinned pf to have the same number of total spikes as the one that it conformed to\n",
    "# assert num_spikes_per_spiketrain_list[0] == num_spikes_per_spiketrain_list[-1] # require the rebinned pf to have the same number of spikes in each spiketrain as the one that it conformed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c6b01-b0eb-4dd6-b820-f0019b85420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO 2023-03-02: plot the three placefields next to each other horizontally (as a single row for comparison):\n",
    "fig, axs = _plot_parameter_sweep(dict(zip([frozenset({'pf':'coarse'}), frozenset({'pf':'original'}), frozenset({'pf':'rebinned'})],[coarse_binned_pf, fine_binned_pf, rebinned_fine_binned_pf])), {'pf':3}, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06843c9-8c6e-4350-979e-fa24ad0550bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73811c-d158-45e6-ac59-9d566463aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ddf44-053c-43df-9855-ab5b2d8ee8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test excluding certain neurons from the placefield\n",
    "original_pf = fine_binned_pf\n",
    "original_pf_neuron_ids = original_pf.included_neuron_IDs.copy()\n",
    "subset_included_neuron_IDXs = np.arange(10) # only get the first 10 neuron_ids\n",
    "subset_included_neuron_ids = original_pf_neuron_ids[subset_included_neuron_IDXs] # only get the first 10 neuron_ids\n",
    "print(f'{original_pf_neuron_ids = }\\n{subset_included_neuron_ids = }')\n",
    "neuron_sliced_pf = deepcopy(fine_binned_pf)\n",
    "neuron_sliced_pf = neuron_sliced_pf.get_by_id(subset_included_neuron_ids)\n",
    "neuron_sliced_pf_neuron_ids = neuron_sliced_pf.included_neuron_IDs\n",
    "print(f'{neuron_sliced_pf_neuron_ids = }')\n",
    "\n",
    "assert np.all(neuron_sliced_pf_neuron_ids == subset_included_neuron_ids) # ensure that the returned neuron ids actually equal the desired subset\n",
    "assert np.all(np.array(neuron_sliced_pf.ratemap.neuron_ids) == subset_included_neuron_ids) # ensure that the ratemap neuron ids actually equal the desired subset\n",
    "assert len(neuron_sliced_pf.ratemap.tuning_curves) == len(subset_included_neuron_ids) # ensure one output tuning curve for each neuron_id\n",
    "np.all(np.isclose(neuron_sliced_pf.ratemap.tuning_curves, [original_pf.ratemap.tuning_curves[idx] for idx in subset_included_neuron_IDXs])) # ensure that the tuning curves built for the neuron_slided_pf are the same as those subset as retrieved from the  original_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3c6ea-8629-499b-8233-33107a246504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_sliced_pf.cell_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f175ff1-ca29-4ba2-865b-9e003d0d1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda42f06-de7d-4f16-9557-5fc596a7fff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf2c25-b755-4407-a65c-23175d6493f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_pf.ratemap.tuning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cf13f-521f-4daa-9f92-0cc330ccaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd846fe-a582-48b1-982b-b0df6cec3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5746da4-a6d7-4f84-90f6-db6802186e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1ce59-7b74-4b58-81c7-679c5df50a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test selecting non-existant neuron_ids for inclusion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728f28f-bfd9-43d2-9099-6fd74a702127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8c7c69-5918-4924-b831-e24e3029405c",
   "metadata": {},
   "source": [
    "## Overflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb1473-77c2-4b0b-8334-44939abfbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all spikes:\n",
    "active_epoch_placefields1Da = PfND(deepcopy(spikes_df), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74209eda-e41f-46e2-95d6-869a225c3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyramidal spikes only:\n",
    "active_epoch_placefields1Db = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # grid_bin=, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef94996-c927-44c7-8826-d0e29b102bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlacefieldComputationParameters\n",
    "# Parameter sweeps:\n",
    "\n",
    "# grid_bin = [(1.0, 1.0)]\n",
    "\n",
    "# 10.0\n",
    "cls.compute_position_grid_bin_size(sess.position.x, sess.position.y, num_bins=(64, 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-poetry",
   "language": "python",
   "name": "spike3d-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
