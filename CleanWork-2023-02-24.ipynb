{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb364483-af18-4721-a504-688311f0a4ab",
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import traceback # for stack trace formatting\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# # os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "# # os.environ[\"MODIN_ENGINE\"] = \"unidist\" # Modin will use Unidist\n",
    "# # os.environ[\"UNIDIST_BACKEND\"] = \"mpi\" # Unidist will use MPI backend\n",
    "# import modin.pandas as pd # alternative to pandas which is much faster\n",
    "# # Installed with `poetry add modin[all]`\n",
    "\n",
    "# from pandas_profiling import ProfileReport ## for dataframe viewing\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables, objsize, print_object_memory_usage, debug_dump_object_member_shapes, TypePrintMode\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_time_str, get_now_time_precise_str\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "from pyphoplacecellanalysis.General.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "global_data_root_parent_path = Path(r'W:\\Data') # Windows Apogee\n",
    "# global_data_root_parent_path = Path(r'/media/MAX/Data') # Diba Lab Workstation Linux\n",
    "# global_data_root_parent_path = Path(r'/Volumes/MoverNew/data') # rMBP\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538e2a-4e39-4d11-90b5-a9fef9258058",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f917bad9-8fe7-4882-b83b-71cf878fffd2",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_session_names_list: ['2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_1-22-43', '2006-6-09_3-23-37', '2006-6-12_15-55-31', '2006-6-13_14-42-6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-07_11-26-53'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-08_14-26-15'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_3-23-37'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-12_15-55-31'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-13_14-42-6'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "## Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "### Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ### Animal `pin01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='pin01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # no exper_name ('one' or 'two') folders for this animal.\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['redundant','showclus','sleep','tmaze'])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155124df-eddb-461c-804f-de8f7acd332e",
   "metadata": {
    "tags": [
     "load",
     "single_session"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.epochs_info.mat... done.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.position_info.mat... done.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.spikes.mat... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\IsolatedSpike3DEnv\\NeuroPy\\neuropy\\core\\session\\Formats\\SessionSpecifications.py:140: UserWarning: WARNING: Optional File: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Computing linear positions for all active epochs for session... Saving updated position results results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.position.npy... 2006-6-08_14-26-15.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.interpolated_spike_positions.npy... 2006-6-08_14-26-15.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\2006-6-08_14-26-15.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\ripple_df.pkl.\n",
      "Loading success: .mua.npy.\n",
      "Loading success: .pbe.npy.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1211.5580800310709)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1211.5580800310709, end: 2093.8978568242164)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2093.8978568242164)\n",
      "computing neurons mua for session...\n",
      "\n",
      "using provided computation_functions_name_whitelist: ['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_two_step_position_decoding_computation']\n",
      "due to whitelist, including only 6 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36668,)\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 6 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26242,)\n",
      "updating computation_results...\n",
      "done.\n",
      "due to whitelist, including only 6 out of 15 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (62911,)\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "# %%viztracer\n",
    "basedir = local_session_paths_list[1] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# epoch_name_whitelist = ['maze']\n",
    "epoch_name_whitelist = None\n",
    "active_computation_functions_name_whitelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        # '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_whitelist=epoch_name_whitelist,\n",
    "                                          computation_functions_name_whitelist=active_computation_functions_name_whitelist,\n",
    "                                          saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=True, skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d77d8-811b-4402-a37d-dcd82efa8fc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "# 2023-02-24 Decoders \n",
    "- [ ] where are cells chosen for inclusion in the input of the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cf00e-cd16-40ab-a8f9-7b0b2c4c36ed",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "long_epoch_name, short_epoch_name, global_epoch_name = long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "\n",
    "recalculate_anyway = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17793403-7842-4c6c-ade7-2a0c487f777d",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "# Make the 1D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf1D = long_results.pf1D\n",
    "short_pf1D = short_results.pf1D\n",
    "global_pf1D = global_results.pf1D\n",
    "\n",
    "# short_pf1D, did_update_bins = short_pf1D.conform_to_position_bins(long_pf1D, force_recompute=True) # not needed because it's done in one_step_decoder_1D.conform_to_position_bins(...)\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D  = [results_data.get('pf1D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_1D, did_recompute = short_one_step_decoder_1D.conform_to_position_bins(long_one_step_decoder_1D, force_recompute=True)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_1D is None) or (short_two_step_decoder_1D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_1D is not None and short_two_step_decoder_1D is not None)\n",
    "\n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "# decoding_time_bin_size = 0.03 # 0.03333333333333333\n",
    "print(f'decoding_time_bin_size: {decoding_time_bin_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fc6cd-84b9-405f-bf53-6b66a6c32525",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "#### Get 2D Decoders for validation and comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b9806-923f-42fd-921f-1f75bdddc5cb",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "# Make the 2D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf2D = long_results.pf2D\n",
    "short_pf2D = short_results.pf2D\n",
    "global_pf2D = global_results.pf2D\n",
    "\n",
    "# long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "# long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "\n",
    "# short_pf2D, did_update_bins = short_pf2D.conform_to_position_bins(long_pf2D)\n",
    "long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_2D, did_recompute = short_one_step_decoder_2D.conform_to_position_bins(long_one_step_decoder_2D)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_2D is None) or (short_two_step_decoder_2D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_2D is not None and short_two_step_decoder_2D is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43499e87-0693-4d11-9d33-100a987737ac",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "# Sums are similar:\n",
    "print(f'{np.sum(long_one_step_decoder_2D.marginal.x.p_x_given_n) =},\\t {np.sum(long_one_step_decoder_1D.p_x_given_n) = }') # 31181.999999999996 vs 31181.99999999999\n",
    "\n",
    "## Validate:\n",
    "assert long_one_step_decoder_2D.marginal.x.p_x_given_n.shape == long_one_step_decoder_1D.p_x_given_n.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.p_x_given_n.shape =} and {long_one_step_decoder_1D.p_x_given_n.shape =}\"\n",
    "assert long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape == long_one_step_decoder_1D.most_likely_positions.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape =} and {long_one_step_decoder_1D.most_likely_positions.shape =}\"\n",
    "\n",
    "## validate values:\n",
    "assert np.allclose(long_one_step_decoder_2D.marginal.x.p_x_given_n, long_one_step_decoder_1D.p_x_given_n), f\"1D Decoder should have an x-posterior equal to its own posterior\"\n",
    "assert np.allclose(curr_epoch_result['marginal_x']['most_likely_positions_1D'], curr_epoch_result['most_likely_positions']), f\"1D Decoder should have an x-posterior with most_likely_positions_1D equal to its own most_likely_positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372d125-bca4-4dba-a4bc-ca1fd83d918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate the spikes that are coming in to build the decoder... those are from the placefields actually.\n",
    "    # so it'll be gating the placefield cells.\n",
    "    \n",
    "active_config_name = 'maze1'\n",
    "active_pf_1D = long_pf1D\n",
    "# curr_active_pipeline.computation_results['maze']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275c444-2439-4d16-bbf3-35d8b777d93c",
   "metadata": {},
   "source": [
    "# Testing Placefields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c086de63-62f8-4226-8d74-9930580b30ff",
   "metadata": {
    "tags": [
     "required_imports"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\IsolatedSpike3DEnv\\NeuroPy\\neuropy\\core\\flattened_spiketrains.py:23: UserWarning: registration of accessor <class 'neuropy.core.flattened_spiketrains.SpikesAccessor'> under name 'spikes' for type <class 'pandas.core.frame.DataFrame'> is overriding a preexisting attribute with the same name.\n",
      "  class SpikesAccessor(TimeSlicedMixin):\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "from neuropy.utils.misc import split_list_of_dicts\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "\n",
    "def _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options: dict) -> dict:\n",
    "    \"\"\" Computes the PfNDs for all the swept parameters (combinations of grid_bin, smooth, etc)\n",
    "    \n",
    "    Usage:\n",
    "        smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "        grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "        all_param_sweep_options = cartesian_product(smooth_options, grid_bin_options)\n",
    "        param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "        output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "\n",
    "    \"\"\"\n",
    "    output_pfs = {} # empty dict\n",
    "\n",
    "    for a_sweep_dict in all_param_sweep_options:\n",
    "        a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
    "        output_pfs[a_sweep_tuple] = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), **a_sweep_dict) # grid_bin=, etc\n",
    "        \n",
    "    return output_pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab4d0e-0df1-42c4-9d4a-7f3596217965",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test that changing the position bins post-hoc is equivalent to initially computing with those position bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7f8f-6f12-4539-be0a-9bc9f7aa07ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get testing variables from `curr_active_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f12de3-87c2-423a-8e55-54ba662cfe57",
   "metadata": {
    "tags": [
     "curr_active_pipeline"
    ]
   },
   "outputs": [],
   "source": [
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "active_pos = curr_active_pipeline.sess.position\n",
    "\n",
    "## Save for NeuroPy testing:\n",
    "finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "sess_identifier_key='sess'\n",
    "spikes_df.to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "active_pos.to_dataframe().to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9344-c10b-488a-855f-b5547a324393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load testing variables from file 'NeuroPy/tests/neuropy_pf_testing.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03923ef5-5e66-4ee8-84a0-eaee7622222f",
   "metadata": {
    "tags": [
     "load",
     "independent"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Corresponding load for Neuropy Testing file 'NeuroPy/tests/neuropy_pf_testing.h5': \n",
    "    ## Save for NeuroPy testing:\n",
    "    finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "    sess_identifier_key='sess'\n",
    "    spikes_df.to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "    active_pos.to_dataframe().to_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df', format='table')\n",
    "\"\"\"\n",
    "finalized_output_cache_file='../NeuroPy/tests/neuropy_pf_testing.h5'\n",
    "sess_identifier_key='sess'\n",
    "# Load the saved .h5 spikes_df and active_pos dataframes for testing:\n",
    "spikes_df = pd.read_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/spikes_df')\n",
    "pyramidal_only_spikes_df = deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal') ## get only the pyramidal spikes\n",
    "\n",
    "active_pos_df = pd.read_hdf(finalized_output_cache_file, key=f'{sess_identifier_key}/pos_df')\n",
    "active_pos = active_pos_df.position.to_Position_obj() # convert back to a full position object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d54730-06da-4290-8d62-c619dbc82ad8",
   "metadata": {},
   "source": [
    "## Conduct Parameter Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21053b39-214b-49be-905c-da33146f27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "# grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "# grid_bin_options = [(5,5)]\n",
    "# param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "\n",
    "smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0)]\n",
    "grid_bin_options = [(0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(smooth=smooth_options, grid_bin=grid_bin_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3cfbc-5fff-41da-ba7e-a6ca1ffc9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_thresh_options = [0.0, 1.0, 25.0, 50.0, 100.0, 200.0]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(speed_thresh=speed_thresh_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "print_aligned_columns(['speed_thresh', 'num_good_neurons', 'num_total_spikes'], [speed_thresh_options, num_good_placefield_neurons_list, num_total_spikes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf913cb9-3cc4-4289-ab0c-39ee28167ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "frate_thresh_options = [0.0, 0.1, 1.0, 5.0, 10.0, 100.0]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(frate_thresh=frate_thresh_options)\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)\n",
    "print_aligned_columns(['frate_thresh', 'num_good_neurons', 'num_total_spikes'], [frate_thresh_options, num_good_placefield_neurons_list, num_total_spikes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999df69-49b9-49f1-9698-1c7ee62afe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337434e7-eb87-48e4-8de6-8245a51c9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(5, 3), smooth=(0.0, 0.0), frate_thresh=0.1) # TODO: FIXME: BUG: when frate_thresh=0.0, there are 0 good placefield_neuronIDs for all computations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a15e13-59c6-4224-acdf-f2d5fc5e22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PfND_TimeDependent Parameter Sweeps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb234b7-7215-4189-818a-2687a6977aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "\n",
    "# PfND_TimeDependent\n",
    "orginal_pf1D_dt = PfND_TimeDependent(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), frate_thresh=0.0)\n",
    "orginal_pf1D_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54e573-d047-4f17-a080-23e656fa26b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-03-03 - Decoder Testing\n",
    "Useful Decoder-related plotting and visuzliation classes:\n",
    "```python\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018a2cad-75e7-47ab-b90c-7783c851ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext viztracer\n",
    "from viztracer import VizTracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b06ffeae-87ba-42ce-96a2-a127361da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e250e4a6-df77-4617-8a99-dd91618be259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "Total Entries: 2788                                                             \n",
      "Use the following command to open the report:\n",
      "\u001b[92mvizviewer C:\\Users\\pho\\repos\\IsolatedSpike3DEnv\\Spike3D\\viztracer_2023-03-03_11-03-build_new_PfND.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-build_new_PfND.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "    ## Build placefield for the decoder to use:\n",
    "    original_pf1D = PfND(deepcopy(pyramidal_only_spikes_df), deepcopy(active_pos.linear_pos_obj), frate_thresh=0.0) # all other settings default\n",
    "\n",
    "    new_decoder_pf_params = deepcopy(original_pf1D.config) # should be a PlacefieldComputationParameters\n",
    "    # override some settings before computation:\n",
    "    new_decoder_pf_params.time_bin_size = 0.1\n",
    "\n",
    "    ## 1D Decoder\n",
    "    new_decoder_pf1D = original_pf1D\n",
    "    new_1D_decoder_spikes_df = new_decoder_pf1D.filtered_spikes_df.copy()\n",
    "\n",
    "    # Why would it need both the pf1D and the spikes? Doesn't the pf1D include the spikes (and determine the placefields, which are all that are used)???\n",
    "    new_1D_decoder = BayesianPlacemapPositionDecoder(new_decoder_pf_params.time_bin_size, new_decoder_pf1D, new_1D_decoder_spikes_df, debug_print=False)\n",
    "    new_1D_decoder.compute_all()\n",
    "\n",
    "    print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca005c14-bd80-451f-b691-7000f72e764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_1D_decoder.time_bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84ece5-564f-4e0a-b806-ebdb1e94d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_decoder = new_1D_decoder # strangely this makes original_pf.included_neuron_IDs wrapped in an extra list!\n",
    "original_neuron_ids = np.array(original_decoder.pf.ratemap.neuron_ids) # original_pf.included_neuron_IDs\n",
    "subset_included_neuron_IDXs = np.arange(10) # only get the first 10 neuron_ids\n",
    "subset_included_neuron_ids = original_neuron_ids[subset_included_neuron_IDXs] # only get the first 10 neuron_ids\n",
    "print(f'{original_neuron_ids = }\\n{subset_included_neuron_ids = }')\n",
    "neuron_sliced_1D_decoder = original_decoder.get_by_id(subset_included_neuron_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d14c5-2b64-4f19-bae4-64910c68b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_1D_decoder.neuron_IDXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e48db7-800a-403a-bf3d-63c91d73647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PfND\n",
    "get_by_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae1ac0-b7f4-40df-957d-c42ad74ee704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc90d44-7231-40e4-851c-a276a810434b",
   "metadata": {},
   "source": [
    "## Specific Epoch Decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af0630a5-3356-4cbc-b14b-4ad641c671a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8e3a8c5-944e-402a-b1c2-177821d70019",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lap-Epochs Decoding:\n",
    "sess = curr_active_pipeline.sess\n",
    "active_decoder = new_1D_decoder\n",
    "decoding_time_bin_size = 0.02\n",
    "global_pos_df = sess.position.to_dataframe()\n",
    "\n",
    "laps_copy = deepcopy(sess.laps)\n",
    "laps_filter_epochs = laps_copy.filtered_by_lap_flat_index(np.arange(6)).as_epoch_obj() # epoch object\n",
    "laps_filter_epochs_decoder_result = active_decoder.decode_specific_epochs(sess.spikes_df, filter_epochs=laps_filter_epochs, decoding_time_bin_size=decoding_time_bin_size, debug_print=False)\n",
    "laps_filter_epochs_decoder_result.epoch_description_list = [f'lap[{epoch_tuple.lap_id}]' for epoch_tuple in laps_filter_epochs.to_dataframe()[['lap_id']].itertuples()] # Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5222861a-9464-4b45-a363-da7bb81ebb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_filter_epochs_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed580ba-bf1d-4a27-a33f-d6eb42ae10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_decoder.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54e59fb0-c693-489c-b2eb-e87c5d3b0932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-87.9073492480395, 151.0926507519605)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.min(active_decoder.xbin), np.max(active_decoder.xbin))\n",
    "# np.ptp(active_decoder.xbin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de91b7a7-7a33-4198-94a9-f43a542efe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_pos_df['lin_pos'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035ce19-485b-4e6e-87c2-c3fd83979a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-87.9073492480395, 150.67256643126836)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  global_pos_df['lin_pos'] lines up with the active_decoder.xbins: (-87.9073492480395, 150.67256643126836) || (-87.9073492480395, 151.0926507519605)\n",
    "_temp_active_pos_arr = global_pos_df['lin_pos'].values\n",
    "(np.nanmin(_temp_active_pos_arr), np.nanmax(_temp_active_pos_arr))\n",
    "# global_pos_df['x'] on the other hand is a VERY BAD MATCH: (22.736279243974774, 261.696733348342)\n",
    "_temp_active_pos_arr = global_pos_df['x'].values\n",
    "(np.nanmin(_temp_active_pos_arr), np.nanmax(_temp_active_pos_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2359b91c-7378-4cdc-8b2d-e1c42e2917fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_plot_tuple = plot_decoded_epoch_slices(laps_filter_epochs, laps_filter_epochs_decoder_result, global_pos_df=global_pos_df, variable_name='lin_pos', xbin=active_decoder.xbin,\n",
    "                                                        name='stacked_epoch_slices_matplotlib_subplots_LAPS', debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ed5cf84-156a-4fbe-8102-2ede8e114138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create legend object and add to figure\n",
    "last_axes = list(laps_plot_tuple[2].axs)[-1]\n",
    "last_axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51317ae-2c9c-4064-aa30-0f1fe2e0e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_plot_tuple[2].fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61a7d6a9-daa9-4cc6-a289-33a9f7adc993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\AppData\\Local\\Temp\\ipykernel_22132\\3653236813.py:1: UserWarning: Legend does not support handles for Axes instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#controlling-the-legend-entries\n",
      "  out_legend = laps_plot_tuple[2].fig.legend(handles=list(laps_plot_tuple[2].axs), labels=[p.get_label() for p in list(laps_plot_tuple[2].axs)], loc='lower center', ncol=4) #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27123b5b6d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_legend = laps_plot_tuple[2].fig.legend(handles=list(laps_plot_tuple[2].axs), labels=[p.get_label() for p in list(laps_plot_tuple[2].axs)], loc='lower center', ncol=4) #\n",
    "out_legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6684d5-5162-4ce0-b563-bf9754c3191b",
   "metadata": {},
   "source": [
    "## \"Leave-one-out\" decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e93b8f75-2bec-4b4b-bfa6-35c889b98f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "def _compute_leave_one_out_decoding(original_decoder):\n",
    "    \"\"\" \"Leave-one-out\" decoding\n",
    "    WARNING: this might suck up a ton of memory! \n",
    "    \"\"\"\n",
    "    original_neuron_ids = np.array(original_decoder.pf.ratemap.neuron_ids) # original_pf.included_neuron_IDs\n",
    "    one_left_out_decoder_dict = {}\n",
    "    for i, aclu_to_omit in enumerate(original_neuron_ids):\n",
    "        subset_included_neuron_ids = np.array([aclu for aclu in original_neuron_ids if aclu != aclu_to_omit]) # get all but the omitted neuron\n",
    "        one_left_out_decoder_dict[aclu_to_omit] = original_decoder.get_by_id(subset_included_neuron_ids, defer_compute_all=True) # skip computations\n",
    "        \n",
    "    return one_left_out_decoder_dict\n",
    "\n",
    "\n",
    "## Build placefield for the decoder to use:\n",
    "original_decoder_pf1D = PfND(deepcopy(pyramidal_only_spikes_df), deepcopy(active_pos.linear_pos_obj), frate_thresh=0.0) # all other settings default\n",
    "## Build the new decoder with custom params:\n",
    "new_decoder_pf_params = deepcopy(original_decoder_pf1D.config) # should be a PlacefieldComputationParameters\n",
    "new_decoder_pf_params.time_bin_size = 0.1\n",
    "original_1D_decoder = BayesianPlacemapPositionDecoder(new_decoder_pf_params.time_bin_size, original_decoder_pf1D, original_decoder_pf1D.filtered_spikes_df.copy(), debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd900512-d015-48f7-9beb-305fcd6b32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty dang inefficient, as there are 70 cells:\n",
    "one_left_out_decoder_dict = _compute_leave_one_out_decoding(original_1D_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0647dd6c-b560-49e4-9e2c-1fd58edbbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lap-Epochs Decoding:\n",
    "sess = curr_active_pipeline.sess\n",
    "decoding_time_bin_size = 0.02\n",
    "global_pos_df = sess.position.to_dataframe()\n",
    "# Common for all decoders:\n",
    "laps_copy = deepcopy(sess.laps)\n",
    "laps_filter_epochs = laps_copy.filtered_by_lap_flat_index(np.arange(6)).as_epoch_obj() # epoch object\n",
    "laps_epoch_description_list = [f'lap[{epoch_tuple.lap_id}]' for epoch_tuple in laps_filter_epochs.to_dataframe()[['lap_id']].itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "827fc992-740f-4662-8e3a-4be691b7d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get just one of the decoders using a left-out cell:\n",
    "# left_out_aclu, curr_aclu_omitted_decoder = list(one_left_out_decoder_dict.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "644f6973-1d06-4d91-a5e7-bbc1c7d1e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_left_out_laps_filter_epochs_decoder_result_dict = {}\n",
    "\n",
    "for left_out_aclu, curr_aclu_omitted_decoder in one_left_out_decoder_dict.items():\n",
    "    laps_filter_epochs_decoder_result = curr_aclu_omitted_decoder.decode_specific_epochs(sess.spikes_df, filter_epochs=laps_filter_epochs, decoding_time_bin_size=decoding_time_bin_size, debug_print=False)\n",
    "    laps_filter_epochs_decoder_result.epoch_description_list = deepcopy(laps_epoch_description_list)\n",
    "    one_left_out_laps_filter_epochs_decoder_result_dict[left_out_aclu] = laps_filter_epochs_decoder_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36a130a8-87f8-4f2b-bde4-b904475843a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the impact leaving each aclu out had on the average encoding performance:\n",
    "### 1. The distance between the actual measured position and the decoded position at each timepoint for each decoder. A larger magnitude difference implies a stronger, more positive effect on the decoding quality.\n",
    "left_out_aclu, curr_aclu_omitted_decoder = list(one_left_out_decoder_dict.items())[0]\n",
    "left_out_decoder_result = one_left_out_laps_filter_epochs_decoder_result_dict[left_out_aclu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8d3ef89-e5da-4712-b675-02404eee8196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_out_aclu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "011f8ffa-0e1c-4e71-be0f-e35b54084c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_decoder.decode_specific_epochs("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "caa8904a-c517-4d64-8528-c776c76819c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95390cc6-4b60-4a98-a479-8a6c06cdace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_filter_epochs_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c46e5-655a-4cae-a920-1c0b26405eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_1D_decoder.compute_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a1e0f93-237b-441c-8667-3977e6c2ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.print_helpers import print_object_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54771eca-b5dd-4250-a621-39a496c4ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(original_1D_decoder) # original_1D_decoder - object size: 90.199349 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51a5dd56-1eee-4434-ade5-1c6a187b2ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object size: 403.150110 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403.150110244751"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_object_memory_usage(one_left_out_laps_filter_epochs_decoder_result_dict) # one_left_out_laps_filter_epochs_decoder_result_dict - object size: 403.150110 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85e916ab-2baf-4a0d-91d2-33e51a33e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object size: 3627.213895 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3627.213894844055"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(one_left_out_decoder_dict) # 70\n",
    "print_object_memory_usage(one_left_out_decoder_dict) # object size: 12696.855616 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cabc31-294f-4397-a200-a24bdbc9f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cache the one-left-out decoder dict:\n",
    "import dill\n",
    "\n",
    "dill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21ea59-269c-48df-a06c-30559680df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decoder = list(one_left_out_decoder_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b7111-5625-485e-8fa5-60b3ffb26042",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decoder.decode_specific_epochs(spikes_df="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad59ae-10a3-4405-9d67-353196a1227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decoder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e50c6-ed5e-43ca-b423-5214f462fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _perform_specific_epochs_decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fd8ca-655e-4395-8020-42b37ff97106",
   "metadata": {},
   "source": [
    "# 2023-02-27 - Test whether conform to active position works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8788d1-134b-4351-93bb-b142a4608b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Placefields with varying bin-sizes:\n",
    "### Here we use frate_thresh=0.0 which ensures that differently binned ratemaps don't have different numbers of spikes or cells.\n",
    "smooth_options = [(None, None)]\n",
    "grid_bin_options = [(0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(grid_bin=grid_bin_options, smooth=smooth_options, frate_thresh=[0.0])\n",
    "output_pfs = _compute_parameter_sweep(spikes_df, active_pos, all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d501f7-0ac3-4cae-ad9e-9f655389bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_good_placefield_neurons_list, num_total_spikes_list, num_spikes_per_spiketrain_list = compare_placefields_info(output_pfs)\n",
    "print_aligned_columns(['grid_bin x smooth', 'num_good_neurons', 'num_total_spikes'], \n",
    "                     [all_param_sweep_options, num_good_placefield_neurons_list, num_total_spikes_list], enable_checking_all_values_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd76742-2059-41d3-8038-6cb4fcb5bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf605a2-24f2-4574-9520-81c0706f45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14fdbc-6a86-4de8-ad5c-0ce86fab500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sweep_option_n_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbea74-64cb-4df0-bd35-fe621395a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_binned_pf = list(output_pfs.values())[0]\n",
    "coarse_binned_pf = list(output_pfs.values())[-1]\n",
    "\n",
    "print(f'{coarse_binned_pf.bin_info = }\\n{fine_binned_pf.bin_info = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51689e35-11d0-40c0-97c4-56daa9120748",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_binned_pf.bin_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0b738-ad5f-46ac-914f-731a497c6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_binned_pf.bin_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5a9bc-3ec1-4d75-b257-3a039be88bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_fine_binned_pf = deepcopy(fine_binned_pf)\n",
    "rebinned_fine_binned_pf.conform_to_position_bins(target_pf1D=coarse_binned_pf, force_recompute=True)\n",
    "assert rebinned_fine_binned_pf.bin_info == coarse_binned_pf.bin_info # the bins must be equal after conforming\n",
    "\n",
    "num_good_placefield_neurons_list, num_total_spikes_list, num_spikes_per_spiketrain_list = compare_placefields_info(dict(zip(['coarse', 'original', 'rebinned'],[coarse_binned_pf, fine_binned_pf, rebinned_fine_binned_pf])))\n",
    "print_aligned_columns(['pf', 'num_good_neurons', 'num_total_spikes'], [['coarse', 'original', 'rebinned'], num_good_placefield_neurons_list, num_total_spikes_list], enable_checking_all_values_width=True)\n",
    "\n",
    "assert num_good_placefield_neurons_list[0] == num_good_placefield_neurons_list[-1] # require the rebinned pf to have the same number of good neurons as the one that it conformed to\n",
    "assert num_total_spikes_list[0] == num_total_spikes_list[-1] # require the rebinned pf to have the same number of total spikes as the one that it conformed to\n",
    "# assert num_spikes_per_spiketrain_list[0] == num_spikes_per_spiketrain_list[-1] # require the rebinned pf to have the same number of spikes in each spiketrain as the one that it conformed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c6b01-b0eb-4dd6-b820-f0019b85420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO 2023-03-02: plot the three placefields next to each other horizontally (as a single row for comparison):\n",
    "fig, axs = _plot_parameter_sweep(dict(zip([frozenset({'pf':'coarse'}), frozenset({'pf':'original'}), frozenset({'pf':'rebinned'})],[coarse_binned_pf, fine_binned_pf, rebinned_fine_binned_pf])), {'pf':3}, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06843c9-8c6e-4350-979e-fa24ad0550bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73811c-d158-45e6-ac59-9d566463aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ddf44-053c-43df-9855-ab5b2d8ee8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test excluding certain neurons from the placefield\n",
    "original_pf = fine_binned_pf\n",
    "original_pf_neuron_ids = original_pf.included_neuron_IDs.copy()\n",
    "subset_included_neuron_IDXs = np.arange(10) # only get the first 10 neuron_ids\n",
    "subset_included_neuron_ids = original_pf_neuron_ids[subset_included_neuron_IDXs] # only get the first 10 neuron_ids\n",
    "print(f'{original_pf_neuron_ids = }\\n{subset_included_neuron_ids = }')\n",
    "neuron_sliced_pf = deepcopy(fine_binned_pf)\n",
    "neuron_sliced_pf = neuron_sliced_pf.get_by_id(subset_included_neuron_ids)\n",
    "neuron_sliced_pf_neuron_ids = neuron_sliced_pf.included_neuron_IDs\n",
    "print(f'{neuron_sliced_pf_neuron_ids = }')\n",
    "\n",
    "assert np.all(neuron_sliced_pf_neuron_ids == subset_included_neuron_ids) # ensure that the returned neuron ids actually equal the desired subset\n",
    "assert np.all(np.array(neuron_sliced_pf.ratemap.neuron_ids) == subset_included_neuron_ids) # ensure that the ratemap neuron ids actually equal the desired subset\n",
    "assert len(neuron_sliced_pf.ratemap.tuning_curves) == len(subset_included_neuron_ids) # ensure one output tuning curve for each neuron_id\n",
    "np.all(np.isclose(neuron_sliced_pf.ratemap.tuning_curves, [original_pf.ratemap.tuning_curves[idx] for idx in subset_included_neuron_IDXs])) # ensure that the tuning curves built for the neuron_slided_pf are the same as those subset as retrieved from the  original_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3c6ea-8629-499b-8233-33107a246504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_sliced_pf.cell_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f175ff1-ca29-4ba2-865b-9e003d0d1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda42f06-de7d-4f16-9557-5fc596a7fff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf2c25-b755-4407-a65c-23175d6493f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_pf.ratemap.tuning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cf13f-521f-4daa-9f92-0cc330ccaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd846fe-a582-48b1-982b-b0df6cec3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_sliced_pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5746da4-a6d7-4f84-90f6-db6802186e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1ce59-7b74-4b58-81c7-679c5df50a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test selecting non-existant neuron_ids for inclusion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728f28f-bfd9-43d2-9099-6fd74a702127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8c7c69-5918-4924-b831-e24e3029405c",
   "metadata": {},
   "source": [
    "## Overflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb1473-77c2-4b0b-8334-44939abfbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all spikes:\n",
    "active_epoch_placefields1Da = PfND(deepcopy(spikes_df), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74209eda-e41f-46e2-95d6-869a225c3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyramidal spikes only:\n",
    "active_epoch_placefields1Db = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # grid_bin=, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef94996-c927-44c7-8826-d0e29b102bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlacefieldComputationParameters\n",
    "# Parameter sweeps:\n",
    "\n",
    "# grid_bin = [(1.0, 1.0)]\n",
    "\n",
    "# 10.0\n",
    "cls.compute_position_grid_bin_size(sess.position.x, sess.position.y, num_bins=(64, 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-poetry",
   "language": "python",
   "name": "spike3d-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
