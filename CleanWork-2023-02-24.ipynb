{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb364483-af18-4721-a504-688311f0a4ab",
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import traceback # for stack trace formatting\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pandas_profiling import ProfileReport ## for dataframe viewing\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables, objsize, print_object_memory_usage, debug_dump_object_member_shapes, TypePrintMode\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_time_str, get_now_time_precise_str\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "from pyphoplacecellanalysis.General.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "global_data_root_parent_path = Path(r'W:\\Data') # Windows Apogee\n",
    "# global_data_root_parent_path = Path(r'/media/MAX/Data') # Diba Lab Workstation Linux\n",
    "# global_data_root_parent_path = Path(r'/Volumes/MoverNew/data') # rMBP\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538e2a-4e39-4d11-90b5-a9fef9258058",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f917bad9-8fe7-4882-b83b-71cf878fffd2",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_session_names_list: ['2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_1-22-43', '2006-6-09_3-23-37', '2006-6-12_15-55-31', '2006-6-13_14-42-6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-07_11-26-53'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-08_14-26-15'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-09_3-23-37'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-12_15-55-31'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/one/2006-6-13_14-42-6'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "## Animal `gor01`:\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "### Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ### Animal `pin01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='pin01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # no exper_name ('one' or 'two') folders for this animal.\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['redundant','showclus','sleep','tmaze'])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155124df-eddb-461c-804f-de8f7acd332e",
   "metadata": {
    "tags": [
     "load",
     "single_session"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\loadedSessPickle.pkl.\n",
      "property already present in pickled version. No need to save.\n",
      "using provided computation_functions_name_whitelist: ['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_two_step_position_decoding_computation']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "# %%viztracer\n",
    "basedir = local_session_paths_list[1] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# epoch_name_whitelist = ['maze']\n",
    "epoch_name_whitelist = None\n",
    "active_computation_functions_name_whitelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        # '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_whitelist=epoch_name_whitelist,\n",
    "                                          computation_functions_name_whitelist=active_computation_functions_name_whitelist,\n",
    "                                          saving_mode=PipelineSavingScheme.SKIP_SAVING, force_reload=False, skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d77d8-811b-4402-a37d-dcd82efa8fc9",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "# 2023-02-24 Decoders \n",
    "- [ ] where are cells chosen for inclusion in the input of the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5cf00e-cd16-40ab-a8f9-7b0b2c4c36ed",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "long_epoch_name, short_epoch_name, global_epoch_name = long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "\n",
    "recalculate_anyway = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17793403-7842-4c6c-ade7-2a0c487f777d",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self will be re-binned to match target_pf1D...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36668,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26242,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "decoding_time_bin_size: 0.03333\n"
     ]
    }
   ],
   "source": [
    "# Make the 1D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf1D = long_results.pf1D\n",
    "short_pf1D = short_results.pf1D\n",
    "global_pf1D = global_results.pf1D\n",
    "\n",
    "# short_pf1D, did_update_bins = short_pf1D.conform_to_position_bins(long_pf1D, force_recompute=True) # not needed because it's done in one_step_decoder_1D.conform_to_position_bins(...)\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D  = [results_data.get('pf1D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_1D, did_recompute = short_one_step_decoder_1D.conform_to_position_bins(long_one_step_decoder_1D, force_recompute=True)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_1D is None) or (short_two_step_decoder_1D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_1D, short_two_step_decoder_1D  = [results_data.get('pf1D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_1D is not None and short_two_step_decoder_1D is not None)\n",
    "\n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "# decoding_time_bin_size = 0.03 # 0.03333333333333333\n",
    "print(f'decoding_time_bin_size: {decoding_time_bin_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fc6cd-84b9-405f-bf53-6b66a6c32525",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "source": [
    "#### Get 2D Decoders for validation and comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7b9806-923f-42fd-921f-1f75bdddc5cb",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self will be re-binned to match target_pf1D...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36668,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "np.isscalar(bin_size): 1.607897707662558\n",
      "prev_one_step_bayesian_decoder.ndim == 1, so using [0.0] as active_ybin_centers.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26242,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m curr_active_pipeline\u001b[38;5;241m.\u001b[39mperform_specific_computation(computation_functions_name_whitelist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_perform_two_step_position_decoding_computation\u001b[39m\u001b[38;5;124m'\u001b[39m], computation_kwargs_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mdict\u001b[39m(ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)], enabled_filter_names\u001b[38;5;241m=\u001b[39m[long_epoch_name, short_epoch_name], fail_on_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, debug_print\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m long_two_step_decoder_2D, short_two_step_decoder_2D  \u001b[38;5;241m=\u001b[39m [results_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpf2D_TwoStepDecoder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m results_data \u001b[38;5;129;01min\u001b[39;00m (long_results, short_results)]\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (long_two_step_decoder_2D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m short_two_step_decoder_2D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make the 2D Placefields and Decoders conform between the long and the short epochs:\n",
    "long_pf2D = long_results.pf2D\n",
    "short_pf2D = short_results.pf2D\n",
    "global_pf2D = global_results.pf2D\n",
    "\n",
    "# long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "# long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "\n",
    "# short_pf2D, did_update_bins = short_pf2D.conform_to_position_bins(long_pf2D)\n",
    "long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "short_one_step_decoder_2D, did_recompute = short_one_step_decoder_2D.conform_to_position_bins(long_one_step_decoder_2D)\n",
    "\n",
    "## Build or get the two-step decoders for both the long and short:\n",
    "long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "if recalculate_anyway or did_recompute or (long_two_step_decoder_2D is None) or (short_two_step_decoder_2D is None):\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_whitelist=['_perform_two_step_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
    "    long_two_step_decoder_2D, short_two_step_decoder_2D  = [results_data.get('pf2D_TwoStepDecoder', None) for results_data in (long_results, short_results)]\n",
    "    assert (long_two_step_decoder_2D is not None and short_two_step_decoder_2D is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43499e87-0693-4d11-9d33-100a987737ac",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "decoder"
    ]
   },
   "outputs": [],
   "source": [
    "# Sums are similar:\n",
    "print(f'{np.sum(long_one_step_decoder_2D.marginal.x.p_x_given_n) =},\\t {np.sum(long_one_step_decoder_1D.p_x_given_n) = }') # 31181.999999999996 vs 31181.99999999999\n",
    "\n",
    "## Validate:\n",
    "assert long_one_step_decoder_2D.marginal.x.p_x_given_n.shape == long_one_step_decoder_1D.p_x_given_n.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.p_x_given_n.shape =} and {long_one_step_decoder_1D.p_x_given_n.shape =}\"\n",
    "assert long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape == long_one_step_decoder_1D.most_likely_positions.shape, f\"Must equal but: {long_one_step_decoder_2D.marginal.x.most_likely_positions_1D.shape =} and {long_one_step_decoder_1D.most_likely_positions.shape =}\"\n",
    "\n",
    "## validate values:\n",
    "assert np.allclose(long_one_step_decoder_2D.marginal.x.p_x_given_n, long_one_step_decoder_1D.p_x_given_n), f\"1D Decoder should have an x-posterior equal to its own posterior\"\n",
    "assert np.allclose(curr_epoch_result['marginal_x']['most_likely_positions_1D'], curr_epoch_result['most_likely_positions']), f\"1D Decoder should have an x-posterior with most_likely_positions_1D equal to its own most_likely_positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e309d-ba28-490c-9172-5ab429166d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate the spikes that are coming in to build the decoder... those are from the placefields actually.\n",
    "    # so it'll be gating the placefield cells.\n",
    "    \n",
    "active_config_name = 'maze1'\n",
    "active_pf_1D = long_pf1D\n",
    "# curr_active_pipeline.computation_results['maze']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a48ea-31e9-499d-b413-929f9f57680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansi2html import Ansi2HTMLConverter # used by DocumentationFilePrinter to build html document from ansi-color coded version\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter\n",
    "\n",
    "doc_printer = DocumentationFilePrinter(doc_output_parent_folder=Path('C:/Users/pho/repos/PhoPy3DPositionAnalysis2021/EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation'), doc_name='PfND')\n",
    "doc_printer.save_documentation('PfND', active_pf_1D, non_expanded_item_keys=['_reverse_cellID_index_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22a5bd08-02e6-4001-a7ed-bb8a6f988a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1c423-0e9c-4732-a0f2-a73b04530ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Test that changing the position bins post-hoc is equivalent to initially computing with those position bins\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences\n",
    "from neuropy.utils.debug_helpers import debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "active_pos = curr_active_pipeline.sess.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b5345f-050f-48ad-97cc-aabfebd888a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.active_sess_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dcb1473-77c2-4b0b-8334-44939abfbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all spikes:\n",
    "active_epoch_placefields1Da = PfND(deepcopy(spikes_df), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74209eda-e41f-46e2-95d6-869a225c3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyramidal spikes only:\n",
    "active_epoch_placefields1Db = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1)) # grid_bin=, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef94996-c927-44c7-8826-d0e29b102bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlacefieldComputationParameters\n",
    "# Parameter sweeps:\n",
    "\n",
    "# grid_bin = [(1.0, 1.0)]\n",
    "\n",
    "# 10.0\n",
    "cls.compute_position_grid_bin_size(sess.position.x, sess.position.y, num_bins=(64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba15d5af-6333-4985-998c-031bff5b8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "output_pfs = {} # empty dict\n",
    "\n",
    "for smooth in smooth_options:\n",
    "    # pyramidal spikes only:\n",
    "    output_pfs[smooth] = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), grid_bin=(1,1), smooth=smooth, speed_thresh=10.0) # grid_bin=, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7561c817-dc0e-4b81-ae17-470d795b6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the differences:\n",
    "fig, axs = plt.subplots(1,5);\n",
    "plt.subplots_adjust(top=0.968,bottom=0.05,left=0.021,right=0.993,hspace=0.2,wspace=0.116)\n",
    "for i, smooth in enumerate(smooth_options):\n",
    "    output_pf = output_pfs[smooth]\n",
    "    output_pf.plot_ratemaps_1D(ax=axs[i])\n",
    "    axs[i].set_title('') # TODO: display the parameter value without losing the number of good cells for each.\n",
    "    debug_print_placefield(output_pf)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7bdf097b-c70e-458b-b3e1-999bab3ea0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import List, Tuple, NamedTuple\n",
    "\n",
    "def parameter_sweeps(**kwargs):\n",
    "    \"\"\" Returns every unique combination of the passed in parameters. Superior to cartesian_product as it preserves the labels (returning a flat list of dicts) and accepts more than 2 inputs.\n",
    "    \n",
    "    Usage:\n",
    "        all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(smooth=[(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)], grid_bin=[(1,1),(5,5),(10,10)])\n",
    "        >> all_param_sweep_options:  [{'smooth': (None, None), 'grid_bin': (1, 1)}, {'smooth': (None, None), 'grid_bin': (5, 5)}, {'smooth': (None, None), 'grid_bin': (10, 10)},\n",
    "        {'smooth': (0.5, 0.5), 'grid_bin': (1, 1)},  {'smooth': (0.5, 0.5), 'grid_bin': (5, 5)},  {'smooth': (0.5, 0.5), 'grid_bin': (10, 10)},\n",
    "        {'smooth': (1.0, 1.0), 'grid_bin': (1, 1)}, ...]\n",
    "        >> param_sweep_option_n_values: {'smooth': 5, 'grid_bin': 3}\n",
    "        \n",
    "    NOTE:\n",
    "        Replaces:\n",
    "        all_param_sweep_options = cartesian_product(grid_bin_options, smooth_options)\n",
    "        param_sweep_option_n_values = dict(grid_bin=len(grid_bin_options), smooth=len(smooth_options)) \n",
    "\n",
    "    \"\"\"\n",
    "    all_param_sweep_options = []\n",
    "    param_sweep_option_n_values = {k:len(v) for k, v in kwargs.items()}\n",
    "    for values in itertools.product(*kwargs.values()):\n",
    "        all_param_sweep_options.append(dict(zip(kwargs.keys(), values))) # Output dictionary\n",
    "        # param_sweep = ParamSweepNamedTuple(params=values)\n",
    "        # all_param_sweep_options.append(param_sweep)\n",
    "    return all_param_sweep_options, param_sweep_option_n_values\n",
    "                      \n",
    "\n",
    "def _compute_parameter_sweep(all_param_sweep_options):\n",
    "    \"\"\" Computes the PfNDs for all the swept parameters (combinations of grid_bin, smooth, etc)\n",
    "    \n",
    "    Usage:\n",
    "        smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "        grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "        all_param_sweep_options = cartesian_product(smooth_options, grid_bin_options)\n",
    "        param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "        output_pfs = _compute_parameter_sweep(all_param_sweep_options)\n",
    "\n",
    "    \"\"\"\n",
    "    output_pfs = {} # empty dict\n",
    "\n",
    "    for a_sweep_tuple in all_param_sweep_options:\n",
    "        # pyramidal spikes only:\n",
    "        if isinstance(a_sweep_tuple, dict):\n",
    "            a_sweep_dict = a_sweep_tuple # already a dict\n",
    "            a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
    "            \n",
    "        else:\n",
    "            # otherwise assume it's a named-tuple that can be converted to dict via a_sweep_tuple._asdict()\n",
    "            a_sweep_dict = a_sweep_tuple._asdict()\n",
    "            \n",
    "        output_pfs[a_sweep_tuple] = PfND(deepcopy(spikes_df).spikes.sliced_by_neuron_type('pyramidal'), deepcopy(active_pos.linear_pos_obj), **a_sweep_dict) # grid_bin=, etc\n",
    "        \n",
    "    return output_pfs\n",
    "\n",
    "def _plot_parameter_sweep(output_pfs, param_sweep_option_n_values):\n",
    "    \"\"\" Sweeps a 1D parameter for the placefields and plots it\n",
    "    \n",
    "    Usage:\n",
    "        fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values)\n",
    "        \n",
    "    \"\"\"\n",
    "    if len(output_pfs)>0:        \n",
    "        if len(param_sweep_option_n_values) > 1:\n",
    "            # more than one variable\n",
    "            num_rows = list(param_sweep_option_n_values.values())[1] # get the first length\n",
    "        else:\n",
    "            # only one variable\n",
    "            num_rows = 1\n",
    "        \n",
    "        num_columns = len(output_pfs) // num_rows\n",
    "        \n",
    "        # remove any singleton variables\n",
    "        formatting_included_items_list = [k for k, v in param_sweep_option_n_values.items() if v>1]\n",
    "\n",
    "        def _plot_title_formatter(x):\n",
    "            # printable_dict = {k:v for k, v in x.items() if k in formatting_included_items_list} # if `x` is dict and has .items() method\n",
    "            printable_dict = {k:v for k, v in x if k in formatting_included_items_list}\n",
    "            return f\"{printable_dict}\"\n",
    "        \n",
    "        plot_title_formatter = _plot_title_formatter\n",
    "        \n",
    "        # plot_title_formatter = lambda x: f\"{x}\" # full NamedTuple item\n",
    "        \n",
    "        # # old namedtuple implementation:\n",
    "        # if num_rows < 2:\n",
    "        #     # suppress the constant variable in the title\n",
    "        #     plot_title_formatter = lambda x: f\"{x._fields[1]}={x[1]}\" # only get the first item\n",
    "        # else:\n",
    "        #     plot_title_formatter = lambda x: f\"{x}\" # full NamedTuple item\n",
    "        \n",
    "    else:\n",
    "        return # empty\n",
    "    \n",
    "    print(f'{num_rows = }, {num_columns = }')\n",
    "    fig, axs = plt.subplots(num_rows, num_columns, sharex=True);\n",
    "    plt.subplots_adjust(top=0.968,bottom=0.05,left=0.021,right=0.993,hspace=0.2,wspace=0.116)\n",
    "    # Flatten the axs array\n",
    "    axs = axs.ravel()\n",
    "    for i, (param_sweep_tuple, output_pf) in enumerate(output_pfs.items()):\n",
    "        output_pf.plot_ratemaps_1D(ax=axs[i])\n",
    "        axs[i].set_title(plot_title_formatter(param_sweep_tuple)) # TODO: display the parameter value without losing the number of good cells for each.\n",
    "        debug_print_placefield(output_pf)\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "476b4c27-a111-4ff2-a116-0cf0d302fec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'smooth': (None, None), 'grid_bin': (1, 1)},\n",
       " {'smooth': (None, None), 'grid_bin': (5, 5)},\n",
       " {'smooth': (None, None), 'grid_bin': (10, 10)},\n",
       " {'smooth': (0.5, 0.5), 'grid_bin': (1, 1)},\n",
       " {'smooth': (0.5, 0.5), 'grid_bin': (5, 5)},\n",
       " {'smooth': (0.5, 0.5), 'grid_bin': (10, 10)},\n",
       " {'smooth': (1.0, 1.0), 'grid_bin': (1, 1)},\n",
       " {'smooth': (1.0, 1.0), 'grid_bin': (5, 5)},\n",
       " {'smooth': (1.0, 1.0), 'grid_bin': (10, 10)},\n",
       " {'smooth': (2.0, 2.0), 'grid_bin': (1, 1)},\n",
       " {'smooth': (2.0, 2.0), 'grid_bin': (5, 5)},\n",
       " {'smooth': (2.0, 2.0), 'grid_bin': (10, 10)},\n",
       " {'smooth': (5.0, 5.0), 'grid_bin': (1, 1)},\n",
       " {'smooth': (5.0, 5.0), 'grid_bin': (5, 5)},\n",
       " {'smooth': (5.0, 5.0), 'grid_bin': (10, 10)}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# all_param_sweep_options = cartesian_product(grid_bin_options, smooth_options)\n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(smooth=[(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)], grid_bin=[(1,1),(5,5),(10,10)])\n",
    "all_param_sweep_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21053b39-214b-49be-905c-da33146f27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth_options = [(None, None), (0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "# grid_bin_options = [(1,1),(5,5),(10,10)]\n",
    "# grid_bin_options = [(5,5)]\n",
    "# param_sweep_option_n_values = dict(smooth=len(smooth_options), grid_bin=len(grid_bin_options)) \n",
    "\n",
    "smooth_options = [(None, None)]\n",
    "grid_bin_options = [(0.5, 0.5), (1.0, 1.0), (2.0, 2.0), (5.0, 5.0)]\n",
    "# all_param_sweep_options = cartesian_product(grid_bin_options, smooth_options)\n",
    "# param_sweep_option_n_values = dict(grid_bin=len(grid_bin_options), smooth=len(smooth_options)) \n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(smooth=smooth_options, grid_bin=grid_bin_options)\n",
    "output_pfs = _compute_parameter_sweep(all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4a3978c0-111d-4311-843e-d0c2565f8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_thresh_options = [25.0, 50.0, 100.0, 200.0]\n",
    "# all_param_sweep_options = cartesian_product(speed_thresh_options, smooth_options)\n",
    "# param_sweep_option_n_values = dict(speed_thresh=len(speed_thresh_options), smooth=len(smooth_options)) \n",
    "all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(speed_thresh=speed_thresh_options)\n",
    "output_pfs = _compute_parameter_sweep(all_param_sweep_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63cca0a3-8c70-4356-99a7-58a454cee062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows = 1, num_columns = 4\n",
      "good_placefield_neuronIDs: (56 good)\n",
      "num_spikes: (24761 total spikes)\n",
      "good_placefield_neuronIDs: (44 good)\n",
      "num_spikes: (11724 total spikes)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43m_plot_parameter_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_pfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_sweep_option_n_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[126], line 101\u001b[0m, in \u001b[0;36m_plot_parameter_sweep\u001b[1;34m(output_pfs, param_sweep_option_n_values)\u001b[0m\n\u001b[0;32m     99\u001b[0m axs \u001b[38;5;241m=\u001b[39m axs\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (param_sweep_tuple, output_pf) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(output_pfs\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m--> 101\u001b[0m     \u001b[43moutput_pf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_ratemaps_1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     axs[i]\u001b[38;5;241m.\u001b[39mset_title(plot_title_formatter(param_sweep_tuple)) \u001b[38;5;66;03m# TODO: display the parameter value without losing the number of good cells for each.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     debug_print_placefield(output_pf)\n",
      "File \u001b[1;32m~\\repos\\NeuroPy\\neuropy\\plotting\\mixins\\placemap_mixins.py:26\u001b[0m, in \u001b[0;36mPfnDPlottingMixin.plot_ratemaps_1D\u001b[1;34m(self, ax, pad, normalize, sortby, cmap, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Note that normalize is required to fit all of the plots on this kind of stacked figure. \"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# returns: ax , sort_ind, colors\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_ratemap_1D(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratemap, ax\u001b[38;5;241m=\u001b[39max, pad\u001b[38;5;241m=\u001b[39mpad, normalize_tuning_curve\u001b[38;5;241m=\u001b[39mnormalize, sortby\u001b[38;5;241m=\u001b[39msortby, cmap\u001b[38;5;241m=\u001b[39mcmap, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\repos\\NeuroPy\\neuropy\\utils\\debug_helpers.py:92\u001b[0m, in \u001b[0;36msafely_accepts_kwargs.<locals>._safe_kwargs_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_kwargs_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverriding_dict_with(lhs_dict\u001b[38;5;241m=\u001b[39mdefault_kwargs_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\repos\\NeuroPy\\neuropy\\plotting\\ratemaps.py:495\u001b[0m, in \u001b[0;36mplot_ratemap_1D\u001b[1;34m(ratemap, normalize_xbin, fignum, fig, ax, pad, normalize_tuning_curve, sortby, cmap, included_unit_indicies, included_unit_neuron_IDs, brev_mode, plot_variable, curve_hatch_style, missing_aclu_string_formatter, single_cell_pfmap_processing_fn, debug_print)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug_print:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalizing tuning curves...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 495\u001b[0m     active_maps \u001b[38;5;241m=\u001b[39m \u001b[43mmathutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_max_scaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_maps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m     pad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m## Sorting (via sortby):\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\NeuroPy\\neuropy\\utils\\mathutil.py:23\u001b[0m, in \u001b[0;36mmin_max_scaler\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmin_max_scaler\u001b[39m(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scales the values x to lie between 0 and 1 along the specfied axis\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m        scaled array\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mptp(\n\u001b[0;32m     24\u001b[0m         x, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-pZq3eobA-py3.9\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2918\u001b[0m, in \u001b[0;36mamin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amin_dispatcher)\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2804\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2806\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[0;32m   2807\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2916\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2919\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-pZq3eobA-py3.9\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "fig, axs = _plot_parameter_sweep(output_pfs, param_sweep_option_n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3471035-ed0a-4037-bfea-d7b8edd82a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epoch_placefields1Da.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b3290-cd77-46b0-9a8b-fb86baf5a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epoch_placefields1Db.cell_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e157e49-a8b7-4910-8c6d-a51caff360a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(5, 3), smooth=(0.0, 0.0), frate_thresh=0.1) # TODO: FIXME: BUG: when frate_thresh=0.0, there are 0 good placefield_neuronIDs for all computations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29110c9-fd37-4b1f-b871-6d867f2d7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All Spikes:\n",
    "active_epoch_session = sess.filtered_by_neuron_type('pyramidal').filtered_by_epoch(active_epoch)\n",
    "print_subsession_neuron_differences(sess.neurons, active_epoch_session.neurons)\n",
    "# print(sess.neurons.n_spikes)\n",
    "\n",
    "# # ## Lap_specific Spikes Only:\n",
    "# active_lap_specific_epoch_session = lap_specific_session.filtered_by_neuron_type('pyramidal').filtered_by_epoch(active_epoch)\n",
    "# print_subsession_neuron_differences(lap_specific_session.neurons, active_lap_specific_epoch_session.neurons)\n",
    "# # print(active_lap_specific_epoch_session.neurons.n_spikes)\n",
    "\n",
    "## Configure Placefield Calc:\n",
    "should_display_2D_plots = False\n",
    "\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=2, smooth=0.5, frate_thresh=2.0)\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=10, smooth=0.5, frate_thresh=2.0) # works well\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=2.5, smooth=1.5, frate_thresh=2.0)\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=(10, 3), smooth=(0.5, 0.5), frate_thresh=0.0)\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(3, 4), smooth=(2, 1), frate_thresh=2.0)\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=(10, 10), smooth=(0.5, 0.5), frate_thresh=2.0) ## Works well for 2D Placemaps\n",
    "# height: 20.0\n",
    "# width: 250.0\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0, grid_bin=(2.0, 0.2), smooth=(0.5, 0.5), frate_thresh=2.0) ## Extremely Slow\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0, grid_bin=(2.0, 1.0), smooth=(0.5, 0.5), frate_thresh=2.0) ## Very slow, doesn't work\n",
    "\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=(10, 3), smooth=(0.0, 0.0), frate_thresh=2.0)\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=(10, 3), smooth=(0.1, 0.1), frate_thresh=2.0)\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=1, grid_bin=(10, 3), smooth=(1.0, 10.0), frate_thresh=2.0)\n",
    "\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(25, 9), smooth=(0.0, 0.0), frate_thresh=2.0)\n",
    "# active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(5, 3), smooth=(0.0, 0.0), frate_thresh=2.0)\n",
    "active_config.computation_config = PlacefieldComputationParameters(speed_thresh=0.0, grid_bin=(5, 3), smooth=(0.0, 0.0), frate_thresh=0.1) # TODO: FIXME: BUG: when frate_thresh=0.0, there are 0 good placefield_neuronIDs for all computations!\n",
    "\n",
    "\n",
    "active_epoch_placefields1D, active_epoch_placefields2D = compute_placefields_masked_by_epochs(active_epoch_session, active_config, included_epochs=None, should_display_2D_plots=should_display_2D_plots)\n",
    "even_lap_specific_placefields1D, even_lap_specific_placefields2D = compute_placefields_masked_by_epochs(active_epoch_session, active_config, included_epochs=even_lap_specific_epochs, should_display_2D_plots=should_display_2D_plots)\n",
    "odd_lap_specific_placefields1D, odd_lap_specific_placefields2D = compute_placefields_masked_by_epochs(active_epoch_session, active_config, included_epochs=odd_lap_specific_epochs, should_display_2D_plots=should_display_2D_plots)\n",
    "any_lap_specific_placefields1D, any_lap_specific_placefields2D = compute_placefields_masked_by_epochs(active_epoch_session, active_config, included_epochs=any_lap_specific_epochs, should_display_2D_plots=should_display_2D_plots)\n",
    "# Compare the results\n",
    "# debug_print_ratemap(active_epoch_placefields1D.ratemap)\n",
    "# num_spikes_per_spiketrain = np.array([np.shape(a_spk_train)[0] for a_spk_train in active_epoch_placefields1D.spk_t])\n",
    "# num_spikes_per_spiketrain\n",
    "# print('placefield_neuronID_spikes: {}; ({} total spikes)'.format(num_spikes_per_spiketrain, np.sum(num_spikes_per_spiketrain)))\n",
    "\n",
    "debug_print_placefield(active_epoch_placefields1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf7f23-d751-4c84-989c-2a61668ffe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "## Build the new decoder with custom params:\n",
    "active_computation_config = curr_active_pipeline.computation_results[active_config_name].computation_config\n",
    "new_decoder_pf_params = deepcopy(active_computation_config.pf_params) # should be a PlacefieldComputationPcurr_active_pipeliners\n",
    "# override some settings before computation:\n",
    "# new_decoder_pf_params.time_bin_size = time_bin_size\n",
    "\n",
    "## 1D Decoder\n",
    "new_decoder_pf1D = active_pf_1D\n",
    "new_1D_decoder_spikes_df = new_decoder_pf1D.filtered_spikes_df.copy()\n",
    "\n",
    "# Why would it need both the pf1D and the spikes? Doesn't the pf1D include the spikes (and determine the placefields, which are all that are used)???\n",
    "new_1D_decoder = BayesianPlacemapPositionDecoder(new_decoder_pf_params.time_bin_size, new_decoder_pf1D, new_1D_decoder_spikes_df, debug_print=False)\n",
    "\n",
    "new_1D_decoder.compute_all() #  --> n = self.\n",
    "\n",
    "\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5dda7-754a-4382-a5ca-7ad032073271",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e50c6-ed5e-43ca-b423-5214f462fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _perform_specific_epochs_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697560e-fdc9-45ec-bf89-87440b22780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fd8ca-655e-4395-8020-42b37ff97106",
   "metadata": {},
   "source": [
    "# 2023-02-27 - Test whether conform to active position works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5924e26-3571-4220-af52-0d3d04437590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder, Zhang_Two_Step\n",
    "\n",
    "prev_output_result.computed_data['pf1D_Decoder'] = BayesianPlacemapPositionDecoder(pf_computation_config.time_bin_size, prev_output_result.computed_data['pf1D'], prev_output_result.computed_data['pf1D'].filtered_spikes_df.copy(), debug_print=False)\n",
    "prev_output_result.computed_data['pf1D_Decoder'].compute_all() #\n",
    "\n",
    "prev_output_result.computed_data['pf2D_Decoder'] = BayesianPlacemapPositionDecoder(pf_computation_config.time_bin_size, prev_output_result.computed_data['pf2D'], prev_output_result.computed_data['pf2D'].filtered_spikes_df.copy(), debug_print=False)\n",
    "prev_output_result.computed_data['pf2D_Decoder'].compute_all() #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-poetry",
   "language": "python",
   "name": "spike3d-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
