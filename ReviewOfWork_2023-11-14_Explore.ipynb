{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, CapturedException, document_active_variables\n",
    "from pyphocorehelpers.general_helpers import GeneratedClassDefinitionType, CodeConversion, inspect_callable_arguments\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.general_helpers import inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables, CapturedException\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement\n",
    "\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, fullwidth_path_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import render_colors\n",
    "\n",
    "global_data_root_parent_path = find_first_extant_path([Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/home/halechr/FastData'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/home/halechr/cloud/turbo/Data')])\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {},
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d2b8a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context: IdentifyingContext = good_contexts_list[1] # select the session from all of the good sessions here.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # DONE. Very good. Many good Pfs, many good replays.\n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # DONE, might be the BEST SESSION, good example session with lots of place cells, clean replays, and clear bar graphs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # DONE, Good Pfs but no good replays\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6') # BAD, 2023-07-14, unsure why still.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # DONE, GREAT, both good Pfs and replays!\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25') # DONE, Added replay selections. Very \"jumpy\" between the starts and ends of the track.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40') # DONE, Added replay selections. A TON of putative replays in general, most bad, but some good.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46') # DONE, added replay selections. Very few (like 12) replays each.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-13_15-22-3') # DONE, Good Pfs, no good epochs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30') # DONE, okay replays (selected)\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50') # DONE, very few replays (selected)\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-19_13-34-40') # BAD\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54') # DONE, one replay each (selected)\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3') # BAD, Good Pfs strangely despite horrible map, no good epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-12_15-25-59') # BAD, No Epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-16_18-47-52')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-17_12-52-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-25_13-20-55')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-28_12-38-13')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44') # DONE, good. Many good pfs, many good replays. Noticed very strange jumping off the track in the 3D behavior/spikes viewer. Is there something wrong with this session?\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0') # DONE, good?, replays selected, few\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # DONE, very few replays\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_12-15-3')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_22-4-5')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # DONE, replays selected, quite a few replays but few are very good.\n",
    "\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['pf_computation',\n",
    "                                            #    'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step', \n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            # 'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba46b6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "### GLOBAL COMPUTATIONS:\n",
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        curr_active_pipeline.load_pickled_global_computation_results()\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results: {e}')\n",
    "        raise\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "extended_computations_include_includelist=['pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "    'pf_dt_sequential_surprise',\n",
    "     'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    # 'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'rank_order_shuffle_analysis'\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True, force_recompute=force_recompute_global, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     exception_info = sys.exc_info()\n",
    "#     e = CapturedException(e, exception_info)\n",
    "#     print(f'second half threw: {e}')\n",
    "\n",
    "\n",
    "# 4m 5.2s for inst fr computations\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, fullwidth_path_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334afa3",
   "metadata": {},
   "source": [
    "newly_computed_values: [('long_short_decoding_analyses', 'maze_any'), ('short_long_pf_overlap_analyses', 'maze_any'), ('long_short_fr_indicies_analyses', 'maze_any'), ('jonathan_firing_rate_analysis', 'maze_any'), ('long_short_post_decoding', 'maze_any'), ('long_short_endcap_analysis', 'maze_any')].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(curr_active_pipeline.active_completed_computation_result_names) # ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.active_incomplete_computation_result_status_dicts\n",
    "curr_active_pipeline.pipeline_compare_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e635746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force New:\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D().values() # included_neuron_ids=EITHER_subset.track_exclusive_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {},
   "source": [
    "# End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a533ba8",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name]['computation_config'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071e94f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## long_short_decoding_analyses:\n",
    "curr_long_short_decoding_analyses = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a929",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name]['computation_config'].pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name]['computation_config'] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84669e5c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# LR_ripple_rank_order_result, RL_ripple_rank_order_result, LR_laps_rank_order_result, RL_laps_rank_order_result = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "\n",
    "rank_order_results = curr_active_pipeline.global_computation_results.computed_data['RankOrder'] # RankOrderComputationsContainer\n",
    "\n",
    "# OLD: ODD/EVEN:\n",
    "# odd_laps_epoch_ranked_aclus_stats_dict, odd_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, odd_laps_long_z_score_values, odd_laps_short_z_score_values, odd_laps_long_short_z_score_diff_values = rank_order_results.odd_laps\n",
    "# even_laps_epoch_ranked_aclus_stats_dict, even_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, even_laps_long_z_score_values, even_laps_short_z_score_values, even_laps_long_short_z_score_diff_values = rank_order_results.even_laps\n",
    "\n",
    "# odd_ripple_evts_epoch_ranked_aclus_stats_dict, odd_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, odd_ripple_evts_long_z_score_values, odd_ripple_evts_short_z_score_values, odd_ripple_evts_long_short_z_score_diff_values = rank_order_results.odd_ripple\n",
    "# even_ripple_evts_epoch_ranked_aclus_stats_dict, even_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, even_ripple_evts_long_z_score_values, even_ripple_evts_short_z_score_values, even_ripple_evts_long_short_z_score_diff_values = rank_order_results.even_ripple\n",
    "\n",
    "# NEW: LR/RL:\n",
    "LR_laps_epoch_ranked_aclus_stats_dict, LR_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_laps_long_z_score_values, LR_laps_short_z_score_values, LR_laps_long_short_z_score_diff_values = rank_order_results.odd_laps # LR_laps\n",
    "RL_laps_epoch_ranked_aclus_stats_dict, RL_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, RL_laps_long_z_score_values, RL_laps_short_z_score_values, RL_laps_long_short_z_score_diff_values = rank_order_results.even_laps # RL_laps\n",
    "\n",
    "LR_ripple_evts_epoch_ranked_aclus_stats_dict, LR_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_ripple_evts_long_z_score_values, LR_ripple_evts_short_z_score_values, LR_ripple_evts_long_short_z_score_diff_values = rank_order_results.odd_ripple # LR_ripple\n",
    "RL_ripple_evts_epoch_ranked_aclus_stats_dict, RL_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, RL_ripple_evts_long_z_score_values, RL_ripple_evts_short_z_score_values, RL_ripple_evts_long_short_z_score_diff_values = rank_order_results.even_ripple # RL_ripple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover from the saved global result:\n",
    "# Unpacking:\n",
    "directional_laps_results = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_lap_specific_configs, split_directional_laps_dict, split_directional_laps_config_names, computed_base_epoch_names = directional_laps_results.directional_lap_specific_configs, directional_laps_results.split_directional_laps_dict, directional_laps_results.split_directional_laps_config_names, directional_laps_results.computed_base_epoch_names\n",
    "long_odd_shared_aclus_only_one_step_decoder_1D, long_even_shared_aclus_only_one_step_decoder_1D, short_odd_shared_aclus_only_one_step_decoder_1D, short_even_shared_aclus_only_one_step_decoder_1D = [directional_laps_results.__dict__[k] for k in ['long_odd_shared_aclus_only_one_step_decoder_1D', 'long_even_shared_aclus_only_one_step_decoder_1D', 'short_odd_shared_aclus_only_one_step_decoder_1D', 'short_even_shared_aclus_only_one_step_decoder_1D']]\n",
    "# track_templates: TrackTemplates = TrackTemplates.init_from_paired_decoders(LR_decoder_pair=(long_odd_shared_aclus_only_one_step_decoder_1D, short_odd_shared_aclus_only_one_step_decoder_1D), RL_decoder_pair=(long_even_shared_aclus_only_one_step_decoder_1D, short_even_shared_aclus_only_one_step_decoder_1D))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "long_odd_shared_aclus_only_one_step_decoder_1D, long_even_shared_aclus_only_one_step_decoder_1D, short_odd_shared_aclus_only_one_step_decoder_1D, short_even_shared_aclus_only_one_step_decoder_1D = [directional_laps_results.__dict__[k] for k in ['long_odd_shared_aclus_only_one_step_decoder_1D', 'long_even_shared_aclus_only_one_step_decoder_1D', 'short_odd_shared_aclus_only_one_step_decoder_1D', 'short_even_shared_aclus_only_one_step_decoder_1D']]\n",
    "long_odd_laps_obj, long_even_laps_obj, short_odd_laps_obj, short_even_laps_obj = list(directional_laps_results.split_directional_laps_dict.values())\n",
    "\n",
    "# print(list(directional_laps_results.__dict__.keys()))\n",
    "# ['directional_lap_specific_configs', 'split_directional_laps_dict', 'split_directional_laps_contexts_dict', 'split_directional_laps_config_names', 'computed_base_epoch_names', 'long_odd_shared_aclus_only_one_step_decoder_1D', 'long_even_shared_aclus_only_one_step_decoder_1D', 'short_odd_shared_aclus_only_one_step_decoder_1D', 'short_even_shared_aclus_only_one_step_decoder_1D']\n",
    "\n",
    "# split_directional_laps_config_names\n",
    "# split_directional_laps_config_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6087c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_LR_pf1D.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df[speed_df['speed'] < speed_thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c47071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cf6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.efficient_interval_search import filter_epochs_by_speed\n",
    "from neuropy.utils.efficient_interval_search import _find_intervals_above_speed, _convert_start_end_tuples_list_to_PortionInterval\n",
    "from neuropy.utils.mixins.time_slicing import add_epochs_id_identity\n",
    "\n",
    "from neuropy.analyses.placefields import PfND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fe7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder = deepcopy(long_LR_pf1D)\n",
    "epochs_df = deepcopy(a_decoder.epochs.to_dataframe())\n",
    "speed_thresh = a_decoder.config.speed_thresh # 10.0\n",
    "position_df = a_decoder.position.to_dataframe()\n",
    "speed_filtered_epochs_df = PfND.filtered_by_speed(epochs_df, position_df, speed_thresh=speed_thresh, debug_print=True)\n",
    "speed_filtered_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7027943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "display(epochs_df)\n",
    "\n",
    "pre_filtering_duration = epochs_df.duration.sum()\n",
    "print(f'pre_filtering_duration: {pre_filtering_duration}') # 135.73698799998965\n",
    "\n",
    "speed_thresh = a_decoder.config.speed_thresh # 10.0\n",
    "\n",
    "# speed_df = global_session.position.to_dataframe()\n",
    "speed_df = a_decoder.position.to_dataframe()\n",
    "\n",
    "epoch_id_string: str = f'decoder_epoch_id'\n",
    "speed_df = add_epochs_id_identity(speed_df, epochs_df, epoch_id_key_name=epoch_id_string, epoch_label_column_name=None, no_interval_fill_value=-1, override_time_variable_name='t')\n",
    "# drop the -1 indicies because they are below the speed:\n",
    "speed_df = speed_df[speed_df[epoch_id_string] != -1] # Drop all non-included position samples\n",
    "n_pre_filtered_samples = len(speed_df)\n",
    "print(f'n_pre_filtered_samples: {n_pre_filtered_samples}')\n",
    "\n",
    "pre_filtered_duration = speed_df\n",
    "speed_df = speed_df[speed_df['speed'] < speed_thresh] # filter the samples by speed_thresh\n",
    "# Performed 3 aggregations grouped on column: 'decoder_epoch_id'\n",
    "speed_filtered_epochs_df = speed_df.groupby(['decoder_epoch_id']).agg(t_first=('t', 'first'), t_last=('t', 'last'), t_count=('t', 'count')).reset_index()\n",
    "# Rename column 't_first' to 'start'\n",
    "speed_filtered_epochs_df = speed_filtered_epochs_df.rename(columns={'t_first': 'start'})\n",
    "# Rename column 't_last' to 'stop'\n",
    "speed_filtered_epochs_df = speed_filtered_epochs_df.rename(columns={'t_last': 'stop'})\n",
    "# Rename column 'decoder_epoch_id' to 'label'\n",
    "speed_filtered_epochs_df = speed_filtered_epochs_df.rename(columns={'decoder_epoch_id': 'label'})\n",
    "# Rename column 't_count' to 'n_samples'\n",
    "speed_filtered_epochs_df = speed_filtered_epochs_df.rename(columns={'t_count': 'n_samples'})\n",
    "speed_filtered_epochs_df['duration'] = speed_filtered_epochs_df['stop'] - speed_filtered_epochs_df['start']\n",
    "\n",
    "\n",
    "speed_filtered_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d151acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_filtering_duration = speed_filtered_epochs_df.duration.sum()\n",
    "print(f'post_filtering_duration: {post_filtering_duration}') # 130.96321400045417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_filtered_epochs_df['duration'] # ~130\n",
    "np.sum(speed_filtered_epochs_df['duration']) # 130.9632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_print = True\n",
    "epoch_args = [deepcopy(a_decoder.epochs)]\n",
    "\n",
    "start_end_tuples_interval_list = _find_intervals_above_speed(speed_df, speed_thresh, is_interpolated=True)\n",
    "above_speed_threshold_intervals = _convert_start_end_tuples_list_to_PortionInterval(start_end_tuples_interval_list)\n",
    "if debug_print:\n",
    "\tprint(f'len(above_speed_threshold_intervals): {len(above_speed_threshold_intervals)}')\n",
    "# find the intervals below the threshold speed by taking the complement:\n",
    "below_speed_threshold_intervals = above_speed_threshold_intervals.complement()\n",
    "if debug_print:\n",
    "\tprint(f'len(below_speed_threshold_intervals): {len(below_speed_threshold_intervals)}')\n",
    "\t# print(f'Pre-speed-filtering: ' + ', '.join([f\"n_epochs: {an_interval.n_epochs}\" for an_interval in epoch_args]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.efficient_interval_search import convert_PortionInterval_to_Epoch_obj\n",
    "\n",
    "epoch_args_Interval = [_convert_start_end_tuples_list_to_PortionInterval(zip(a_replays_epoch_obj.starts, a_replays_epoch_obj.stops)) for a_replays_epoch_obj in epoch_args] # returns P.Interval objects\n",
    "## Filter *_replays_Interval by requiring them to be below the speed:\n",
    "# epoch_args_Interval = [below_speed_threshold_intervals.intersection(a_replays_Interval_obj) for a_replays_Interval_obj in epoch_args_Interval] # returns P.Interval objects\n",
    "epoch_args_Interval = [above_speed_threshold_intervals.intersection(a_replays_Interval_obj) for a_replays_Interval_obj in epoch_args_Interval] # returns P.Interval objects\n",
    "## Convert back to Epoch objects:\n",
    "epoch_args = [convert_PortionInterval_to_Epoch_obj(a_replays_Interval_obj) for a_replays_Interval_obj in epoch_args_Interval] # returns P.Interval objects\n",
    "epoch_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd227e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de674d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# long_replays, short_replays, global_replays, above_speed_threshold_intervals, below_speed_threshold_intervals = filter_epochs_by_speed(speed_df, long_replays, short_replays, global_replays, speed_thresh=speed_thresh, debug_print=True)\n",
    "out_epochs, above_speed_threshold_intervals, below_speed_threshold_intervals = filter_epochs_by_speed(speed_df, long_LR_pf1D.epochs, speed_thresh=speed_thresh, debug_print=True)\n",
    "out_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_epochs.duration # 1011.887555000023\n",
    "\n",
    "out_epochs.durations.sum() # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_odd')\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_even')\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze2_even')\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze2_odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9070e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display(DirectionalPlacefieldGlobalDisplayFunctions._display_directional_laps_overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be784fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = _out['win']\n",
    "win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "area: pg.DockArea  = win.area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win.ui.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(_out.keys())\n",
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all exportable items:\n",
    "_out.plots["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out['plots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "import attrs\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "def create_class_from_dict(class_name, input_dict):\n",
    "    attributes = {}\n",
    "    for key, value in input_dict.items():\n",
    "        attributes[key] = attr.ib(type=type(value), default=value) # , repr=False\n",
    "\n",
    "    return attrs.make_class(class_name, attributes)\n",
    "\n",
    "TempGraphicsOutput = create_class_from_dict('TempGraphicsOutput', _out)\n",
    "TempGraphicsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2391753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = TempGraphicsOutput()\n",
    "print(instance)\n",
    "\n",
    "grl = instance.plots\n",
    "grl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyqtgraph.exporters\n",
    "\n",
    "exporter = pg.exporters.ImageExporter( grl.scene() ) # graphics layout widget (grl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d38dcc",
   "metadata": {},
   "source": [
    "## 2023-10-19 - Test Instantaneous Spike Rates and their visualizations during the Replay Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import BinningContainer\n",
    "\n",
    "\n",
    "replay_instantaneous_time_bin_size_seconds = 0.01\n",
    "inst_replay_frs: SpikeRateTrends = SpikeRateTrends.init_from_spikes_and_epochs(spikes_df=global_session.spikes_df, filter_epochs=global_replays, included_neuron_ids=EITHER_subset.track_exclusive_aclus.copy(), instantaneous_time_bin_size_seconds=replay_instantaneous_time_bin_size_seconds)\n",
    "neuron_labels = [str(aclu) for aclu in inst_replay_frs.included_neuron_ids] # labels for plotting the epochs using `BasicBinnedImageRenderingWindow`\n",
    "# 26 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_idx = 1\n",
    "a_replay_inst_frs = inst_replay_frs.inst_fr_df_list[epoch_idx].to_numpy() # (n_epoch_time_bins, n_cells)   ## OLD: #.T # (n_cells, n_epoch_time_bins)\n",
    "display(a_replay_inst_frs.shape)\n",
    "a_epoch_timebin_labels = [str(v) for v in np.arange(np.shape(a_replay_inst_frs)[0])]\n",
    "assert len(a_epoch_timebin_labels) == np.shape(a_replay_inst_frs)[0]\n",
    "assert len(neuron_labels) == np.shape(a_replay_inst_frs)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow, LayoutScrollability\n",
    "\n",
    "out = BasicBinnedImageRenderingWindow(a_replay_inst_frs, None, None, name='a_replay_inst_frs', title=\"Inst Fr for Replay Epoch\", variable_label='Inst FR', scrollability_mode=LayoutScrollability.NON_SCROLLABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Entropy/Surprise Results:\n",
    "active_extended_stats = global_results['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['pf_dt_sequential_surprise'] # DynamicParameters\n",
    "historical_snapshots = active_relative_entropy_results['historical_snapshots']\n",
    "post_update_times: np.ndarray = active_relative_entropy_results['post_update_times'] # (4152,) = (n_post_update_times,)\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals: np.ndarray = active_relative_entropy_results['time_intervals']\n",
    "surprise_time_bin_duration = (post_update_times[2]-post_update_times[1])\n",
    "long_short_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['long_short_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "short_long_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['short_long_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "flat_relative_entropy_results: np.ndarray = active_relative_entropy_results['flat_relative_entropy_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_results: np.ndarray = active_relative_entropy_results['flat_jensen_shannon_distance_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_across_all_positions: np.ndarray = np.sum(np.abs(flat_jensen_shannon_distance_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "flat_surprise_across_all_positions: np.ndarray = np.sum(np.abs(flat_relative_entropy_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "\n",
    "## Get the placefield dt matrix:\n",
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\t## Compute it if missing:\n",
    "\toccupancy_weighted_tuning_maps_over_time = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in historical_snapshots.values()])\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = occupancy_weighted_tuning_maps_over_time\n",
    "else:\n",
    "\toccupancy_weighted_tuning_maps_over_time = active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] # (n_post_update_times, n_neurons, n_xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5597bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483230a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "# appearing_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_distant_remapping_endcap_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aba9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import fig_remapping_cells\n",
    "\n",
    "graphics_output_dict = fig_remapping_cells(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-11-10 - Adds \"LxC_PBEsDeltaMinus\" for PBEs \n",
    "\n",
    "\n",
    "temp = add_extra_spike_rate_trends(curr_active_pipeline)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0ef2b",
   "metadata": {},
   "source": [
    "# 🟢 2023-10-20 - Z-Score Comparisons with Neuron_ID Shuffled templates\n",
    "1. Take the intersection of the long and short templates to get only the common cells\n",
    "2. Determine the long and short \"tempaltes\": this is done by ranking the aclus for each by their placefields' center of mass. `compute_placefield_center_of_masses`\n",
    "\t2a. `long_pf_peak_ranks`, `short_pf_peak_ranks` - there are one of each of these for each shared aclu.\n",
    "3. Generate the unit_id shuffled (`shuffled_aclus`, `shuffle_IDXs`) ahead of time to use to shuffle the two templates during the epochs.\n",
    "4. For each replay event, take each shuffled template\n",
    "\t4a. Iterate through each shuffle and obtain the shuffled templates like `long_pf_peak_ranks[epoch_specific_shuffled_indicies]`, `short_pf_peak_ranks[epoch_specific_shuffled_indicies]`\n",
    "\t4b. compute the spearman rank-order of the event and each shuffled template, and accumulate the results in `long_spearmanr_rank_stats_results`, `short_spearmanr_rank_stats_results`\n",
    "\n",
    "5. After we're done with the shuffle loop, accumulate the results and convert to the right output format.\n",
    "\n",
    "6. When all epochs are done, loop through the results (the epochs again) and compute the z-scores for each epoch so they can be compared to each other. Keep track of the means and std_dev for comparisons later, and subtract the two sets of z-scores (long/short) to get the delta_Z for each template.\n",
    "\n",
    "7. TODO: Next figure out what to do with the array of z-scores and delta_Z. We have:\n",
    "\tn_epochs sets of results\n",
    "\t\tn_shuffles scores of delta_Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd9d61",
   "metadata": {},
   "source": [
    "## Convo with Kamran 2023-10-23:\n",
    "- Use directional templates **\n",
    "- No need to worry about re-ranking\n",
    "[X] Plot the long and short separately in addition to the difference, so we show significant reqplay on each as a sanity check\n",
    "[X] Absolute value difference?\n",
    "[X] Fisher transform the correlation values (check if there is a difference) because correlation coefficients aren't going to be normally distributed.\n",
    "\t[ ] Then Z-score releative to fisher.\n",
    "\n",
    "- T-test to compare to mean of zero (if looking at the difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ffd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concerns:\n",
    "# 1. Permutation recommended over shuffling for small numbers of ids\n",
    "# 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nptyping import NDArray\n",
    "from attrs import define, field, Factory, astuple\n",
    "import scipy.stats\n",
    "from scipy import ndimage\n",
    "from neuropy.utils.misc import build_shuffled_ids # used in _SHELL_analyze_leave_one_out_decoding_results\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import pho_stats_paired_t_test\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import compute_shuffled_rankorder_analyses, build_track_templates_for_shuffle, compute_shuffled_rankorder_analyses\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TrackTemplates, RankOrderAnalyses, ShuffleHelper, Zscorer\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import build_pyqtgraph_epoch_indicator_regions\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.LongShortTrackComparingDisplayFunctions import _helper_add_long_short_session_indicator_regions\n",
    "from pyphocorehelpers.DataStructure.general_parameter_containers import VisualizationParameters, RenderPlotsData, RenderPlots # PyqtgraphRenderPlots\n",
    "from pyphocorehelpers.gui.PhoUIContainer import PhoUIContainer\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.PyqtgraphRenderPlots import PyqtgraphRenderPlots\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderDebugger import GenericPyQtGraphContainer\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderDebugger import GenericPyQtGraphScatterClicker, RankOrderDebugger\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper, PhoDockAreaContainingWindow\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer, RankOrderResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17402af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover from the saved global result:\n",
    "directional_laps_results = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "long_odd_shared_aclus_only_one_step_decoder_1D, long_even_shared_aclus_only_one_step_decoder_1D, short_odd_shared_aclus_only_one_step_decoder_1D, short_even_shared_aclus_only_one_step_decoder_1D = [directional_laps_results.__dict__[k] for k in ['long_odd_shared_aclus_only_one_step_decoder_1D', 'long_even_shared_aclus_only_one_step_decoder_1D', 'short_odd_shared_aclus_only_one_step_decoder_1D', 'short_even_shared_aclus_only_one_step_decoder_1D']]\n",
    "track_templates: TrackTemplates = TrackTemplates.init_from_paired_decoders(LR_decoder_pair=(long_odd_shared_aclus_only_one_step_decoder_1D, short_odd_shared_aclus_only_one_step_decoder_1D), RL_decoder_pair=(long_even_shared_aclus_only_one_step_decoder_1D, short_even_shared_aclus_only_one_step_decoder_1D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name]['computation_config'].pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name]['computation_config'] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs: curr_active_pipeline, track_templates, global_replays, owning_pipeline_reference\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "# global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].spikes_df)\n",
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps) # .trimmed_to_non_overlapping()\n",
    "global_laps_epochs_df = global_laps.to_dataframe()\n",
    "active_epochs_df = global_laps_epochs_df.copy()\n",
    "\n",
    "rod_display = RankOrderDebugger(global_spikes_df, active_epochs_df, track_templates, RL_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "rod_display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131a972",
   "metadata": {},
   "source": [
    "# ❇️🆕 READY/NEXT: 2023-11-10 - All directional pf1D works for merging all four 1D templates!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "# Use the four epochs to make to a pseudo-y:\n",
    "all_directional_decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "all_directional_pf1D = PfND.build_merged_directional_placefields(deepcopy(long_LR_pf1D), deepcopy(long_RL_pf1D), deepcopy(short_LR_pf1D), deepcopy(short_RL_pf1D), debug_print=False)\n",
    "all_directional_pf1D_Decoder = BasePositionDecoder(all_directional_pf1D, setup_on_init=True, post_load_on_init=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Combine the non-directional PDFs and renormalize to get the directional PDF:\n",
    "# Inputs: long_LR_pf1D, long_RL_pf1D\n",
    "all_directional_decoder_names = ['long_LR', 'long_RL']\n",
    "long_directional_pf1D = PfND.build_merged_directional_placefields(deepcopy(long_LR_pf1D), deepcopy(long_RL_pf1D), debug_print=False)\n",
    "long_directional_pf1D_Decoder = BasePositionDecoder(long_directional_pf1D, setup_on_init=True, post_load_on_init=True, debug_print=False)\n",
    "# takes 6.3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_decoder_names = ['short_LR', 'short_RL']\n",
    "short_directional_pf1D = PfND.build_merged_directional_placefields(deepcopy(short_LR_pf1D), deepcopy(short_RL_pf1D), debug_print=False)\n",
    "short_directional_pf1D_Decoder = BasePositionDecoder(short_directional_pf1D, setup_on_init=True, post_load_on_init=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_directional_pf1D_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c649977",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_pf1D_Decoder.ratemap.dims_coord_tuple \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc66666",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_pf1D_Decoder.ratemap.ndim\n",
    "all_directional_pf1D_Decoder.ratemap.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f959b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_pf1D_Decoder.pf.occupancy #.shape: (56, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should easily be able to plot the placefields, the occupancy:\n",
    "\n",
    "all_directional_pf1D_Decoder.ratemap\n",
    "\n",
    "np.print_options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper, PhoDockAreaContainingWindow\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum, LongShortDisplayConfigManager\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.EpochsEditorItem import EpochsEditor # perform_plot_laps_diagnoser\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.dockarea.Dock import Dock, DockDisplayConfig\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TrackTemplates # _display_directional_laps_overview\n",
    "\n",
    "# uses `global_session`\n",
    "epochs_editor = EpochsEditor.init_from_session(global_session, include_velocity=False, include_accel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea764a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization:\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap, visualize_heatmap_pyqtgraph\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow, LayoutScrollability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode using long_directional_decoder\n",
    "global_spikes_df, (odd_shuffle_helper, even_shuffle_helper) = RankOrderAnalyses.common_analysis_helper(curr_active_pipeline=curr_active_pipeline, num_shuffles=1000)\n",
    "spikes_df = deepcopy(global_spikes_df) #.spikes.sliced_by_neuron_id(track_templates.shared_aclus_only_neuron_IDs)\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# long_directional_decoding_result: DecodedFilterEpochsResult = long_directional_pf1D_Decoder.decode_specific_epochs(spikes_df, global_replays, decoding_time_bin_size=0.01)\n",
    "all_directional_decoding_result: DecodedFilterEpochsResult = all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df, global_replays, decoding_time_bin_size=0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8702b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_directional_decoding_result.marginal_y_list.p_x_given_n_list\n",
    "list(all_directional_decoding_result.marginal_y_list[0].keys()) # ['p_x_given_n', 'most_likely_positions_1D']\n",
    "\n",
    "\n",
    "_out_marginal_y_p_x_given_n = [all_directional_decoding_result.marginal_y_list[i]['p_x_given_n'] for i in np.arange(len(all_directional_decoding_result.marginal_y_list))]\n",
    "_out_marginal_y_p_x_given_n[0].shape # (4, 26)\n",
    "\n",
    "\n",
    "_out_marginal_y_p_x_given_n_arr = np.stack(_out_marginal_y_p_x_given_n, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_decoding_result.p_x_given_n_list\n",
    "\n",
    "def find_marginal_best_decoder_direction(active_decoding_result):\n",
    "\t\"\"\"marginalize over the direction to find which direction is most likely\"\"\"\n",
    "\ttime_bins = active_decoding_result.time_bin_containers[epoch_idx]\n",
    "\t# len(time_bins.centers)\n",
    "\t# time_bins.num_bins\n",
    "\t#.shape # (2, n_bins)\n",
    "\t# sum across timebins to get total likelihood for each of the two directions\n",
    "\tlong_relative_direction_likelihoods = np.vstack([(np.sum(active_decoding_result.marginal_y_list[epoch_idx].p_x_given_n, axis=1)/active_decoding_result.time_bin_containers[epoch_idx].num_bins) for epoch_idx in np.arange(active_decoding_result.num_filter_epochs)]) # should get 2 values\n",
    "\tdisplay(long_relative_direction_likelihoods.shape) # (n_epochs, 2)\n",
    "\tlong_relative_direction_likelihoods\n",
    "\t# np.all(np.sum(long_relative_direction_likelihoods, axis=1) == 1)\n",
    "\t# np.sum(long_relative_direction_likelihoods, axis=1) # not sure why some NaN values are getting in there\n",
    "\tis_good_epoch = np.isfinite(np.sum(long_relative_direction_likelihoods, axis=1))\n",
    "\tmost_likely_template_index = long_relative_direction_likelihoods.argmax(axis=-1) # one per epoch, indicates which template is the most likely given the data from all timebins within that epoch\n",
    "\treturn most_likely_template_index\n",
    "\n",
    "\n",
    "\n",
    "long_relative_direction_likelihoods = find_marginal_best_decoder_direction(long_directional_decoding_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginalize over the direction to find which direction is most likely\n",
    "epoch_idx = 0\n",
    "\n",
    "# long_directional_marginalized\n",
    "active_decoding_result.marginal_y_list[epoch_idx].p_x_given_n\n",
    "\n",
    "# np.sum(long_directional_decoding_result.marginal_y_list[epoch_idx].p_x_given_n, axis=0)\n",
    "\n",
    "time_bins = active_decoding_result.time_bin_containers[epoch_idx]\n",
    "len(time_bins.centers)\n",
    "time_bins.num_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " #.shape # (2, n_bins)\n",
    "\n",
    "# sum across timebins to get total likelihood for each of the two directions\n",
    "long_relative_direction_likelihoods = np.vstack([(np.sum(active_decoding_result.marginal_y_list[epoch_idx].p_x_given_n, axis=1)/active_decoding_result.time_bin_containers[epoch_idx].num_bins) for epoch_idx in np.arange(active_decoding_result.num_filter_epochs)]) # should get 2 values\n",
    "display(long_relative_direction_likelihoods.shape) # (n_epochs, 2)\n",
    "long_relative_direction_likelihoods\n",
    "\n",
    "# np.all(np.sum(long_relative_direction_likelihoods, axis=1) == 1)\n",
    "# np.sum(long_relative_direction_likelihoods, axis=1) # not sure why some NaN values are getting in there\n",
    "\n",
    "is_good_epoch = np.isfinite(np.sum(long_relative_direction_likelihoods, axis=1))\n",
    "is_good_epoch\n",
    "most_likely_template_index = long_relative_direction_likelihoods.argmax(axis=-1) # one per epoch, indicates which template is the most likely given the data from all timebins within that epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = BasicBinnedImageRenderingWindow(long_relative_direction_likelihoods, None, None, name='long_relative_direction_likelihoods', title=\"Directional Decoder Prob for Replay Epochs\", variable_label='P_x_given_n', scrollability_mode=LayoutScrollability.NON_SCROLLABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_relative_direction_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_relative_direction_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsWindow()\n",
    "win.setWindowTitle('DOTS')\n",
    "# pg.plot(long_relative_direction_likelihoods.T[0], pen=None, symbol='o', symbolPen=None, symbolSize=4, symbolBrush=('b'), name='LR', win=win)\n",
    "# pg.plot(long_relative_direction_likelihoods.T[1], pen=None, symbol='o', symbolPen=None, symbolSize=4, symbolBrush=('r'), name='RL', win=win)\n",
    "p1 = win.addPlot() \n",
    "curve1 = p1.plot()\n",
    "curve2 = p1.plot()\n",
    "\n",
    "#Blue Dots\n",
    "curve1.setData(long_relative_direction_likelihoods.T[0], pen=None, symbol='o', symbolPen=None, symbolSize=4, symbolBrush=('b'), name='LR')\n",
    "#Red Dots\n",
    "curve2.setData(long_relative_direction_likelihoods.T[1], pen=None, symbol='o', symbolPen=None, symbolSize=4, symbolBrush=('r'), name='RL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blue Dots\n",
    "curve1.setData(long_relative_direction_likelihoods.T[0], pen=None, symbol='o', symbolPen=None, symbolSize=4, symbolBrush=('b'), name='LR')\n",
    "#Red Dots\n",
    "curve2.setData(long_relative_direction_likelihoods.T[1], pen=None, symbol='o', symbolPen=None, symbolSize=4, symbolBrush=('r'), name='RL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2f309",
   "metadata": {},
   "source": [
    "# PhoKamran2023Paper Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d813d",
   "metadata": {},
   "source": [
    "## Figure 1) pf1D Ratemaps, Active set, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273696fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_1_add_replay_epoch_rasters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.reload_default_display_functions()\n",
    "\n",
    "pf1d_compare_graphics, (example_epoch_rasters_L, example_epoch_rasters_S), example_stacked_epoch_graphics, fig_1c_figures_out_dict = PAPER_FIGURE_figure_1_full(curr_active_pipeline) # did not display the pf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = jonathan_firing_rate_analysis_result.rdf.rdf\n",
    "rdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba3c7a",
   "metadata": {},
   "source": [
    "## Figure 2) `PaperFigureTwo`: LxC/SxC Analyses\n",
    "Note: this fails when SxC or LxC are empty for this session (as it's not meaningful to produce a comparison bar plot). In this case, aggregate across multiple sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PaperFigureTwo\n",
    "\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.003) # 3ms\n",
    "_out_fig_2.compute(curr_active_pipeline=curr_active_pipeline)\n",
    "_out_fig_2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a52142",
   "metadata": {},
   "source": [
    "## Figure 3) `PAPER_FIGURE_figure_3`: Firing Rate Index and Long/Short Firing Rate Replays v. Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f765ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.LongShortTrackComparingDisplayFunctions import _plot_long_short_firing_rate_indicies\n",
    "# curr_active_pipeline.reload_default_display_functions()\n",
    "\n",
    "_out, _out2 = PAPER_FIGURE_figure_3(curr_active_pipeline, defer_render=False, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0d9b8",
   "metadata": {},
   "source": [
    "##  🖼️ Plot a debug view for directional laps. Shows the laps, the pf1Ds, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ececec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper, PhoDockAreaContainingWindow\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum, LongShortDisplayConfigManager\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.EpochsEditorItem import EpochsEditor # perform_plot_laps_diagnoser\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.dockarea.Dock import Dock, DockDisplayConfig\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig\n",
    "\n",
    "epochs_editor = EpochsEditor.init_from_session(global_session, include_velocity=True, include_accel=False)\n",
    "root_dockAreaWindow, app = DockAreaWrapper.wrap_with_dockAreaWindow(epochs_editor.plots.win, None, title='Pho Directional Laps Templates')\n",
    "\n",
    "# track_templates.long_LR_decoder.pf\n",
    "\n",
    "def _get_decoder_sorted_pfs(a_decoder):\n",
    "\tratemap = a_decoder.pf.ratemap\n",
    "\tCoM_sort_indicies = np.argsort(ratemap.peak_tuning_curve_center_of_masses) # get the indicies to sort the placefields by their center-of-mass (CoM) location\n",
    "\t# CoM_sort_indicies.shape # (n_neurons,)\n",
    "\treturn ratemap.pdf_normalized_tuning_curves[CoM_sort_indicies, :]\n",
    "\n",
    "decoders_dict = {'long_LR': track_templates.long_LR_decoder,\n",
    "\t'long_RL': track_templates.long_RL_decoder,\n",
    "\t'short_LR': track_templates.short_LR_decoder,\n",
    "\t'short_RL': track_templates.short_RL_decoder,\n",
    "}\n",
    "\n",
    "## Plot the placefield 1Ds as heatmaps and then wrap them in docks and add them to the window:\n",
    "_out_pf1D_heatmaps = {}\n",
    "for a_decoder_name, a_decoder in decoders_dict.items():\n",
    "\t_out_pf1D_heatmaps[a_decoder_name] = visualize_heatmap_pyqtgraph(_get_decoder_sorted_pfs(a_decoder), title=f'{a_decoder_name}_pf1Ds', show_value_labels=False, show_xticks=False, show_yticks=False, show_colorbar=False, win=None, defer_show=True)\n",
    "\n",
    "even_dock_config = CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_even_dock_colors)\n",
    "odd_dock_config = CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_odd_dock_colors)\n",
    "\n",
    "\n",
    "_out_dock_widgets = {}\n",
    "dock_configs = (even_dock_config, odd_dock_config, even_dock_config, odd_dock_config)\n",
    "dock_add_locations = (['left'], ['left'], ['right'], ['right'])\n",
    "\n",
    "for i, (a_decoder_name, a_heatmap) in enumerate(_out_pf1D_heatmaps.items()):\n",
    "\t_out_dock_widgets[a_decoder_name] = root_dockAreaWindow.add_display_dock(identifier=a_decoder_name, widget=a_heatmap[0], dockSize=(300,200), dockAddLocationOpts=dock_add_locations[i], display_config=dock_configs[i])\n",
    "\n",
    "\n",
    "# Outputs: root_dockAreaWindow, app, epochs_editor, _out_pf1D_heatmaps, _out_dock_widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774ec4c",
   "metadata": {},
   "source": [
    "# 🟢🖼️ 2023-10-27 - Example validation of directional shuffles with multi-raster plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.DirectionalTemplatesRastersDebugger import _debug_plot_directional_template_rasters\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.DirectionalTemplatesRastersDebugger import build_selected_spikes_df\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.GraphicsWidgets.DirectionalTemplatesRastersDebugger import add_selected_spikes_df_points_to_scatter_plot\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import DockAreaWrapper\n",
    "\n",
    "from pyphocorehelpers.print_helpers import generate_html_string # used for `plot_long_short_surprise_difference_plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5664c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs: curr_active_pipeline, track_templates, global_replays, owning_pipeline_reference\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "# global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].spikes_df)\n",
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps) # .trimmed_to_non_overlapping()\n",
    "global_laps_epochs_df = global_laps.to_dataframe()\n",
    "active_epochs_df = global_laps_epochs_df.copy()\n",
    "\n",
    "odd_display_outputs, even_display_outputs = _debug_plot_directional_template_rasters(global_spikes_df, active_epochs_df, track_templates)\n",
    "odd_app, odd_win, odd_plots, odd_plots_data, odd_on_update_active_epoch, odd_on_update_active_scatterplot_kwargs = odd_display_outputs\n",
    "even_app, even_win, even_plots, even_plots_data, even_on_update_active_epoch, even_on_update_active_scatterplot_kwargs = even_display_outputs\n",
    "# Build the wrapping window using `DockAreaWrapper`:\n",
    "active_root_main_widget = odd_win.window()\n",
    "root_dockAreaWindow, app = DockAreaWrapper.wrap_with_dockAreaWindow(active_root_main_widget, even_win, title='Pho Rank-Order Epochs Debugger')\n",
    "\n",
    "\n",
    "def on_update_active_epoch(an_epoch_idx, an_epoch):\n",
    "    \"\"\" captures: odd_on_update_active_epoch, even_on_update_active_epoch, root_dockAreaWindow \"\"\"\n",
    "    odd_on_update_active_epoch(an_epoch_idx, an_epoch=an_epoch)\n",
    "    even_on_update_active_epoch(an_epoch_idx, an_epoch=an_epoch)\n",
    "    root_dockAreaWindow.setWindowTitle(f'Pho Rank-Order Epochs Debugger: Epoch[{an_epoch_idx}]')\n",
    "\n",
    "def on_update_epoch_IDX(an_epoch_idx):\n",
    "    \"\"\" captures on_update_active_epoch, active_epochs_df to extract the epoch time range and call `on_update_active_epoch` \"\"\"\n",
    "    # curr_epoch_spikes = spikes_df[(spikes_df.new_lap_IDX == an_epoch_idx)]\n",
    "    curr_epoch_df = active_epochs_df[(active_epochs_df.lap_id == (an_epoch_idx+1))]\n",
    "    curr_epoch = list(curr_epoch_df.itertuples())[0]\n",
    "\n",
    "    on_update_active_epoch(an_epoch_idx, curr_epoch)\n",
    "\n",
    "## Build the selected spikes df:\n",
    "\n",
    "## Laps: RL_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict\n",
    "## Ripples: RL_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict\n",
    "\n",
    "(even_selected_spike_df, even_neuron_id_to_new_IDX_map), (odd_selected_spike_df, odd_neuron_id_to_new_IDX_map) = build_selected_spikes_df(track_templates, active_epochs_df, RL_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "\n",
    "## Add the spikes\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=odd_plots_data, plots=odd_plots, selected_spikes_df=deepcopy(odd_selected_spike_df), _active_plot_identifier = 'long_odd')\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=odd_plots_data, plots=odd_plots, selected_spikes_df=deepcopy(odd_selected_spike_df), _active_plot_identifier = 'short_odd')\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=even_plots_data, plots=even_plots, selected_spikes_df=deepcopy(even_selected_spike_df), _active_plot_identifier = 'long_even')\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=even_plots_data, plots=even_plots, selected_spikes_df=deepcopy(even_selected_spike_df), _active_plot_identifier = 'short_even')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89205ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_display_outputs\n",
    "\n",
    "print(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62166f04",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "all_plots = odd_plots + even_plots # TypeError: unsupported operand type(s) for +: 'RenderPlots' and 'RenderPlots'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c7ecb",
   "metadata": {},
   "source": [
    "### Ripples Debugger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs: curr_active_pipeline, track_templates, global_replays, owning_pipeline_reference\n",
    "\n",
    "def _get_shared_data(epoch_idx: int):\n",
    "    # i_str = generate_html_string('i', color='white', bold=True)\n",
    "    # j_str = generate_html_string('j', color='red', bold=True)\n",
    "    # title_str = generate_html_string(f'JSD(p_x_given_n, pf[{i_str}]) - JSD(p_x_given_n, pf[{j_str}]) where {j_str} non-firing')\n",
    "\treturn [generate_html_string(str(round(arr[epoch_idx], ndigits=3)), color='white', bold=True) for arr in (LR_ripple_evts_long_z_score_values, RL_ripple_evts_long_z_score_values, LR_ripple_evts_short_z_score_values, RL_ripple_evts_short_z_score_values)]\n",
    "\n",
    "\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].spikes_df)\n",
    "\n",
    "global_ripples_epochs_df = global_replays.to_dataframe()\n",
    "active_epochs_df = global_ripples_epochs_df.copy()\n",
    "\n",
    "odd_display_outputs, even_display_outputs = _debug_plot_directional_template_rasters(global_spikes_df, active_epochs_df, track_templates)\n",
    "odd_app, odd_win, odd_plots, odd_plots_data, odd_on_update_active_epoch, odd_on_update_active_scatterplot_kwargs = odd_display_outputs\n",
    "even_app, even_win, even_plots, even_plots_data, even_on_update_active_epoch, even_on_update_active_scatterplot_kwargs = even_display_outputs\n",
    "\n",
    "# Build the wrapping window using `DockAreaWrapper`:\n",
    "active_root_main_widget = odd_win.window()\n",
    "root_dockAreaWindow, app = DockAreaWrapper.wrap_with_dockAreaWindow(active_root_main_widget, even_win, title='Pho Rank-Order Epochs Debugger')\n",
    "\n",
    "def on_update_active_epoch(an_epoch_idx, an_epoch):\n",
    "    \"\"\" captures: odd_on_update_active_epoch, even_on_update_active_epoch, root_dockAreaWindow \"\"\"\n",
    "    odd_on_update_active_epoch(an_epoch_idx, an_epoch=an_epoch)\n",
    "    even_on_update_active_epoch(an_epoch_idx, an_epoch=an_epoch)\n",
    "    root_dockAreaWindow.setWindowTitle(f'Pho Rank-Order Epochs Debugger: Epoch[{an_epoch_idx}]')\n",
    "\n",
    "\n",
    "def on_update_epoch_IDX(an_epoch_idx):\n",
    "    \"\"\" captures on_update_active_epoch, active_epochs_df to extract the epoch time range and call `on_update_active_epoch`, _get_shared_data, odd_plots, even_plots \"\"\"\n",
    "    # curr_epoch_spikes = spikes_df[(spikes_df.new_lap_IDX == an_epoch_idx)]\n",
    "    # curr_epoch_df = active_epochs_df[(active_epochs_df.lap_id == (an_epoch_idx+1))]\n",
    "    # curr_epoch_df = active_epochs_df[(active_epochs_df.label.astype(float) == float(an_epoch_idx))]\n",
    "    # curr_epoch_df = active_epochs_df[(active_epochs_df.index.astype(float) == float(an_epoch_idx))]\n",
    "    curr_epoch_df = active_epochs_df[active_epochs_df.index == active_epochs_df.index.to_numpy()[an_epoch_idx]]\n",
    "    curr_epoch = list(curr_epoch_df.itertuples())[0] # Pandas(Index=233, start=799.9181619619485, stop=800.1395608885214, label='235', duration=0.2213989265728742)\n",
    "    \n",
    "    ## Add values as labels:\n",
    "    curr_epoch_raw_values = _get_shared_data(an_epoch_idx)\n",
    "    long_odd_val, long_even_val, short_odd_val, short_even_val = curr_epoch_raw_values\n",
    "    odd_plots['ax']['long_odd'].getAxis('left').setLabel(f'long_odd: {long_odd_val}')\n",
    "    odd_plots['ax']['short_odd'].getAxis('left').setLabel(f'short_odd: {short_odd_val}')\n",
    "    even_plots['ax']['long_even'].getAxis('left').setLabel(f'long_even: {long_even_val}')\n",
    "    even_plots['ax']['short_even'].getAxis('left').setLabel(f'short_even: {short_even_val}')\n",
    "    \n",
    "    # for ax_name in ('long_odd', 'short_odd'):\n",
    "    # \todd_plots['ax'][ax_name].getAxis('left').setLabel(f'{ax_name}: {}')\n",
    "\n",
    "    on_update_active_epoch(an_epoch_idx, curr_epoch)\n",
    "\n",
    "## Build the selected spikes df:\n",
    "\n",
    "## Laps: RL_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict\n",
    "## Ripples: RL_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict\n",
    "\n",
    "(even_selected_spike_df, even_neuron_id_to_new_IDX_map), (odd_selected_spike_df, odd_neuron_id_to_new_IDX_map) = build_selected_spikes_df(track_templates, active_epochs_df, RL_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "\n",
    "## Add the spikes\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=odd_plots_data, plots=odd_plots, selected_spikes_df=deepcopy(odd_selected_spike_df), _active_plot_identifier = 'long_odd')\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=odd_plots_data, plots=odd_plots, selected_spikes_df=deepcopy(odd_selected_spike_df), _active_plot_identifier = 'short_odd')\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=even_plots_data, plots=even_plots, selected_spikes_df=deepcopy(even_selected_spike_df), _active_plot_identifier = 'long_even')\n",
    "add_selected_spikes_df_points_to_scatter_plot(plots_data=even_plots_data, plots=even_plots, selected_spikes_df=deepcopy(even_selected_spike_df), _active_plot_identifier = 'short_even')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_idx = 299\n",
    "on_update_epoch_IDX(epoch_idx)\n",
    "# programmatically_select_epoch(epoch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c11b66",
   "metadata": {},
   "source": [
    "## 🟢 Ripple Rank-Order Z-Score Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "## Show Ripple Helper:\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch = curr_active_pipeline.filtered_epochs[long_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "\n",
    "global_spikes_df, (odd_shuffle_helper, even_shuffle_helper) = RankOrderAnalyses.common_analysis_helper(curr_active_pipeline=curr_active_pipeline, num_shuffles=1000)\n",
    "spikes_df = deepcopy(global_spikes_df) #.spikes.sliced_by_neuron_id(track_templates.shared_aclus_only_neuron_IDs)\n",
    "global_replays = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay)\n",
    "if isinstance(global_replays, pd.DataFrame):\n",
    "\tglobal_replays = Epoch(global_replays.epochs.get_valid_df())\n",
    "\n",
    "# epoch_identifiers = global_replays.to_dataframe().index.astype(int) # 0, ... max\n",
    "epoch_identifiers = np.arange(global_replays.n_epochs) # 0, ... max\n",
    "\n",
    "# x_values = global_replays.labels.astype(float)\n",
    "# x_axis_name_suffix='Index'\n",
    "x_values = global_replays.midtimes\n",
    "x_axis_name_suffix='Mid-time (Sec)'\n",
    "_display_replay_z_score_diff_outputs = RankOrderAnalyses._perform_plot_z_score_diff(x_values, RL_ripple_evts_long_short_z_score_diff_values, LR_ripple_evts_long_short_z_score_diff_values, variable_name='Ripple', x_axis_name_suffix=x_axis_name_suffix, point_data_values=epoch_identifiers)\n",
    "_display_replay_z_score_raw_outputs = RankOrderAnalyses._perform_plot_z_score_raw(x_values, LR_ripple_evts_long_z_score_values, LR_ripple_evts_short_z_score_values, RL_ripple_evts_long_z_score_values, RL_ripple_evts_short_z_score_values, variable_name='Ripple', x_axis_name_suffix=x_axis_name_suffix, point_data_values=epoch_identifiers)\n",
    "\n",
    "ripple_app, ripple_win, ripple_diff_p1, (ripple_even_out_plot_1D, ripple_odd_out_plot_1D) = _display_replay_z_score_diff_outputs # unwrap\n",
    "long_epoch_indicator_region_items, short_epoch_indicator_region_items = _helper_add_long_short_session_indicator_regions(ripple_diff_p1, long_epoch, short_epoch) # add long/short epoch indicator regions\n",
    "ripple_raw_app, ripple_raw_win, ripple_raw_p1, (ripple_long_even_out_plot_1D, ripple_long_odd_out_plot_1D, ripple_short_even_out_plot_1D, ripple_short_odd_out_plot_1D) = _display_replay_z_score_raw_outputs\n",
    "long_epoch_indicator_region_items, short_epoch_indicator_region_items = _helper_add_long_short_session_indicator_regions(ripple_raw_p1, long_epoch, short_epoch) # add long/short epoch indicator regions\n",
    "\n",
    "active_connections_dict = {} # for holding connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56488a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(odd_ripple_evts_long_z_score_values, even_ripple_evts_long_z_score_values, odd_ripple_evts_short_z_score_values, even_ripple_evts_short_z_score_values)\n",
    "LR_laps_epoch_ranked_aclus_stats_dict, LR_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_laps_long_z_score_values, LR_laps_short_z_score_values, LR_laps_long_short_z_score_diff_values = rank_order_results.odd_laps # LR_laps\n",
    "RL_laps_epoch_ranked_aclus_stats_dict, RL_laps_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, RL_laps_long_z_score_values, RL_laps_short_z_score_values, RL_laps_long_short_z_score_diff_values = rank_order_results.even_laps # RL_laps\n",
    "\n",
    "LR_ripple_evts_epoch_ranked_aclus_stats_dict, LR_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_ripple_evts_long_z_score_values, LR_ripple_evts_short_z_score_values, LR_ripple_evts_long_short_z_score_diff_values = rank_order_results.odd_ripple # LR_ripple\n",
    "RL_ripple_evts_epoch_ranked_aclus_stats_dict, RL_ripple_evts_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, RL_ripple_evts_long_z_score_values, RL_ripple_evts_short_z_score_values, RL_ripple_evts_long_short_z_score_diff_values = rank_order_results.even_ripple # RL_ripple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lastClicked = []\n",
    "def _test_scatter_plot_clicked(plot, evt):\n",
    "\t\"\"\" captures `lastClicked` , `x_values`, `on_update_epoch_IDX`\n",
    "\tplot: <pyphoplacecellanalysis.External.pyqtgraph.graphicsItems.PlotDataItem.PlotDataItem object at 0x0000023C7D74C8B0>\n",
    "\tclicked points <MouseClickEvent (78.6115,-2.04825) button=1>\n",
    "\n",
    "\t\"\"\"\n",
    "\tglobal lastClicked  # Declare lastClicked as a global variable\n",
    "\t# for p in lastClicked:\n",
    "\t# \tp.resetPen()\n",
    "\tprint(f'plot: {plot}') # plot: <pyphoplacecellanalysis.External.pyqtgraph.graphicsItems.PlotDataItem.PlotDataItem object at 0x0000023C7D74C8B0>\n",
    "\tprint(f'\\tevt: {evt}')\t\n",
    "\tprint(\"clicked points\", evt.pos()) # clicked points <MouseClickEvent (48.2713,1.32425) button=1>\n",
    "\t# print(f'args: {args}')\n",
    "\tpt_x, pt_y = evt.pos()\n",
    "\tprint(f'\\tpt_x: {pt_x}')\n",
    "\tnearest_epoch_idx = find_nearest_idx(x_values, pt_x)\n",
    "\tprint(f'\\tnearest_epoch_idx: {nearest_epoch_idx}')\n",
    "\t\n",
    "\t# idx_x = int(round(pt_x))\n",
    "\t# print(f'\\tidx_x: {idx_x}')\n",
    "\tpts = plot.pointsAt(evt.pos())\n",
    "\tprint(f'pts: {pts}')\n",
    "\t# for p in points:\n",
    "\t# \tp.setPen(clickedPen)\n",
    "\n",
    "\ton_update_epoch_IDX(nearest_epoch_idx)\n",
    "\n",
    "\tlastClicked = [idx_x]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "sigPointsClicked(self, points, ev)  Emitted when a plot point is clicked. Sends the list of points under the mouse.\n",
    "sigPointsHovered(self, points, ev)  Emitted when a plot point is hovered over. Sends the list of points under the mouse.\n",
    "\n",
    "\"\"\"\n",
    "def _test_scatter_plot_points_clicked(plot, points, evt):\n",
    "\tprint(f'_test_scatter_plot_points_clicked(...): \\n\\tplot: {plot}') # plot: <pyphoplacecellanalysis.External.pyqtgraph.graphicsItems.PlotDataItem.PlotDataItem object at 0x0000023C7D74C8B0>\n",
    "\tprint(f'\\tpoints: {points}')\t\n",
    "\tprint(f'\\tevt: {evt}')\t\n",
    "\n",
    "def _test_scatter_plot_points_hovered(plot, points, evt):\n",
    "\tprint(f'_test_scatter_plot_points_hovered(...): \\n\\tplot: {plot}') # plot: <pyphoplacecellanalysis.External.pyqtgraph.graphicsItems.PlotDataItem.PlotDataItem object at 0x0000023C7D74C8B0>\n",
    "\tprint(f'\\tpoints: {points}')\t\n",
    "\tprint(f'\\tevt: {evt}')\t\n",
    "\n",
    "\n",
    "\n",
    "for a_plot in _display_replay_z_score_raw_outputs[3]:\n",
    "\tif a_plot in active_connections_dict:\n",
    "\t\t# remove existing first\n",
    "\t\tprint(f'removing existing connection for {a_plot}')\n",
    "\t\ta_plot.sigClicked.disconnect(active_connections_dict[a_plot])\n",
    "\t\tdel active_connections_dict[a_plot]\n",
    "\n",
    "\t# main_scatter_clicked_connection = a_plot.sigClicked.connect(_test_scatter_plot_clicked)\n",
    "\t# main_scatter_clicked_connection = a_plot.sigPointsClicked.connect(_test_scatter_plot_points_clicked)\n",
    "\tmain_scatter_clicked_connection = a_plot.sigPointsHovered.connect(_test_scatter_plot_points_hovered)\n",
    "\n",
    "\tactive_connections_dict[a_plot] = main_scatter_clicked_connection\n",
    "\n",
    "for a_plot in _display_replay_z_score_diff_outputs[3]:\n",
    "\tif a_plot in active_connections_dict:\n",
    "\t\t# remove existing first\n",
    "\t\tprint(f'removing existing connection for {a_plot}')\n",
    "\t\ta_plot.sigClicked.disconnect(active_connections_dict[a_plot])\n",
    "\t\tdel active_connections_dict[a_plot]\n",
    "\t# main_scatter_clicked_connection = a_plot.sigClicked.connect(_test_scatter_plot_clicked)\n",
    "\t# main_scatter_clicked_connection = a_plot.sigPointsClicked.connect(_test_scatter_plot_points_clicked)\n",
    "\tmain_scatter_clicked_connection = a_plot.sigPointsHovered.connect(_test_scatter_plot_points_hovered)\n",
    "\n",
    "\tactive_connections_dict[a_plot] = main_scatter_clicked_connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50045836",
   "metadata": {},
   "outputs": [],
   "source": [
    "even_out_plot_1D, odd_out_plot_1D = _display_replay_z_score_diff_outputs[3] # pyphoplacecellanalysis.External.pyqtgraph.graphicsItems.PlotDataItem.PlotDataItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even_out_plot_1D.points\n",
    "\n",
    "def programmatically_select_epoch(epoch_idx: int):\n",
    "\t\"\"\" captures: _display_replay_z_score_diff_outputs, _display_replay_z_score_raw_outputs, \"\"\"\n",
    "\n",
    "\tdefaultPen = None\n",
    "\thoverPen = pg.mkPen('w', width=2)\n",
    "\thoverBrush = pg.mkBrush('w')\n",
    "\n",
    "\tfor a_plot in _display_replay_z_score_diff_outputs[3]:\n",
    "\t\ta_scatter = a_plot.scatter # ScatterPlotItem\n",
    "\t\ta_scatter.setPen(defaultPen) # reset all points in scatter to default (TODO: efficiency)\n",
    "\t\t# Set the selected point's pen:\n",
    "\t\tpts = a_scatter.points()\n",
    "\t\t# pts.shape # (626,)\n",
    "\t\tpts[epoch_idx].setPen(hoverPen)\n",
    "\n",
    "\tfor a_plot in _display_replay_z_score_raw_outputs[3]:\n",
    "\t\ta_scatter = a_plot.scatter # ScatterPlotItem\n",
    "\t\ta_scatter.setPen(defaultPen) # reset all points in scatter to default (TODO: efficiency)\n",
    "\t\t# Set the selected point's pen:\n",
    "\t\tpts = a_scatter.points()\n",
    "\t\t# pts.shape # (626,)\n",
    "\t\tpts[epoch_idx].setPen(hoverPen)\n",
    "\n",
    "\n",
    "programmatically_select_epoch(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
