{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "# Renaming to `BatchInteractiveProcessing_*.ipynb` from `BatchGenerateOutputs_*.ipynb` because it makes more sense\n",
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "# %gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.exception_helpers import CapturedException\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionTables, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult, BatchSessionCompletionHandler\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_file_metadata\n",
    "from pyphocorehelpers.Filesystem.path_helpers import discover_data_files, generate_copydict, copy_movedict, copy_file, save_copydict_to_text_file, read_copydict_from_text_file, invert_filedict\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import get_file_str_if_file_exists\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import check_output_h5_files, copy_files_in_filelist_to_dest\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import ConcreteSessionFolder, BackupMethods\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots, BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import build_merged_neuron_firing_rate_indicies\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsHelpers\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalLapsHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "# BATCH_DATE_TO_USE = '2023-04-10' # used for filenames throught the notebook\n",
    "# BATCH_DATE_TO_USE = '2023-04-10_Apogee' # used for filenames throught the notebook\n",
    "BATCH_DATE_TO_USE = '2024-04-11_GL' # used for filenames throught the notebook\n",
    "\n",
    "# collected_outputs_path = Path('/nfs/turbo/umms-kdiba/Data/Output/collected_outputs').resolve() # Linux\n",
    "collected_outputs_path: Path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs').resolve() # GreatLakes\n",
    "# collected_outputs_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs').resolve() # Apogee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/global_batch_result_2024-04-11_GL.pkl... done.\n",
      "no difference between provided and internal paths.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-07_11-26-53</td>\n",
       "      <td>kdiba_gor01_one_2006-6-07_11-26-53</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 11:26:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_3-23-37</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_3-23-37</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 03:23:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-04_21-20-3</td>\n",
       "      <td>kdiba_pin01_one_fet11-04_21-20-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-04 21:20:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>redundant</td>\n",
       "      <td>kdiba_pin01_one_redundant</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>showclus</td>\n",
       "      <td>kdiba_pin01_one_showclus</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>sleep</td>\n",
       "      <td>kdiba_pin01_one_sleep</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>tmaze</td>\n",
       "      <td>kdiba_pin01_one_tmaze</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "0        kdiba  gor01        one  2006-6-07_11-26-53   \n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "3        kdiba  gor01        one   2006-6-09_3-23-37   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "..         ...    ...        ...                 ...   \n",
       "67       kdiba  pin01        one    fet11-04_21-20-3   \n",
       "68       kdiba  pin01        one           redundant   \n",
       "69       kdiba  pin01        one            showclus   \n",
       "70       kdiba  pin01        one               sleep   \n",
       "71       kdiba  pin01        one               tmaze   \n",
       "\n",
       "                               context  \\\n",
       "0   kdiba_gor01_one_2006-6-07_11-26-53   \n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "3    kdiba_gor01_one_2006-6-09_3-23-37   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "..                                 ...   \n",
       "67    kdiba_pin01_one_fet11-04_21-20-3   \n",
       "68           kdiba_pin01_one_redundant   \n",
       "69            kdiba_pin01_one_showclus   \n",
       "70               kdiba_pin01_one_sleep   \n",
       "71               kdiba_pin01_one_tmaze   \n",
       "\n",
       "                                             basedirs  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...   \n",
       "69  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...   \n",
       "70   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep   \n",
       "71   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze   \n",
       "\n",
       "                              status errors    session_datetime  n_long_laps  \\\n",
       "0   SessionBatchProgress.NOT_STARTED   None 2006-06-07 11:26:53            0   \n",
       "1   SessionBatchProgress.NOT_STARTED   None 2006-06-08 14:26:15            0   \n",
       "2   SessionBatchProgress.NOT_STARTED   None 2006-06-09 01:22:43            0   \n",
       "3   SessionBatchProgress.NOT_STARTED   None 2006-06-09 03:23:37            0   \n",
       "4   SessionBatchProgress.NOT_STARTED   None 2006-06-12 15:55:31            0   \n",
       "..                               ...    ...                 ...          ...   \n",
       "67  SessionBatchProgress.NOT_STARTED   None 2009-11-04 21:20:03            0   \n",
       "68  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "69  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "70  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "71  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "0                0             0                0     False   \n",
       "1                0             0                0     False   \n",
       "2                0             0                0     False   \n",
       "3                0             0                0     False   \n",
       "4                0             0                0     False   \n",
       "..             ...           ...              ...       ...   \n",
       "67               0             0                0     False   \n",
       "68               0             0                0     False   \n",
       "69               0             0                0     False   \n",
       "70               0             0                0     False   \n",
       "71               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "0                         False                                  True  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "3                         False                                  True  \n",
       "4                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "67                        False                                  True  \n",
       "68                        False                                 False  \n",
       "69                        False                                 False  \n",
       "70                        False                                 False  \n",
       "71                        False                                 False  \n",
       "\n",
       "[72 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/media/MAX/Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/home/halechr/FastData')] # , Path(r'/Volumes/MoverNew/data', Path(r'/home/halechr/FastData'), Path(r'/home/halechr/turbo/Data'), Path(r'W:\\Data')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "# try to load an existing batch result:\n",
    "global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    # display(batch_progress_df)\n",
    "    # display(good_only_batch_progress_df)\n",
    "    display(batch_progress_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019afbbd-70d2-4e75-9548-b6f22d2e31ca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>kdiba_gor01_two_2006-6-07_16-40-19</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>kdiba_gor01_two_2006-6-08_21-16-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-10_12-58-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>kdiba_pin01_one_11-02_17-46-44</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>kdiba_pin01_one_11-02_19-28-0</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>kdiba_pin01_one_11-03_12-3-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>kdiba_pin01_one_fet11-01_12-58-54</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "6        kdiba  gor01        two  2006-6-07_16-40-19   \n",
       "8        kdiba  gor01        two  2006-6-08_21-16-25   \n",
       "..         ...    ...        ...                 ...   \n",
       "32       kdiba  vvp01        two   2006-4-10_12-58-3   \n",
       "52       kdiba  pin01        one      11-02_17-46-44   \n",
       "53       kdiba  pin01        one       11-02_19-28-0   \n",
       "54       kdiba  pin01        one       11-03_12-3-25   \n",
       "64       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "\n",
       "                               context  \\\n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "6   kdiba_gor01_two_2006-6-07_16-40-19   \n",
       "8   kdiba_gor01_two_2006-6-08_21-16-25   \n",
       "..                                 ...   \n",
       "32   kdiba_vvp01_two_2006-4-10_12-58-3   \n",
       "52      kdiba_pin01_one_11-02_17-46-44   \n",
       "53       kdiba_pin01_one_11-02_19-28-0   \n",
       "54       kdiba_pin01_one_11-03_12-3-25   \n",
       "64   kdiba_pin01_one_fet11-01_12-58-54   \n",
       "\n",
       "                                             basedirs  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                              status errors    session_datetime  n_long_laps  \\\n",
       "1   SessionBatchProgress.NOT_STARTED   None 2006-06-08 14:26:15            0   \n",
       "2   SessionBatchProgress.NOT_STARTED   None 2006-06-09 01:22:43            0   \n",
       "4   SessionBatchProgress.NOT_STARTED   None 2006-06-12 15:55:31            0   \n",
       "6   SessionBatchProgress.NOT_STARTED   None 2006-06-07 16:40:19            0   \n",
       "8   SessionBatchProgress.NOT_STARTED   None 2006-06-08 21:16:25            0   \n",
       "..                               ...    ...                 ...          ...   \n",
       "32  SessionBatchProgress.NOT_STARTED   None 2006-04-10 12:58:03            0   \n",
       "52  SessionBatchProgress.NOT_STARTED   None 2009-11-02 17:46:44            0   \n",
       "53  SessionBatchProgress.NOT_STARTED   None 2009-11-02 19:28:00            0   \n",
       "54  SessionBatchProgress.NOT_STARTED   None 2009-11-03 12:03:25            0   \n",
       "64  SessionBatchProgress.NOT_STARTED   None 2009-11-01 12:58:54            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "1                0             0                0     False   \n",
       "2                0             0                0     False   \n",
       "4                0             0                0     False   \n",
       "6                0             0                0     False   \n",
       "8                0             0                0     False   \n",
       "..             ...           ...              ...       ...   \n",
       "32               0             0                0     False   \n",
       "52               0             0                0     False   \n",
       "53               0             0                0     False   \n",
       "54               0             0                0     False   \n",
       "64               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "4                          True                                  True  \n",
       "6                          True                                  True  \n",
       "8                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "32                         True                                  True  \n",
       "52                         True                                  True  \n",
       "53                         True                                  True  \n",
       "54                         True                                  True  \n",
       "64                         True                                  True  \n",
       "\n",
       "[15 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # \n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # Other Notebook\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # \n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # \n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'), # Other Notebook\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), #\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # \n",
    "]\n",
    "\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "good_session_concrete_folders\n",
    "\n",
    "included_session_batch_progress_df = batch_progress_df[np.isin(batch_progress_df['context'].values, included_session_contexts)]\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(included_session_batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71584edc",
   "metadata": {},
   "source": [
    "# Execute Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6ae279",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(included_session_contexts): 15\n",
      "Beginning processing with len(included_session_contexts): 15\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15\"):build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43\"):\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43.log\n",
      "\n",
      "\n",
      "========================== runBatch STARTING ==========================\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31.log========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\n",
      "\tsession_context: kdiba_gor01_one_2006-6-08_14-26-15\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15========================== runBatch STARTING ==========================\n",
      "\tsession_context: kdiba_gor01_one_2006-6-09_1-22-43\n",
      "__________________________________________________________________\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl... \tsession_context: kdiba_gor01_one_2006-6-12_15-55-31\n",
      "\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31__________________________________________________________________\n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43__________________________________________________________________\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl...\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31\n",
      " active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "69    0\n",
      "70    1\n",
      "71    0\n",
      "72    1\n",
      "73    0\n",
      "Name: lap_dir, Length: 74, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "79    0\n",
      "80    1\n",
      "81    0\n",
      "82    1\n",
      "83    0\n",
      "Name: lap_dir, Length: 84, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-NoneSaving (file mode 'None') saved session pickle file results : None... \n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "77    0\n",
      "78    1\n",
      "79    0\n",
      "80    1\n",
      "81    0\n",
      "Name: lap_dir, Length: 82, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-08_14-26-15, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/20240411103731-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/20240411103735-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl'\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/20240411103738-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields...\t done. \n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8921454205980747\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (34393,) should be less than time_window_edges: (30460,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (24701,) should be less than time_window_edges: (18541,)!\n",
      "\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21257,) should be less than time_window_edges: (18538,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (59094,) should be less than time_window_edges: (49657,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (5) == ybin_indicies[-1] (5): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:548: UserWarning: \tWARNING: This REALLY should not happen! peak_y_bin_idx: 1, matching_vertical_scan_y_idxs: [1]!!\n",
      "  warn(f'\\tWARNING: This REALLY should not happen! peak_y_bin_idx: {peak_y_bin_idx}, matching_vertical_scan_y_idxs: {matching_vertical_scan_y_idxs}!!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:549: NotImplementedError\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:55.296956\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_15-55-31'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x14881b4fd140>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x14881cd64b80>) while trying to build the session HDF output for kdiba_gor01_one_2006-6-12_15-55-31\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'jonathan_firing_rate_analysis' ::::: (<class 'KeyError'>, KeyError('jonathan_firing_rate_analysis'), <traceback object at 0x14889aa61d80>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_gor01_one_2006-6-12_15-55-31]\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14881d22a340>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-07_16-40-19\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "76    1\n",
      "77    0\n",
      "78    1\n",
      "79    0\n",
      "80    1\n",
      "Name: lap_dir, Length: 81, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/20240411104518-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8938844725727578\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49660248476264324\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910495161195622\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49502750895531233\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8914204801222696\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49523360006792755\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8914306270153439\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910038673692668\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910023340225576\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8931068608807453\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4961704782670807\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8920268037179523\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4955704465099735\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910003391088342\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8954385439553483\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49746585775297125\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8912550089632353\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49514167164624184\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:593: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 57)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:599: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:616: UserWarning: \tWARNING: peak_x_bin_idx (56) == xbin_indicies[-1] (56): setting matching_horizontal_scan_x_idxs = (-1, 57)\n",
      "  warn(f'\\tWARNING: peak_x_bin_idx ({peak_x_bin_idx}) == xbin_indicies[-1] ({xbin_indicies[-1]}): setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (5) == ybin_indicies[-1] (5): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8957101901631331\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.89101370175517\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:599: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8911996409412434\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910023940432917\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49500133002405095\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 6 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8912549664438852\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:593: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 57)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:599: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (5) == ybin_indicies[-1] (5): setting matching_vertical_scan_y_idxs = (-1, 6)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (524075,) should be less than time_window_edges: (2060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15323,) should be less than time_window_edges: (1996,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (344561,) should be less than time_window_edges: (1417,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10660,) should be less than time_window_edges: (1226,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (868636,) should be less than time_window_edges: (3476,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (25983,) should be less than time_window_edges: (3298,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (524075,) should be less than time_window_edges: (2060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11292,) should be less than time_window_edges: (1896,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (344561,) should be less than time_window_edges: (1417,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10597,) should be less than time_window_edges: (1219,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (868636,) should be less than time_window_edges: (3476,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21889,) should be less than time_window_edges: (3176,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (524075,) should be less than time_window_edges: (2060,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26615,) should be less than time_window_edges: (2029,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (344561,) should be less than time_window_edges: (1417,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21257,) should be less than time_window_edges: (1237,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (868636,) should be less than time_window_edges: (3476,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47872,) should be less than time_window_edges: (3309,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARNING: filtered_contexts['maze1_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.023809523809523808\n",
      "percent_laps_estimated_correctly: 0.023809523809523808\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "83    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (488487,) should be less than time_window_edges: (2474,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6335,) should be less than time_window_edges: (2388,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (471751,) should be less than time_window_edges: (2705,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9833,) should be less than time_window_edges: (2485,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (960238,) should be less than time_window_edges: (5177,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16168,) should be less than time_window_edges: (5107,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (488487,) should be less than time_window_edges: (2474,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9001,) should be less than time_window_edges: (2247,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (471751,) should be less than time_window_edges: (2705,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11391,) should be less than time_window_edges: (2538,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (960238,) should be less than time_window_edges: (5177,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20392,) should be less than time_window_edges: (5050,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (488487,) should be less than time_window_edges: (2474,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15336,) should be less than time_window_edges: (2388,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (471751,) should be less than time_window_edges: (2705,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21224,) should be less than time_window_edges: (2554,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (960238,) should be less than time_window_edges: (5177,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (36560,) should be less than time_window_edges: (5107,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARNING: filtered_contexts['maze1_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.028). This used to be enforced but continuing anyway.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 1.0\n",
      "percent_laps_estimated_correctly: 1.0\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "laps_paired_tests: [TtestResult(statistic=-12.583660012048691, pvalue=6.193825884773257e-21, df=83), TtestResult(statistic=-13.83468369297005, pvalue=2.924330222742681e-23, df=83)]\n",
      "\tdone. building global result.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "laps_paired_tests: [TtestResult(statistic=14.440349169878825, pvalue=5.41854114105888e-24, df=80), TtestResult(statistic=12.875642162850026, pvalue=3.416717990709997e-21, df=80)]\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (596822,) should be less than time_window_edges: (2425,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11239,) should be less than time_window_edges: (2365,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (410073,) should be less than time_window_edges: (1766,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11039,) should be less than time_window_edges: (1610,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1006895,) should be less than time_window_edges: (4189,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (22278,) should be less than time_window_edges: (4075,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (596822,) should be less than time_window_edges: (2425,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12949,) should be less than time_window_edges: (2345,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (410073,) should be less than time_window_edges: (1766,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11652,) should be less than time_window_edges: (1622,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1006895,) should be less than time_window_edges: (4189,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (24601,) should be less than time_window_edges: (4075,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (596822,) should be less than time_window_edges: (2425,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (24188,) should be less than time_window_edges: (2397,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (410073,) should be less than time_window_edges: (1766,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (22691,) should be less than time_window_edges: (1664,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1006895,) should be less than time_window_edges: (4189,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (46879,) should be less than time_window_edges: (4128,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARNING: filtered_contexts['maze1_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.028). This used to be enforced but continuing anyway.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "fails due to some types thing?\n",
      "\terr: Length of values (82) does not match length of index (80)\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "laps_paired_tests: [TtestResult(statistic=-14.071410803304241, pvalue=3.166489794923787e-23, df=79), TtestResult(statistic=-15.867856262084928, pvalue=2.7755757477399216e-26, df=79)]\n",
      "\tdone. building global result.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ripple_evts_paired_tests: [TtestResult(statistic=-2.474512578276508, pvalue=0.014008865586446241, df=249), TtestResult(statistic=-3.4035279276348023, pvalue=0.0007750729515407897, df=249)]\n",
      "\tdone. building global result.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ripple_evts_paired_tests: [TtestResult(statistic=0.11075282091831637, pvalue=0.9118658994441743, df=415), TtestResult(statistic=-0.3550981080132294, pvalue=0.7226964909068281, df=415)]\n",
      "\tdone. building global result.\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=0.2429006468985795, pvalue=0.8082365395834719, df=323), TtestResult(statistic=-1.0954816716992632, pvalue=0.27412182641762173, df=323)]\n",
      "\tdone. building global result.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 66, n_all_epoch_timebins = 1618)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 66, n_all_epoch_timebins = 1618)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2679: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t44 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t224 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t156 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t224 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t379 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t224 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t156 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t224 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t156 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t44 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t44 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([138.16397565, 146.119753  ]), 'short_ylim': array([138.02062831, 146.26310034])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 0/28\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 28/28\n",
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 7/28\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8054171165052444\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8054171165052444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8054171165052444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 15/84\n",
      "\tagreeing_rows_ratio: 0.17857142857142858\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 35/412\n",
      "\tagreeing_rows_ratio: 0.08495145631067962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 57/84\n",
      "\tagreeing_rows_ratio: 0.6785714285714286\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 148/412\n",
      "\tagreeing_rows_ratio: 0.3592233009708738\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 74/84\n",
      "\tagreeing_rows_ratio: 0.8809523809523809\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 128/412\n",
      "\tagreeing_rows_ratio: 0.3106796116504854\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.468855\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_1-22-43'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error expected str, bytes or os.PathLike object, not AttributeManager\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-09_1-22-43...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t224 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t156 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t224 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t156 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t44 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t44 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14874d0de380>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-08_21-16-25\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "75    0\n",
      "76    1\n",
      "77    0\n",
      "78    1\n",
      "79    0\n",
      "Name: lap_dir, Length: 80, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/20240411124338-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (13037,) should be less than time_window_edges: (12336,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (32443,) should be less than time_window_edges: (20105,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28471,) should be less than time_window_edges: (20105,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (23066,) should be less than time_window_edges: (12846,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18262,) should be less than time_window_edges: (12846,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (55509,) should be less than time_window_edges: (34393,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (46733,) should be less than time_window_edges: (34393,)!\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8956195547635818\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8967583758174363\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 62, n_all_epoch_timebins = 2178)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891006533516873\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 39, n_all_epoch_timebins = 3090)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910037019192092\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 62, n_all_epoch_timebins = 2178)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2679: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8911631738308479\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4950906521282488\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "(n_neurons = 39, n_all_epoch_timebins = 3090)\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t296 remain.\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t245 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t41 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8965885294414875\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910323075479959\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t217 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t393 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8918139240407162\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49545218002262015\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t296 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8915288674690897\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8985171103141391\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 8 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 8)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "DropShorterMode:Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\n",
      "\t217 remain.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t530 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t480 remain.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t217 remain.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t393 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t41 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([138.39266724, 146.94706035]), 'short_ylim': array([139.5560647 , 146.19427375])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 0/15\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 15/15\n",
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 2/15\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "\n",
      "long_short_inst_spike_rate_groups missing.\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t296 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t245 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t296 remain.\n",
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.775693131596347\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.775693131596347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t245 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([137.92544712, 145.16448777]), 'short_ylim': array([137.99712079, 145.02114043])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 4/32\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 28/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 8/28\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.793023081021702\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.793023081021702\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (372221,) should be less than time_window_edges: (1447,)!\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15874,) should be less than time_window_edges: (1296,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (246159,) should be less than time_window_edges: (958,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10732,) should be less than time_window_edges: (824,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (618380,) should be less than time_window_edges: (2404,)!\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26606,) should be less than time_window_edges: (2260,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (372221,) should be less than time_window_edges: (1447,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12597,) should be less than time_window_edges: (1263,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (246159,) should be less than time_window_edges: (958,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7530,) should be less than time_window_edges: (803,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (618380,) should be less than time_window_edges: (2404,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20127,) should be less than time_window_edges: (2215,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (372221,) should be less than time_window_edges: (1447,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28471,) should be less than time_window_edges: (1342,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (246159,) should be less than time_window_edges: (958,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18262,) should be less than time_window_edges: (858,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (618380,) should be less than time_window_edges: (2404,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (46733,) should be less than time_window_edges: (2294,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARNING: filtered_contexts['maze1_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.047). This used to be enforced but continuing anyway.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "fails due to some types thing?\n",
      "\terr: Length of values (80) does not match length of index (78)\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.775693131596347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 7/81\n",
      "\tagreeing_rows_ratio: 0.08641975308641975\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 64/718\n",
      "\tagreeing_rows_ratio: 0.08913649025069638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 41/81\n",
      "\tagreeing_rows_ratio: 0.5061728395061729\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 216/718\n",
      "\tagreeing_rows_ratio: 0.3008356545961003\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 60/81\n",
      "\tagreeing_rows_ratio: 0.7407407407407407\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 223/718\n",
      "\tagreeing_rows_ratio: 0.31058495821727017\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.480791\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-07_16-40-19'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "laps_paired_tests: [TtestResult(statistic=9.635918746107393, pvalue=7.182159016623526e-15, df=77), TtestResult(statistic=16.09981603858911, pvalue=2.3325191364414886e-26, df=77)]\n",
      "\tdone. building global result.\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-07_16-40-19...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t217 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t393 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t41 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x148784bb9900>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-09_22-24-40\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "83    0\n",
      "84    1\n",
      "85    0\n",
      "86    1\n",
      "87    0\n",
      "Name: lap_dir, Length: 88, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/20240411131800-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.793023081021702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 18/80\n",
      "\tagreeing_rows_ratio: 0.225\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 29/611\n",
      "\tagreeing_rows_ratio: 0.04746317512274959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 41/80\n",
      "\tagreeing_rows_ratio: 0.5125\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 227/611\n",
      "\tagreeing_rows_ratio: 0.37152209492635024\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 68/80\n",
      "\tagreeing_rows_ratio: 0.85\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 210/611\n",
      "\tagreeing_rows_ratio: 0.3436988543371522\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.384378\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_14-26-15'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error expected str, bytes or os.PathLike object, not AttributeManager\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-08_14-26-15...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t296 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t245 remain.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t296 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t245 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_gor01_one_2006-6-08_14-26-15, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14881c1489c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-12_16-53-46\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "76    1\n",
      "77    0\n",
      "78    1\n",
      "79    0\n",
      "80    1\n",
      "Name: lap_dir, Length: 81, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/20240411132201-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16577,) should be less than time_window_edges: (13015,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12579,) should be less than time_window_edges: (9137,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10115,) should be less than time_window_edges: (8944,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (29156,) should be less than time_window_edges: (22719,)!\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (30931,) should be less than time_window_edges: (26889,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8936327664152072\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8941631061622644\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8911675359905024\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 10)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 10 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8961208756197983\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4978449308998879\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 10)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 10)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 10 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8917872441653794\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=-0.9571845271444935, pvalue=0.34080577912381405, df=99), TtestResult(statistic=-1.1534624191677212, pvalue=0.2514978286739236, df=99)]\n",
      "\tdone. building global result.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8952491071949483\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (4) == ybin_indicies[-1] (4): setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910030423261578\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910905376632813\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49505029870182293\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 10)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 10 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:599: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 10)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (174940,) should be less than time_window_edges: (943,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8917937537080264\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7475,) should be less than time_window_edges: (844,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110870,) should be less than time_window_edges: (630,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6715,) should be less than time_window_edges: (581,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (285810,) should be less than time_window_edges: (1572,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14190,) should be less than time_window_edges: (1486,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (174940,) should be less than time_window_edges: (943,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (4788,) should be less than time_window_edges: (802,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110870,) should be less than time_window_edges: (630,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3400,) should be less than time_window_edges: (565,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (285810,) should be less than time_window_edges: (1572,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8188,) should be less than time_window_edges: (1436,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (174940,) should be less than time_window_edges: (943,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12263,) should be less than time_window_edges: (869,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110870,) should be less than time_window_edges: (630,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10115,) should be less than time_window_edges: (598,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (285810,) should be less than time_window_edges: (1572,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (22378,) should be less than time_window_edges: (1503,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARNING: filtered_contexts['maze1_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.036). This used to be enforced but continuing anyway.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8955746384417013\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49754146580094516\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 10)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:599: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 10)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 10 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "laps_paired_tests: [TtestResult(statistic=11.9131930767579, pvalue=2.532630915754349e-19, df=79), TtestResult(statistic=10.14037339814309, pvalue=5.905373322131459e-16, df=79)]\n",
      "\tdone. building global result.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 61, n_all_epoch_timebins = 613)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 61, n_all_epoch_timebins = 613)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t38 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1766: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t76 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1766: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t96 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t76 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t76 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t38 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t38 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([139.89578723, 148.51861548]), 'short_ylim': array([140.37483324, 148.38174519])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 7/33\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 26/33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 3/26\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.826320568641476\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.826320568641476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.826320568641476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 2/78\n",
      "\tagreeing_rows_ratio: 0.02564102564102564\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 6/139\n",
      "\tagreeing_rows_ratio: 0.04316546762589928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 48/78\n",
      "\tagreeing_rows_ratio: 0.6153846153846154\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 38/139\n",
      "\tagreeing_rows_ratio: 0.2733812949640288\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 43/78\n",
      "\tagreeing_rows_ratio: 0.5512820512820513\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 29/139\n",
      "\tagreeing_rows_ratio: 0.20863309352517986\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.974030\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_21-16-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error expected str, bytes or os.PathLike object, not AttributeManager\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-08_21-16-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t76 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t76 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t38 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t38 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14881a28b400>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-09_17-29-30\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0      1\n",
      "1      0\n",
      "2      1\n",
      "3      0\n",
      "4      1\n",
      "      ..\n",
      "96     1\n",
      "97     0\n",
      "98     1\n",
      "99     0\n",
      "100    1\n",
      "Name: lap_dir, Length: 101, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/20240411140038-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.WARNING: PREVIOUSLY ASSERT: \n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\t spikes_df[time_variable_name]: (489077,) should be less than time_window_edges: (1825,)!\n",
      "\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11389,) should be less than time_window_edges: (1791,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\t done.\n",
      "Recomputing active_epoch_placefields2D... \n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (686401,) should be less than time_window_edges: (3325,)!\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5527,) should be less than time_window_edges: (3258,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1175478,) should be less than time_window_edges: (5148,)!\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16916,) should be less than time_window_edges: (5117,)!\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (489077,) should be less than time_window_edges: (1825,)!\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (13362,) should be less than time_window_edges: (1726,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (686401,) should be less than time_window_edges: (3325,)!\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (4427,) should be less than time_window_edges: (2923,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1175478,) should be less than time_window_edges: (5148,)!\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (17789,) should be less than time_window_edges: (4721,)!\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (489077,) should be less than time_window_edges: (1825,)!\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=1.8842095598328856, pvalue=0.06287617824701691, df=87), TtestResult(statistic=2.043418097216416, pvalue=0.044036292933062535, df=87)]\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (24751,) should be less than time_window_edges: (1791,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (686401,) should be less than time_window_edges: (3325,)!\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9954,) should be less than time_window_edges: (3292,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1175478,) should be less than time_window_edges: (5148,)!\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (34705,) should be less than time_window_edges: (5117,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\t done.\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.034). This used to be enforced but continuing anyway.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.9772727272727273\n",
      "percent_laps_estimated_correctly: 0.9772727272727273\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "83    0\n",
      "85    0\n",
      "87    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "84    1\n",
      "86    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "laps_paired_tests: [TtestResult(statistic=-11.73869567086926, pvalue=1.260029686231092e-19, df=87), TtestResult(statistic=-9.564428412015477, pvalue=3.138073008753577e-15, df=87)]\n",
      "\tdone. building global result.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 36, n_all_epoch_timebins = 407)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 36, n_all_epoch_timebins = 407)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t51 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t45 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t51 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t73 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t51 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t45 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t51 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t45 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([139.91706986, 148.74520355]), 'short_ylim': array([140.05394015, 147.85554667])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 1/11\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 10/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 4/10\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (187994,) should be less than time_window_edges: (1749,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5370,) should be less than time_window_edges: (1543,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110977,) should be less than time_window_edges: (1038,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3786,) should be less than time_window_edges: (918,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (298971,) should be less than time_window_edges: (2785,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9156,) should be less than time_window_edges: (2686,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (187994,) should be less than time_window_edges: (1749,)!\n",
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3139,) should be less than time_window_edges: (1618,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110977,) should be less than time_window_edges: (1038,)!\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8108462429929406\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8108462429929406\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2446,) should be less than time_window_edges: (894,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (298971,) should be less than time_window_edges: (2785,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5585,) should be less than time_window_edges: (2660,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (187994,) should be less than time_window_edges: (1749,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8509,) should be less than time_window_edges: (1656,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110977,) should be less than time_window_edges: (1038,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6232,) should be less than time_window_edges: (931,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (298971,) should be less than time_window_edges: (2785,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14741,) should be less than time_window_edges: (2699,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "fails due to some types thing?\n",
      "\terr: Length of values (101) does not match length of index (86)\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "83    0\n",
      "85    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "84    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laps_paired_tests: [TtestResult(statistic=-7.54986490453549, pvalue=6.969499507638417e-11, df=78), TtestResult(statistic=-10.576242421539526, pvalue=9.970897068519366e-17, df=78)]\n",
      "\tdone. building global result.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8108462429929406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 1/81\n",
      "\tagreeing_rows_ratio: 0.012345679012345678\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 6/121\n",
      "\tagreeing_rows_ratio: 0.049586776859504134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 51/81\n",
      "\tagreeing_rows_ratio: 0.6296296296296297\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 32/121\n",
      "\tagreeing_rows_ratio: 0.2644628099173554\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 73/81\n",
      "\tagreeing_rows_ratio: 0.9012345679012346\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 31/121\n",
      "\tagreeing_rows_ratio: 0.256198347107438\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.366825\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_16-53-46'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error expected str, bytes or os.PathLike object, not AttributeManager\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t51 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t45 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t51 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t45 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x148725e0a780>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-10_12-25-50\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "79    0\n",
      "80    1\n",
      "81    0\n",
      "82    1\n",
      "83    0\n",
      "Name: lap_dir, Length: 84, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/20240411142332-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (3) == ybin_indicies[-1] (3): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 4 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 4 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (3) == ybin_indicies[-1] (3): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 4 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 4 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8919951739265211\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (3) == ybin_indicies[-1] (3): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 4 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:599: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (3) == ybin_indicies[-1] (3): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 4 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:545: UserWarning: \tWARNING: peak_y_bin_idx (3) == ybin_indicies[-1] (3): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[-1] ({ybin_indicies[-1]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:541: UserWarning: \tWARNING: peak_y_bin_idx (0) == ybin_indicies[0] (0): setting matching_vertical_scan_y_idxs = (-1, 4)\n",
      "  warn(f'\\tWARNING: peak_y_bin_idx ({peak_y_bin_idx}) == ybin_indicies[0] ({ybin_indicies[0]}): setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:599: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = -1 to matching_horizontal_scan_x_idxs\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 1: missing lower extrema, adding xbin_outer_extrema[0] = {xbin_outer_extrema[0]} to matching_horizontal_scan_x_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 4 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=-3.015905017234945, pvalue=0.003711298648038475, df=62), TtestResult(statistic=-3.7058924066504004, pvalue=0.00045114464589090115, df=62)]\n",
      "\tdone. building global result.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=-3.8509461833341123, pvalue=0.00013429901282042674, df=461), TtestResult(statistic=-3.5273083761705406, pvalue=0.0004618574058817142, df=461)]\n",
      "\tdone. building global result.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (160181,) should be less than time_window_edges: (1769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2965,) should be less than time_window_edges: (1700,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (88598,) should be less than time_window_edges: (1060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2773,) should be less than time_window_edges: (844,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (248779,) should be less than time_window_edges: (2828,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5738,) should be less than time_window_edges: (2753,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (160181,) should be less than time_window_edges: (1769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (4245,) should be less than time_window_edges: (1663,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (88598,) should be less than time_window_edges: (1060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3141,) should be less than time_window_edges: (853,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (248779,) should be less than time_window_edges: (2828,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7386,) should be less than time_window_edges: (2720,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (160181,) should be less than time_window_edges: (1769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7210,) should be less than time_window_edges: (1733,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (88598,) should be less than time_window_edges: (1060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5914,) should be less than time_window_edges: (881,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (248779,) should be less than time_window_edges: (2828,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (13124,) should be less than time_window_edges: (2790,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARNING: filtered_contexts['maze1_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_odd']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_odd'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_even']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_even'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze1_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze1_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze2_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze2' != desired_filter_name: 'maze2_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: filtered_contexts['maze_any']'s actual context name is incorrect. \n",
      "\ta_filtered_ctxt.filter_name: 'maze' != desired_filter_name: 'maze_any'\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "83    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 21, n_all_epoch_timebins = 584)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 21, n_all_epoch_timebins = 584)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t60 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t70 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t60 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([108.13933163, 115.13913719]), 'short_ylim': array([108.35144695, 114.2199708 ])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 0/4\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 4/4\n",
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 3/4\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8185290701194465\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8185290701194465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laps_paired_tests: [TtestResult(statistic=-9.885367982280652, pvalue=1.257890732042188e-15, df=82), TtestResult(statistic=-3.6298429998667996, pvalue=0.000492157951875825, df=82)]\n",
      "\tdone. building global result.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8185290701194465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 5/86\n",
      "\tagreeing_rows_ratio: 0.05813953488372093\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 13/124\n",
      "\tagreeing_rows_ratio: 0.10483870967741936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 62/86\n",
      "\tagreeing_rows_ratio: 0.7209302325581395\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 42/124\n",
      "\tagreeing_rows_ratio: 0.3387096774193548\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 63/86\n",
      "\tagreeing_rows_ratio: 0.7325581395348837\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 50/124\n",
      "\tagreeing_rows_ratio: 0.4032258064516129\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.353078\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_17-29-30'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-09_17-29-30...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t60 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t42 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14881b781180>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-09_16-40-54\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl... done.Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl.\n",
      "\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "93    0\n",
      "94    1\n",
      "95    0\n",
      "96    1\n",
      "97    0\n",
      "Name: lap_dir, Length: 98, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/20240411145445-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910004670304068\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49500025946133713\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910018456542161\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4950010253634534\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=0.16840142467367844, pvalue=0.8671161292595706, df=40), TtestResult(statistic=-0.42318940703367336, pvalue=0.6744238026011956, df=40)]\n",
      "\tdone. building global result.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "(n_neurons = 23, n_all_epoch_timebins = 276)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 23, n_all_epoch_timebins = 276)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2679: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1766: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t26 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t22 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1766: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t26 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t43 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t26 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t22 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t26 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t22 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t42 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([107.81777896, 113.75700792]), 'short_ylim': array([107.88848407, 113.61559771])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 0/1\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 1/1\n",
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 1/1\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n",
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8571811021501654\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8571811021501654\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (218208,) should be less than time_window_edges: (2313,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5010,) should be less than time_window_edges: (1680,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (100496,) should be less than time_window_edges: (1138,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3641,) should be less than time_window_edges: (958,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (318704,) should be less than time_window_edges: (3449,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8651,) should be less than time_window_edges: (2815,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (218208,) should be less than time_window_edges: (2313,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3958,) should be less than time_window_edges: (1679,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (100496,) should be less than time_window_edges: (1138,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2643,) should be less than time_window_edges: (931,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (318704,) should be less than time_window_edges: (3449,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6601,) should be less than time_window_edges: (2790,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (218208,) should be less than time_window_edges: (2313,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8968,) should be less than time_window_edges: (1729,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (100496,) should be less than time_window_edges: (1138,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6284,) should be less than time_window_edges: (983,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (318704,) should be less than time_window_edges: (3449,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15252,) should be less than time_window_edges: (2840,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.039). This used to be enforced but continuing anyway.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "83    0\n",
      "85    0\n",
      "87    0\n",
      "89    0\n",
      "91    0\n",
      "93    0\n",
      "95    0\n",
      "97    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "84    1\n",
      "86    1\n",
      "88    1\n",
      "90    1\n",
      "92    1\n",
      "94    1\n",
      "96    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laps_paired_tests: [TtestResult(statistic=16.57685994275281, pvalue=2.7904587465555393e-27, df=78), TtestResult(statistic=15.228411348807056, pvalue=4.461223297489366e-25, df=78)]\n",
      "\tdone. building global result.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8571811021501654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 8/84\n",
      "\tagreeing_rows_ratio: 0.09523809523809523\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 6/55\n",
      "\tagreeing_rows_ratio: 0.10909090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 40/84\n",
      "\tagreeing_rows_ratio: 0.47619047619047616\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 20/55\n",
      "\tagreeing_rows_ratio: 0.36363636363636365\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 46/84\n",
      "\tagreeing_rows_ratio: 0.5476190476190477\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 32/55\n",
      "\tagreeing_rows_ratio: 0.5818181818181818\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.280804\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-25-50'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error expected str, bytes or os.PathLike object, not AttributeManager\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-10_12-25-50...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t26 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t22 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t26 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t22 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t42 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t42 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14881bf52380>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-10_12-58-3\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "76    1\n",
      "77    0\n",
      "78    1\n",
      "79    0\n",
      "80    1\n",
      "Name: lap_dir, Length: 81, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/20240411151630-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=0.962923384689127, pvalue=0.3508535135320998, df=15), TtestResult(statistic=2.162667011671865, pvalue=0.04712865382616771, df=15)]\n",
      "\tdone. building global result.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/pandas/core/nanops.py:1569: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 472\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 22, n_all_epoch_timebins = 317)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 22, n_all_epoch_timebins = 317)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2679: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t45 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t50 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t29 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t25 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t29 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t44 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t29 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t25 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t45 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t50 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([122.9663429, 130.4610842]), 'short_ylim': array([122.82493268, 130.03685356])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 1/4\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 0/3\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.911561277209492\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.911561277209492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.911561277209492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...Performance: Radon Transform:\n",
      "\tLaps:\n",
      "\n",
      "agreeing_rows_count/num_total_epochs: 1/98\n",
      "\tagreeing_rows_ratio: 0.01020408163265306\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 8/70\n",
      "\tagreeing_rows_ratio: 0.11428571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 42/98\n",
      "\tagreeing_rows_ratio: 0.42857142857142855\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 30/70\n",
      "\tagreeing_rows_ratio: 0.42857142857142855\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 71/98\n",
      "\tagreeing_rows_ratio: 0.7244897959183674\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 32/70\n",
      "\tagreeing_rows_ratio: 0.45714285714285713\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.371473\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_16-40-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-09_16-40-54...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t29 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t25 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t45 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t50 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x148813e8b800>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_17-46-44\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "78    1\n",
      "79    0\n",
      "80    1\n",
      "81    0\n",
      "82    1\n",
      "Name: lap_dir, Length: 83, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/20240411153800-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (127947,) should be less than time_window_edges: (1867,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2791,) should be less than time_window_edges: (1724,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (77409,) should be less than time_window_edges: (1053,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2871,) should be less than time_window_edges: (937,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (205356,) should be less than time_window_edges: (2919,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5662,) should be less than time_window_edges: (2842,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (127947,) should be less than time_window_edges: (1867,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3427,) should be less than time_window_edges: (1745,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (77409,) should be less than time_window_edges: (1053,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3060,) should be less than time_window_edges: (824,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (205356,) should be less than time_window_edges: (2919,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6487,) should be less than time_window_edges: (2758,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (127947,) should be less than time_window_edges: (1867,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6218,) should be less than time_window_edges: (1785,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (77409,) should be less than time_window_edges: (1053,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5931,) should be less than time_window_edges: (937,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (205356,) should be less than time_window_edges: (2919,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12149,) should be less than time_window_edges: (2842,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.06). This used to be enforced but continuing anyway.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "percent_laps_estimated_correctly: 0.0\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "laps_paired_tests: [TtestResult(statistic=11.186523683468218, pvalue=5.263889380452137e-16, df=57), TtestResult(statistic=3.6963769463532015, pvalue=0.0004927875455813251, df=57)]\n",
      "\tdone. building global result.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:593: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 61)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8911101756761406\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49506120870896697\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:593: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 61)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:593: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 61)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "(n_neurons = 56, n_all_epoch_timebins = 3696)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:593: UserWarning: \tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = (-1, 61)\n",
      "  warn(f'\\tWARNING: len(matching_horizontal_scan_x_idxs) == 0: setting matching_horizontal_scan_x_idxs = {xbin_outer_extrema}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 56, n_all_epoch_timebins = 3696)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2679: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t47 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t41 remain.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t107 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t432 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t107 remain.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t406 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t107 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t432 remain.\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=1.0133683448898865, pvalue=0.31668677390355054, df=42), TtestResult(statistic=1.0533901112637434, pvalue=0.29818488645242774, df=42)]\n",
      "\tdone. building global result.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t47 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t41 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([139.30536443, 147.99662782]), 'short_ylim': array([140.12658616, 147.44914666])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 3/33\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 30/33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 24/30\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (172300,) should be less than time_window_edges: (2290,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2520,) should be less than time_window_edges: (1903,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (148480,) should be less than time_window_edges: (1596,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (4292,) should be less than time_window_edges: (1460,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (320780,) should be less than time_window_edges: (3884,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6812,) should be less than time_window_edges: (3738,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (172300,) should be less than time_window_edges: (2290,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2674,) should be less than time_window_edges: (1929,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (148480,) should be less than time_window_edges: (1596,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3446,) should be less than time_window_edges: (1352,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (320780,) should be less than time_window_edges: (3884,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6120,) should be less than time_window_edges: (3721,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (172300,) should be less than time_window_edges: (2290,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5194,) should be less than time_window_edges: (1931,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (148480,) should be less than time_window_edges: (1596,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7738,) should be less than time_window_edges: (1460,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (320780,) should be less than time_window_edges: (3884,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12932,) should be less than time_window_edges: (3738,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.059). This used to be enforced but continuing anyway.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "fails due to some types thing?\n",
      "\terr: Length of values (83) does not match length of index (81)\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8632841399651463\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.8632841399651463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laps_paired_tests: [TtestResult(statistic=0.039322158756795185, pvalue=0.9689882015900099, df=22), TtestResult(statistic=0.21618698328611297, pvalue=0.8308339633517008, df=22)]\n",
      "\tdone. building global result.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ripple_evts_paired_tests: [TtestResult(statistic=-1.810139233242567, pvalue=0.32131309501173194, df=1), TtestResult(statistic=0.27127941255248295, pvalue=0.8313569198829778, df=1)]\n",
      "\tdone. building global result.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 20, n_all_epoch_timebins = 2015)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 27, n_all_epoch_timebins = 542)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 20, n_all_epoch_timebins = 2015)\n",
      "(n_neurons = 27, n_all_epoch_timebins = 542)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2679: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.long_short_fr_indicies_analyses missing.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t41 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t44 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1766: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t57 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t136 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t28 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1766: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t56 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t36 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t82 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t74 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t57 remain.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.8632841399651463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t28 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t41 remain.\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t44 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t136 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([122.84017432, 129.69856966]), 'short_ylim': array([122.628059  , 129.13292881])}Performance: Radon Transform:\n",
      "\tLaps:\n",
      "\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 1/9\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 8/9agreeing_rows_count/num_total_epochs: 11/88\n",
      "\tagreeing_rows_ratio: 0.125\n",
      "\n",
      "\tPerformance: Ripple: Radon Transform:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreeing_rows_count/num_total_epochs: 41/707\n",
      "\tagreeing_rows_ratio: 0.05799151343705799\n",
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 3/8\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t44 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t136 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t33 remain."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t40 remain.\n",
      "Performance: WCorr:\n",
      "\tLaps:\t done.\n",
      "agreeing_rows_count/num_total_epochs: 52/88\n",
      "\tagreeing_rows_ratio: 0.5909090909090909\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 205/707\n",
      "\tagreeing_rows_ratio: 0.28995756718529\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 57/88\n",
      "\tagreeing_rows_ratio: 0.6477272727272727\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 247/707\n",
      "\tagreeing_rows_ratio: 0.3493635077793494\n",
      "\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\t done.\n",
      "\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([136.1009553, 144.5925227]), 'short_ylim': array([137.20171404, 143.96351771])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 1/11\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 10/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 5/10\n",
      "\t done.\n",
      "no changes in global results.2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.474607\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_22-24-40'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.65558372671137\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.65558372671137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.7694216188180962\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.7694216188180962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-09_22-24-40...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t107 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t432 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t47 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t41 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x1488151d9a00>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_19-28-0\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl... neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "89    0\n",
      "90    1\n",
      "91    0\n",
      "92    1\n",
      "93    0\n",
      "Name: lap_dir, Length: 94, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/20240411160911-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl'\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.Recomputing active_epoch_placefields2D...\n",
      " \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.65558372671137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 0/81\n",
      "\tagreeing_rows_ratio: 0.0\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 7/95\n",
      "\tagreeing_rows_ratio: 0.07368421052631578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 40/81\n",
      "\tagreeing_rows_ratio: 0.49382716049382713\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 23/95\n",
      "\tagreeing_rows_ratio: 0.24210526315789474\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 61/81\n",
      "\tagreeing_rows_ratio: 0.7530864197530864\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 29/95\n",
      "\tagreeing_rows_ratio: 0.30526315789473685\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.278347\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-58-3'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910015631697795\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4950008684276553\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-10_12-58-3...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t57 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t28 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...DropShorterMode:\n",
      "\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t41 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x148814d63f80>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_pin01_one_11-03_12-3-25\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "79    0\n",
      "80    1\n",
      "81    0\n",
      "82    1\n",
      "83    0\n",
      "Name: lap_dir, Length: 84, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/20240411161431-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7694216188180962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 6/81\n",
      "\tagreeing_rows_ratio: 0.07407407407407407\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 15/260\n",
      "\tagreeing_rows_ratio: 0.057692307692307696\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 21/81\n",
      "\tagreeing_rows_ratio: 0.25925925925925924\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 70/260\n",
      "\tagreeing_rows_ratio: 0.2692307692307692\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 37/81\n",
      "\tagreeing_rows_ratio: 0.4567901234567901\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 106/260\n",
      "\tagreeing_rows_ratio: 0.4076923076923077\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.339832\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_17-46-44'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error expected str, bytes or os.PathLike object, not AttributeManager\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... could not output time-dependent placefields to .h5. Skipping.\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_17-46-44...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t44 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t136 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t44 remain.\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t136 remain.\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t33 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t40 remain.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14875ae42440>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3419.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3419.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /nfs/turbo/umms-kdiba/Data\n",
      "\tsession_context: kdiba_pin01_one_fet11-01_12-58-54\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "     ..\n",
      "67    0\n",
      "68    1\n",
      "69    0\n",
      "70    1\n",
      "71    0\n",
      "Name: lap_dir, Length: 72, dtype: int64 using old even/odd 'lap_dir' determination.\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "No existing loaded_track_limits parameters! Assigning them!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.position_info.mat... done.\n",
      "were pipeline preprocessing parameters missing and updated?: True\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910005884798784\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "done.\n",
      "moving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/20240411161630-loadedSessPickle.pkl' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl'\n",
      "WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'ratemap_peaks_prominence2d', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_train_test_split', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - pf_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\t done.\n",
      "\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Recomputing active_epoch_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "\t done.\n",
      "2024-01-02 - pfdt_computation _perform_try_computation_if_needed, remove_provided_keys\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t done.\n",
      "2024-01-02 - position_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910561998386689\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8916440623989182\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.49535781244384347\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "2024-01-02 - ratemap_peaks_prominence2d _perform_try_computation_if_needed, remove_provided_keys\n",
      "ratemap_peaks_prominence2d missing.\n",
      "\t Recomputing ratemap_peaks_prominence2d...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910779184498424\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910000620723857\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4950000344846587\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8912289875784684\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (155661,) should be less than time_window_edges: (1577,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3904,) should be less than time_window_edges: (1468,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (78036,) should be less than time_window_edges: (782,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (4673,) should be less than time_window_edges: (732,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (233697,) should be less than time_window_edges: (2357,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8577,) should be less than time_window_edges: (2251,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (155661,) should be less than time_window_edges: (1577,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3043,) should be less than time_window_edges: (1411,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (78036,) should be less than time_window_edges: (782,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2492,) should be less than time_window_edges: (708,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (233697,) should be less than time_window_edges: (2357,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5535,) should be less than time_window_edges: (2187,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (155661,) should be less than time_window_edges: (1577,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6947,) should be less than time_window_edges: (1485,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (78036,) should be less than time_window_edges: (782,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7165,) should be less than time_window_edges: (743,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (233697,) should be less than time_window_edges: (2357,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14112,) should be less than time_window_edges: (2261,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.041). This used to be enforced but continuing anyway.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.02127659574468085\n",
      "percent_laps_estimated_correctly: 0.02127659574468085\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.495\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "83    0\n",
      "85    0\n",
      "87    0\n",
      "89    0\n",
      "91    0\n",
      "93    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "84    1\n",
      "86    1\n",
      "88    1\n",
      "90    1\n",
      "92    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910000245147935\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8910014385865044\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.4950007992147247\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "laps_paired_tests: [TtestResult(statistic=3.723611692838699, pvalue=0.00041281998981888184, df=65), TtestResult(statistic=3.644543271483432, pvalue=0.0005334675604948362, df=65)]\n",
      "\tdone. building global result.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.891\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:420: UserWarning: n_contours is 0 for level: 0.8912734993944789\n",
      "  warn( f\"n_contours is 0 for level: {levii}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: a_slice is None. 2023-09-25 - Unsure if this is okay, used to be a fatal error.\n",
      "\t done.\n",
      "2024-01-02 - extended_stats _perform_try_computation_if_needed, remove_provided_keys\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - pf_dt_sequential_surprise _perform_try_computation_if_needed, remove_provided_keys\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (89683,) should be less than time_window_edges: (1340,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3869,) should be less than time_window_edges: (1247,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44874,) should be less than time_window_edges: (674,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1861,) should be less than time_window_edges: (579,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (134557,) should be less than time_window_edges: (2012,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5730,) should be less than time_window_edges: (1859,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (89683,) should be less than time_window_edges: (1340,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3506,) should be less than time_window_edges: (1162,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44874,) should be less than time_window_edges: (674,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (3694,) should be less than time_window_edges: (606,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (134557,) should be less than time_window_edges: (2012,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7200,) should be less than time_window_edges: (1816,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (89683,) should be less than time_window_edges: (1340,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7375,) should be less than time_window_edges: (1247,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44874,) should be less than time_window_edges: (674,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (5555,) should be less than time_window_edges: (606,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (134557,) should be less than time_window_edges: (2012,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12930,) should be less than time_window_edges: (1873,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.076). This used to be enforced but continuing anyway.\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.2619047619047619\n",
      "percent_laps_estimated_correctly: 0.2619047619047619\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "73    0\n",
      "75    0\n",
      "77    0\n",
      "79    0\n",
      "81    0\n",
      "83    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "72    1\n",
      "74    1\n",
      "76    1\n",
      "78    1\n",
      "80    1\n",
      "82    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "laps_paired_tests: [TtestResult(statistic=0.8517256763816744, pvalue=0.39764429515127586, df=62), TtestResult(statistic=-5.487303385324744, pvalue=8.022771802160096e-07, df=62)]\n",
      "\tdone. building global result.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ripple_evts_paired_tests: [TtestResult(statistic=-0.7252417912750373, pvalue=0.4741157025234052, df=29), TtestResult(statistic=-1.0268814755870044, pvalue=0.312965788346686, df=29)]\n",
      "\tdone. building global result.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 22, n_all_epoch_timebins = 914)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 22, n_all_epoch_timebins = 914)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t43 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t50 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t19 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t43 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t19 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t29 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t19 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t43 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t43 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t50 remain.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([141.44749774, 147.73754767]), 'short_ylim': array([141.60474899, 147.58029642])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 2/10\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 3/8\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - firing_rate_trends _perform_try_computation_if_needed, remove_provided_keys\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (120546,) should be less than time_window_edges: (4115,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (61833,) should be less than time_window_edges: (1950,)!\n",
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2179,) should be less than time_window_edges: (1779,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (182379,) should be less than time_window_edges: (6063,)!\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.7378933289508005\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.7378933289508005\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (120546,) should be less than time_window_edges: (4115,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (4524,) should be less than time_window_edges: (3014,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (61833,) should be less than time_window_edges: (1950,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (2641,) should be less than time_window_edges: (1734,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (182379,) should be less than time_window_edges: (6063,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7165,) should be less than time_window_edges: (5720,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (120546,) should be less than time_window_edges: (4115,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7420,) should be less than time_window_edges: (3232,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (61833,) should be less than time_window_edges: (1950,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (4820,) should be less than time_window_edges: (1874,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "WARNING: PREVIOUSLY ASSERT: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t spikes_df[time_variable_name]: (182379,) should be less than time_window_edges: (6063,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12240,) should be less than time_window_edges: (5938,)!\n",
      "\t done.\n",
      "2024-01-02 - split_to_directional_laps _perform_try_computation_if_needed, remove_provided_keys\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: True\n",
      "\t done.\n",
      "2024-01-02 - merged_directional_placefields _perform_try_computation_if_needed, remove_provided_keys\n",
      "merged_directional_placefields missing.\n",
      "\t Recomputing merged_directional_placefields...\n",
      "WARN: ripple_decoding_time_bin_size 0.025 < min_possible_time_bin_size (0.03). This used to be enforced but continuing anyway.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "WARN: Laps._compute_lap_dir_from_smoothed_velocity(...): the velocity-determined lap direction (\"is_LR_dir\") substantially differs from the previous (\"lap_dir\") column. This might be because it initially used simple ODD/EVEN determination for the direction.\n",
      "\tWARN: overwriting the \"lap_dir\" column of Laps with the \"is_LR_dir\" column. Do things need to be recomputed after this?\n",
      "percent_laps_estimated_correctly: 0.2638888888888889\n",
      "percent_laps_estimated_correctly: 0.2638888888888889\n",
      "\t done.\n",
      "2024-01-02 - rank_order_shuffle_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "rank_order_shuffle_analysis missing.\n",
      "\t Recomputing rank_order_shuffle_analysis...\n",
      "WARN: perform_rank_order_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_rank_order_shuffle_analysis(..., num_shuffles=500)\n",
      "\tcomputing Laps rank-order shuffles:\n",
      "\t\tnum_shuffles: 500, minimum_inclusion_fr_Hz: 5.0 Hz\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 1     0\n",
      "3     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "25    0\n",
      "27    0\n",
      "29    0\n",
      "31    0\n",
      "33    0\n",
      "35    0\n",
      "37    0\n",
      "39    0\n",
      "41    0\n",
      "43    0\n",
      "45    0\n",
      "47    0\n",
      "49    0\n",
      "51    0\n",
      "53    0\n",
      "55    0\n",
      "57    0\n",
      "59    0\n",
      "61    0\n",
      "63    0\n",
      "65    0\n",
      "67    0\n",
      "69    0\n",
      "71    0\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARNING: No global_session or position passed but replace_existing == True!\n",
      "\tWould replace existing 'lap_dir' column: 0     1\n",
      "2     1\n",
      "4     1\n",
      "6     1\n",
      "8     1\n",
      "10    1\n",
      "12    1\n",
      "14    1\n",
      "16    1\n",
      "18    1\n",
      "20    1\n",
      "22    1\n",
      "24    1\n",
      "26    1\n",
      "28    1\n",
      "30    1\n",
      "32    1\n",
      "34    1\n",
      "36    1\n",
      "38    1\n",
      "40    1\n",
      "42    1\n",
      "44    1\n",
      "46    1\n",
      "48    1\n",
      "50    1\n",
      "52    1\n",
      "54    1\n",
      "56    1\n",
      "58    1\n",
      "60    1\n",
      "62    1\n",
      "64    1\n",
      "66    1\n",
      "68    1\n",
      "70    1\n",
      "Name: lap_dir, dtype: int64 using old even/odd 'lap_dir' determination.\n",
      "WARN: .trim_overlapping_laps(...): need to recompute  ['start_position_index', 'end_position_index', 'start_spike_index', 'end_spike_index', 'num_spikes'] for the laps after calling self.trim_overlapping_laps()!\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laps_paired_tests: [TtestResult(statistic=-1.2653844086811743, pvalue=0.21106817698474545, df=55), TtestResult(statistic=-2.778146262512074, pvalue=0.0074639215246556385, df=55)]\n",
      "\tdone. building global result.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=1.099079705979632, pvalue=0.30370556999509085, df=8), TtestResult(statistic=-0.6913897331879039, pvalue=0.5088834318500286, df=8)]\n",
      "\tdone. building global result.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.7378933289508005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 0/94\n",
      "\tagreeing_rows_ratio: 0.0\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 6/121\n",
      "\tagreeing_rows_ratio: 0.049586776859504134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 18, n_all_epoch_timebins = 446)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 36/94\n",
      "\tagreeing_rows_ratio: 0.3829787234042553\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 32/121\n",
      "\tagreeing_rows_ratio: 0.2644628099173554\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 76/94\n",
      "\tagreeing_rows_ratio: 0.8085106382978723\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 22/121\n",
      "\tagreeing_rows_ratio: 0.18181818181818182\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.370129\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_19-28-0'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "(n_neurons = 18, n_all_epoch_timebins = 446)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_19-28-0...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t19 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t43 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t43 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.003).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.003.\t50 remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x14881b2e1ec0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t41 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t39 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t5 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t15 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t5 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t5 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([138.93147777, 146.63678893]), 'short_ylim': array([139.24598026, 146.32228643])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 0/5\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 5/5\n",
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 4/5\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.720848254723061\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.720848254723061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tcomputing Ripple rank-order shuffles:\n",
      "ERROR! BUG?! 2023-12-21 - Need to update the previous 'DirectionalLaps' object with the qclu-restricted one right? on filter\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.720848254723061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 13/84\n",
      "\tagreeing_rows_ratio: 0.15476190476190477\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 7/48\n",
      "\tagreeing_rows_ratio: 0.14583333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 46/84\n",
      "\tagreeing_rows_ratio: 0.5476190476190477\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 16/48\n",
      "\tagreeing_rows_ratio: 0.3333333333333333\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 50/84\n",
      "\tagreeing_rows_ratio: 0.5952380952380952\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 10/48\n",
      "\tagreeing_rows_ratio: 0.20833333333333334\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.243976\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-03_12-3-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-03_12-3-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x1487bacee280>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "ripple_evts_paired_tests: [TtestResult(statistic=0.8571503228418295, pvalue=0.391842618109586, df=427), TtestResult(statistic=-3.0384723350277634, pvalue=0.002523808909775633, df=427)]\n",
      "\tdone. building global result.\n",
      "n_valid_shuffles: 500\n",
      "combined_variable_names: ['short_RL_spearman', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman', 'long_LR_pearson', 'short_RL_pearson', 'short_LR_pearson', 'long_LR_spearman']\n",
      "combined_variable_z_score_column_names: ['short_RL_spearman_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_pearson_Z', 'short_LR_pearson_Z', 'long_LR_spearman_Z']\n",
      "done!\n",
      "\tdone. building global result.\n",
      "\t done.\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 21, n_all_epoch_timebins = 5002)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 21, n_all_epoch_timebins = 5002)\n",
      "\t done.\n",
      "2024-01-02 - short_long_pf_overlap_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2679: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "2024-01-02 - long_short_fr_indicies_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:660: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t47 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t22 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t207 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t223 remain.\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t207 remain.\n",
      "DropShorterMode:\n",
      "\tminimum_event_duration present (minimum_event_duration=0.01).\n",
      "\tdropping 0 that are shorter than our minimum_event_duration of 0.01.\t306 remain.\n",
      "\t done.\n",
      "2024-01-02 - jonathan_firing_rate_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Time window 79 has no spikes.\n",
      "Time window 444 has no spikes.\n",
      "Time window 452 has no spikes.\n",
      "Time window 529 has no spikes.\n",
      "Time window 690 has no spikes.\n",
      "Time window 724 has no spikes.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "2024-01-02 - long_short_post_decoding _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n",
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n",
      "\t done.\n",
      "2024-01-02 - long_short_inst_spike_rate_groups _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "2024-01-02 - long_short_endcap_analysis _perform_try_computation_if_needed, remove_provided_keys\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "loaded_track_limits: {'long_xlim': array([ 59.07738974, 228.69004399]), 'short_xlim': array([ 94.01556371, 193.7565709 ]), 'long_ylim': array([140.14211943, 147.27457298]), 'short_ylim': array([140.28335614, 146.14467935])}\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 2/7\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 5/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= min_significant_remapping_x_distance) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 4/5\n",
      "\t done.\n",
      "2024-01-02 - directional_train_test_split _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_train_test_split missing.\n",
      "\t Recomputing directional_train_test_split...\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "training_data_portion: 0.9, test_data_portion: 0.09999999999999998\n",
      "a_modern_name: long_LR, old_directional_lap_name: maze1_odd\n",
      "a_modern_name: long_RL, old_directional_lap_name: maze1_even\n",
      "a_modern_name: short_LR, old_directional_lap_name: maze2_odd\n",
      "a_modern_name: short_RL, old_directional_lap_name: maze2_even\n",
      "['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train']\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_decode_continuous _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_decode_continuous missing.\n",
      "\t Recomputing directional_decoders_decode_continuous...\n",
      "should_disable_cache == True so setting had_existing_DirectionalDecodersDecoded_result = False\n",
      "\thad_existing_DirectionalDecodersDecoded_result == False. New DirectionalDecodersDecodedResult will be built...\n",
      "\ttime_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t computation done. Creating new DirectionalDecodersDecodedResult....\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_evaluate_epochs _perform_try_computation_if_needed, remove_provided_keys\n",
      "directional_decoders_evaluate_epochs missing.\n",
      "\t Recomputing directional_decoders_evaluate_epochs...\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.727113289456203\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.727113289456203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:333: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours will be calculated from margin and pos_bin_size. neighbours: 4 = int(margin: 16 / pos_bin_size: 3.727113289456203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 4/72\n",
      "\tagreeing_rows_ratio: 0.05555555555555555\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 81/765\n",
      "\tagreeing_rows_ratio: 0.10588235294117647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/flattened_spiketrains.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._obj[epoch_id_key_name] = no_interval_fill_value # initialize the column to -1\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spk_df[epoch_id_key_name] = spike_epoch_identity_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 25/72\n",
      "\tagreeing_rows_ratio: 0.3472222222222222\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 180/765\n",
      "\tagreeing_rows_ratio: 0.23529411764705882\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 37/72\n",
      "\tagreeing_rows_ratio: 0.5138888888888888\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 217/765\n",
      "\tagreeing_rows_ratio: 0.28366013071895424\n",
      "\t done.\n",
      "2024-01-02 - directional_decoders_epoch_heuristic_scoring _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['DirectionalDecodersEpochsEvaluations'] because force_recompute was True.\n",
      "directional_decoders_epoch_heuristic_scoring missing.\n",
      "\t Recomputing directional_decoders_epoch_heuristic_scoring...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'DirectionalDecodersEpochsEvaluations'\n",
      "/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py:33: KeyError: 'DirectionalDecodersEpochsEvaluations'\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 0:00:00.312485\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/Library/VSCode/green/.venv_green/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'fet11-01_12-58-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:284: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:290: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups failed to save to HDF5 due to type issues and will be skipped. This is usually caused by Python None values. Error Object dtype dtype('O') has no native HDF5 equivalent\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_fet11-01_12-58-54...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: compute_and_export_marginals_dfs_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "compute_and_export_marginals_dfs_completion_function(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'NoneType' object has no attribute 'exists' ::::: (<class 'AttributeError'>, AttributeError(\"'NoneType' object has no attribute 'exists'\"), <traceback object at 0x1488176e0480>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import determine_computation_datetimes_completion_function, compute_and_export_marginals_dfs_completion_function, perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function, compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function, reload_exported_kdiba_session_position_info_mat_completion_function\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# BEGIN USER COMPLETION FUNCTIONS                                                                                      #\n",
    "# ==================================================================================================================== #\n",
    "custom_user_completion_functions = []\n",
    "\n",
    "custom_user_completion_functions.append(compute_and_export_marginals_dfs_completion_function)\n",
    "custom_user_completion_functions.append(perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function)\n",
    "custom_user_completion_functions.append(compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function)\n",
    "# custom_user_completion_functions.append(reload_exported_kdiba_session_position_info_mat_completion_function)\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# END USER COMPLETION FUNCTIONS                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "# %pdb on\n",
    "# multiprocessing_kwargs = dict(use_multiprocessing=False, num_processes=1)\n",
    "multiprocessing_kwargs = dict(use_multiprocessing=True, num_processes=3)\n",
    "\n",
    "# Whether to output figures:\n",
    "should_perform_figure_generation_to_file=False\n",
    "# should_perform_figure_generation_to_file=True\n",
    "\n",
    "## Included Session Contexts:\n",
    "included_session_contexts = included_session_contexts # ALL\n",
    "\n",
    "if included_session_contexts is not None:\n",
    "    print(f'len(included_session_contexts): {len(included_session_contexts)}')\n",
    "else:\n",
    "    print(f'included_session_contexts is None so all session contexts will be included.')\n",
    "\n",
    "# included_session_contexts\n",
    "\n",
    "# # No recomputing at all:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER), # , override_file=\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "# No Reloading\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "                                                session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "                                                force_recompute_override_computations_includelist=None, # ['split_to_directional_laps', 'rank_order_shuffle_analysis']\n",
    "                                                **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "# # Forced Reloading:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=True,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE, force_global_recompute=True,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "active_post_run_callback_fn = result_handler.on_complete_success_execution_session\n",
    "# active_post_run_callback_fn = _temp_on_complete_success_execution_session\n",
    "\n",
    "\n",
    "## Append the desired completion functions:\n",
    "result_handler.completion_functions = custom_user_completion_functions\n",
    "# result_handler.completion_functions.append(compute_and_export_marginals_dfs_completion_function) # determine_computation_datetimes_completion_function\n",
    "# result_handler.completion_functions.append(determine_computation_datetimes_completion_function)\n",
    "\n",
    "\n",
    "## Specific Setup for 2023-09-28 Changes to LxC/SxC \"refinements\"\n",
    "result_handler.extended_computations_include_includelist = ['pf_computation', \n",
    "                                                            'pfdt_computation',\n",
    "                                                            'firing_rate_trends',\n",
    "                                                'pf_dt_sequential_surprise',\n",
    "                                                'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # # 'position_decoding_two_step',\n",
    "                                                'extended_stats',\n",
    "                                                'long_short_decoding_analyses',\n",
    "                                                'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', #, 'long_short_rate_remapping',\n",
    "                                                'long_short_inst_spike_rate_groups',\n",
    "                                                'long_short_endcap_analysis',\n",
    "                                                # 'spike_burst_detection',\n",
    "                                                'split_to_directional_laps',\n",
    "                                                'merged_directional_placefields',\n",
    "                                                'rank_order_shuffle_analysis',\n",
    "                                                'directional_train_test_split',\n",
    "                                                'directional_decoders_decode_continuous',\n",
    "                                                'directional_decoders_evaluate_epochs',    \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'directional_decoders_epoch_heuristic_scoring',\n",
    "                                                ]\n",
    "\n",
    "\n",
    "basic_local_computations = ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
    " \n",
    "# basic_local_computations = [] # set to empty so we don't overwrite these computations\n",
    "# result_handler.extended_computations_include_includelist = ['split_to_directional_laps', 'rank_order_shuffle_analysis'] # OVERRIDE with specific computations to do\n",
    "\n",
    "result_handler.enable_hdf5_output = True # output the HDF5 when done.\n",
    "# result_handler.enable_hdf5_output = False\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "global_batch_run.execute_all(force_reload=result_handler.force_reload_all, saving_mode=result_handler.saving_mode, skip_extended_batch_computations=True, post_run_callback_fn=active_post_run_callback_fn,\n",
    "                             fail_on_exception=False, included_session_contexts=included_session_contexts,\n",
    "                                                                                        **{'computation_functions_name_includelist': basic_local_computations,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}, **multiprocessing_kwargs) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f130e5c9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/global_batch_result_2024-04-11_GL.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/global_batch_result_2024-04-11_GL.pkl... \tmoving new output at '/nfs/turbo/umms-kdiba/Data/20240411182226-global_batch_result_2024-04-11_GL.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/global_batch_result_2024-04-11_GL.pkl'\n",
      "done.\n",
      "done outputting HDF file.\n"
     ]
    }
   ],
   "source": [
    "# Save to pickle:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f74fd4",
   "metadata": {},
   "source": [
    "# 2024-01-18 - Extracts the callback results 'determine_computation_datetimes_completion_function':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5a6244",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_split_csv_path: /nfs/turbo/umms-kdiba/Data/Output/collected_outputs/2024-04-11_GL_t_split_df.csv\n"
     ]
    }
   ],
   "source": [
    "extracted_callback_fn_results = {a_sess_ctxt:a_result.across_session_results.get('determine_session_t_delta_completion_function', {}) for a_sess_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "extracted_callback_fn_results\n",
    "\n",
    "# {'session': list(extracted_callback_fn_results.keys()),\n",
    "# Saves them to dataframe:\n",
    "t_split_df = pd.DataFrame(extracted_callback_fn_results).T # .T to get 't_start', 't_delta', 't_end' as columns instead of rows and the index as the session name\n",
    "t_split_df\n",
    "\n",
    "t_split_csv_path = collected_outputs_path.joinpath(f'{BATCH_DATE_TO_USE}_t_split_df.csv').resolve()\n",
    "print(f't_split_csv_path: {t_split_csv_path}')\n",
    "t_split_df.to_csv(t_split_csv_path)\n",
    "# t_split_df.to_csv(f'output/across_session_results/{BATCH_DATE_TO_USE}_t_split_df.csv')\n",
    "# t_split_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868a1ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recover across_session_results_extended_dict\n",
    "result_handler.across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0981cde1",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [format_name, animal, exper_name, session_name, context, basedirs, status, errors, session_datetime, n_long_laps, n_long_replays, n_short_laps, n_short_replays, is_ready, global_computation_result_file, loaded_session_pickle_file, ripple_result_file, has_user_replay_annotations, has_user_grid_bin_bounds_annotations]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbb196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_files_metadata\n",
    "\n",
    "# # Stores no global times\n",
    "# global_pickle_path = curr_active_pipeline.global_computation_results_pickle_path.resolve()\n",
    "# assert global_pickle_path.exists()\n",
    "# global_pickle_path\n",
    "\n",
    "# file_metadata = FilesystemMetadata.get_file_metadata(global_pickle_path)\n",
    "# file_metadata['modification_time']\n",
    "\n",
    "collected_global_computations_pickle_paths: List[Path] = good_only_batch_progress_df['global_computation_result_file'].to_list()\n",
    "\n",
    "get_files_metadata(collected_global_computations_pickle_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import os\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "## recieves: max_parallel_executions, script_paths\n",
    "\"\"\"\n",
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 5\n",
    "# List of your script paths\n",
    "script_paths = output_python_scripts\n",
    "\"\"\"\n",
    "\n",
    "# Function to execute a script\n",
    "def run_script(script_path):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.isfile(script_path):\n",
    "            return script_path, None, f\"Error: File {script_path} does not exist\"\n",
    "\n",
    "        result = subprocess.run(['python', script_path], capture_output=True, text=True, timeout=1800) # 30 minutes timeout\n",
    "        return script_path, result.stdout, result.stderr\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        return script_path, None, f\"FileNotFoundError: {str(fnf_error)}\"\n",
    "    except subprocess.TimeoutExpired as timeout_error:\n",
    "        return script_path, None, f\"TimeoutError: {str(timeout_error)}\"\n",
    "    except Exception as e:\n",
    "        return script_path, None, f\"Error: {str(e)}\\nTraceback: {traceback.format_exc()}\"\n",
    "\n",
    "# Retry mechanism\n",
    "num_retries = 3\n",
    "retry_count = {path : 0 for path in script_paths}\n",
    "\n",
    "# Run scripts in parallel with a limit on the number of parallel instances\n",
    "with ProcessPoolExecutor(max_workers=max_parallel_executions) as executor:\n",
    "    futures = {executor.submit(run_script, path): path for path in script_paths}\n",
    "    for future in as_completed(futures):\n",
    "        script, stdout, stderr = future.result()\n",
    "        # Retry if error and retry count is not exceeded.\n",
    "        if stderr and retry_count[script] < num_retries:\n",
    "            retry_count[script] += 1\n",
    "            futures[executor.submit(run_script, script)] = script\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Across Sessions After Batching Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dce885b-5b99-4a7a-9f72-2eed2e45ae18",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5',\n",
       " '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch_progress_df = included_session_batch_progress_df.copy()\n",
    "\n",
    "good_session_concrete_folders = [ConcreteSessionFolder(a_context, a_basedir) for a_context, a_basedir in zip(list(a_batch_progress_df.context.values), list(a_batch_progress_df.basedirs.values))]\n",
    "\n",
    "# good_only_batch_progress_df.batch_results\n",
    "# included_h5_paths = [get_file_str_if_file_exists(v.joinpath('output','pipeline_results.h5').resolve()) for v in list(good_only_batch_progress_df.basedirs.values)]\n",
    "# included_h5_paths = [a_dir.joinpath('output','pipeline_results.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']]\n",
    "included_h5_paths = [get_file_str_if_file_exists(v.pipeline_results_h5) for v in good_session_concrete_folders]\n",
    "included_h5_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39738fa-f8f3-46d2-a937-0fb08c0ccb43",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results_2024-04-11_GL.pkl')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_dir = Path('output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-10-03').resolve()\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, target_dir=target_dir)\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-05', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix=BATCH_DATE_TO_USE, only_include_file_types=['local_pkl', 'global_pkl'])\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-07', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "copy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "135cb2d8-65b3-405b-a41b-22b2fa7cb28e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "copying \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl\"\n",
      "\t\t -> \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results_2024-04-11_GL.pkl\"...\n",
      "done.\n",
      "done copying 30 of 30 files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle_2024-04-11_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results_2024-04-11_GL.pkl')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moved_files_dict_h5_files = copy_movedict(copy_dict)\n",
    "moved_files_dict_h5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243e3954-15a4-449c-b927-56d5d79153c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved_files_copydict_file_path: /nfs/turbo/umms-kdiba/Data/backed_up_files_copydict_2024-04-11_GL.csv\n",
      "src_file, operation, _out_path\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle_2024-04-11_GL.pkl\"\n",
      "\"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl\", \"->\", \"/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results_2024-04-11_GL.pkl\"\n",
      "saving out to \"/nfs/turbo/umms-kdiba/Data/backed_up_files_copydict_2024-04-11_GL.csv\"...\n"
     ]
    }
   ],
   "source": [
    "moved_files_copydict_output_filename=f'backed_up_files_copydict_{BATCH_DATE_TO_USE}.csv'\n",
    "moved_files_copydict_file_path = Path(global_data_root_parent_path).joinpath(moved_files_copydict_output_filename).resolve() # Use Default\n",
    "print(f'moved_files_copydict_file_path: {moved_files_copydict_file_path}')\n",
    "\n",
    "_out_string, filedict_out_path = save_copydict_to_text_file(moved_files_dict_h5_files, moved_files_copydict_file_path, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab869730",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_moved_files_dict_files = read_copydict_from_text_file(moved_files_copydict_file_path, debug_print=False)\n",
    "read_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_moved_files_dict_files\n",
    "restore_moved_files_dict_files = invert_filedict(read_moved_files_dict_files)\n",
    "restore_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d067d8f2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>modification_time</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>file_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>2024-04-11 13:21:17.901109</td>\n",
       "      <td>2024-04-11 13:21:17.901109</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>2024-04-11 12:43:00.500121</td>\n",
       "      <td>2024-04-11 12:43:00.500121</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>2024-04-11 13:17:19.229701</td>\n",
       "      <td>2024-04-11 13:17:19.229701</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>2024-04-11 14:00:12.742732</td>\n",
       "      <td>2024-04-11 14:00:12.742732</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>2024-04-11 16:08:38.360741</td>\n",
       "      <td>2024-04-11 16:08:38.360741</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>2024-04-11 14:23:06.206275</td>\n",
       "      <td>2024-04-11 14:23:06.206275</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>2024-04-11 14:54:27.331616</td>\n",
       "      <td>2024-04-11 14:54:27.331616</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>2024-04-11 15:15:58.312797</td>\n",
       "      <td>2024-04-11 15:15:58.312797</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>2024-04-11 15:37:31.133416</td>\n",
       "      <td>2024-04-11 15:37:31.133416</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>2024-04-11 16:14:14.538885</td>\n",
       "      <td>2024-04-11 16:14:14.538885</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>2024-04-11 16:15:50.236778</td>\n",
       "      <td>2024-04-11 16:16:20.176890</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>2024-04-11 16:50:37.158819</td>\n",
       "      <td>2024-04-11 16:50:37.158819</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>2024-04-11 16:55:56.406086</td>\n",
       "      <td>2024-04-11 16:56:26.101935</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>2024-04-11 18:22:14.788078</td>\n",
       "      <td>2024-04-11 18:22:14.788078</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "5   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "7   /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "9   /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "10  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "11  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "12  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "13  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "            modification_time              creation_time  file_size  \n",
       "0  2024-04-11 13:21:17.901109 2024-04-11 13:21:17.901109       1.13  \n",
       "1  2024-04-11 12:43:00.500121 2024-04-11 12:43:00.500121       1.00  \n",
       "2  2024-04-11 13:17:19.229701 2024-04-11 13:17:19.229701       1.06  \n",
       "3  2024-04-11 14:00:12.742732 2024-04-11 14:00:12.742732       0.72  \n",
       "4  2024-04-11 16:08:38.360741 2024-04-11 16:08:38.360741       1.37  \n",
       "5  2024-04-11 14:23:06.206275 2024-04-11 14:23:06.206275       0.34  \n",
       "6  2024-04-11 14:54:27.331616 2024-04-11 14:54:27.331616       0.36  \n",
       "7  2024-04-11 15:15:58.312797 2024-04-11 15:15:58.312797       0.31  \n",
       "8  2024-04-11 15:37:31.133416 2024-04-11 15:37:31.133416       0.38  \n",
       "9  2024-04-11 16:14:14.538885 2024-04-11 16:14:14.538885       0.34  \n",
       "10 2024-04-11 16:15:50.236778 2024-04-11 16:16:20.176890       0.48  \n",
       "11 2024-04-11 16:50:37.158819 2024-04-11 16:50:37.158819       0.31  \n",
       "12 2024-04-11 16:55:56.406086 2024-04-11 16:56:26.101935       0.23  \n",
       "13 2024-04-11 18:22:14.788078 2024-04-11 18:22:14.788078       0.56  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_output_h5_files(included_h5_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0953",
   "metadata": {},
   "source": [
    "## Extract `across_sessions_instantaneous_fr_dict` from the computation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39691fe2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sessions: 0\n"
     ]
    }
   ],
   "source": [
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "across_sessions_recomputed_instantaneous_fr_dict = {}\n",
    "\n",
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "        a_good_recomp_result = a_result.across_session_results.get('recomputed_inst_fr_comps', None)\n",
    "        if a_good_recomp_result is not None:\n",
    "            across_sessions_recomputed_instantaneous_fr_dict[a_ctxt] = a_good_recomp_result\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "\n",
    "## Outputs: across_sessions_instantaneous_fr_dict, across_sessions_recomputed_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbeb9efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "across_sessions_recomputed_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09136799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "# AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "#                                                  inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "across_session_result_long_short_recomputed_inst_firing_rate_filename: str = f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_recomputed_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "                                                 inst_fr_output_filename=across_session_result_long_short_recomputed_inst_firing_rate_filename)\n",
    "\n",
    "\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c123baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed for file path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table. wth exception group ``/`` does not have a child named ``/kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table``. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "failed for file path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/long_short_fr_indicies_analysis/table. wth exception group ``/`` does not have a child named ``/kdiba/gor01/one/2006-6-12_15-55-31/global_computations/long_short_fr_indicies_analysis/table``. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "failed for file path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/jonathan_fr_analysis/neuron_replay_stats_df/table. wth exception group ``/`` does not have a child named ``/kdiba/gor01/one/2006-6-12_15-55-31/global_computations/jonathan_fr_analysis/neuron_replay_stats_df/table``. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "a_name: neuron_identities_table\n",
      "writing /nfs/turbo/umms-kdiba/Data/2024-04-11_GL/neuron_identities_table.pkl.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/2024-04-11_GL/neuron_identities_table.pkl... done.\n",
      "a_name: long_short_fr_indicies_analysis_table\n",
      "writing /nfs/turbo/umms-kdiba/Data/2024-04-11_GL/long_short_fr_indicies_analysis_table.pkl.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/2024-04-11_GL/long_short_fr_indicies_analysis_table.pkl... done.\n",
      "a_name: neuron_replay_stats_table\n",
      "writing /nfs/turbo/umms-kdiba/Data/2024-04-11_GL/neuron_replay_stats_table.pkl.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/2024-04-11_GL/neuron_replay_stats_table.pkl... done.\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "# neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True, )\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_and_save_all_combined_tables(included_session_contexts, included_h5_paths, override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'{BATCH_DATE_TO_USE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a859bff-8cdb-4281-a64f-251d24db7cb9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed for file path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table. wth exception group ``/`` does not have a child named ``/kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table``. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "failed for file path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/long_short_fr_indicies_analysis/table. wth exception group ``/`` does not have a child named ``/kdiba/gor01/one/2006-6-12_15-55-31/global_computations/long_short_fr_indicies_analysis/table``. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n",
      "failed for file path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, table_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/jonathan_fr_analysis/neuron_replay_stats_df/table. wth exception group ``/`` does not have a child named ``/kdiba/gor01/one/2006-6-12_15-55-31/global_computations/jonathan_fr_analysis/neuron_replay_stats_df/table``. Skipping.\n",
      "concatenating dataframes from 14 of 15 files\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True)\n",
    "# neuron_replay_stats_table['is_refined_LxC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b2430-c9ec-4adb-81a8-1ffbf8a0cb3c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95013ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_identities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(neuron_replay_stats_table['is_refined_LxC'])\n",
    "# np.isnan(neuron_replay_stats_table['is_refined_LxC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>session_name</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>track_membership</th>\n",
       "      <th>is_refined_LxC</th>\n",
       "      <th>is_refined_SxC</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>2006-06-12 16:53:46</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>2006-06-12 16:53:46</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>2006-06-12 16:53:46</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal        session_name    session_datetime  \\\n",
       "0        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "1        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "2        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "3        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "4        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "5        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "6        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "7        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "8        kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "9        kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "10       kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "11       kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "12       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "13       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "14       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "15       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "16       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "17       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "18       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "19       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "20       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "21       kdiba  gor01  2006-6-12_16-53-46 2006-06-12 16:53:46   \n",
       "22       kdiba  gor01  2006-6-12_16-53-46 2006-06-12 16:53:46   \n",
       "23       kdiba  gor01  2006-6-12_16-53-46 2006-06-12 16:53:46   \n",
       "24       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "25       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "26       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "27       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "28       kdiba  pin01       11-02_19-28-0 2009-11-02 19:28:00   \n",
       "29       kdiba  pin01       11-02_19-28-0 2009-11-02 19:28:00   \n",
       "30       kdiba  pin01       11-03_12-3-25 2009-11-03 12:03:25   \n",
       "31       kdiba  pin01       11-03_12-3-25 2009-11-03 12:03:25   \n",
       "32       kdiba  pin01       11-03_12-3-25 2009-11-03 12:03:25   \n",
       "33       kdiba  pin01   fet11-01_12-58-54 2009-11-01 12:58:54   \n",
       "34       kdiba  pin01   fet11-01_12-58-54 2009-11-01 12:58:54   \n",
       "35       kdiba  pin01   fet11-01_12-58-54 2009-11-01 12:58:54   \n",
       "36       kdiba  vvp01  2006-4-09_16-40-54 2006-04-09 16:40:54   \n",
       "37       kdiba  vvp01  2006-4-09_16-40-54 2006-04-09 16:40:54   \n",
       "38       kdiba  vvp01  2006-4-09_16-40-54 2006-04-09 16:40:54   \n",
       "39       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "40       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "41       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "42       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "43       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "44       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "45       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "46       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "47       kdiba  vvp01   2006-4-10_12-58-3 2006-04-10 12:58:03   \n",
       "48       kdiba  vvp01   2006-4-10_12-58-3 2006-04-10 12:58:03   \n",
       "49       kdiba  vvp01   2006-4-10_12-58-3 2006-04-10 12:58:03   \n",
       "\n",
       "   track_membership  is_refined_LxC  is_refined_SxC  count  \n",
       "0         LEFT_ONLY           False           False      1  \n",
       "1        RIGHT_ONLY           False           False      2  \n",
       "2        RIGHT_ONLY           False            True      3  \n",
       "3            SHARED           False           False     58  \n",
       "4         LEFT_ONLY           False           False      7  \n",
       "5        RIGHT_ONLY           False           False      5  \n",
       "6        RIGHT_ONLY           False            True      2  \n",
       "7            SHARED           False           False     94  \n",
       "8         LEFT_ONLY           False           False     10  \n",
       "9        RIGHT_ONLY           False           False      3  \n",
       "10       RIGHT_ONLY           False            True      3  \n",
       "11           SHARED           False           False     86  \n",
       "12        LEFT_ONLY           False           False      7  \n",
       "13       RIGHT_ONLY           False           False      2  \n",
       "14       RIGHT_ONLY           False            True      3  \n",
       "15           SHARED           False           False     91  \n",
       "16        LEFT_ONLY           False           False      4  \n",
       "17        LEFT_ONLY            True           False      1  \n",
       "18       RIGHT_ONLY           False           False      2  \n",
       "19       RIGHT_ONLY           False            True      3  \n",
       "20           SHARED           False           False     86  \n",
       "21        LEFT_ONLY           False           False      5  \n",
       "22       RIGHT_ONLY           False           False      2  \n",
       "23           SHARED           False           False     52  \n",
       "24        LEFT_ONLY           False           False      6  \n",
       "25       RIGHT_ONLY           False           False      3  \n",
       "26       RIGHT_ONLY           False            True      4  \n",
       "27           SHARED           False           False     29  \n",
       "28        LEFT_ONLY           False           False      4  \n",
       "29           SHARED           False           False     34  \n",
       "30        LEFT_ONLY           False           False      3  \n",
       "31       RIGHT_ONLY           False           False      3  \n",
       "32           SHARED           False           False     23  \n",
       "33        LEFT_ONLY           False           False      5  \n",
       "34       RIGHT_ONLY           False           False      1  \n",
       "35           SHARED           False           False     25  \n",
       "36        LEFT_ONLY           False           False      5  \n",
       "37       RIGHT_ONLY           False           False      1  \n",
       "38           SHARED           False           False     29  \n",
       "39        LEFT_ONLY           False           False      6  \n",
       "40       RIGHT_ONLY           False           False      3  \n",
       "41       RIGHT_ONLY           False            True      2  \n",
       "42           SHARED           False           False     33  \n",
       "43        LEFT_ONLY           False           False      4  \n",
       "44       RIGHT_ONLY           False           False      1  \n",
       "45       RIGHT_ONLY           False            True      1  \n",
       "46           SHARED           False           False     32  \n",
       "47        LEFT_ONLY           False           False      6  \n",
       "48       RIGHT_ONLY           False           False      2  \n",
       "49           SHARED           False           False     38  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "# _out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "# _out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership','is_refined_LxC', 'is_refined_SxC'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'is_refined_LxC', 'is_refined_SxC', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6af57298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>session_name</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>track_membership</th>\n",
       "      <th>is_refined_LxC</th>\n",
       "      <th>is_refined_SxC</th>\n",
       "      <th>count</th>\n",
       "      <th>session_datetime_first</th>\n",
       "      <th>time_since_first_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 21:45:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 21:45:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 21:45:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0 days 21:45:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 04:36:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 04:36:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 04:36:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>86</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 04:36:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 08:42:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 08:42:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 08:42:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>91</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>1 days 08:42:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>2 days 05:44:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>2 days 05:44:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>2 days 05:44:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>2 days 05:44:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>86</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>2 days 05:44:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>2006-06-12 16:53:46</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>5 days 00:13:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>2006-06-12 16:53:46</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>5 days 00:13:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>2006-06-12 16:53:46</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>5 days 00:13:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 01:41:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>34</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 01:41:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 18:16:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 18:16:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0 days 18:16:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>-2 days +19:12:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>-2 days +19:12:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>-2 days +19:12:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 00:48:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 00:48:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 00:48:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 00:48:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 19:44:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 19:44:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 19:44:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 19:44:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>LEFT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 20:17:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>RIGHT_ONLY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 20:17:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>SHARED</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>38</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>0 days 20:17:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal        session_name    session_datetime  \\\n",
       "0        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "1        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "2        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "3        kdiba  gor01  2006-6-07_16-40-19 2006-06-07 16:40:19   \n",
       "4        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "5        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "6        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "7        kdiba  gor01  2006-6-08_14-26-15 2006-06-08 14:26:15   \n",
       "8        kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "9        kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "10       kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "11       kdiba  gor01  2006-6-08_21-16-25 2006-06-08 21:16:25   \n",
       "12       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "13       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "14       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "15       kdiba  gor01   2006-6-09_1-22-43 2006-06-09 01:22:43   \n",
       "16       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "17       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "18       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "19       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "20       kdiba  gor01  2006-6-09_22-24-40 2006-06-09 22:24:40   \n",
       "21       kdiba  gor01  2006-6-12_16-53-46 2006-06-12 16:53:46   \n",
       "22       kdiba  gor01  2006-6-12_16-53-46 2006-06-12 16:53:46   \n",
       "23       kdiba  gor01  2006-6-12_16-53-46 2006-06-12 16:53:46   \n",
       "24       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "25       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "26       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "27       kdiba  pin01      11-02_17-46-44 2009-11-02 17:46:44   \n",
       "28       kdiba  pin01       11-02_19-28-0 2009-11-02 19:28:00   \n",
       "29       kdiba  pin01       11-02_19-28-0 2009-11-02 19:28:00   \n",
       "30       kdiba  pin01       11-03_12-3-25 2009-11-03 12:03:25   \n",
       "31       kdiba  pin01       11-03_12-3-25 2009-11-03 12:03:25   \n",
       "32       kdiba  pin01       11-03_12-3-25 2009-11-03 12:03:25   \n",
       "33       kdiba  pin01   fet11-01_12-58-54 2009-11-01 12:58:54   \n",
       "34       kdiba  pin01   fet11-01_12-58-54 2009-11-01 12:58:54   \n",
       "35       kdiba  pin01   fet11-01_12-58-54 2009-11-01 12:58:54   \n",
       "36       kdiba  vvp01  2006-4-09_16-40-54 2006-04-09 16:40:54   \n",
       "37       kdiba  vvp01  2006-4-09_16-40-54 2006-04-09 16:40:54   \n",
       "38       kdiba  vvp01  2006-4-09_16-40-54 2006-04-09 16:40:54   \n",
       "39       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "40       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "41       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "42       kdiba  vvp01  2006-4-09_17-29-30 2006-04-09 17:29:30   \n",
       "43       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "44       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "45       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "46       kdiba  vvp01  2006-4-10_12-25-50 2006-04-10 12:25:50   \n",
       "47       kdiba  vvp01   2006-4-10_12-58-3 2006-04-10 12:58:03   \n",
       "48       kdiba  vvp01   2006-4-10_12-58-3 2006-04-10 12:58:03   \n",
       "49       kdiba  vvp01   2006-4-10_12-58-3 2006-04-10 12:58:03   \n",
       "\n",
       "   track_membership  is_refined_LxC  is_refined_SxC  count  \\\n",
       "0         LEFT_ONLY           False           False      1   \n",
       "1        RIGHT_ONLY           False           False      2   \n",
       "2        RIGHT_ONLY           False            True      3   \n",
       "3            SHARED           False           False     58   \n",
       "4         LEFT_ONLY           False           False      7   \n",
       "5        RIGHT_ONLY           False           False      5   \n",
       "6        RIGHT_ONLY           False            True      2   \n",
       "7            SHARED           False           False     94   \n",
       "8         LEFT_ONLY           False           False     10   \n",
       "9        RIGHT_ONLY           False           False      3   \n",
       "10       RIGHT_ONLY           False            True      3   \n",
       "11           SHARED           False           False     86   \n",
       "12        LEFT_ONLY           False           False      7   \n",
       "13       RIGHT_ONLY           False           False      2   \n",
       "14       RIGHT_ONLY           False            True      3   \n",
       "15           SHARED           False           False     91   \n",
       "16        LEFT_ONLY           False           False      4   \n",
       "17        LEFT_ONLY            True           False      1   \n",
       "18       RIGHT_ONLY           False           False      2   \n",
       "19       RIGHT_ONLY           False            True      3   \n",
       "20           SHARED           False           False     86   \n",
       "21        LEFT_ONLY           False           False      5   \n",
       "22       RIGHT_ONLY           False           False      2   \n",
       "23           SHARED           False           False     52   \n",
       "24        LEFT_ONLY           False           False      6   \n",
       "25       RIGHT_ONLY           False           False      3   \n",
       "26       RIGHT_ONLY           False            True      4   \n",
       "27           SHARED           False           False     29   \n",
       "28        LEFT_ONLY           False           False      4   \n",
       "29           SHARED           False           False     34   \n",
       "30        LEFT_ONLY           False           False      3   \n",
       "31       RIGHT_ONLY           False           False      3   \n",
       "32           SHARED           False           False     23   \n",
       "33        LEFT_ONLY           False           False      5   \n",
       "34       RIGHT_ONLY           False           False      1   \n",
       "35           SHARED           False           False     25   \n",
       "36        LEFT_ONLY           False           False      5   \n",
       "37       RIGHT_ONLY           False           False      1   \n",
       "38           SHARED           False           False     29   \n",
       "39        LEFT_ONLY           False           False      6   \n",
       "40       RIGHT_ONLY           False           False      3   \n",
       "41       RIGHT_ONLY           False            True      2   \n",
       "42           SHARED           False           False     33   \n",
       "43        LEFT_ONLY           False           False      4   \n",
       "44       RIGHT_ONLY           False           False      1   \n",
       "45       RIGHT_ONLY           False            True      1   \n",
       "46           SHARED           False           False     32   \n",
       "47        LEFT_ONLY           False           False      6   \n",
       "48       RIGHT_ONLY           False           False      2   \n",
       "49           SHARED           False           False     38   \n",
       "\n",
       "   session_datetime_first time_since_first_session  \n",
       "0     2006-06-07 16:40:19          0 days 00:00:00  \n",
       "1     2006-06-07 16:40:19          0 days 00:00:00  \n",
       "2     2006-06-07 16:40:19          0 days 00:00:00  \n",
       "3     2006-06-07 16:40:19          0 days 00:00:00  \n",
       "4     2006-06-07 16:40:19          0 days 21:45:56  \n",
       "5     2006-06-07 16:40:19          0 days 21:45:56  \n",
       "6     2006-06-07 16:40:19          0 days 21:45:56  \n",
       "7     2006-06-07 16:40:19          0 days 21:45:56  \n",
       "8     2006-06-07 16:40:19          1 days 04:36:06  \n",
       "9     2006-06-07 16:40:19          1 days 04:36:06  \n",
       "10    2006-06-07 16:40:19          1 days 04:36:06  \n",
       "11    2006-06-07 16:40:19          1 days 04:36:06  \n",
       "12    2006-06-07 16:40:19          1 days 08:42:24  \n",
       "13    2006-06-07 16:40:19          1 days 08:42:24  \n",
       "14    2006-06-07 16:40:19          1 days 08:42:24  \n",
       "15    2006-06-07 16:40:19          1 days 08:42:24  \n",
       "16    2006-06-07 16:40:19          2 days 05:44:21  \n",
       "17    2006-06-07 16:40:19          2 days 05:44:21  \n",
       "18    2006-06-07 16:40:19          2 days 05:44:21  \n",
       "19    2006-06-07 16:40:19          2 days 05:44:21  \n",
       "20    2006-06-07 16:40:19          2 days 05:44:21  \n",
       "21    2006-06-07 16:40:19          5 days 00:13:27  \n",
       "22    2006-06-07 16:40:19          5 days 00:13:27  \n",
       "23    2006-06-07 16:40:19          5 days 00:13:27  \n",
       "24    2009-11-02 17:46:44          0 days 00:00:00  \n",
       "25    2009-11-02 17:46:44          0 days 00:00:00  \n",
       "26    2009-11-02 17:46:44          0 days 00:00:00  \n",
       "27    2009-11-02 17:46:44          0 days 00:00:00  \n",
       "28    2009-11-02 17:46:44          0 days 01:41:16  \n",
       "29    2009-11-02 17:46:44          0 days 01:41:16  \n",
       "30    2009-11-02 17:46:44          0 days 18:16:41  \n",
       "31    2009-11-02 17:46:44          0 days 18:16:41  \n",
       "32    2009-11-02 17:46:44          0 days 18:16:41  \n",
       "33    2009-11-02 17:46:44        -2 days +19:12:10  \n",
       "34    2009-11-02 17:46:44        -2 days +19:12:10  \n",
       "35    2009-11-02 17:46:44        -2 days +19:12:10  \n",
       "36    2006-04-09 16:40:54          0 days 00:00:00  \n",
       "37    2006-04-09 16:40:54          0 days 00:00:00  \n",
       "38    2006-04-09 16:40:54          0 days 00:00:00  \n",
       "39    2006-04-09 16:40:54          0 days 00:48:36  \n",
       "40    2006-04-09 16:40:54          0 days 00:48:36  \n",
       "41    2006-04-09 16:40:54          0 days 00:48:36  \n",
       "42    2006-04-09 16:40:54          0 days 00:48:36  \n",
       "43    2006-04-09 16:40:54          0 days 19:44:56  \n",
       "44    2006-04-09 16:40:54          0 days 19:44:56  \n",
       "45    2006-04-09 16:40:54          0 days 19:44:56  \n",
       "46    2006-04-09 16:40:54          0 days 19:44:56  \n",
       "47    2006-04-09 16:40:54          0 days 20:17:09  \n",
       "48    2006-04-09 16:40:54          0 days 20:17:09  \n",
       "49    2006-04-09 16:40:54          0 days 20:17:09  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3537135/4006058737.py:25: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,) (14,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb Cell 37\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m num_Shared \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mtrack_membership\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSHARED\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m num_SxCs \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mtrack_membership\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRIGHT_ONLY\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m num_TotalCs \u001b[39m=\u001b[39m num_LxCs \u001b[39m+\u001b[39;49m num_Shared \u001b[39m+\u001b[39m num_SxCs\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m num_TotalCs\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,) (14,) "
     ]
    }
   ],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: missing 3 keys from context: ['animal', 'exper_name', 'session_name']. Building path anyway.\n",
      "register_output_file(output_path: /home/halechr/repos/Spike3D/EXTERNAL/Screenshots/ProgrammaticDisplayFunctionTesting/2024-04-11/kdiba/0_display_long_short_laps.png, ...)\n",
      "\t saved \"file:///home/halechr/repos/Spike3D/EXTERNAL/Screenshots/ProgrammaticDisplayFunctionTesting/2024-04-11/kdiba/0_display_long_short_laps.png\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47cb0cd9-3e60-4425-9351-dfc903f3f067",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving out to \"/nfs/turbo/umms-kdiba/Data/fileList_Greatlakes_HDF5_2024-04-11_GL.txt\"...\n",
      "\n",
      "saving out to \"/nfs/turbo/umms-kdiba/Data/fileList_Greatlakes_pkls_2024-04-11_GL.txt\"...\n",
      "\n",
      "saving out to \"/nfs/turbo/umms-kdiba/Data/dest_fileList_Apogee_2024-04-11_GL.txt\"...\n"
     ]
    }
   ],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import convert_filelist_to_new_parent\n",
    "\n",
    "def save_filelist_to_text_file(hdf5_output_paths, filelist_path: Path):\n",
    "    _out_string = '\\n'.join([str(a_file) for a_file in hdf5_output_paths])\n",
    "    print(f'{_out_string}')\n",
    "    print(f'saving out to \"{filelist_path}\"...')\n",
    "    with open(filelist_path, 'w') as f:\n",
    "        f.write(_out_string)\n",
    "    return _out_string, filelist_path\n",
    "\n",
    "# Save output filelist:\n",
    "\n",
    "# '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'\n",
    "\n",
    "# kdiba_vvp01_two_2006-4-10_12-58-3\n",
    "# \toutputs_local ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl')}\n",
    "# \toutputs_global ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'), 'hdf5': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5')}\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths = global_batch_run.build_output_files_lists()\n",
    "\n",
    "h5_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_HDF5_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_HDF5_savepath = save_filelist_to_text_file(hdf5_output_paths, h5_filelist_path)\n",
    "\n",
    "pkls_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_pkls_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_pkls_savepath = save_filelist_to_text_file(pkl_output_paths, pkls_filelist_path)\n",
    "\n",
    "# source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "source_parent_path = Path(r'/nfs/turbo/umms-kdiba/Data')\n",
    "dest_parent_path = Path(r'/~/W/Data/')\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_source = hdf5_output_paths\n",
    "filelist_dest_paths = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest_paths\n",
    "\n",
    "dest_Apogee_h5_filelist_path = global_data_root_parent_path.joinpath(f'dest_fileList_Apogee_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, dest_filelist_savepath = save_filelist_to_text_file(filelist_dest_paths, dest_Apogee_h5_filelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a8e69a3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done outputting HDF file.\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "file_path\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282ce774-98fb-4e69-a7e2-e94cbff1b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sessions: 0\n",
      "global_batch_result_inst_fr_file_path: /nfs/turbo/umms-kdiba/Data/across_session_result_long_short_inst_firing_rate_2024-04-11_GL.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n"
     ]
    }
   ],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "\n",
    "# list(global_batch_run.session_batch_outputs.keys())\n",
    "\n",
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "# good_session_batch_outputs = global_batch_run.session_batch_outputs\n",
    "\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d19a7-5a89-43f1-aa33-3a7450d1f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178426c-54df-47ac-8103-a66f114c77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[a_ctxt.get_initialization_code_string() for a_ctxt in sessions_with_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28828512",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d9a7",
   "metadata": {},
   "source": [
    "# 2023-10-06 - `joined_neruon_fri_df` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE = '2023-10-05_NewParameters'\n",
    "BATCH_DATE_TO_USE = '2023-10-07'\n",
    "all_sessions_joined_neruon_fri_df, out_path = build_and_merge_all_sessions_joined_neruon_fri_df(global_data_root_parent_path, BATCH_DATE_TO_USE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_neruon_fri_df_basename = f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df'\n",
    "AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=joined_neruon_fri_df_basename, include_csv=False)\n",
    "print(f'>>\\t done with {output_file_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c4c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-10-04 - Load Saved across-sessions-data and testing Batch-computed inst_firing_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "# from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PaperFigureTwo, InstantaneousSpikeRateGroupsComputation\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "# from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import list_of_dicts_to_dict_of_lists\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34549c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved across-session results:\n",
    "# inst_fr_output_filename = 'long_short_inst_firing_rate_result_handlers_2023-07-12.pkl'\n",
    "# inst_fr_output_filename = 'across_session_result_long_short_inst_firing_rate.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-07-21.pkl'\n",
    "# inst_fr_output_filename=f'across_session_result_handler_{BATCH_DATE_TO_USE}.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-08-09_Test.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-10-04-GL.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-04-GL-Recomp.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-07.pkl'\n",
    "inst_fr_output_filename: str = f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abea5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    " \n",
    "## Load all across-session tables from the pickles:\n",
    "# output_path_suffix: str = f'2023-10-07'\n",
    "output_path_suffix: str = f'{BATCH_DATE_TO_USE}'\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.load_all_combined_tables(override_output_parent_path=global_data_root_parent_path, output_path_suffix=output_path_suffix) # output_path_suffix=f'2023-10-04-GL-Recomp'\n",
    "num_sessions = len(neuron_replay_stats_table.session_uid.unique().to_numpy())\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager, SessionCellExclusivityRecord\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "annotation_man = UserAnnotationsManager()\n",
    "\n",
    "LxC_uids = []\n",
    "SxC_uids = []\n",
    "\n",
    "for a_ctxt in included_session_contexts:\n",
    "\tsession_uid = a_ctxt.get_description(separator=\"|\", include_property_names=False)\n",
    "\tsession_uid\n",
    "\tsession_cell_exclusivity: SessionCellExclusivityRecord = annotation_man.annotations[a_ctxt].get('session_cell_exclusivity', None)\n",
    "\tLxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.LxC])\n",
    "\tSxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.SxC])\n",
    "\t\n",
    "# [a_ctxt.get_description(separator=\"|\", include_property_names=False) for a_ctxt in included_session_contexts]\n",
    "\n",
    "long_short_fr_indicies_analysis_table['XxC_status'] = 'Shared'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, LxC_uids), 'XxC_status'] = 'LxC'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, SxC_uids), 'XxC_status'] = 'SxC'\n",
    "\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Get the long peak location\n",
    "\n",
    "long_short_fr_indicies_analysis_table['long_pf_peak_x'] = neuron_replay_stats_table['long_pf_peak_x']\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1641aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='x_frs_index', title='Pf Peak position vs. LapsFRI', ylabel='Lap FRI')\n",
    "\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='y_frs_index', title='Pf Peak position vs. ReplayFRI', ylabel='Replay FRI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9987dc",
   "metadata": {},
   "source": [
    " #TODO 2023-10-05 11:40: - [ ] Extract the \"contrarian cells\", the ones that have a strong exclusivity on the laps but the opposite tendency on the replays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_short_fr_indicies_analysis_table_filename = 'output/2023-10-07_long_short_fr_indicies_analysis_table.csv'\n",
    "long_short_fr_indicies_analysis_table_filename: str = 'output/{BATCH_DATE_TO_USE}_long_short_fr_indicies_analysis_table.csv'\n",
    "long_short_fr_indicies_analysis_table.to_csv(long_short_fr_indicies_analysis_table_filename)\n",
    "print(f'saved: {long_short_fr_indicies_analysis_table_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427597f",
   "metadata": {},
   "source": [
    "# 2023-10-10 - Statistics for `across_sessions_bar_graphs`, analysing `across_session_inst_fr_computation` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e33422d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_total: 832, n_above_diagonal: 459, n_exact_on_diagonal: 0, n_below_diagonal: 373\n",
      "binom_test_chance_result: BinomTestResult(k=459, n=832, alternative='two-sided', statistic=0.5516826923076923, pvalue=0.003186790241593849)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'across_session_inst_fr_computation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb Cell 64\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#Y120sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m binom_test_chance_result \u001b[39m=\u001b[39m pho_stats_perform_diagonal_line_binomial_test(long_short_fr_indicies_analysis_table)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#Y120sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinom_test_chance_result: \u001b[39m\u001b[39m{\u001b[39;00mbinom_test_chance_result\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#Y120sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m LxC_Laps_T_result, SxC_Laps_T_result, LxC_Replay_T_result, SxC_Replay_T_result \u001b[39m=\u001b[39m pho_stats_bar_graph_t_tests(across_session_inst_fr_computation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'across_session_inst_fr_computation' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import pho_stats_perform_diagonal_line_binomial_test, pho_stats_bar_graph_t_tests\n",
    "\n",
    "binom_test_chance_result = pho_stats_perform_diagonal_line_binomial_test(long_short_fr_indicies_analysis_table)\n",
    "print(f'binom_test_chance_result: {binom_test_chance_result}')\n",
    "\n",
    "LxC_Laps_T_result, SxC_Laps_T_result, LxC_Replay_T_result, SxC_Replay_T_result = pho_stats_bar_graph_t_tests(across_session_inst_fr_computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea5a5",
   "metadata": {},
   "source": [
    "## 2023-10-04 - Run `AcrossSessionsVisualizations` corresponding to the PhoDibaPaper2023 figures for all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cc1152c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'across_session_inst_fr_computation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb Cell 66\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchInteractiveProcessing_2024-04-11.ipynb#Y122sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m global_multi_session_context, _out_aggregate_fig_2 \u001b[39m=\u001b[39m AcrossSessionsVisualizations\u001b[39m.\u001b[39macross_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, enable_hover_labels\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'across_session_inst_fr_computation' is not defined"
     ]
    }
   ],
   "source": [
    "## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \n",
    "global_multi_session_context, _out_aggregate_fig_2 = AcrossSessionsVisualizations.across_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels=False, enable_hover_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_long_and_short_firing_rate_replays_v_laps_figure(neuron_replay_stats_table=neuron_replay_stats_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_man = UserAnnotationsManager()\n",
    "included_annotations = {ctxt:ann_man.annotations[ctxt].get('session_cell_exclusivity', None) for ctxt in included_session_contexts}\n",
    "\n",
    "all_LxCs = []\n",
    "all_SxCs = []\n",
    "\n",
    "for ctxt, an_ann in included_annotations.items():\n",
    "\tsession_ctxt_key:str = ctxt.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "\tall_LxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.LxC])\n",
    "\tall_SxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.SxC])\n",
    "\t\n",
    "all_LxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1222a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_SxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381fcf5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "across_session_inst_fr_computation.LxC_scatter_props\n",
    "across_session_inst_fr_computation.SxC_scatter_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate across all of the sessions to build a new combined `InstantaneousSpikeRateGroupsComputation`, which can be used to plot the \"PaperFigureTwo\", bar plots for many sessions.\n",
    "global_multi_session_context = IdentifyingContext(format_name='kdiba', num_sessions=num_sessions) # some global context across all of the sessions, not sure what to put here.\n",
    "\n",
    "# To correctly aggregate results across sessions, it only makes sense to combine entries at the `.cell_agg_inst_fr_list` variable and lower (as the number of cells can be added across sessions, treated as unique for each session).\n",
    "\n",
    "## Display the aggregate across sessions:\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # WARNING: we didn't save this info\n",
    "_out_fig_2.computation_result = across_session_inst_fr_computation # the result loaded from the file\n",
    "_out_fig_2.active_identifying_session_ctx = across_session_inst_fr_computation.active_identifying_session_ctx\n",
    "# Set callback, the only self-specific property\n",
    "# _out_fig_2._pipeline_file_callback_fn = curr_active_pipeline.output_figure # lambda args, kwargs: self.write_to_file(args, kwargs, curr_active_pipeline)\n",
    "_out_fig_2.scatter_props_fn = _return_scatter_props_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LxC_aclus = _out_fig_2.computation_result.LxC_aclus\n",
    "SxC_aclus = _out_fig_2.computation_result.SxC_aclus\n",
    "\n",
    "LxC_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureOutputManager, FigureOutputLocation, ContextToPathMode\n",
    "\n",
    "registered_output_files = {}\n",
    "\n",
    "def output_figure(final_context: IdentifyingContext, fig, write_vector_format:bool=False, write_png:bool=True, debug_print=True):\n",
    "    \"\"\" outputs the figure using the provided context. \"\"\"\n",
    "    from pyphoplacecellanalysis.General.Mixins.ExportHelpers import build_and_write_to_file\n",
    "    def register_output_file(output_path, output_metadata=None):\n",
    "        \"\"\" registers a new output file for the pipeline \"\"\"\n",
    "        print(f'register_output_file(output_path: {output_path}, ...)')\n",
    "        registered_output_files[output_path] = output_metadata or {}\n",
    "\n",
    "    fig_out_man = FigureOutputManager(figure_output_location=FigureOutputLocation.DAILY_PROGRAMMATIC_OUTPUT_FOLDER, context_to_path_mode=ContextToPathMode.HIERARCHY_UNIQUE)\n",
    "    active_out_figure_paths = build_and_write_to_file(fig, final_context, fig_out_man, write_vector_format=write_vector_format, write_png=write_png, register_output_file_fn=register_output_file)\n",
    "    return active_out_figure_paths, final_context\n",
    "\n",
    "\n",
    "# Set callback, the only self-specific property\n",
    "_out_fig_2._pipeline_file_callback_fn = output_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing\n",
    "restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# Perform interactive Matplotlib operations with 'Qt5Agg' backend\n",
    "_fig_2_theta_out, _fig_2_replay_out = _out_fig_2.display(active_context=global_multi_session_context, title_modifier_fn=lambda original_title: f\"{original_title} ({num_sessions} sessions)\", save_figure=True)\n",
    "\t\n",
    "_out_fig_2.perform_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Surprise Shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d829b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a87449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-green",
   "language": "python",
   "name": "spike3d-green"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
