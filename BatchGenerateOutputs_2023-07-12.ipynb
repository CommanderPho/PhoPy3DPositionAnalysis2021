{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "# from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "# from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchSessionCompletionHandler\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef5938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_global_batch_result_filename='global_batch_result_2023-07-12.pkl'\n",
    "# active_global_batch_result_filename='global_batch_result_2023-07-07.pkl'\n",
    "active_global_batch_result_filename='global_batch_result_2023-07-07_extra.pkl'\n",
    "\n",
    "debug_print=True\n",
    "enable_neptune = False\n",
    "\n",
    "if enable_neptune:\n",
    "    import neptune # for logging progress and results\n",
    "    from neptune.types import File\n",
    "    from pyphoplacecellanalysis.General.Batch.NeptuneAiHelpers import set_environment_variables, neptune_output_figures\n",
    "    neptune_kwargs = {'project':\"commander.pho/PhoDibaLongShort2023\",\n",
    "    'api_token':\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGIxODU2My1lZTNhLTQ2ZWMtOTkzNS02ZTRmNzM5YmNjNjIifQ==\"}\n",
    "    set_environment_variables(neptune_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4442b4ad-6384-47ec-a248-185b35b08a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalized_loaded_global_batch_result_pickle_path: W:\\Data\\global_batch_result_2023-07-07_extra.pkl\n",
      "Loading loaded session pickle file results : W:\\Data\\global_batch_result_2023-07-07_extra.pkl... done.\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Memo value not found at index 847",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Loading.py:44\u001b[0m, in \u001b[0;36mloadData\u001b[1;34m(pkl_path, debug_print, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     db \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(dbfile, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\.venv\\lib\\site-packages\\dill\\_dill.py:373\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, ignore, **kwds)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39mUnpickle an object from a file.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[39mSee :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m \u001b[39mreturn\u001b[39;00m Unpickler(file, ignore\u001b[39m=\u001b[39;49mignore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[1;32mc:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\.venv\\lib\\site-packages\\dill\\_dill.py:646\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m--> 646\u001b[0m     obj \u001b[39m=\u001b[39m StockUnpickler\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    647\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39mgetattr\u001b[39m(_main_module, \u001b[39m'\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pathlib.py:1084\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flavour\u001b[39m.\u001b[39mis_supported:\n\u001b[1;32m-> 1084\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot instantiate \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m                               \u001b[39m%\u001b[39m (\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,))\n\u001b[0;32m   1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m df_path \u001b[39m=\u001b[39m global_data_root_parent_path\u001b[39m.\u001b[39mjoinpath(active_global_batch_result_filename)\u001b[39m.\u001b[39mresolve()\n\u001b[0;32m     27\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m \tglobal_batch_run \u001b[39m=\u001b[39m BatchRun\u001b[39m.\u001b[39;49mtry_init_from_file(global_data_root_parent_path, active_global_batch_result_filename\u001b[39m=\u001b[39;49mactive_global_batch_result_filename,\n\u001b[0;32m     29\u001b[0m \t\t\t\t\t\tskip_root_path_conversion\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, debug_print\u001b[39m=\u001b[39;49mdebug_print) \u001b[39m# on_needs_create_callback_fn=run_diba_batch\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     32\u001b[0m \terror_message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(err)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:97\u001b[0m, in \u001b[0;36mBatchRun.try_init_from_file\u001b[1;34m(cls, global_data_root_parent_path, active_global_batch_result_filename, skip_root_path_conversion, debug_print)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m global_batch_run\n\u001b[0;32m     95\u001b[0m \u001b[39m##\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m global_batch_run \u001b[39m=\u001b[39m _try_load_global_batch_result()\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m (global_batch_run \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m skip_root_path_conversion):\n\u001b[0;32m     99\u001b[0m     \u001b[39m# One was loaded from file, meaning it has the potential to have the wrong paths. Check.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     global_batch_run\u001b[39m.\u001b[39mchange_global_root_path(global_data_root_parent_path) \u001b[39m# Convert the paths to work on the new system:\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:77\u001b[0m, in \u001b[0;36mBatchRun.try_init_from_file.<locals>._try_load_global_batch_result\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m# try to load an existing batch result:\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     global_batch_run \u001b[39m=\u001b[39m loadData(finalized_loaded_global_batch_result_pickle_path, debug_print\u001b[39m=\u001b[39;49mdebug_print)\n\u001b[0;32m     79\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39m# Fixes issue with pickled POSIX_PATH on windows for path.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     posix_backup \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPosixPath \u001b[39m# backup the PosixPath definition\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Loading.py:62\u001b[0m, in \u001b[0;36mloadData\u001b[1;34m(pkl_path, debug_print, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     pathlib\u001b[39m.\u001b[39mPosixPath \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPurePosixPath\n\u001b[1;32m---> 62\u001b[0m     db \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(dbfile, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m# Fails this time if it still throws an error\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     pathlib\u001b[39m.\u001b[39mPosixPath \u001b[39m=\u001b[39m posix_backup \u001b[39m# restore the backup posix path definition\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\.venv\\lib\\site-packages\\dill\\_dill.py:373\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, ignore, **kwds)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(file, ignore\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m    368\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39m    Unpickle an object from a file.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[39m    See :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mreturn\u001b[39;00m Unpickler(file, ignore\u001b[39m=\u001b[39;49mignore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[1;32mc:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\.venv\\lib\\site-packages\\dill\\_dill.py:646\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m--> 646\u001b[0m     obj \u001b[39m=\u001b[39m StockUnpickler\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    647\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39mgetattr\u001b[39m(_main_module, \u001b[39m'\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    648\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore:\n\u001b[0;32m    649\u001b[0m             \u001b[39m# point obj class to main\u001b[39;00m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Memo value not found at index 847"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import main, BatchRun, run_diba_batch, run_specific_batch\n",
    "\n",
    "\"\"\"\n",
    "global_data_root_parent_path = find_first_extant_path([Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')])\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\n",
    "## NEPTUNE:\n",
    "if enable_neptune:\n",
    "\tproject = neptune.init_project(**neptune_kwargs)\n",
    "\tproject[\"general/global_batch_result_filename\"] = active_global_batch_result_filename\n",
    "\tproject[\"general/global_data_root_parent_path\"] = global_data_root_parent_path.as_posix()\n",
    "\n",
    "## TODO: load the batch result initially:\n",
    "\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "if enable_neptune:\n",
    "\trun = neptune.init_run() # rember to run.stop()\n",
    "\trun['parameters/global_batch_result_file_path'] = global_batch_result_file_path.as_posix()\n",
    "\t# project[\"general/data_analysis\"].upload(\"data_analysis.ipynb\")\n",
    "\trun[\"dataset/latest\"].track_files(f\"file://{global_batch_result_file_path}\") # \"s3://datasets/images\"\n",
    "\n",
    "# try to load an existing batch result:\n",
    "df_path = global_data_root_parent_path.joinpath(active_global_batch_result_filename).resolve()\n",
    "try:\n",
    "\tglobal_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=True, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\t\n",
    "except NotImplementedError as err:\n",
    "\terror_message = str(err)\n",
    "\tif 'WindowsPath' in error_message:  # Check if WindowsPath is missing\n",
    "\t\tprint(\"Issue with saved WindowsPath on Linux for path {}, performing pathlib workaround...\".format(df_path))\n",
    "\t\twin_backup = pathlib.WindowsPath  # Backup the WindowsPath definition\n",
    "\t\ttry:\n",
    "\t\t\tpathlib.WindowsPath = pathlib.PureWindowsPath\n",
    "\t\t\tglobal_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\t\t\tskip_root_path_conversion=True, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\t\tfinally:\n",
    "\t\t\tpathlib.WindowsPath = win_backup  # Restore the backup WindowsPath definition\n",
    "\t\t\t\n",
    "\telif 'PosixPath' in error_message:  # Check if PosixPath is missing\n",
    "\t\t# Fixes issue with saved POSIX_PATH on windows for path.\n",
    "\t\tposix_backup = pathlib.PosixPath # backup the PosixPath definition\n",
    "\t\ttry:\n",
    "\t\t\tpathlib.PosixPath = pathlib.PurePosixPath\n",
    "\t\t\tglobal_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\tskip_root_path_conversion=True, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\t\tfinally:\n",
    "\t\t\tpathlib.PosixPath = posix_backup # restore the backup posix path definition\n",
    "\telse:\n",
    "\t\tprint(\"Unknown issue with saved path for path {}, performing pathlib workaround...\".format(df_path))\n",
    "\t\traise\n",
    "except Exception as e:\n",
    "\t# unhandled exception\n",
    "\traise\n",
    "\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "\n",
    "if enable_neptune:\n",
    "\trun[\"dataset/global_batch_run_progress_df\"].upload(File.as_html(batch_progress_df)) # \"path/to/test_preds.csv\"\n",
    "\trun[\"dataset/global_batch_run_good_only_df\"].upload(File.as_html(good_only_batch_progress_df)) # \"path/to/test_preds.csv\"\n",
    "\n",
    "batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85809c31-2e4e-4bbc-a313-5f40fa0c05dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    # display(batch_progress_df)\n",
    "    good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "    display(good_only_batch_progress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e5298-4374-458f-8473-8dbc84b4689c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "[v.resolve().exists() for v in list(good_only_batch_progress_df.basedirs.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348205cf-4373-4c92-8305-335beafe506a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.user_annotations import UserAnnotationsManager\n",
    "\n",
    "user_is_replay_good_annotations = UserAnnotationsManager.get_user_annotations()\n",
    "\n",
    "good_only_batch_progress_df['has_user_replay_annotations'] = [(a_row_context.adding_context_if_missing(display_fn_name='DecodedEpochSlices',epochs='replays',decoder='long_results_obj',user_annotation='selections') in user_is_replay_good_annotations) for a_row_context in good_only_batch_progress_df['context']]\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab6e6b-e289-43ba-b35a-d6c837887026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in good_only_batch_progress_df.iterrows():\n",
    "    if row['is_ready']:\n",
    "        row_context = row['context']\n",
    "        print(f\"context: {row_context}\")\n",
    "        row_context in user_is_replay_good_annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f42b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build `global_batch_run` pre-loading results (before execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a7c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global_batch_result = loadData('global_batch_result.pkl')\n",
    "global_batch_run = run_diba_batch(global_data_root_parent_path, execute_all=False, extant_batch_run=global_batch_run, debug_print=False)\n",
    "# print(f'global_batch_result: {global_batch_run}')\n",
    "global_batch_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d043e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Skip sessions that aren't already computed. Refuse to do any computations (fail if computations are not done already)\n",
    "# Compute as needed, and if computations were done save the result (caching it for future loading)\n",
    "\n",
    "# Force Recompute:\n",
    "#   - Compute always (this mode would be used after a bug was fixed in the computations and we want to be sure the latest version is used to produce the result) regardless of whether extant results exist. Once computation is complete, overwrite extant files if they exist.\n",
    "#   - Same as above but do not save at all (just use in-memory result for something)\n",
    "#   - Same as first one but save the result with a special filename (a variant) as not to overwrite the original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd73b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I got it doing the bare-minimum loading and computations, so it should be ready to update the laps and constraint the placefields to those. Then we should be able to set up the replays at the same time.\n",
    "# finally, we then finish by computing.\n",
    "# force_reload = True\n",
    "force_reload = False\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=force_reload, should_perform_figure_generation_to_file=False, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False)\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "active_computation_functions_name_includelist=['_perform_baseline_placefield_computation',\n",
    "                                        # '_perform_time_dependent_placefield_computation',\n",
    "                                        '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "# active_computation_functions_name_includelist=['_perform_baseline_placefield_computation']\n",
    "global_batch_run.execute_all(force_reload=force_reload, saving_mode=PipelineSavingScheme.SKIP_SAVING, skip_extended_batch_computations=True, post_run_callback_fn=result_handler.on_complete_success_execution_session,\n",
    "                                                                                        **{'computation_functions_name_includelist': active_computation_functions_name_includelist,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45172b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last completed:\n",
    "r'W:\\Data\\KDIBA\\pin01\\one\\11-03_12-3-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d2321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to file:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "inst_fr_output_filename = 'long_short_inst_firing_rate_result.pkl'\n",
    "global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# Save the all sessions instantaneous firing rate dict to the path:\n",
    "saveData(global_batch_result_inst_fr_file_path, result_handler.across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a180ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 2023-07-12 10:12: - [ ] New save way after we save out current result and reload\n",
    "result_handler.save_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename='across_session_result_long_short_inst_firing_rate.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0e3f6-5c37-42e1-b06d-7a8028cb9512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_sessions = len(result_handler.across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32781c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Single Session testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_out = global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, computation_functions_name_includelist =['_perform_baseline_placefield_computation'], active_session_computation_configs=None) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "_test_out\n",
    "\n",
    "# global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, **{'computation_functions_name_includelist': ['_perform_baseline_placefield_computation'], 'active_session_computation_configs': None}) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 23.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2bb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "full_good_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is None]\n",
    "bad_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is not None]\n",
    "full_good_dirs\n",
    "bad_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f73f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status\n",
    "global_batch_run.session_batch_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faf2bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed516134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf02efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the list of sessions that are completely ready to process:\n",
    "full_good_ready_to_process_sessions = list(good_only_batch_progress_df['context'].to_numpy())\n",
    "full_good_ready_to_process_sessions\n",
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ad809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run[\"good_sessions_list\"].extend(full_good_ready_to_process_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636e3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.stop()\n",
    "project.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\",\\n\".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # List definitions\n",
    "\n",
    "# [IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a4bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\ncurr_context = \".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # Line definitions\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5689d1d-6400-4f87-be0e-a184a1d5bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82aedf-b024-4f07-b1a9-350730b4db8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "save_time = datetime.now()\n",
    " \n",
    "print(\"save_time =\", save_time)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = save_time.strftime(\"%Y-%m-%d_%I-%M%p\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc6bd-5b34-41d0-b906-b0ad9927b08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get output file paths:\n",
    "completed_pipeline_filename = 'loadedSessPickle.pkl'\n",
    "completed_global_computations_filename = 'outputs/global_computation_results.pkl'\n",
    "\n",
    "full_good_ready_to_process_session_paths = list(good_only_batch_progress_df['basedirs'].to_numpy())\n",
    "session_paths_output_folders = [sess_path.joinpath('outputs').resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "\n",
    "\n",
    "\n",
    "completed_pipeline_file_paths = [sess_path.joinpath(completed_pipeline_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths = [sess_path.joinpath(completed_global_computations_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
