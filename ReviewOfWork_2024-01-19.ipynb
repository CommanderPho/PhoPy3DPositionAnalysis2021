{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:20.608442900Z",
     "start_time": "2023-11-16T23:21:20.217442100Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n",
      "DAY_DATE_STR: 2024-01-19, DAY_DATE_TO_USE: 2024-01-19\n",
      "NOW_DATETIME: 2024-01-19_0520AM, NOW_DATETIME_TO_USE: 2024-01-19_0520AM\n",
      "global_data_root_parent_path changed to W:\\Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f01794c20c45d098d043e70532d3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Data Root:', layout=Layout(width='auto'), options=(WindowsPath('W:/Data'),), style=…"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext viztracer\n",
    "from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, CapturedException, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.general_helpers import inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables, CapturedException\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer, RankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "\n",
    "all_paths = [Path(r'/home/halechr/FastData'), Path(r'/media/MAX/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')]\n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "\tglobal global_data_root_parent_path\n",
    "\tnew_global_data_root_parent_path = new_path.resolve()\n",
    "\tglobal_data_root_parent_path = new_global_data_root_parent_path\n",
    "\tprint(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "\tassert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\t\t\t\n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {},
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basedir: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\loadedSessPickle.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:com.PhoHale.Spike3D.pipeline:NeuropyPipeline.__setstate__(state=\"{'pipeline_name': 'kdiba_pipeline', 'session_data_type': 'kdiba', '_stage': <pyphoplacecellanalysis.General.Pipeline.Stages.Display.DisplayPipelineStage object at 0x000001C7E1737430>}\")\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:NeuropyPipeline.on_stage_changed(new_stage=\"PipelineStage.Displayed\")\n",
      "WARNING:com.PhoHale.Spike3D.pipeline:WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context: IdentifyingContext = good_contexts_list[1] # select the session from all of the good sessions here.\n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # DONE. Very good. Many good Pfs, many good replays.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # DONE, might be the BEST SESSION, good example session with lots of place cells, clean replays, and clear bar graphs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # DONE, Good Pfs but no good replays ---- VERY weird effect of the replays, a sharp drop to strongly negative values more than 3/4 through the experiment.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6') # BAD, 2023-07-14, unsure why still.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # DONE, GREAT, both good Pfs and replays! Interesting see-saw!\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25') # DONE, Added replay selections. Very \"jumpy\" between the starts and ends of the track.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40') # 2024-01-10 new RANKORDER APOGEE | DONE, Added replay selections. A TON of putative replays in general, most bad, but some good. LOOKIN GOOD!\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='twolong_LR_pf1Dsession_name='2006-4-12_15-25-59') # BAD, No Epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-16_18-47-52')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-17_12-52-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-25_13-20-55')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-28_12-38-13')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44') # DONE, good. Many good pfs, many good replays. Noticed very strange jumping off the track in the 3D behavior/spikes viewer. Is there something wrong with this session?\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0') # DONE, good?, replays selected, few --- \"ZeroDivisionError: float division by zero\"\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # DONE, very few replays\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_12-15-3') ### KeyError: 'maze1_odd'\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_22-4-5') ### \n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # DONE, replays selected, quite a few replays but few are very good.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "# \n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['pf_computation',\n",
    "                                            #    'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step', \n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            # 'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acba46b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.574268400Z",
     "start_time": "2023-11-16T23:21:35.966373700Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\output\\global_computation_results.pkl... done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['DirectionalLaps',\n",
       "  'DirectionalMergedDecoders',\n",
       "  'RankOrder',\n",
       "  'long_short_leave_one_out_decoding_analysis',\n",
       "  'short_long_pf_overlap_analyses',\n",
       "  'long_short_fr_indicies_analysis',\n",
       "  'jonathan_firing_rate_analysis',\n",
       "  'DirectionalDecodersDecoded'],\n",
       " ['DirectionalLaps',\n",
       "  'DirectionalMergedDecoders',\n",
       "  'RankOrder',\n",
       "  'long_short_leave_one_out_decoding_analysis',\n",
       "  'short_long_pf_overlap_analyses',\n",
       "  'long_short_fr_indicies_analysis',\n",
       "  'jonathan_firing_rate_analysis',\n",
       "  'DirectionalDecodersDecoded'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['pf_computation', 'firing_rate_trends', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "missing required value, so we don't need to call .validate_computation_test(...) to know it isn't valid!\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'NoneType' object has no attribute 'active_filter_epochs'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'active_filter_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m force_recompute_global \u001b[38;5;241m=\u001b[39m force_reload\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# force_recompute_global = True\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m newly_computed_values \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_extended_computations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_active_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_computations_include_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_global_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_recompute_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_recompute_override_computations_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_recompute_override_computations_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(newly_computed_values) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewly_computed_values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewly_computed_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\NonInteractiveProcessing.py:371\u001b[0m, in \u001b[0;36mbatch_extended_computations\u001b[1;34m(curr_active_pipeline, include_includelist, included_computation_filter_names, include_global_functions, fail_on_exception, progress_print, debug_print, force_recompute, force_recompute_override_computations_includelist, dry_run)\u001b[0m\n\u001b[0;32m    369\u001b[0m _curr_force_recompute \u001b[38;5;241m=\u001b[39m force_recompute \u001b[38;5;129;01mor\u001b[39;00m ((_comp_specifier\u001b[38;5;241m.\u001b[39mshort_name \u001b[38;5;129;01min\u001b[39;00m force_recompute_override_computations_includelist) \u001b[38;5;129;01mor\u001b[39;00m (_comp_specifier\u001b[38;5;241m.\u001b[39mcomputation_fn_name \u001b[38;5;129;01min\u001b[39;00m force_recompute_override_computations_includelist)) \u001b[38;5;66;03m# force_recompute for this specific result if either of its name is included in `force_recompute_override_computations_includelist`\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dry_run:\n\u001b[1;32m--> 371\u001b[0m     newly_computed_values \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_comp_specifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_computation_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_active_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_filter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_epoch_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_already_computed_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_subfn_on_already_computed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_print\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_curr_force_recompute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdry-run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_comp_specifier\u001b[38;5;241m.\u001b[39mshort_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, force_recompute=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforce_recompute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, curr_force_recompute=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curr_force_recompute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Model\\SpecificComputationValidation.py:140\u001b[0m, in \u001b[0;36mSpecificComputationValidator.try_computation_if_needed\u001b[1;34m(self, curr_active_pipeline, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_computation_if_needed\u001b[39m(\u001b[38;5;28mself\u001b[39m, curr_active_pipeline, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_try_computation_if_needed(\u001b[38;5;28mself\u001b[39m, curr_active_pipeline, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Model\\SpecificComputationValidation.py:259\u001b[0m, in \u001b[0;36mSpecificComputationValidator._perform_try_computation_if_needed\u001b[1;34m(cls, comp_specifier, curr_active_pipeline, computation_filter_name, on_already_computed_fn, fail_on_exception, progress_print, debug_print, force_recompute)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured while computing (`perform_specific_computation(...)`):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Inner exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fail_on_exception:\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m inner_e\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;66;03m# unhandled exception\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Model\\SpecificComputationValidation.py:252\u001b[0m, in \u001b[0;36mSpecificComputationValidator._perform_try_computation_if_needed\u001b[1;34m(cls, comp_specifier, curr_active_pipeline, computation_filter_name, on_already_computed_fn, fail_on_exception, progress_print, debug_print, force_recompute)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# When this fails due to unwrapping from the load, add `, computation_kwargs_list=[{'perform_cache_load': False}]` as an argument to the `perform_specific_computation` call below\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[43mcurr_active_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_specific_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcomp_specifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputation_fn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcomp_specifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputation_fn_kwargs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fail_on_exception MUST be True or error handling is all messed up \u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m progress_print \u001b[38;5;129;01mor\u001b[39;00m debug_print:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:1101\u001b[0m, in \u001b[0;36mPipelineWithComputedPipelineStageMixin.perform_specific_computation\u001b[1;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, debug_print)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" perform a specific computation (specified in computation_functions_name_includelist) in a minimally destructive manner using the previously recomputed results:\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;124;03mPassthrough wrapper to self.stage.perform_specific_computation(...) with the same arguments.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03mUpdates:\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;124;03m    curr_active_pipeline.computation_results\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;66;03m# self.stage is of type ComputedPipelineStage\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_specific_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_computation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_computation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_filter_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_filter_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:689\u001b[0m, in \u001b[0;36mComputedPipelineStage.perform_specific_computation\u001b[1;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, debug_print)\u001b[0m\n\u001b[0;32m    685\u001b[0m     global_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(owning_pipeline_reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, global_computation_results\u001b[38;5;241m=\u001b[39mprevious_computation_result, computation_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_results, active_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_configs, include_includelist\u001b[38;5;241m=\u001b[39menabled_filter_names, debug_print\u001b[38;5;241m=\u001b[39mdebug_print)\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m has_custom_kwargs_list), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#TODO 2023-11-22 23:41: - [ ] perform_specific_computation(...) computation_kwargs_list seems to have no effect in global functions, maybe fix? For now, just throw an error so you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt think your custom kwargs are working when they aren\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_computation_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_specific_computations_single_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mare_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# was there a reason I didn't pass `computation_kwargs_list` to the global version?\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# Non-global functions:\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a_select_config_name, a_filtered_session \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiltered_sessions\u001b[38;5;241m.\u001b[39mitems():                \n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:330\u001b[0m, in \u001b[0;36mComputedPipelineStage.run_specific_computations_single_context\u001b[1;34m(self, previous_computation_result, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[0;32m    328\u001b[0m     progress_logger_callback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_specific_computations_single_context(including only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_computation_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregistered_computation_function_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registered computation functions): active_computation_functions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_computation_functions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Perform the computations:\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mComputedPipelineStage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_computation_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_computation_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_computation_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprevious_computation_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mare_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mare_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:797\u001b[0m, in \u001b[0;36mComputedPipelineStage._execute_computation_functions\u001b[1;34m(active_computation_functions, previous_computation_result, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_logger_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     progress_logger_callback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecuting [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_num_funcs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 797\u001b[0m previous_computation_result \u001b[38;5;241m=\u001b[39m f(previous_computation_result, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomputation_kwargs_list[i])\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Log the computation copmlete time:\u001b[39;00m\n\u001b[0;32m    799\u001b[0m computation_times[computation_times_key_fn(f)] \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\ComputationFunctions\\MultiContextComputationFunctions\\MultiContextComputationFunctions.py:19\u001b[0m, in \u001b[0;36m_wrap_multi_context_computation_function.<locals>._\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(global_comp_fcn) \u001b[38;5;66;03m# @wraps ensures that the functions name, docs, etc are accessible in the wrapped version of the function.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_\u001b[39m(x):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m     x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_comp_fcn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# update global_computation_results\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\ComputationFunctions\\MultiContextComputationFunctions\\LongShortTrackComputations.py:1040\u001b[0m, in \u001b[0;36mLongShortTrackComputations._perform_long_short_post_decoding_analysis\u001b[1;34m(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist, debug_print)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m## Common to both Long and Short:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m active_pos_df \u001b[38;5;241m=\u001b[39m global_session\u001b[38;5;241m.\u001b[39mposition\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[43mlong_results_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_filter_epochs\u001b[49m\u001b[38;5;241m.\u001b[39mas_array() \u001b[38;5;241m==\u001b[39m short_results_obj\u001b[38;5;241m.\u001b[39mactive_filter_epochs\u001b[38;5;241m.\u001b[39mas_array())\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;66;03m# ensure that the active_filter_epochs for both are the same.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m active_filter_epochs \u001b[38;5;241m=\u001b[39m long_results_obj\u001b[38;5;241m.\u001b[39mactive_filter_epochs\n\u001b[0;32m   1042\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m active_filter_epochs\u001b[38;5;241m.\u001b[39mn_epochs\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'active_filter_epochs'"
     ]
    }
   ],
   "source": [
    "### GLOBAL COMPUTATIONS:\n",
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        curr_active_pipeline.load_pickled_global_computation_results()\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results: {e}')\n",
    "        raise\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "extended_computations_include_includelist=['pf_computation', 'firing_rate_trends',# 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    # 'long_short_inst_spike_rate_groups',\n",
    "    # 'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    # 'directional_decoders_decode_continuous'\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'] # , 'directional_decoders_decode_continuous'\n",
    "\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     exception_info = sys.exc_info()\n",
    "#     e = CapturedException(e, exception_info)\n",
    "#     print(f'second half threw: {e}')\n",
    "\n",
    "# 4m 5.2s for inst fr computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3d4f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['pf_computation', 'firing_rate_trends', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "2024-01-02 - long_short_decoding_analyses _perform_try_computation_if_needed, remove_provided_keys\n",
      "removed results: ['long_short_leave_one_out_decoding_analysis'] because force_recompute was True.\n",
      "missing required value, so we don't need to call .validate_computation_test(...) to know it isn't valid!\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\FastSwap\\AppData\\VSCode\\yellow\\.venv_yellow\\lib\\site-packages\\scipy\\spatial\\distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 63, n_all_epoch_timebins = 2178)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\FastSwap\\AppData\\VSCode\\yellow\\.venv_yellow\\lib\\site-packages\\scipy\\spatial\\distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 63, n_all_epoch_timebins = 2178)\n",
      "\t done.\n",
      "missing required value, so we don't need to call .validate_computation_test(...) to know it isn't valid!\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: 'NoneType' object has no attribute 'active_filter_epochs'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'active_filter_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m      1\u001b[0m extended_computations_include_includelist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpf_computation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiring_rate_trends\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;66;03m# 'pfdt_computation',\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# 'pf_dt_sequential_surprise',\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#  'ratemap_peaks_prominence2d',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirectional_decoders_decode_continuous\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m ] \u001b[38;5;66;03m# do only specified\u001b[39;00m\n\u001b[0;32m     19\u001b[0m force_recompute_override_computations_includelist \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong_short_decoding_analyses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 21\u001b[0m newly_computed_values \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_extended_computations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_active_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_computations_include_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_global_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_recompute_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_recompute_override_computations_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_recompute_override_computations_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m newly_computed_values\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\NonInteractiveProcessing.py:371\u001b[0m, in \u001b[0;36mbatch_extended_computations\u001b[1;34m(curr_active_pipeline, include_includelist, included_computation_filter_names, include_global_functions, fail_on_exception, progress_print, debug_print, force_recompute, force_recompute_override_computations_includelist, dry_run)\u001b[0m\n\u001b[0;32m    369\u001b[0m _curr_force_recompute \u001b[38;5;241m=\u001b[39m force_recompute \u001b[38;5;129;01mor\u001b[39;00m ((_comp_specifier\u001b[38;5;241m.\u001b[39mshort_name \u001b[38;5;129;01min\u001b[39;00m force_recompute_override_computations_includelist) \u001b[38;5;129;01mor\u001b[39;00m (_comp_specifier\u001b[38;5;241m.\u001b[39mcomputation_fn_name \u001b[38;5;129;01min\u001b[39;00m force_recompute_override_computations_includelist)) \u001b[38;5;66;03m# force_recompute for this specific result if either of its name is included in `force_recompute_override_computations_includelist`\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dry_run:\n\u001b[1;32m--> 371\u001b[0m     newly_computed_values \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_comp_specifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_computation_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_active_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_filter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_epoch_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_already_computed_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_subfn_on_already_computed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_print\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_curr_force_recompute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdry-run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_comp_specifier\u001b[38;5;241m.\u001b[39mshort_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, force_recompute=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforce_recompute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, curr_force_recompute=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curr_force_recompute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Model\\SpecificComputationValidation.py:140\u001b[0m, in \u001b[0;36mSpecificComputationValidator.try_computation_if_needed\u001b[1;34m(self, curr_active_pipeline, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_computation_if_needed\u001b[39m(\u001b[38;5;28mself\u001b[39m, curr_active_pipeline, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_try_computation_if_needed(\u001b[38;5;28mself\u001b[39m, curr_active_pipeline, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Model\\SpecificComputationValidation.py:259\u001b[0m, in \u001b[0;36mSpecificComputationValidator._perform_try_computation_if_needed\u001b[1;34m(cls, comp_specifier, curr_active_pipeline, computation_filter_name, on_already_computed_fn, fail_on_exception, progress_print, debug_print, force_recompute)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured while computing (`perform_specific_computation(...)`):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Inner exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fail_on_exception:\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m inner_e\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;66;03m# unhandled exception\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Model\\SpecificComputationValidation.py:252\u001b[0m, in \u001b[0;36mSpecificComputationValidator._perform_try_computation_if_needed\u001b[1;34m(cls, comp_specifier, curr_active_pipeline, computation_filter_name, on_already_computed_fn, fail_on_exception, progress_print, debug_print, force_recompute)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# When this fails due to unwrapping from the load, add `, computation_kwargs_list=[{'perform_cache_load': False}]` as an argument to the `perform_specific_computation` call below\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[43mcurr_active_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_specific_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcomp_specifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputation_fn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcomp_specifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputation_fn_kwargs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fail_on_exception MUST be True or error handling is all messed up \u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m progress_print \u001b[38;5;129;01mor\u001b[39;00m debug_print:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:1101\u001b[0m, in \u001b[0;36mPipelineWithComputedPipelineStageMixin.perform_specific_computation\u001b[1;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, debug_print)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" perform a specific computation (specified in computation_functions_name_includelist) in a minimally destructive manner using the previously recomputed results:\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;124;03mPassthrough wrapper to self.stage.perform_specific_computation(...) with the same arguments.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03mUpdates:\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;124;03m    curr_active_pipeline.computation_results\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;66;03m# self.stage is of type ComputedPipelineStage\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_specific_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_computation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_computation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_filter_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_filter_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:689\u001b[0m, in \u001b[0;36mComputedPipelineStage.perform_specific_computation\u001b[1;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, debug_print)\u001b[0m\n\u001b[0;32m    685\u001b[0m     global_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(owning_pipeline_reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, global_computation_results\u001b[38;5;241m=\u001b[39mprevious_computation_result, computation_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_results, active_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_configs, include_includelist\u001b[38;5;241m=\u001b[39menabled_filter_names, debug_print\u001b[38;5;241m=\u001b[39mdebug_print)\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m has_custom_kwargs_list), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#TODO 2023-11-22 23:41: - [ ] perform_specific_computation(...) computation_kwargs_list seems to have no effect in global functions, maybe fix? For now, just throw an error so you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt think your custom kwargs are working when they aren\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_computation_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_specific_computations_single_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mare_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# was there a reason I didn't pass `computation_kwargs_list` to the global version?\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# Non-global functions:\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a_select_config_name, a_filtered_session \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiltered_sessions\u001b[38;5;241m.\u001b[39mitems():                \n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:330\u001b[0m, in \u001b[0;36mComputedPipelineStage.run_specific_computations_single_context\u001b[1;34m(self, previous_computation_result, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[0;32m    328\u001b[0m     progress_logger_callback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_specific_computations_single_context(including only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_computation_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregistered_computation_function_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registered computation functions): active_computation_functions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_computation_functions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Perform the computations:\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mComputedPipelineStage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_computation_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_computation_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_computation_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprevious_computation_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mare_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mare_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Computation.py:797\u001b[0m, in \u001b[0;36mComputedPipelineStage._execute_computation_functions\u001b[1;34m(active_computation_functions, previous_computation_result, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_logger_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     progress_logger_callback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecuting [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_num_funcs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 797\u001b[0m previous_computation_result \u001b[38;5;241m=\u001b[39m f(previous_computation_result, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomputation_kwargs_list[i])\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Log the computation copmlete time:\u001b[39;00m\n\u001b[0;32m    799\u001b[0m computation_times[computation_times_key_fn(f)] \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\ComputationFunctions\\MultiContextComputationFunctions\\MultiContextComputationFunctions.py:19\u001b[0m, in \u001b[0;36m_wrap_multi_context_computation_function.<locals>._\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(global_comp_fcn) \u001b[38;5;66;03m# @wraps ensures that the functions name, docs, etc are accessible in the wrapped version of the function.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_\u001b[39m(x):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m     x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_comp_fcn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# update global_computation_results\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\ComputationFunctions\\MultiContextComputationFunctions\\LongShortTrackComputations.py:1040\u001b[0m, in \u001b[0;36mLongShortTrackComputations._perform_long_short_post_decoding_analysis\u001b[1;34m(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist, debug_print)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m## Common to both Long and Short:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m active_pos_df \u001b[38;5;241m=\u001b[39m global_session\u001b[38;5;241m.\u001b[39mposition\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[43mlong_results_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_filter_epochs\u001b[49m\u001b[38;5;241m.\u001b[39mas_array() \u001b[38;5;241m==\u001b[39m short_results_obj\u001b[38;5;241m.\u001b[39mactive_filter_epochs\u001b[38;5;241m.\u001b[39mas_array())\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;66;03m# ensure that the active_filter_epochs for both are the same.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m active_filter_epochs \u001b[38;5;241m=\u001b[39m long_results_obj\u001b[38;5;241m.\u001b[39mactive_filter_epochs\n\u001b[0;32m   1042\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m active_filter_epochs\u001b[38;5;241m.\u001b[39mn_epochs\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'active_filter_epochs'"
     ]
    }
   ],
   "source": [
    "extended_computations_include_includelist=['pf_computation', 'firing_rate_trends',# 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    # 'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    # 'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    'directional_decoders_decode_continuous'\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_override_computations_includelist = ['long_short_decoding_analyses']\n",
    "\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "newly_computed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results() # newly_computed_values: [('pfdt_computation', 'maze_any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'pf1D_dt'\n",
    "# 'pf1D_dt', 'pf2D_dt'\n",
    "# curr_active_pipeline.computation_results['maze_any'].computed_data['pf1D_dt']\n",
    "# curr_active_pipeline.computation_results['maze_any'].computed_data['pf2D_dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9378762",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results_pickle_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine most recent computation times:\n",
    "curr_active_pipeline.get_computation_times(debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bacec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f215007",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_perform_PBE_stats_analyses'], enabled_filter_names=[global_epoch_name], fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a869b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n",
    "curr_active_pipeline.clear_registered_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine most recent computation times:\n",
    "out_times = curr_active_pipeline.get_computation_times(debug_print=True)\n",
    "out_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata\n",
    "\n",
    "# Stores no global times\n",
    "global_pickle_path = curr_active_pipeline.global_computation_results_pickle_path.resolve()\n",
    "assert global_pickle_path.exists()\n",
    "global_pickle_path\n",
    "\n",
    "file_metadata = FilesystemMetadata.get_file_metadata(global_pickle_path)\n",
    "file_metadata['modification_time']\n",
    "\n",
    "collected_global_computations_pickle_paths: List[Path] = []\n",
    "\n",
    "FilesystemMetadata.get_files_metadata(collected_global_computations_pickle_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eff113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Global computations:\n",
    "\n",
    "# global_computation_completion_times = {k.__name__:v for k,v in self.global_computation_results.computation_times.items()}\n",
    "# global_computations_latest_computation_time: datetime = max(list(global_computation_completion_times.values()), default=datetime.min)\n",
    "\n",
    "# ## Any (global or non-global) computation most recent time):\n",
    "# any_most_recent_computation_time: datetime = max(non_global_any_most_recent_computation_time, global_computations_latest_computation_time, datetime.min) # returns `datetime.min` if the first arguments are empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {},
   "source": [
    "# Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {},
   "source": [
    "# End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a533ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.700275900Z",
     "start_time": "2023-11-16T23:21:40.584273Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "t_start, t_delta, t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have several python variables I want to print: t_start, t_delta, t_end\n",
    "# I want to generate a print statement that explicitly lists the variable name prior to its value like `print(f't_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')`\n",
    "# Currently I have to t_start, t_delta, t_end\n",
    "curr_active_pipeline.get_session_context()\n",
    "\n",
    "print(f'{curr_active_pipeline.session_name}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071e94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:43.601382Z",
     "start_time": "2023-11-16T23:21:40.702275600Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## long_short_decoding_analyses:\n",
    "curr_long_short_decoding_analyses = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83acf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_v_observed_result.observed_from_expected_diff_ptp_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f5d4f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104fc37",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "directional_laps_results = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']   \n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: float = rank_order_results.included_qclu_values\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65751b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the decoded epochs to a file so they can be compared across sessions?\n",
    "directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.to_hdf(Path('output/all_directional_laps_filter_epochs_decoder_result.hdf').resolve(), 'all_directional_laps_filter_epochs_decoder_result', enable_hdf_testing_mode=True, debug_print=True)\n",
    "directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs.to_hdf('output/all_directional_laps_filter_epochs_decoder_result-filter_epochs.hdf', 'filter_epochs')\n",
    "directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f67cb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersDecodedResult\n",
    "\n",
    "directional_decoders_decode_result: DirectionalDecodersDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "# continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "time_bin_size: float = directional_decoders_decode_result.last_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = directional_decoders_decode_result.continuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "DirectionalDecodersDecodedResult.validate_has_directional_decoded_continuous_epochs(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Type\n",
    "from neuropy.utils.mixins.HDF5_representable import HDFSerializationRegister\n",
    "\n",
    "a_register = HDFSerializationRegister()\n",
    "\n",
    "a_register.converion_registery[pd.DataFrame] = lambda x, *hdf_args, **hdf_kwargs: x.to_hdf(*hdf_args, **hdf_kwargs)\n",
    "a_register.converion_registery[Epoch] = lambda x, *hdf_args, **hdf_kwargs: x.to_dataframe().to_hdf(*hdf_args, **hdf_kwargs)\n",
    "\n",
    "\n",
    "# works!\n",
    "a_register.to_hdf(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs, 'output/all_directional_laps_filter_epochs_decoder_result-filter_epochs.hdf', 'filter_epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print_keys_if_possible('DirectionalMergedDecoders', directional_merged_decoders_result)\n",
    "\n",
    "from ansi2html import Ansi2HTMLConverter # used by DocumentationFilePrinter to build html document from ansi-color coded version\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter\n",
    "\n",
    "doc_printer = DocumentationFilePrinter(doc_output_parent_folder=Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation'), doc_name='DirectionalMergedDecodersResult')\n",
    "doc_printer.save_documentation('DirectionalMergedDecodersResult', directional_merged_decoders_result, non_expanded_item_keys=['_reverse_cellID_index_map'], additional_excluded_item_classes='neuropy.analyses.PfND', max_depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dc5fe",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "# `LongShortStatsItem` form (2024-01-02):\n",
    "# LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "# RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_LR_pf1D_Decoder.time_bin_size\n",
    "long_LR_pf1D_Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260739a4f36c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "# active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1c6006aba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Entropy/Surprise Results:\n",
    "active_extended_stats = global_results['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['pf_dt_sequential_surprise'] # DynamicParameters\n",
    "historical_snapshots = active_relative_entropy_results['historical_snapshots']\n",
    "post_update_times: np.ndarray = active_relative_entropy_results['post_update_times'] # (4152,) = (n_post_update_times,)\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals: np.ndarray = active_relative_entropy_results['time_intervals']\n",
    "surprise_time_bin_duration = (post_update_times[2]-post_update_times[1])\n",
    "long_short_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['long_short_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "short_long_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['short_long_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "flat_relative_entropy_results: np.ndarray = active_relative_entropy_results['flat_relative_entropy_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_results: np.ndarray = active_relative_entropy_results['flat_jensen_shannon_distance_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_across_all_positions: np.ndarray = np.sum(np.abs(flat_jensen_shannon_distance_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "flat_surprise_across_all_positions: np.ndarray = np.sum(np.abs(flat_relative_entropy_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "\n",
    "## Get the placefield dt matrix:\n",
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\t## Compute it if missing:\n",
    "\toccupancy_weighted_tuning_maps_over_time = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in historical_snapshots.values()])\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = occupancy_weighted_tuning_maps_over_time\n",
    "else:\n",
    "\toccupancy_weighted_tuning_maps_over_time = active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] # (n_post_update_times, n_neurons, n_xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d3bf5955d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c62d5c18c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "# appearing_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf30c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_odd') # , 'maze1_odd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc832fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import programmatic_render_to_file\n",
    "\n",
    "programmatic_render_to_file(curr_active_pipeline, curr_display_function_name='_display_1d_placefields', write_vector_format=True, write_png=True, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_1d_placefields', 'maze_any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefields', 'maze_any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_placemaps_pyqtplot_2D', 'maze2_odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefields', 'maze2_odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_3d_interactive_spike_and_behavior_browser', 'maze1_odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust layout to make space for the footer\n",
    "# plt.subplots_adjust(bottom=0.35)\n",
    "\n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ecb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_display_placemaps_pyqtplot_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_multitab import MplMultiTab, MplMultiTab2D\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import programmatic_display_to_PDF, programmatic_render_to_file\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.placefield import plot_single_cell_1D_placecell_validation\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.placefield import plot_1d_placecell_validations\n",
    "\n",
    "\n",
    "# matplotlib_configuration_update(is_interactive=True)\n",
    "\n",
    "# curr_active_pipeline.display('_display_grid_bin_bounds_validation')\n",
    "_out = curr_active_pipeline.display('_display_1d_placefield_validations', 'maze1_odd')\n",
    "_out.ui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f301b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "programmatic_display_to_PDF(curr_active_pipeline, curr_display_function_name='_display_1d_placefield_validations', filter_name='maze1_odd', debug_print=True)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f824cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "placefield_cell_index = 0\n",
    "active_epoch_placefields1D = deepcopy(long_pf1D)\n",
    "curr_cell_normalized_tuning_curve = active_epoch_placefields1D.ratemap.normalized_tuning_curves[placefield_cell_index, :].squeeze()\n",
    "{'xbin_centers': active_epoch_placefields1D.ratemap.xbin_centers, 'curr_cell_normalized_tuning_curve': curr_cell_normalized_tuning_curve}\n",
    "\n",
    "{'xbin_centers': np.array([31.0565, 34.8495, 38.6426, 42.4356, 46.2286, 50.0216, 53.8147, 57.6077, 61.4007, 65.1937, 68.9867, 72.7798, 76.5728, 80.3658, 84.1588, 87.9519, 91.7449, 95.5379, 99.3309, 103.124, 106.917, 110.71, 114.503, 118.296, 122.089, 125.882, 129.675, 133.468, 137.261, 141.054, 144.847, 148.64, 152.433, 156.226, 160.019, 163.812, 167.605, 171.398, 175.191, 178.984, 182.777, 186.57, 190.363, 194.157, 197.95, 201.743, 205.536, 209.329, 213.122, 216.915, 220.708, 224.501, 228.294, 232.087, 235.88, 239.673, 243.466, 247.259, 251.052, 254.845, 258.638, 262.431]),\n",
    " 'curr_cell_normalized_tuning_curve': np.array([5.92979e-05, 0.000150933, 0.00036895, 0.000736517, 0.00121915, 0.00173714, 0.0022042, 0.00252859, 0.0026496, 0.0027108, 0.00312627, 0.00423033, 0.00579314, 0.00709557, 0.00766535, 0.00789647, 0.00884807, 0.0115452, 0.0165549, 0.0238423, 0.0323681, 0.039895, 0.0442459, 0.0452642, 0.0449909, 0.0457691, 0.0485138, 0.0525281, 0.0562324, 0.0581433, 0.0575758, 0.0544383, 0.0486438, 0.0404683, 0.0315115, 0.0243731, 0.0207242, 0.0199181, 0.0197507, 0.0183449, 0.0153819, 0.0119837, 0.00951012, 0.00827676, 0.00740415, 0.00596512, 0.00396809, 0.00210018, 0.000875453, 0.000302685, 0.000153468, 0.00027615, 0.000667689, 0.00135676, 0.00224608, 0.00305331, 0.0034339, 0.0031979, 0.0024518, 0.00153458, 0.00079294, 0.000405152])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_extended_programmatic_figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40083faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77344ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefields', 'maze2_even')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fdc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO 2023-11-29 09:18: - [ ] Not good, the self.filtered_contexts are not unique!\n",
    "list(curr_active_pipeline.filtered_contexts.values())\n",
    "# [IdentifyingContext<(... 'maze2')>, IdentifyingContext<(... 'maze2')>, IdentifyingContext<(..., 'maze')>, IdentifyingContext<(... 'maze2')>, IdentifyingContext<(... 'maze2')>, IdentifyingContext<(..., 'maze')>, IdentifyingContext<(...ze1_any')>, IdentifyingContext<(... 'maze2')>, IdentifyingContext<(..., 'maze')>]\n",
    "[(v == curr_active_pipeline.filtered_contexts['maze1_even']) for v in list(curr_active_pipeline.filtered_contexts.values())]\n",
    "# [True, True, False, True, True, False, False, True, False]\n",
    "# meaning `curr_active_pipeline.display('_display_1d_placefields', curr_active_pipeline.filtered_contexts['maze1_even'])` doesn't work\n",
    "curr_active_pipeline.filtered_contexts.index(curr_active_pipeline.filtered_contexts['maze1_even'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefields', 'maze2_odd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vector_format = False\n",
    "write_png = True\n",
    "debug_print = True\n",
    "from neuropy.plotting.ratemaps import BackgroundRenderingOptions\n",
    "\n",
    "programmatic_render_to_file(curr_active_pipeline, curr_display_function_name='_display_2d_placefield_result_plot_ratemaps_2D', write_vector_format=write_vector_format, write_png=write_png, debug_print=debug_print, bg_rendering_mode=BackgroundRenderingOptions.EMPTY) #  🟢✅ Now seems to be working and saving to PDF!! Still using matplotlib.use('Qt5Agg') mode and plots still appear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_2d_placefield_occupancy', 'maze2_any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e90109",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_2d_placefield_occupancy', 'maze1_any')\n",
    "occupancy_ax = _out.axes #.get_aspect()\n",
    "pf = long_pf2D\n",
    "# pf.xbin\n",
    "# pf.ybin\n",
    "# pf.xbin_centers\n",
    "# pf.ybin_centers\n",
    "\n",
    "# aspect_ratio = np.ptp(pf.xbin) / np.ptp(pf.ybin)  # ptp: peak to peak (range)\n",
    "# aspect_ratio = 0.102803738317757\n",
    "# print(f'aspect_ratio: {aspect_ratio}')\n",
    "# occupancy_ax.set_aspect(aspect_ratio, adjustable='box') # If 'box', change the physical dimensions of the Axes. If 'datalim', change the x or y data limits.\n",
    "\n",
    "\n",
    "## See \n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_anchor.html#\n",
    "\n",
    "\n",
    "occupancy_ax.set_aspect('equal', adjustable=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_ax.set_aspect('equal', adjustable='datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ecc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_ax.set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42513c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "programmatic_render_to_file(curr_active_pipeline, curr_display_function_name='_display_2d_placefield_occupancy', write_vector_format=write_vector_format, write_png=write_png, debug_print=debug_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a572825",
   "metadata": {},
   "source": [
    "# EVEN: \"RL\", ODD: \"LR\"\n",
    "Starts with Even (idx=0)\n",
    "- EVEN: \"RL\"\n",
    "shared_RL_aclus_only_neuron_IDs\n",
    "`is_even = (an_epoch.lap_dir == 0)`\n",
    "- ODD: \"LR\"\n",
    "shared_LR_aclus_only_neuron_IDs\n",
    "`is_odd = (an_epoch.lap_dir == 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0ef2b",
   "metadata": {},
   "source": [
    "# 🟢 2023-10-20 - Z-Score Comparisons with Neuron_ID Shuffled templates\n",
    "1. Take the intersection of the long and short templates to get only the common cells\n",
    "2. Determine the long and short \"tempaltes\": this is done by ranking the aclus for each by their placefields' center of mass. `compute_placefield_center_of_masses`\n",
    "\t2a. `long_pf_peak_ranks`, `short_pf_peak_ranks` - there are one of each of these for each shared aclu.\n",
    "3. Generate the unit_id shuffled (`shuffled_aclus`, `shuffle_IDXs`) ahead of time to use to shuffle the two templates during the epochs.\n",
    "4. For each replay event, take each shuffled template\n",
    "\t4a. Iterate through each shuffle and obtain the shuffled templates like `long_pf_peak_ranks[epoch_specific_shuffled_indicies]`, `short_pf_peak_ranks[epoch_specific_shuffled_indicies]`\n",
    "\t4b. compute the spearman rank-order of the event and each shuffled template, and accumulate the results in `long_spearmanr_rank_stats_results`, `short_spearmanr_rank_stats_results`\n",
    "\n",
    "5. After we're done with the shuffle loop, accumulate the results and convert to the right output format.\n",
    "\n",
    "6. When all epochs are done, loop through the results (the epochs again) and compute the z-scores for each epoch so they can be compared to each other. Keep track of the means and std_dev for comparisons later, and subtract the two sets of z-scores (long/short) to get the delta_Z for each template.\n",
    "\n",
    "7. TODO: Next figure out what to do with the array of z-scores and delta_Z. We have:\n",
    "\tn_epochs sets of results\n",
    "\t\tn_shuffles scores of delta_Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd9d61",
   "metadata": {},
   "source": [
    "## Convo with Kamran 2023-10-23:\n",
    "- Use directional templates **\n",
    "- No need to worry about re-ranking\n",
    "[X] Plot the long and short separately in addition to the difference, so we show significant reqplay on each as a sanity check\n",
    "[X] Absolute value difference?\n",
    "[X] Fisher transform the correlation values (check if there is a difference) because correlation coefficients aren't going to be normally distributed.\n",
    "\t[ ] Then Z-score releative to fisher.\n",
    "\n",
    "- T-test to compare to mean of zero (if looking at the difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ffd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concerns:\n",
    "# 1. Permutation recommended over shuffling for small numbers of ids\n",
    "# 2.\n",
    "\n",
    "# 5Hz thresholding of templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86cb20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:22:34.093953500Z",
     "start_time": "2023-11-16T23:22:33.960957900Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nptyping import NDArray\n",
    "from attrs import define, field, Factory, astuple\n",
    "import scipy.stats\n",
    "from scipy import ndimage\n",
    "from neuropy.utils.misc import build_shuffled_ids # used in _SHELL_analyze_leave_one_out_decoding_results\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import pho_stats_paired_t_test\n",
    "\n",
    "# minimum_inclusion_fr_Hz: float = 2.0\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "\n",
    "# Recover from the saved global result:\n",
    "directional_laps_results = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "## Pre 2023-11-22 method: building a TrackTemplates object after getting the raw decoders:\n",
    "# long_LR_one_step_decoder_1D, long_RL_one_step_decoder_1D, short_LR_one_step_decoder_1D, short_RL_one_step_decoder_1D = directional_laps_results.get_decoders()\n",
    "# long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = directional_laps_results.get_shared_aclus_only_decoders()\n",
    "# track_templates: TrackTemplates = TrackTemplates.init_from_paired_decoders(LR_decoder_pair=(long_LR_decoder, short_LR_decoder), RL_decoder_pair=(long_RL_decoder, short_RL_decoder))\n",
    "# # track_templates: TrackTemplates = TrackTemplates.init_from_paired_decoders(LR_decoder_pair=(long_LR_one_step_decoder_1D, short_LR_one_step_decoder_1D), RL_decoder_pair=(long_RL_one_step_decoder_1D, short_RL_one_step_decoder_1D)) # NOTE: now use the un-constrained versions\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "all_directional_decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = dict(zip(all_directional_decoder_names, [deepcopy(long_LR_pf1D_Decoder), deepcopy(long_RL_pf1D_Decoder), deepcopy(short_LR_pf1D_Decoder), deepcopy(short_RL_pf1D_Decoder)]))\n",
    "\n",
    "\n",
    "# `LongShortStatsItem` form (2024-01-02):\n",
    "# LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "# RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb454c",
   "metadata": {
    "tags": [
     "histogram"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_rank_order_histograms\n",
    "\n",
    "# Plot histograms:\n",
    "post_title_info: str = f'{minimum_inclusion_fr_Hz} Hz\\n{curr_active_pipeline.get_session_context().get_description()}'\n",
    "_out_z_score, _out_real, _out_most_likely_z = plot_rank_order_histograms(rank_order_results, post_title_info=post_title_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b641e1f6",
   "metadata": {},
   "source": [
    "#TODO 2023-12-10 19:56: - [ ] Histogram Display Helpers\n",
    "\n",
    "#TODO 2023-12-10 19:56: - [ ] Pf1D Helpers\n",
    "\n",
    "#TODO 2023-12-10 19:57: - [ ] Variant Saving\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ed6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=0.0) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# filtered_decoder_list = [filtered_by_frate(a_decoder, minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, debug_print=True) for a_decoder in (long_LR_one_step_decoder_1D, long_RL_one_step_decoder_1D, short_LR_one_step_decoder_1D, short_RL_one_step_decoder_1D)]\n",
    "original_neuron_ids_list = [a_decoder.pf.ratemap.neuron_ids for a_decoder in (long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder)]\n",
    "is_aclu_included_list = [a_decoder.pf.ratemap.tuning_curve_unsmoothed_peak_firing_rates >= minimum_inclusion_fr_Hz for a_decoder in (long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder)]\n",
    "filtered_aclus_list = [np.array(a_decoder.pf.ratemap.neuron_ids)[a_decoder.pf.ratemap.tuning_curve_unsmoothed_peak_firing_rates >= minimum_inclusion_fr_Hz] for a_decoder in (long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder)]\n",
    "\n",
    "## For a given run direction (LR/RL) let's require inclusion in either (OR) long v. short to be included.\n",
    "filtered_included_LR_aclus = np.union1d(filtered_aclus_list[0], filtered_aclus_list[2])\n",
    "filtered_included_RL_aclus = np.union1d(filtered_aclus_list[1], filtered_aclus_list[3])\n",
    "# build the final shared aclus:\n",
    "filtered_direction_shared_aclus_list = [filtered_included_LR_aclus, filtered_included_RL_aclus, filtered_included_LR_aclus, filtered_included_RL_aclus] # contains the shared aclus for that direction\n",
    "# rebuild the is_aclu_included_list from the shared aclus\n",
    "is_aclu_included_list = [np.isin(an_original_neuron_ids, a_filtered_neuron_ids) for an_original_neuron_ids, a_filtered_neuron_ids in zip(original_neuron_ids_list, filtered_direction_shared_aclus_list)]\n",
    "\n",
    "# is_aclu_included_list[0]\n",
    "filtered_direction_shared_aclus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017813cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for 5Hz:\n",
    "# [array([  5,   7,  31,  39,  41,  45,  46,  48,  50,  55,  61,  62,  64,  69,  72,  75,  76,  78,  79,  83,  84,  86,  88,  90,  91,  92,  95,  99, 100, 108]),\n",
    "#  array([  5,   7,   9,  31,  32,  39,  41,  45,  46,  48,  50,  55,  61,  62,  64,  69,  72,  75,  76,  78,  79,  83,  84,  86,  88,  90,  91,  92,  93,  95,  99, 101, 108]),\n",
    "#  array([  5,   7,  31,  39,  41,  45,  46,  48,  50,  55,  61,  62,  64,  69,  72,  75,  76,  78,  79,  83,  84,  86,  88,  90,  91,  92,  95,  99, 100, 108]),\n",
    "#  array([  5,   7,   9,  31,  32,  39,  41,  45,  46,  48,  50,  55,  61,  62,  64,  69,  72,  75,  76,  78,  79,  83,  84,  86,  88,  90,  91,  92,  93,  95,  99, 101, 108])]\n",
    "\n",
    "# # for 20Hz:\n",
    "# [array([  5,  41,  46,  48,  69,  78,  79,  83,  86,  88,  90, 108]),\n",
    "#  array([ 62,  64,  75,  78,  83,  91, 101]),\n",
    "#  array([  5,  41,  46,  48,  69,  78,  79,  83,  86,  88,  90, 108]),\n",
    "#  array([ 62,  64,  75,  78,  83,  91, 101])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ccfe1f",
   "metadata": {},
   "source": [
    "# 2023-11-22 - RECOMPUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalLapsHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "## clear the old values to prepare for the new ones:\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'] = None\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'] = None\n",
    "del curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "del curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'] = DirectionalLapsHelpers.build_global_directional_result_from_natural_epochs(curr_active_pipeline, progress_print=True) # repalce the directional laps object\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "\n",
    "num_shuffles = 500\n",
    "\n",
    "minimum_inclusion_fr_Hz = 5.0\n",
    "included_qclu_values = [1,2]\n",
    "\n",
    "# minimum_inclusion_fr_Hz = 1.0\n",
    "# included_qclu_values = [1,2,4,9]\n",
    "\n",
    "# perform_rank_order_shuffle_analysis\n",
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-perform_rank_order_shuffle_analysis_{curr_active_pipeline.session_name}_num_shuffles-{num_shuffles}.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "    ## DO ALL:\n",
    "    RankOrderGlobalComputationFunctions.perform_rank_order_shuffle_analysis(curr_active_pipeline, curr_active_pipeline.global_computation_results, None, None, include_includelist=None, debug_print=False,\n",
    "                                                                            num_shuffles=num_shuffles, minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, skip_laps=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715580af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom `RankOrderAnalyses.most_likely_directional_rank_order_shuffling(...)`\n",
    "# Requires \"New method 2023-12-15\" result\n",
    "# Set the global result:\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "try:\n",
    "\tprint(f'\\tdone. building global result.')\n",
    "\tcurr_active_pipeline.global_computation_results.computed_data['RankOrder'].adding_active_aclus_info()\n",
    "\tcurr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].laps_most_likely_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline)\n",
    "\n",
    "except (AssertionError, BaseException) as e:\n",
    "\tprint(f'Issue with `RankOrderAnalyses.most_likely_directional_rank_order_shuffling(...)` e: {e}')\n",
    "\traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].laps_most_likely_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222cfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "## Extract the rank_order_results:\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "rank_order_results.adding_active_aclus_info()\n",
    "\n",
    "directional_laps_results = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=rank_order_results.minimum_inclusion_fr_Hz) # non-shared-only\n",
    "decoders_dict = track_templates.get_decoders_dict() # decoders_dict = {'long_LR': track_templates.long_LR_decoder, 'long_RL': track_templates.long_RL_decoder, 'short_\n",
    "# LR': track_templates.short_LR_decoder, 'short_RL': track_templates.short_RL_decoder, }\n",
    "\n",
    "# Get the `directional_merged_decoders_result` to determining most-likely direction from the merged pseudo-2D decoder:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "# directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "# directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_marginals = DirectionalMergedDecodersResult.determine_directional_likelihoods(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir = laps_marginals\n",
    "\n",
    "ripple_marginals = DirectionalMergedDecodersResult.determine_directional_likelihoods(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir = ripple_marginals\n",
    "\n",
    "\n",
    "# directional_merged_decoders_result.\n",
    "# ripple_most_likely_result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08112e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_directional_all_epoch_bins_marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e450b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_replay_epochs_df # 'Long_normed_LR_evidence', 'Long_normed_RL_evidence', 'Short_normed_LR_evidence', 'Short_normed_RL_evidence'\n",
    "# active_replay_epochs_df[['Long_normed_LR_evidence', 'Short_normed_RL_evidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-01-04 - DirectionalMergedDecoders version:\n",
    "# NOTE: ripple_most_likely_direction_from_decoder comes with with more epochs than the already filtered `rank_order_results.ripple_combined_epoch_stats_df` version. We'll get only the active indicies from `rank_order_results.ripple_combined_epoch_stats_df.index`\n",
    "# needs: rank_order_results, ripple_most_likely_direction_from_decoder, ripple_directional_all_epoch_bins_marginal, \n",
    "combined_best_direction_indicies = deepcopy(ripple_most_likely_direction_from_decoder) # .shape (611,)\n",
    "# np.shape(combined_best_direction_indicies)\n",
    "combined_best_direction_indicies = combined_best_direction_indicies[rank_order_results.ripple_combined_epoch_stats_df['label'].to_numpy()] # get only the indicies for the active epochs\n",
    "# np.shape(combined_best_direction_indicies)\n",
    "assert np.shape(combined_best_direction_indicies)[0] == np.shape(rank_order_results.ripple_combined_epoch_stats_df)[0]\n",
    "long_best_direction_indicies = combined_best_direction_indicies.copy() # use same (globally best) indicies for Long/Short\n",
    "short_best_direction_indicies = combined_best_direction_indicies.copy() # use same (globally best) indicies for Long/Short\n",
    "\n",
    "# gets the LR likelihood for each of these (long/short)\n",
    "long_relative_direction_likelihoods = ripple_directional_all_epoch_bins_marginal[rank_order_results.ripple_combined_epoch_stats_df['label'].to_numpy(), 0] # (n_epochs, 2)\n",
    "short_relative_direction_likelihoods = ripple_directional_all_epoch_bins_marginal[rank_order_results.ripple_combined_epoch_stats_df['label'].to_numpy(), 0] # (n_epochs, 2)\n",
    "\n",
    "ripple_directional_likelihoods_tuple: DirectionalRankOrderLikelihoods = DirectionalRankOrderLikelihoods(long_relative_direction_likelihoods=long_relative_direction_likelihoods,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshort_relative_direction_likelihoods=short_relative_direction_likelihoods,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlong_best_direction_indices=long_best_direction_indicies, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshort_best_direction_indices=short_best_direction_indicies,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3976821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "## Main\n",
    "ripple_result_tuple, laps_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline, decoding_time_bin_size=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0145e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-01-04 - DirectionalMergedDecoders version:\n",
    "# NOTE: laps_most_likely_direction_from_decoder comes with with more epochs than the already filtered `rank_order_results.laps_combined_epoch_stats_df` version. We'll get only the active indicies from `rank_order_results.ripple_combined_epoch_stats_df.index`\n",
    "# needs: rank_order_results, laps_most_likely_direction_from_decoder, laps_directional_all_epoch_bins_marginal, \n",
    "laps_marginals = DirectionalMergedDecodersResult.determine_directional_likelihoods(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir = laps_marginals\n",
    "\n",
    "combined_best_direction_indicies = deepcopy(laps_most_likely_direction_from_decoder) # .shape (611,)\n",
    "# np.shape(combined_best_direction_indicies)\n",
    "combined_best_direction_indicies = combined_best_direction_indicies[rank_order_results.laps_combined_epoch_stats_df['label'].to_numpy()] # get only the indicies for the active epochs\n",
    "# np.shape(combined_best_direction_indicies)\n",
    "assert np.shape(combined_best_direction_indicies)[0] == np.shape(rank_order_results.laps_combined_epoch_stats_df)[0]\n",
    "long_best_direction_indicies = combined_best_direction_indicies.copy() # use same (globally best) indicies for Long/Short\n",
    "short_best_direction_indicies = combined_best_direction_indicies.copy() # use same (globally best) indicies for Long/Short\n",
    "\n",
    "# gets the LR likelihood for each of these (long/short)\n",
    "long_relative_direction_likelihoods = laps_directional_all_epoch_bins_marginal[rank_order_results.laps_combined_epoch_stats_df['label'].to_numpy(), 0] # (n_epochs, 2)\n",
    "short_relative_direction_likelihoods = laps_directional_all_epoch_bins_marginal[rank_order_results.laps_combined_epoch_stats_df['label'].to_numpy(), 0] # (n_epochs, 2)\n",
    "\n",
    "laps_directional_likelihoods_tuple: DirectionalRankOrderLikelihoods = DirectionalRankOrderLikelihoods(long_relative_direction_likelihoods=long_relative_direction_likelihoods,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshort_relative_direction_likelihoods=short_relative_direction_likelihoods,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlong_best_direction_indices=long_best_direction_indicies, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshort_best_direction_indices=short_best_direction_indicies,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "RankOrderAnalyses.percentiles_computations(rank_order_results=rank_order_results)\n",
    "laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalLapsHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "## DO just Pandas-based method and post-processing for best directions:\n",
    "RankOrderGlobalComputationFunctions.perform_pandas_based_rank_order_shuffle_analysis(curr_active_pipeline, curr_active_pipeline.global_computation_results, None, None, include_includelist=None, debug_print=True,\n",
    "                                                                        num_shuffles=1000, minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, skip_laps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c50e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n",
    "\n",
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-pandas_df_based_correlation_computations.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "\tripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)\n",
    "\n",
    "ripple_combined_epoch_stats_df\n",
    "\n",
    "# n_shuffles: [5, 50]\n",
    "# time: [\"10.4s\", \"1m 15.9s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ripple_combined_epoch_stats_df.columns)) # ['long_RL_spearman', 'long_LR_pearson', 'short_RL_spearman', 'short_RL_pearson', 'long_LR_spearman', 'short_LR_pearson', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_spearman_Z', 'short_RL_pearson_Z', 'long_LR_spearman_Z', 'short_LR_pearson_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'label']\n",
    "\n",
    "['LR_Long_spearman_Z', 'LR_Long_spearman_Z', 'LR_Long_spearman_Z', 'LR_Long_spearman_Z']\n",
    "\n",
    "{'long_LR':'LR_Long'}\n",
    "\n",
    "decoder_name_to_column_name_prefix_map: Dict[str, str] = dict(zip(['long_LR', 'long_RL', 'short_LR', 'short_RL'], ['LR_Long', 'RL_Long', 'LR_Short', 'RL_Short']))\n",
    "\n",
    "rename_fn = lambda a_name: a_name.replace(\n",
    "\n",
    "[a_name.replace( for a_name in list(ripple_combined_epoch_stats_df.columns)]\n",
    "\n",
    "\n",
    "['long_RL_spearman', 'long_LR_pearson', 'short_RL_spearman', 'short_RL_pearson', 'long_LR_spearman', 'short_LR_pearson', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_spearman_Z', 'short_RL_pearson_Z', 'long_LR_spearman_Z', 'short_LR_pearson_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z', 'label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef655eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_column_rename_dict(column_names: List[str], decoder_name_to_column_name_prefix_map:Optional[Dict[str,str]]=None) -> Dict[str,str]:\n",
    "  \"\"\" \n",
    "  \n",
    "  column_names = ['long_RL_spearman', 'long_LR_pearson', 'short_RL_spearman', 'short_RL_pearson', 'long_LR_spearman', 'short_LR_pearson', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_spearman_Z', 'short_RL_pearson_Z', 'long_LR_spearman_Z', 'short_LR_pearson_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z']\n",
    "  decoder_name_to_column_name_prefix_map = dict(zip(['long_LR', 'long_RL', 'short_LR', 'short_RL'], ['LR_Long', 'RL_Long', 'LR_Short', 'RL_Short']))\n",
    "\n",
    "  old_to_new_names = build_column_rename_dict(column_names, decoder_name_to_column_name_prefix_map.copy())\n",
    "  print(old_to_new_names)\n",
    "\n",
    "  {'long_RL_spearman': 'RL_Long_spearman', 'long_LR_pearson': 'LR_Long_pearson', 'short_RL_spearman': 'RL_Short_spearman', 'short_RL_pearson': 'RL_Short_pearson', 'long_LR_spearman': 'LR_Long_spearman', 'short_LR_pearson': 'LR_Short_pearson', 'short_LR_spearman': 'LR_Short_spearman', 'long_RL_pearson': 'RL_Long_pearson', 'long_RL_spearman_Z': 'RL_Long_spearman_Z', 'long_LR_pearson_Z': 'LR_Long_pearson_Z', 'short_RL_spearman_Z': 'RL_Short_spearman_Z', 'short_RL_pearson_Z': 'RL_Short_pearson_Z', 'long_LR_spearman_Z': 'LR_Long_spearman_Z', 'short_LR_pearson_Z': 'LR_Short_pearson_Z', 'short_LR_spearman_Z': 'LR_Short_spearman_Z', 'long_RL_pearson_Z': 'RL_Long_pearson_Z'}\n",
    "  \"\"\"\n",
    "  if decoder_name_to_column_name_prefix_map is None:\n",
    "    decoder_name_to_column_name_prefix_map = dict(zip(['long_LR', 'long_RL', 'short_LR', 'short_RL'], ['LR_Long', 'RL_Long', 'LR_Short', 'RL_Short']))\n",
    "  \n",
    "  old_to_new_names = {}\n",
    "  for col in column_names:\n",
    "    for decoder_name, prefix in decoder_name_to_column_name_prefix_map.items():\n",
    "      if decoder_name in col:\n",
    "        new_col = prefix + col.split(decoder_name)[-1]\n",
    "        old_to_new_names[col] = new_col\n",
    "  return old_to_new_names\n",
    "  \n",
    "column_names = ['long_RL_spearman', 'long_LR_pearson', 'short_RL_spearman', 'short_RL_pearson', 'long_LR_spearman', 'short_LR_pearson', 'short_LR_spearman', 'long_RL_pearson', 'long_RL_spearman_Z', 'long_LR_pearson_Z', 'short_RL_spearman_Z', 'short_RL_pearson_Z', 'long_LR_spearman_Z', 'short_LR_pearson_Z', 'short_LR_spearman_Z', 'long_RL_pearson_Z']\n",
    "old_to_new_names = build_column_rename_dict(column_names)\n",
    "print(old_to_new_names)\n",
    "ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df.rename(columns=old_to_new_names, inplace=False)\n",
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aac86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df.LR_Long_spearman_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "override_decoder_aclu_peak_map_dict = deepcopy(decoder_aclu_peak_map_dict)\n",
    "active_selected_spikes_df = RankOrderAnalyses._subfn_build_all_pf_peak_x_columns(track_templates, selected_spikes_df=selected_spikes_df, override_decoder_aclu_peak_map_dict=override_decoder_aclu_peak_map_dict)\n",
    "epoch_id_grouped_selected_spikes_df =  active_selected_spikes_df.groupby('Probe_Epoch_id') # I can even compute this outside the loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_selected_spikes_df = deepcopy(epoch_id_grouped_selected_spikes_df)\n",
    "active_selected_spikes_df = RankOrderAnalyses._subfn_build_all_pf_peak_x_columns(track_templates, selected_spikes_df=active_selected_spikes_df, override_decoder_aclu_peak_map_dict=override_decoder_aclu_peak_map_dict)\n",
    "active_selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ff169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#TODO 2023-12-18 13:20: - [ ] This assumes that `'Probe_Epoch_id'` is correct and consistent for both directions, yeah?\n",
    "\n",
    "## Compute real values here:\n",
    "decoder_names = track_templates.get_decoder_names()\n",
    "\n",
    "epoch_id_grouped_selected_spikes_df =  active_selected_spikes_df.groupby('Probe_Epoch_id') # I can even compute this outside the loop?\n",
    "spearman_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='spearman', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "pearson_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='pearson', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "\n",
    "real_stats_df = pd.concat((spearman_correlations, pearson_correlations), axis='columns')\n",
    "real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()] # drop duplicated 'Probe_Epoch_id' column\n",
    "# Change column type to uint64 for column: 'Probe_Epoch_id'\n",
    "real_stats_df = real_stats_df.astype({'Probe_Epoch_id': 'uint64'})\n",
    "# Rename column 'Probe_Epoch_id' to 'label'\n",
    "real_stats_df = real_stats_df.rename(columns={'Probe_Epoch_id': 'label'})\n",
    "real_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d5ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "\n",
    "# 10m 29.5s for 1000 shuffles.  c:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\viztracer_2023-11-22_16-11-perform_rank_order_shuffle_analysis.json\n",
    "\n",
    "# 3m 33.9s - 500\n",
    "# 3m 26.4s - 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d74122",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ripples_outputs = RankOrderAnalyses.main_ripples_analysis(curr_active_pipeline, num_shuffles=500, rank_alignment='median', minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values)\n",
    "(LR_ripple_outputs, RL_ripple_outputs, ripple_evts_paired_tests) = _ripples_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76178217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR_ripple_outputs.epochs_df\n",
    "LR_ripple_outputs.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_ripple_outputs.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_ripple_outputs.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b664cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_ripple_outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc41020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure equivalence of the two LR_ripple_outputs and RL_ripple_outputs for the fields that matter:\n",
    "assert LR_ripple_outputs.spikes_df.equals(RL_ripple_outputs.spikes_df), f\"spikes_df are not equal\"\n",
    "assert LR_ripple_outputs.selected_spikes_df.equals(RL_ripple_outputs.selected_spikes_df), f\"selected_spikes_df are not equal\"\n",
    "assert LR_ripple_outputs.epochs_df.equals(RL_ripple_outputs.epochs_df), f\"epochs_df are not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_rank_order_event_raster_debugger = RankOrderRastersDebugger.init_rank_order_debugger(deepcopy(LR_ripple_outputs.selected_spikes_df), deepcopy(LR_ripple_outputs.epochs_df), track_templates, rank_order_results, None, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe68671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeError: <lambda>() missing 1 required positional argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recompute just the `most_likely_directional_rank_order_shuffling` part:\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderLikelihoods\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "## Main\n",
    "ripple_result_tuple, laps_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline)\n",
    "ripple_result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc170273",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple = ripple_result_tuple, laps_result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_likelihoods_tuple: DirectionalRankOrderLikelihoods = deepcopy(ripple_result_tuple.directional_likelihoods_tuple)\n",
    "directional_likelihoods_tuple.long_best_direction_indices\n",
    "directional_likelihoods_tuple.short_best_direction_indices\n",
    "# directional_likelihoods_tuple.long_relative_direction_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c292d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Saving/Loading `DirectionalLaps_2Hz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6c50e",
   "metadata": {
    "tags": [
     "save",
     "persistance"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import save_rank_order_results\n",
    "\n",
    "# DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "# DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "# print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "# NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "# NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "# print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "formatted_time = get_now_rounded_time_str()\n",
    "print(formatted_time)\n",
    "save_rank_order_results(curr_active_pipeline, day_date=f\"{formatted_time}\") # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm\n",
    "# '2024-01-09_0125PM-minimum_inclusion_fr-5-included_qclu_values-[1, 2]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_rank_order_results(owning_active_pipeline, day_date=f\"{formatted_time}_partial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_path = Path('/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/').resolve()\n",
    "sorted(search_path.glob(f\"{DAY_DATE_TO_USE}*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da8a7c",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import SaveStringGenerator\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadData\n",
    "\n",
    "# Load the data from a file into the pipeline:\n",
    "# out_filename_str: str = '2023-12-11-minimum_inclusion_fr_Hz_2_included_qclu_values_1-2_' # specific\n",
    "\n",
    "minimum_inclusion_fr_Hz: float = 5.0\n",
    "included_qclu_values: List[int] = [1,2]\n",
    "out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date=f'{DAY_DATE_TO_USE}_11am') # '2023-12-21_349am'\n",
    "# out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date='2023-12-22_312pm') # '2023-12-21_349am'\n",
    "print(f'out_filename_str: \"{out_filename_str}\"')\n",
    "# day_date_str: str = '2023-12-11_with_tuple_newer_'\n",
    "# day_date_str: str = ''\n",
    "directional_laps_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}DirectionalLaps.pkl').resolve()\n",
    "assert directional_laps_output_path.exists()\n",
    "# loaded_directional_laps, loaded_rank_order = loadData(directional_laps_output_path)\n",
    "loaded_directional_laps = loadData(directional_laps_output_path)\n",
    "assert (loaded_directional_laps is not None)\n",
    "# assert (loaded_rank_order is not None)\n",
    "\n",
    "rank_order_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}RankOrder.pkl').resolve()\n",
    "loaded_rank_order = loadData(rank_order_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the loaded data to the pipeline:\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'], curr_active_pipeline.global_computation_results.computed_data['RankOrder'] = loaded_directional_laps, loaded_rank_order\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd521ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec24467335a760",
   "metadata": {},
   "source": [
    "# POST-Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c46e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": [
     "unwrap"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multi_sort_raster_browser\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons, paired_incremental_sort_neurons # _display_directional_template_debugger\n",
    "from neuropy.utils.indexing_helpers import paired_incremental_sorting, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ScrollBarWithSpinBox.ScrollBarWithSpinBox import ScrollBarWithSpinBox\n",
    "\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "## Display Testing\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph import QtGui\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import pyqtplot_build_image_bounds_extent, pyqtplot_plot_image\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "# ripple_result_tuple\n",
    "\n",
    "## Unpacks `rank_order_results`: \n",
    "# global_replays = Epoch(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# active_replay_epochs, active_epochs_df, active_selected_spikes_df = combine_rank_order_results(rank_order_results, global_replays, track_templates=track_templates)\n",
    "# active_epochs_df\n",
    "\n",
    "# ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices\n",
    "dir_index_to_direction_name_map: Dict[int, str] = {0:'LR', 1:\"RL\"}\n",
    "\n",
    "\n",
    "## All three DataFrames are the same number of rows, each with one row corresponding to an Epoch:\n",
    "active_replay_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "# active_replay_epochs_df\n",
    "\n",
    "# Change column type to int8 for columns: 'long_best_direction_indices', 'short_best_direction_indices'\n",
    "# directional_likelihoods_df = pd.DataFrame.from_dict(ripple_result_tuple.directional_likelihoods_tuple._asdict()).astype({'long_best_direction_indices': 'int8', 'short_best_direction_indices': 'int8'})\n",
    "directional_likelihoods_df = ripple_result_tuple.directional_likelihoods_df\n",
    "# directional_likelihoods_df\n",
    "\n",
    "# 2023-12-15 - Newest method:\n",
    "# laps_combined_epoch_stats_df = rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "# ripple_combined_epoch_stats_df: pd.DataFrame  = rank_order_results.ripple_combined_epoch_stats_df\n",
    "# ripple_combined_epoch_stats_df\n",
    "\n",
    "\n",
    "# # Concatenate the three DataFrames along the columns axis:\n",
    "# # Assert that all DataFrames have the same number of rows:\n",
    "# assert len(active_replay_epochs_df) == len(directional_likelihoods_df) == len(ripple_combined_epoch_stats_df), \"DataFrames have different numbers of rows.\"\n",
    "# # Assert that all DataFrames have at least one row:\n",
    "# assert len(active_replay_epochs_df) > 0, \"active_replay_epochs_df is empty.\"\n",
    "# assert len(directional_likelihoods_df) > 0, \"directional_likelihoods_df is empty.\"\n",
    "# assert len(ripple_combined_epoch_stats_df) > 0, \"ripple_combined_epoch_stats_df is empty.\"\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = pd.concat([active_replay_epochs_df.reset_index(drop=True, inplace=False), directional_likelihoods_df.reset_index(drop=True, inplace=False), ripple_combined_epoch_stats_df.reset_index(drop=True, inplace=False)], axis=1)\n",
    "# merged_complete_epoch_stats_df = merged_complete_epoch_stats_df.set_index(active_replay_epochs_df.index, inplace=False)\n",
    "\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "# merged_complete_epoch_stats_df.to_csv('output/2023-12-21_merged_complete_epoch_stats_df.csv')\n",
    "# merged_complete_epoch_stats_df\n",
    "\n",
    "laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_directional_decoder_dict_value)\n",
    "list(all_directional_decoder_dict_value.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdabd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ripple_result_tuple) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.DirectionalRankOrderResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "\n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps, partial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def register_type_display(func_to_register, type_to_register):\n",
    "\t\"\"\" adds the display function (`func_to_register`) it decorates to the class (`type_to_register) as a method\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\t@wraps(func_to_register)\n",
    "\tdef wrapper(*args, **kwargs):\n",
    "\t\treturn func_to_register(*args, **kwargs)\n",
    "\n",
    "\tfunction_name: str = func_to_register.__name__ # get the name of the function to be added as the property\n",
    "\tsetattr(type_to_register, function_name, wrapper) # set the function as a method with the same name as the decorated function on objects of the class.\t\n",
    "\treturn wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots \n",
    "\n",
    "# @register_type_display(DirectionalRankOrderResult)\n",
    "def plot_histograms(self: DirectionalRankOrderResult, **kwargs) -> \"MatplotlibRenderPlots\":\n",
    "\t\"\"\" \n",
    "\tnum='RipplesRankOrderZscore'\n",
    "\t\"\"\"\n",
    "\tprint(f'.plot_histograms(..., kwargs: {kwargs})')\n",
    "\tfig = plt.figure(layout=\"constrained\", **kwargs)\n",
    "\tax_dict = fig.subplot_mosaic(\n",
    "\t\t[\n",
    "\t\t\t[\"long_short_best_z_score_diff\", \"long_short_best_z_score_diff\"],\n",
    "\t\t\t[\"long_best_z_scores\", \"short_best_z_scores\"],\n",
    "\t\t],\n",
    "\t)\n",
    "\tplots = (pd.DataFrame({'long_best_z_scores': self.long_best_dir_z_score_values}).hist(ax=ax_dict['long_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'short_best_z_scores': self.short_best_dir_z_score_values}).hist(ax=ax_dict['short_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'long_short_best_z_score_diff': self.long_short_best_dir_z_score_diff_values}).hist(ax=ax_dict['long_short_best_z_score_diff'], bins=21, alpha=0.8),\n",
    "\t)\n",
    "\treturn MatplotlibRenderPlots(name='plot_histogram_figure', figures=[fig], axes=ax_dict)\n",
    "\n",
    "\n",
    "register_type_display(plot_histograms, DirectionalRankOrderResult)\n",
    "## Call the newly added `plot_histograms` function on the `ripple_result_tuple` object which is of type `DirectionalRankOrderResult`:\n",
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import Zscorer\n",
    "\n",
    "from pyphocorehelpers.general_helpers import GeneratedClassDefinitionType, CodeConversion\n",
    "\n",
    "CodeConversion.convert_dictionary_to_class_defn(ripple_result_tuple, class_name='DirectionalRankOrderResult', class_definition_mode=GeneratedClassDefinitionType.ATTRS_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "\n",
    "CodeConversion._isinstance_namedtuple(ripple_result_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788190b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def convert_to_parsable_type(an_obj_instance):\n",
    "\tif isinstance(an_obj_instance, (namedtuple, )):\n",
    "\t\t# use namedtuple's built-in `._asdict()` property:\n",
    "\t\treturn an_obj_instance._asdict()\n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('DirectionalRankOrderResultBase', ripple_result_tuple._asdict(), depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d47bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple._asdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1bdb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\t try saving to CSV...')\n",
    "merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "merged_complete_ripple_epoch_stats_df_output_path = curr_active_pipeline.get_output_path().joinpath(f'{DAY_DATE_TO_USE}_1247pm_merged_complete_epoch_stats_df.csv').resolve()\n",
    "merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
    "print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import reorder_columns\n",
    "\n",
    "dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4))\n",
    "reorder_columns(merged_complete_epoch_stats_df, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dceda30",
   "metadata": {},
   "source": [
    "## 2023-12-21 - Computing Spearman Percentiles as an alternative to the Z-score from shuffling, which does not seem to work for small numbers of active cells in an event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45aa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values, shuffled_results_output_dict, combined_variable_names, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles = rank_order_results.ripple_new_output_tuple\n",
    "# shuffled_results_output_dict['short_LR_pearson_Z']\n",
    "print(list(shuffled_results_output_dict.keys())) # ['short_LR_pearson_Z', 'short_LR_spearman_Z', 'short_RL_pearson_Z', 'short_RL_spearman_Z', 'long_LR_pearson_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_spearman_Z']\n",
    "\n",
    "['long_LR_pearson_Z', 'long_RL_pearson_Z', 'short_LR_pearson_Z', 'short_RL_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b40bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-12-22 - Add the LR-LR, RL-RL differences\n",
    "merged_complete_epoch_stats_df['LongShort_LR_quantile_diff'] = merged_complete_epoch_stats_df['LR_Long_rank_percentile'] - merged_complete_epoch_stats_df['LR_Short_rank_percentile']\n",
    "merged_complete_epoch_stats_df['LongShort_RL_quantile_diff'] = merged_complete_epoch_stats_df['RL_Long_rank_percentile'] - merged_complete_epoch_stats_df['RL_Short_rank_percentile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import _plot_significant_event_quantile_fig\n",
    "\n",
    "# active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "# if isinstance(global_events, pd.DataFrame):\n",
    "#     active_replay_epochs = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df())\n",
    "\n",
    "\n",
    "# _out = _plot_significant_event_quantile_fig(curr_active_pipeline, significant_ripple_combined_epoch_stats_df=significant_ripple_combined_epoch_stats_df)\n",
    "# _out\n",
    "\n",
    "marker_style = dict(linestyle='None', color='#ff7f0eff', markersize=6, markerfacecolor='#ff7f0eb4', markeredgecolor='#ff7f0eff')\n",
    "\n",
    "    # dict(facecolor='#ff7f0eb4', size=8.0)\n",
    "    # fignum='best_quantiles'\n",
    "\n",
    "# ripple_combined_epoch_stats_df['combined_best_direction_indicies']\n",
    "\n",
    "_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y='LongShort_BestDir_quantile_diff', title='Sig. (>0.95) Best Quantile Diff', **marker_style, marker='o')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd89199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "perform_update_title_subtitle(fig=fig_long_pf_1D, ax=ax_long_pf_1D, title_string=title_string, subtitle_string=subtitle_string, active_context=active_context, use_flexitext_titles=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neuropy.utils.matplotlib_helpers import draw_epoch_regions\n",
    "epochs_collection, epoch_labels = draw_epoch_regions(curr_active_pipeline.sess.epochs, ax, defer_render=False, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecb4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(significant_BestDir_quantile_stats_df.columns))\n",
    "['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']\n",
    "\n",
    "for a_name in ['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']:\n",
    "\t_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y=a_name, title=f'Sig. (>0.95) {a_name}', **marker_style, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "\n",
    "df = quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].copy()\n",
    "# Create the subplots and loop through columns\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 10))\n",
    "for i, col in enumerate(df.columns):\n",
    "    df[col].plot.hist(ax=axes[i], bins=21)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Adjust layout and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd61a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsLayoutWidget(show=True)\n",
    "win.resize(800,350)\n",
    "win.setWindowTitle('Z-Scorer: Histogram')\n",
    "plt1 = win.addPlot()\n",
    "vals = quantile_results_df.LR_Long_rank_percentile\n",
    "fisher_z_transformed_vals = np.arctanh(vals)\n",
    "\n",
    "## compute standard histogram\n",
    "y, x = np.histogram(vals) # , bins=np.linspace(-3, 8, 40)\n",
    "# fisher_z_transformed_y, x = np.histogram(fisher_z_transformed_vals, bins=x)\n",
    "\n",
    "## Using stepMode=\"center\" causes the plot to draw two lines for each sample.\n",
    "## notice that len(x) == len(y)+1\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "# plt1.plot(x, fisher_z_transformed_y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,255,100,50), name='fisher_z_values')\n",
    "\n",
    "# ## Now draw all points as a nicely-spaced scatter plot\n",
    "y = pg.pseudoScatter(vals, spacing=0.15)\n",
    "# #plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5)\n",
    "plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5, symbolPen=(255,255,255,200), symbolBrush=(0,0,255,150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30cb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.concat((ripple_combined_epoch_stats_df, ripple_p_values_epoch_stats_df), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43327521",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_not(np.isnan(rank_order_results.ripple_combined_epoch_stats_df.index).any())\n",
    "# ripple_combined_epoch_stats_df.label.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.label).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.index).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60749347",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "print(f'\\tdone. building global result.')\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_output_tuple (output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles) = ripple_new_output_tuple\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "unique_Probe_Epoch_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "\t# probe_epoch_df = active_selected_spikes_df[a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id']]\n",
    "\t# epoch_unique_aclus = probe_epoch_df.aclu.unique()\n",
    "\tmask = (a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id'])\n",
    "\t# epoch_unique_aclus = active_selected_spikes_df.loc[mask, 'aclu'].unique()\n",
    "\tfor a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "\t\t# Shuffle aclus here:\n",
    "\t\tactive_selected_spikes_df.loc[mask, 'aclu'] = active_selected_spikes_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\t\tactive_selected_spikes_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# ## Shuffle aclus here:\n",
    "\t\t# # probe_epoch_df.aclu.sample(1000)\n",
    "\t\t# # a_aclu_peak_map\n",
    "\t\t# # Assuming 'df' is your DataFrame and 'column_name' is the column you want to shuffle\n",
    "\t\t# probe_epoch_df['aclu'] = probe_epoch_df['aclu'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\t# probe_epoch_df[f'{a_decoder_name}_pf_peak_x'] = probe_epoch_df.aclu.map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.aclu.map(a_aclu_peak_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d43ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of shuffles you want to do\n",
    "num_shuffles = 5\n",
    "\n",
    "# Create a list to hold the shuffled DataFrames\n",
    "shuffled_dfs = []\n",
    "\n",
    "for i in range(num_shuffles):\n",
    "    # Working on a copy of the DataFrame\n",
    "    shuffled_df = active_selected_spikes_df.copy()\n",
    "    \n",
    "    for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "        mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "        shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "        \n",
    "    # Adding the shuffled DataFrame to the list\n",
    "    shuffled_dfs.append(shuffled_df)\n",
    "\n",
    "# Now applying the mapping\n",
    "for i in range(num_shuffles):\n",
    "    shuffled_df = shuffled_dfs[i]\n",
    "    \n",
    "    for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "        mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "        \n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "        \n",
    "    # Replacing the shuffled DataFrame in the list after mapping has been applied\n",
    "    shuffled_dfs[i] = shuffled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dfs\n",
    "\n",
    "'polars[pandas,numpy,pyarrow,fsspec,connectorx,plot]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c306ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-01-09 - More Efficient\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df: pd.DataFrame):\n",
    "    \"\"\" 2024-01-09 - Candidate for moving into RankOrderComputations \n",
    "    captures: decoder_names\n",
    "    \n",
    "    Usage:\n",
    "    \n",
    "    shuffled_dfs = _perform_efficient_shuffle(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "    \n",
    "    \"\"\"\n",
    "    decoder_names = track_templates.get_decoder_names()\n",
    "    \n",
    "    ## Compute real values here:\n",
    "    epoch_id_grouped_selected_spikes_df = active_selected_spikes_df.groupby('Probe_Epoch_id') # I can even compute this outside the loop?\n",
    "\n",
    "    # spearman_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='spearman', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "    # pearson_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='pearson', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "\n",
    "    # real_stats_df = pd.concat((spearman_correlations, pearson_correlations), axis='columns')\n",
    "    # real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()] # drop duplicated 'Probe_Epoch_id' column\n",
    "    # # Change column type to uint64 for column: 'Probe_Epoch_id'\n",
    "    # real_stats_df = real_stats_df.astype({'Probe_Epoch_id': 'uint64'})\n",
    "    # # Rename column 'Probe_Epoch_id' to 'label'\n",
    "    # real_stats_df = real_stats_df.rename(columns={'Probe_Epoch_id': 'label'})\n",
    "    \n",
    "    # Parallelize correlation computations if required\n",
    "    correlations = []\n",
    "    for method in ['spearman', 'pearson']:\n",
    "        correlations.append(\n",
    "            epoch_id_grouped_selected_spikes_df.apply(\n",
    "                lambda group: RankOrderAnalyses._subfn_calculate_correlations(\n",
    "                    group, method=method, decoder_names=decoder_names)\n",
    "            )\n",
    "        )\n",
    "  \n",
    "    # Adjust and join all calculated correlations\n",
    "    real_stats_df = pd.concat(correlations, axis='columns').reset_index()\n",
    "    real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()]\n",
    "\n",
    "    real_stats_df.rename(columns={'Probe_Epoch_id': 'label'}, inplace=True)\n",
    "    real_stats_df['label'] = real_stats_df['label'].astype('uint64')  # in-place type casting\n",
    "    \n",
    "    return real_stats_df\n",
    "\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "def _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    \"\"\" 2024-01-09 - Performs the shuffles in a simple way\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "\n",
    "    # Create a list to hold the shuffled dataframes\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        # Working on a copy of the DataFrame\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "            \n",
    "            # Shuffle 'aclu' values\n",
    "            shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "            \n",
    "            # # Apply aclu peak map dictionary to 'aclu' column\n",
    "            # for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            #     shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "            \n",
    "\n",
    "        # end `for a_probe_epoch_ID`\n",
    "        # Once done, apply the aclu peak maps to shuffled_df's 'aclu' column:\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df[f'{a_decoder_name}_pf_peak_x'] = shuffled_df.aclu.map(a_aclu_peak_map)\n",
    "            \n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "        \n",
    "        # Adding the shuffled DataFrame to the list\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "        \n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "def _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles: int = 5):\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    def map_dict_to_group(group, a_dict, column):\n",
    "        group[column] = group[column].map(a_dict)\n",
    "        return group\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'] = shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df = shuffled_df.groupby('Probe_Epoch_id').apply(map_dict_to_group, a_dict=a_aclu_peak_map, column=f'{a_decoder_name}_pf_peak_x')\n",
    "\n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "\n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-suggested_perform_efficient_shuffle.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "shuffled_dfs, shuffled_stats_dfs = _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 50, 1m 21.2s, 10, 16.1s\n",
    "# shuffled_dfs, shuffled_stats_dfs = _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 10, 12.8s\n",
    "\n",
    "\n",
    "shuffled_dfs\n",
    "shuffled_stats_dfs\n",
    "# 5, 4.1 sec\n",
    "# 0.5s!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values = shuffled_stats_dfs\n",
    "# Build the output `stacked_arrays`: _________________________________________________________________________________ #\n",
    "\n",
    "stacked_arrays = np.stack([a_shuffle_real_stats_df[combined_variable_names].to_numpy() for a_shuffle_real_stats_df in output_active_epoch_computed_values], axis=0) # for compatibility: .shape (n_shuffles, n_epochs, n_columns)\n",
    "# stacked_df = pd.concat(output_active_epoch_computed_values, axis='index')\n",
    "\n",
    "## Drop any shuffle indicies where NaNs are returned for any of the stats values.\n",
    "is_valid_row = np.logical_not(np.isnan(stacked_arrays)).all(axis=(1,2)) # row [0, 66, :] is bad, ... so is [1, 66, :], ... [20, 66, :], ... they are repeated!!\n",
    "n_valid_shuffles = np.sum(is_valid_row)\n",
    "if debug_print:\n",
    "\tprint(f'n_valid_shuffles: {n_valid_shuffles}')\n",
    "valid_stacked_arrays = stacked_arrays[is_valid_row] ## Get only the rows where all elements along both axis (1, 2) are True\n",
    "\n",
    "# Need: valid_stacked_arrays, real_stacked_arrays, combined_variable_names\n",
    "combined_epoch_stats_df: pd.DataFrame = pd.DataFrame(real_stacked_arrays, columns=combined_variable_names)\n",
    "combined_variable_z_score_column_names = [f\"{a_name}_Z\" for a_name in combined_variable_names] # combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']\n",
    "\n",
    "## Extract the stats values for each shuffle from `valid_stacked_arrays`:\n",
    "n_epochs = np.shape(real_stacked_arrays)[0]\n",
    "n_variables = np.shape(real_stacked_arrays)[1]\n",
    "\n",
    "# valid_stacked_arrays.shape: (n_shuffles, n_epochs, n_variables)\n",
    "assert n_epochs == np.shape(valid_stacked_arrays)[-2]\n",
    "assert n_variables == np.shape(valid_stacked_arrays)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "num_shuffles = 5\n",
    "\n",
    "# Define the operation to be run in parallel for a shuffle iteration\n",
    "def shuffle_iteration(i):\n",
    "    # Working on a copy of the DataFrame\n",
    "    shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "    for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "        mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "\n",
    "        # Shuffle 'aclu' values\n",
    "        shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        # Apply aclu peak map dictionary to 'aclu' column\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    # Return the shuffled DataFrame\n",
    "    return shuffled_df\n",
    "\n",
    "# Create a list to hold the shuffled dataframes\n",
    "shuffled_dfs = Parallel(n_jobs=-1)(delayed(shuffle_iteration)(i) for i in range(num_shuffles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()]\n",
    "print(peak_column_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    # Apply aclu peak map dictionary to each decoder name\n",
    "    for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "        active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df['aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffles = {}\n",
    "    for i in range(num_shuffles):\n",
    "        shuffles[i] = active_selected_spikes_df.copy()\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffles[i]['Probe_Epoch_id'])\n",
    "            # Shuffle multiple columns here:\n",
    "            for a_decoder_name in decoder_aclu_peak_map_dict.keys():\n",
    "                shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'].sample(frac=1).values\n",
    "    return shuffles\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "shuffled_dfs = _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "# shuffled_dfs\n",
    "# 5, 1.5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle 'aclu' values\n",
    "shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "\n",
    "# Shuffle aclu and their corresponding peaks: ['aclu', 'long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()] # ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']] = shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']].sample(frac=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d641689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(output_active_epoch_computed_values) # 0.946189 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #TODO 2023-12-13 02:07: - [ ] Figure out how 'Probe_Epoch_id' maps to `ripple_result_tuple.active_epochs`\n",
    "ripple_result_tuple.active_epochs\n",
    "rank_order_results.LR_ripple.ranked_aclus_stats_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the pf_x information for each aclu:\n",
    "## 2023-10-11 - Get the long/short peak locations\n",
    "# decoder_peak_coms_list = [a_decoder.pf.ratemap.peak_tuning_curve_center_of_masses[is_good_aclus] for a_decoder in decoder_args]\n",
    "decoder_aclu_peak_location_dict_list = [dict(zip(neuron_IDs, peak_locations)) for neuron_IDs, peak_locations in zip(track_templates.decoder_neuron_IDs_list, track_templates.decoder_peak_location_list)]\n",
    "decoder_aclu_peak_location_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de234ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_tuning_curve_center_of_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1305ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.decoder_LR_pf_peak_ranks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b179f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replays:\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "if isinstance(global_replays, pd.DataFrame):\n",
    "\tglobal_replays = Epoch(global_replays.epochs.get_valid_df())\n",
    "\n",
    "# get the aligned epochs and the z-scores aligned to them:\n",
    "active_replay_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_replays.to_dataframe().copy(), is_laps=False)\n",
    "active_replay_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6384a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laps:\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps).trimmed_to_non_overlapping()\n",
    "active_laps_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_laps.to_dataframe(), is_laps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find only the significant events (|z| > 1.96):\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "filtered_z_score_df, (n_events, n_significant_events, percent_significant_events) = RankOrderAnalyses.find_only_significant_events(rank_order_results, high_z_criteria=1.96)\n",
    "filtered_z_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_z_score_df.index.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86532662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-11-20 - Finding high-significance periods for Kamran:\n",
    "z_threshold = 1.96\n",
    "is_greater_than_z_threshold_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) > z_threshold)\n",
    "is_greater_than_z_threshold_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) > z_threshold)\n",
    "is_significant_either = np.logical_or(is_greater_than_z_threshold_long, is_greater_than_z_threshold_short)\n",
    "is_significant_either\n",
    "\n",
    "# is_greater_than_3std_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) >= 3.0)\n",
    "# is_greater_than_3std_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) >= 3.0)\n",
    "# is_significant_either = np.logical_or(is_greater_than_3std_long, is_greater_than_3std_short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f925cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_ripple_epochs = deepcopy(Epoch(ripple_result_tuple.active_epochs)).boolean_indicies_slice(is_significant_either)\n",
    "# significant_ripple_epochs = deepcopy(global_replays).boolean_indicies_slice(is_significant_either)\n",
    "significant_ripple_epochs.to_dataframe()\n",
    "\n",
    "# significant_ripple_epochs.filename = Path(f'output/2023-11-27_SignificantReplayRipples').resolve()\n",
    "# significant_ripple_epochs.to_neuroscope()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8beece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_epochs = ripple_result_tuple.active_epochs\n",
    "active_epochs: Epoch = rank_order_results.RL_ripple.epochs_df # Epoch(rank_order_results.RL_ripple.epochs_df)\n",
    "# type(active_epochs)\n",
    "active_epochs.n_epochs\n",
    "# rank_order_results.RL_ripple.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ffe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.epochs_df\n",
    "rank_order_results.LR_ripple.spikes_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39525572",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_variable_names: ['LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson']\n",
    "combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f47973",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.build_display_context_for_filtered_session(filtered_session_name='maze_any', display_fn_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b319650",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca979a6",
   "metadata": {},
   "source": [
    "# `RankOrderRastersDebugger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616af69",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "figure",
     "RankOrderRastersDebugger"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import build_adjusted_color\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "# from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import a_debug_callback_fn, debug_update_long_short_info_titles, debug_update_plot_titles\n",
    "\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "\n",
    "## RankOrderRastersDebugger: \n",
    "# _out_rank_order_event_raster_debugger = RankOrderRastersDebugger.init_rank_order_debugger(spikes_df, ripple_result_tuple.active_epochs, track_templates, rank_order_results.RL_ripple.selected_spikes_fragile_linear_neuron_IDX_dict, rank_order_results.LR_ripple.selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "\n",
    "## Required prereqs:\n",
    "# active_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "# active_selected_spikes_df, active_epochs_df = RankOrderAnalyses.new_compute_correlations(selected_spikes_df=selected_spikes_df, active_epochs=active_epochs, track_templates=track_templates)\n",
    "## 2023-12-13 11am - uses `active_epochs_df` passed in that has been augmented with all the old results:\n",
    "# _out_rank_order_event_raster_debugger = RankOrderRastersDebugger.init_rank_order_debugger(spikes_df, active_epochs_df, track_templates, rank_order_results, rank_order_results.RL_ripple.selected_spikes_fragile_linear_neuron_IDX_dict, rank_order_results.LR_ripple.selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "# _out_rank_order_event_raster_debugger = RankOrderRastersDebugger.init_rank_order_debugger(spikes_df, active_epochs_df, track_templates, rank_order_results, rank_order_results.RL_ripple.selected_spikes_df, rank_order_results.LR_ripple.selected_spikes_df)\n",
    "# _out_rank_order_event_raster_debugger.params.enable_show_Z_values = False\n",
    "\n",
    "_out_rank_order_selected_spikes_only_event_raster_debugger = RankOrderRastersDebugger.init_rank_order_debugger(deepcopy(rank_order_results.LR_ripple.selected_spikes_df), deepcopy(rank_order_results.LR_ripple.epochs_df),\n",
    "                                                                                                                track_templates, rank_order_results, None, None, debug_print=True, debug_draw=True) # rank_order_results.RL_ripple.selected_spikes_fragile_linear_neuron_IDX_dict, rank_order_results.LR_ripple.selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_directional_template_pfs_debugger, debug_update_paired_directional_template_pfs_debugger = _out_rank_order_selected_spikes_only_event_raster_debugger.plot_attached_directional_templates_pf_debugger(curr_active_pipeline)\n",
    "active_plotter = _out_rank_order_selected_spikes_only_event_raster_debugger\n",
    "active_plotter.on_update_epoch_IDX(17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "_out_directional_template_pfs_debugger, debug_update_paired_directional_template_pfs_debugger = _out_rank_order_selected_spikes_only_event_raster_debugger.plot_attached_directional_templates_pf_debugger(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_directional_template_pfs_debugger: TemplateDebugger = _out_directional_template_pfs_debugger['obj']\n",
    "a_directional_template_pfs_debugger.plots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b433ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_directional_template_pfs_debugger.plots_data.sorted_pf_peak_location_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3a2d3",
   "metadata": {},
   "source": [
    "### Exporting `rank_order_event_raster_debugge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot, ExportFiletype\n",
    "\n",
    "export_path = Path(r'C:\\Users\\pho\\Desktop\\2023-12-19 Exports').resolve()\n",
    "\n",
    "_out_rank_order_event_raster_debugger.save_figure(export_path=export_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ebf52",
   "metadata": {},
   "source": [
    "#### Iterates through the epochs (via the slider) and saves out the images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f73ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = Path(r'C:\\Users\\pho\\Desktop\\2023-12-19 Exports').resolve()\n",
    "all_save_paths = _out_rank_order_event_raster_debugger.export_figure_all_slider_values(export_path=export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_y_values_dict = {_active_plot_identifier:{int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs} for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()}\n",
    "aclu_max_y_values_dict = {_active_plot_identifier:np.max(list({int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs}.values())) for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()} # {'long_LR': 51.48039215686274, 'long_RL': 53.5, 'short_LR': 51.48039215686274, 'short_RL': 53.5}\n",
    "global_max_y_value = np.max(list(aclu_max_y_values_dict.values()))\n",
    "global_max_y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01518ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_neurons = np.max([len(v) for v in _out_rank_order_event_raster_debugger.plots_data.unsorted_original_neuron_IDs_lists])\n",
    "max_n_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots.all_separate_plots['long_LR']['root_plot']\n",
    "\n",
    "\n",
    "root_plots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56dbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_separate_root_plots = {a_decoder_name:a_raster_setup_tuple.plots.root_plot for a_decoder_name, a_raster_setup_tuple in rasters_display_outputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_selected_spikes_only_event_raster_debugger.ui.tableView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import a_debug_callback_fn, debug_update_long_short_info_titles, debug_update_plot_titles\n",
    "\n",
    "_out_LAPS_rank_order_event_raster_debugger = RankOrderRastersDebugger.init_rank_order_debugger(spikes_df, deepcopy(rank_order_results.LR_laps.epochs_df), track_templates, rank_order_results, None, None)\n",
    "_out_LAPS_rank_order_event_raster_debugger.params.is_laps = True\n",
    "_out_LAPS_rank_order_event_raster_debugger.on_idx_changed_callback_function_dict['debug_update_plot_titles_callback'] = debug_update_plot_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15491f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc00fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll to the specified row\n",
    "tableView.scrollTo(tableView.model().index(row_index, 0))\n",
    "\n",
    "# Select the entire row\n",
    "tableView.selectRow(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots_data.LR_selected_spike_df #.LR_active_epochs_selected_spikes_fragile_linear_neuron_IDX_dict\n",
    "# _out_rank_order_event_raster_debugger.plots_data.LR_neuron_id_to_new_IDX_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a79c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(_out_rank_order_event_raster_debugger.plots_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c62746",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots_data['all_selected_spots_dict']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b093b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots_data['seperate_all_spots_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48888cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_active_plot_identifier = 'long_LR'\n",
    "new_sorted_raster = _out_rank_order_event_raster_debugger.plots_data['seperate_new_sorted_rasters_dict'][_active_plot_identifier]\n",
    "# selected_spikes_df = new_sorted_raster.update_spikes_df_visualization_columns(spikes_df=selected_spikes_df)\n",
    "# selected_spikes_df\n",
    "new_sorted_raster.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a57b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the source data (spikes_df) to the plot_data\n",
    "plots_data.spikes_df = deepcopy(spikes_df)    \n",
    "# Update the dataframe\n",
    "plots_data.spikes_df = new_sorted_raster.update_spikes_df_visualization_columns(spikes_df=plots_data.spikes_df)\n",
    "## Build the spots for the raster plot:\n",
    "_out_rank_order_event_raster_debugger.plots_data['all_selected_spots_dict'][_active_plot_identifier], _out_rank_order_event_raster_debugger.plots_data.all_selected_scatterplot_tooltips_kwargs_dict[_active_plot_identifier] = new_sorted_raster.build_spikes_all_spots_from_df(spikes_df=plots_data.spikes_df, should_return_data_tooltips_kwargs=True, generate_debug_tuples=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots_data.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86793de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.global_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce652a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_selected_spikes_df_points_to_scatter_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0cb69",
   "metadata": {},
   "source": [
    "## Connects TemplatesDebugger to RasterDebugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7136b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import a_debug_callback_fn, debug_update_long_short_info_titles, debug_update_plot_titles\n",
    "\n",
    "\n",
    "# active_plotter = _out_rank_order_event_raster_debugger\n",
    "active_plotter = _out_rank_order_selected_spikes_only_event_raster_debugger\n",
    "# _out_directional_template_pfs_debugger = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaafb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_active_aclus = deepcopy(active_plotter.get_epoch_active_aclus())\n",
    "epoch_active_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68aeeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import a_debug_callback_fn, debug_update_long_short_info_titles, debug_update_plot_titles\n",
    "\n",
    "# active_plotter = _out_rank_order_event_raster_debugger\n",
    "active_plotter = _out_rank_order_selected_spikes_only_event_raster_debugger\n",
    "\n",
    "# _out_directional_template_pfs_debugger = None\n",
    "_out_directional_template_pfs_debugger, debug_update_paired_directional_template_pfs_debugger = active_plotter.plot_attached_directional_templates_pf_debugger(curr_active_pipeline)\n",
    "\n",
    "active_plotter.on_idx_changed_callback_function_dict['a_debug_callback'] = a_debug_callback_fn\n",
    "active_plotter.on_idx_changed_callback_function_dict['debug_update_plot_titles_callback'] = debug_update_plot_titles\n",
    "active_plotter.on_idx_changed_callback_function_dict['debug_update_paired_directional_template_pfs_debugger'] = debug_update_paired_directional_template_pfs_debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a278553",
   "metadata": {},
   "source": [
    "# 2023-12-20 - Spearman Investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d242421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import CurrTesting\n",
    "\n",
    "active_plotter.active_epoch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a933b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_plotter.active_epoch_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_plotter.active_epoch_df.RL_Long_rel_num_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33545624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "rank_order_results.ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best direction for the current epoch:\n",
    "curr_best_dir_index: int = directional_likelihoods_df.long_best_direction_indices.iloc[active_plotter.active_epoch_IDX]\n",
    "print(f'curr_best_dir_index: {curr_best_dir_index}')\n",
    "curr_best_dir_name: str = dir_index_to_direction_name_map[curr_best_dir_index]\n",
    "print(f'curr_best_dir_name: {curr_best_dir_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_plotter.active_epoch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_plotter.global_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch_spikes_df = deepcopy(active_plotter.get_active_epoch_spikes_df())[['t_rel_seconds', 'aclu', 'shank', 'cluster', 'qclu', 'maze_id', 'flat_spike_idx', 'Probe_Epoch_id']]\n",
    "curr_epoch_spikes_df[\"spike_rank\"] = curr_epoch_spikes_df[\"t_rel_seconds\"].rank(method=\"average\")\n",
    "# Sort by column: 'aclu' (ascending)\n",
    "# curr_epoch_spikes_df = curr_epoch_spikes_df.sort_values(['aclu'])\n",
    "curr_epoch_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spikes = np.shape(curr_epoch_spikes_df)[0]\n",
    "curr_epoch_spikes_aclus = deepcopy(curr_epoch_spikes_df.aclu.to_numpy())\n",
    "curr_epoch_spikes_aclu_ranks = deepcopy(curr_epoch_spikes_df.spike_rank.to_numpy())\n",
    "curr_epoch_spikes_aclu_rank_map = dict(zip(curr_epoch_spikes_aclus, curr_epoch_spikes_aclu_ranks))\n",
    "n_unique_aclus = np.shape(curr_epoch_spikes_df.aclu.unique())[0]\n",
    "assert n_spikes == n_unique_aclus, f\"there is more than one spike in curr_epoch_spikes_df for an aclu! n_spikes: {n_spikes}, n_unique_aclus: {n_unique_aclus}\"\n",
    "n_spikes\n",
    "n_unique_aclus\n",
    "# curr_epoch_spikes_aclu_rank_map\n",
    "# is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df[rank_order_results.ripple_combined_epoch_stats_df.label == active_plotter.active_epoch_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df[['LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z']].iloc[[active_plotter.active_epoch_IDX]]\n",
    "\n",
    "# ['long_RL_spearman', 'long_LR_spearman', 'short_RL_spearman', 'short_LR_spearman', 'long_RL_spearman_Z', 'long_LR_spearman_Z', 'short_RL_spearman_Z', 'short_LR_spearman_Z']\n",
    "# ['long_RL_spearman', 'long_LR_spearman', 'short_RL_spearman', 'short_LR_spearman', 'long_RL_spearman_Z', 'long_LR_spearman_Z', 'short_RL_spearman_Z', 'short_LR_spearman_Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b872dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "\n",
    "# curr_epoch_spikes_df = deepcopy(active_plotter.get_active_epoch_spikes_df())[['t_rel_seconds', 'aclu', 'shank', 'cluster', 'qclu', 'maze_id', 'flat_spike_idx', 'Probe_Epoch_id']]\n",
    "\n",
    "\n",
    "# curr_epoch_spikes_df\n",
    "\n",
    "# curr_epoch_spikes_df[\"spike_rank\"] = curr_epoch_spikes_df[\"t_rel_seconds\"].rank(method=\"average\")\n",
    "# curr_epoch_spikes_df = curr_epoch_spikes_df.sort_values(['aclu']) # Sort by column: 'aclu' (ascending)\n",
    "template_spearman_real_results = CurrTesting.pho_compute_rank_order(track_templates, curr_epoch_spikes_df, rank_method=\"average\", stats_nan_policy='omit')\n",
    "template_spearman_real_results\n",
    "# {'long_LR': (-0.2922074234830293, 4),\n",
    "#  'long_RL': (-0.4582233402727546, 2),\n",
    "#  'short_LR': (-0.12496330329017007, 4),\n",
    "#  'short_RL': (-0.582141093511449, 2)}\n",
    "\n",
    "# {'long_LR': (-0.3312693498452012, 0.17932214439926802, 4),\n",
    "#  'long_RL': (-0.48872180451127817, 0.028772030054996085, 2),\n",
    "#  'short_LR': (-0.16408668730650156, 0.5152947310342115, 4),\n",
    "#  'short_RL': (-0.5954887218045113, 0.005601441030074217, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbd0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CurrTesting.pho_compute_rank_order(track_templates, curr_epoch_spikes_df, rank_method=\"average\", stats_nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891225b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_spikes_df = deepcopy(rank_order_results.LR_ripple.selected_spikes_df)\n",
    "selected_spikes_df = deepcopy(curr_epoch_spikes_df)\n",
    "real_stats_df = RankOrderAnalyses._compute_single_rank_order_shuffle(track_templates, selected_spikes_df=selected_spikes_df)\n",
    "real_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_variable_names = list(set(real_stats_df.columns) - set(['label'])) # ['RL_Short_spearman', 'RL_Long_pearson', 'RL_Short_pearson', 'LR_Long_spearman', 'LR_Short_pearson', 'LR_Long_pearson', 'LR_Short_spearman', 'RL_Long_spearman']\n",
    "real_stacked_arrays = real_stats_df[combined_variable_names].to_numpy() # for compatibility\n",
    "real_stacked_arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c078b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "\n",
    "list(decoder_aclu_peak_map_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45778248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## USES selected_spikes_df\n",
    "all_decoder_aclus_map_keys_list = [np.array(list(a_map.keys())) for a_map in (long_LR_aclu_peak_map, long_RL_aclu_peak_map, short_LR_aclu_peak_map, short_RL_aclu_peak_map)] # list of four elements\n",
    "all_shuffled_decoder_aclus_map_keys_list = [build_shuffled_ids(a_map_keys, num_shuffles=num_shuffles, seed=None)[0] for a_map_keys in all_decoder_aclus_map_keys_list] # [0] only gets the shuffled_aclus themselves, which are of shape .shape: ((num_shuffles, n_neurons[i]) where i is the decoder_index\n",
    "\n",
    "long_LR_epoch_specific_shuffled_aclus, long_RL_epoch_specific_shuffled_aclus, short_LR_epoch_specific_shuffled_aclus, short_RL_epoch_specific_shuffled_aclus = all_shuffled_decoder_aclus_map_keys_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b07a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "# _NaN_Type = np.nan\n",
    "_NaN_Type = pd.NA\n",
    "\n",
    "# long_LR_aclu_peak_map, long_RL_aclu_peak_map, short_LR_aclu_peak_map, short_RL_aclu_peak_map = track_templates.get_decoder_aclu_peak_maps()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': 'int'}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[_NaN_Type, _NaN_Type, _NaN_Type, _NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "## Normal:\n",
    "# active_selected_spikes_df['LR_Long_pf_peak_x'] = active_selected_spikes_df.aclu.map(long_LR_aclu_peak_map)\n",
    "# active_selected_spikes_df['RL_Long_pf_peak_x'] = active_selected_spikes_df.aclu.map(long_RL_aclu_peak_map)\n",
    "# active_selected_spikes_df['LR_Short_pf_peak_x'] = active_selected_spikes_df.aclu.map(short_LR_aclu_peak_map)\n",
    "# active_selected_spikes_df['RL_Short_pf_peak_x'] = active_selected_spikes_df.aclu.map(short_RL_aclu_peak_map)\n",
    "\n",
    "for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "\tactive_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.aclu.map(a_aclu_peak_map)\n",
    "\n",
    "active_selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be11f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_shuffles:int = 10\n",
    "\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "all_decoder_aclus_map_keys_dict = {a_decoder_name:np.array(list(a_map.keys())) for a_decoder_name, a_map in decoder_aclu_peak_map_dict.items()} # list of four elements\n",
    "all_decoder_aclus_map_values_dict = {a_decoder_name:np.array(list(a_map.values())) for a_decoder_name, a_map in decoder_aclu_peak_map_dict.items()} # list of four elements\n",
    "all_shuffled_decoder_aclus_map_keys_dict = {a_decoder_name:build_shuffled_ids(a_map_keys, num_shuffles=num_shuffles, seed=None)[0] for a_decoder_name, a_map_keys in all_decoder_aclus_map_keys_dict.items()} # [0] only gets the shuffled_aclus themselves, which are of shape .shape: ((num_shuffles, n_neurons[i]) where i is the decoder_index\n",
    "\n",
    "# all_shuffled_override_decoder_aclu_peak_map_dict: one for each shuffle.\n",
    "all_shuffled_override_decoder_aclu_peak_map_dict = [{a_decoder_name:dict(zip(a_decoder_specific_shuffled_aclus_arr[shuffle_IDX], all_decoder_aclus_map_values_dict[a_decoder_name])) for a_decoder_name, a_decoder_specific_shuffled_aclus_arr in all_shuffled_decoder_aclus_map_keys_dict.items()} for shuffle_IDX in np.arange(num_shuffles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1642789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_dataframe_memory_usage(all_shuffled_override_decoder_aclu_peak_map_dict)\n",
    "print_object_memory_usage(all_shuffled_override_decoder_aclu_peak_map_dict) # 0.21150970458984375\n",
    "\n",
    "single_shuffle_result_mem_usage_MB = print_object_memory_usage(all_shuffled_override_decoder_aclu_peak_map_dict)/float(10.0)\n",
    "print(f'single_shuffle_result_mem_usage_MB: {single_shuffle_result_mem_usage_MB}')\n",
    "\n",
    "single_shuffle_result_mem_usage_MB * 1000.0 # 21MB for 1000 shuffles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "# long_LR_epoch_specific_shuffled_aclus, long_RL_epoch_specific_shuffled_aclus, short_LR_epoch_specific_shuffled_aclus, short_RL_epoch_specific_shuffled_aclus = all_shuffled_decoder_aclus_map_keys_list\n",
    "\n",
    "output_active_epoch_computed_values = []\n",
    "\n",
    "for shuffle_IDX in np.arange(num_shuffles):\n",
    "\tshuffle_real_stats_df = RankOrderAnalyses._compute_single_rank_order_shuffle(track_templates, selected_spikes_df=selected_spikes_df, override_decoder_aclu_peak_map_dict=all_shuffled_override_decoder_aclu_peak_map_dict[shuffle_IDX])\n",
    "\toutput_active_epoch_computed_values.append(shuffle_real_stats_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4742c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_dfs = pd.concat(output_active_epoch_computed_values, axis='index')\n",
    "print_dataframe_memory_usage(stacked_dfs)\n",
    "stacked_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Compute real values here:\n",
    "# epoch_id_grouped_selected_spikes_df =  active_selected_spikes_df.groupby('Probe_Epoch_id') # I can even compute this outside the loop?\n",
    "# spearman_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='spearman', enable_shuffle=False)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "# pearson_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='pearson', enable_shuffle=False)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "\n",
    "\n",
    "group = \n",
    "method='spearman'\n",
    "enable_shuffle: bool=False\n",
    "\n",
    "if enable_shuffle:\n",
    "\trng = np.random.default_rng()\n",
    "\tshuffle_fn = lambda x: pd.Series(rng.permuted(x.values), index=x.index)  # x.values\n",
    "else:\n",
    "\tshuffle_fn = lambda x: x # no-op\n",
    "\n",
    "# correlations = {\n",
    "#     f'LR_Long_{method}': shuffle_fn(group['t_rel_seconds']).corr(group['LR_Long_pf_peak_x'], method=method),\n",
    "#     f'RL_Long_{method}': shuffle_fn(group['t_rel_seconds']).corr(group['RL_Long_pf_peak_x'], method=method),\n",
    "#     f'LR_Short_{method}': shuffle_fn(group['t_rel_seconds']).corr(group['LR_Short_pf_peak_x'], method=method),\n",
    "#     f'RL_Short_{method}': shuffle_fn(group['t_rel_seconds']).corr(group['RL_Short_pf_peak_x'], method=method)\n",
    "# }\n",
    "\n",
    "_decoder_names = track_templates.get_decoder_names()\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in _decoder_names]\n",
    "_output_column_names = [f'{a_decoder_name}_{method}' for a_decoder_name in _decoder_names]\n",
    "# correlations = {f'{a_decoder_name}_{method}': shuffle_fn(group['t_rel_seconds']).rank(method=\"dense\").corr(group[f'{a_decoder_name}_pf_peak_x'], method=method) for a_decoder_name in _decoder_names}\n",
    "correlations = {an_output_col_name: shuffle_fn(group['t_rel_seconds']).rank(method=\"dense\").corr(group[a_pf_peak_x_column_name], method=method) for a_decoder_name, a_pf_peak_x_column_name, an_output_col_name in zip(_decoder_names, _pf_peak_x_column_names, _output_column_names)}\n",
    "\n",
    "\n",
    "pd.Series(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_stats_df.columns # Index(['Probe_Epoch_id', 'LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'Probe_Epoch_id', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2bc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_stats_df[real_stats_df.label == active_plotter.active_epoch_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26da2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR_only_aclus, RL_only_aclus = bidirectional_setdiff1d(track_templates.shared_LR_aclus_only_neuron_IDs, track_templates.shared_RL_aclus_only_neuron_IDs)\n",
    "# (array([ 2,  5,  6,  8, 19, 29, 30, 33, 34, 35, 43, 58, 83, 85, 86]),\n",
    "#  array([  4,  12,  14,  18,  20,  27,  28,  32,  38,  57,  59,  62,  63,  67,  70,  71,  90,  91,  95, 101]))\n",
    "len(LR_only_aclus)\n",
    "len(RL_only_aclus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_epoch_spikes_df = deepcopy(active_plotter.get_active_epoch_spikes_df())[['t_rel_seconds', 'aclu', 'shank', 'cluster', 'qclu', 'maze_id', 'flat_spike_idx', 'Probe_Epoch_id']]\n",
    "# curr_epoch_spikes_df[\"spike_rank\"] = curr_epoch_spikes_df[\"t_rel_seconds\"].rank(method=\"average\")\n",
    "# curr_epoch_spikes_df = curr_epoch_spikes_df.sort_values(['aclu']) # Sort by column: 'aclu' (ascending)\n",
    "template_df: pd.DataFrame = pd.DataFrame(track_templates.decoder_aclu_peak_rank_dict_dict)\n",
    "template_df = template_df.add_prefix('rank_').reset_index(names='aclu')\n",
    "spearman_rho_df = CurrTesting.compute_spearman_rank_order(spike_df=curr_epoch_spikes_df, template_df=template_df)\n",
    "print(spearman_rho_df)\n",
    "\n",
    "# template_df.columns # ['aclu', 'rank_long_LR', 'rank_long_RL', 'rank_short_LR', 'rank_short_RL']\n",
    "# template_df.reset_index(names='aclu')\n",
    "# template_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77043562",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = curr_epoch_spikes_df.merge(template_df, on=\"aclu\")\n",
    "merged_df[\"spike_rank\"] = merged_df[\"t_rel_seconds\"].rank(method=\"dense\")\n",
    "merged_df\n",
    "\n",
    "# list(track_templates.decoder_aclu_peak_rank_dict_dict.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "\n",
    "# curr_rank_column_name: str = 'rank_long_LR' # \"rank\"\n",
    "# spearman_rho_df = merged_df.apply(lambda x: pd.Series({\"spearman_rho\": x[\"spike_rank\"]}, name=x.name), axis='index')\n",
    "\n",
    "spearman_rho_dict = {}\n",
    "for curr_rank_column_name in ['rank_long_LR', 'rank_long_RL', 'rank_short_LR', 'rank_short_RL']:\n",
    "\tspearman_rho_dict[curr_rank_column_name] = [merged_df[\"spike_rank\"].corr(merged_df[curr_rank_column_name], method=\"spearman\"), ]\n",
    "\n",
    "spearman_rho_dict\n",
    "spearman_rho_df = pd.DataFrame(spearman_rho_dict) # \n",
    "spearman_rho_df.columns = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "spearman_rho_df = spearman_rho_df.add_suffix('_spearman')\n",
    "# spearman_rho_df = merged_df.apply(\n",
    "# \tlambda x: pd.DataFrame(\n",
    "# \t\t{\"spearman_rho\": x[\"spike_rank\"]}, #.corr(x[curr_rank_column_name], method=\"spearman\")},\n",
    "# \t\tcolumns=[x.name],\n",
    "# \t),\n",
    "#     axis='index',\n",
    "# )\n",
    "spearman_rho_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "spike_df = pd.DataFrame(\n",
    "    {\n",
    "        \"t_rel_seconds\": [10, 20, 30, 40, 50, 60],\n",
    "        \"aclu\": [1, 2, 1, 3, 2, 1],\n",
    "    }\n",
    ")\n",
    "template_df = pd.DataFrame(\n",
    "    {\n",
    "        \"aclu\": [1, 2, 3],\n",
    "        \"rank\": [3, 1, 2],\n",
    "    }\n",
    ")\n",
    "\n",
    "spearman_rho_df = CurrTesting.compute_spearman_rank_order(spike_df, template_df)\n",
    "\n",
    "print(spearman_rho_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f066a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_long_rank_stats = scipy.stats.spearmanr(curr_epoch_spikes_aclus, template_corresponding_aclu_rank_list, nan_policy='omit')\n",
    "real_long_rank_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9fb882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the ranks for the active aclus in the epoch:\n",
    "short_LR_peak_x_map = deepcopy(active_plotter.track_templates.get_decoder_aclu_peak_maps().short_LR)\n",
    "# np.unique(np.array(list(short_LR_peak_x_map.values())))\n",
    "\n",
    "assert CurrTesting.debug_detect_repeated_values(np.array(list(short_LR_peak_x_map.values()))) == {}, f\"No repeats allowed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import compute_placefield_center_of_mass_positions\n",
    "\n",
    "compute_placefield_center_of_mass_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23636d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratemap: Ratemap = deepcopy(track_templates.long_LR_decoder.pf.ratemap)\n",
    "ratemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fe1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def map_CoM_to_position_interp_array(xbin, tuning_curve_CoM_coordinates, xbin_edge_labels) -> NDArray:\n",
    "  \"\"\"\n",
    "  Maps center-of-mass coordinates back to actual positions as a flat numpy array.\n",
    "\n",
    "  Args:\n",
    "      xbin: np.array, the bin centers (not edges).\n",
    "      tuning_curve_CoM_coordinates: np.array, the center-of-mass coordinates for each tuning curve.\n",
    "      xbin_edge_labels: np.array, the bin edge labels (not center).\n",
    "\n",
    "  Returns:\n",
    "      positions: np.array, the estimated positions corresponding to the input center-of-mass values.\n",
    "  \"\"\"\n",
    "  positions = np.full_like(tuning_curve_CoM_coordinates, np.nan)\n",
    "  for i, CoM in enumerate(tuning_curve_CoM_coordinates):\n",
    "    # Find the bin containing the center-of-mass\n",
    "    bin_idx = (xbin_edge_labels <= CoM).sum() - 1\n",
    "    \n",
    "    # Check if CoM falls within the valid range\n",
    "    if bin_idx == 0 or bin_idx == len(xbin) - 1:\n",
    "      continue  # Skip invalid values and leave them as NaN\n",
    "\n",
    "    # Interpolate between bin edges\n",
    "    interpolator = interp1d(xbin_edge_labels[bin_idx:bin_idx+2], xbin[bin_idx:bin_idx+2], kind=\"linear\")\n",
    "    positions[i] = interpolator(CoM)\n",
    "  return positions\n",
    "\n",
    "def map_CoM_to_position_weighted_average(xbin, tuning_curve_CoM_coordinates, xbin_edge_labels):\n",
    "  \"\"\"\n",
    "  Maps center-of-mass coordinates back to actual positions using inverse weighted average.\n",
    "\n",
    "  Args:\n",
    "      xbin: np.array, the bin centers (not edges).\n",
    "      tuning_curve_CoM_coordinates: np.array, the center-of-mass coordinates for each tuning curve.\n",
    "      xbin_edge_labels: np.array, the bin edge labels (not center).\n",
    "\n",
    "  Returns:\n",
    "      positions: np.array, the estimated positions corresponding to the input center-of-mass values.\n",
    "  \"\"\"\n",
    "  positions = np.full_like(tuning_curve_CoM_coordinates, np.nan)\n",
    "  for i, CoM in enumerate(tuning_curve_CoM_coordinates):\n",
    "    # Find contributing bins\n",
    "    bin_idx1 = (xbin_edge_labels < CoM).sum() - 1\n",
    "    bin_idx2 = (xbin_edge_labels >= CoM).sum() - 1\n",
    "    \n",
    "    # Calculate weighted average if valid range\n",
    "    if bin_idx1 != -1 and bin_idx2 != len(xbin):\n",
    "      weights = tuning_curve_CoM_coordinates[bin_idx1:bin_idx2+1] - CoM\n",
    "      positions[i] = (weights * xbin[bin_idx1:bin_idx2+1]).sum() / weights.sum()\n",
    "\n",
    "  return positions\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def map_CoM_to_position_smooth_interp(xbin, tuning_curve_CoM_coordinates, xbin_edge_labels, sigma=1):\n",
    "  \"\"\"\n",
    "  Maps center-of-mass coordinates back to actual positions using Gaussian smoothing and interpolation.\n",
    "\n",
    "  Args:\n",
    "      xbin: np.array, the bin centers (not edges).\n",
    "      tuning_curve_CoM_coordinates: np.array, the center-of-mass coordinates for each tuning curve.\n",
    "      xbin_edge_labels: np.array, the bin edge labels (not center).\n",
    "      sigma: float, the sigma parameter for Gaussian smoothing.\n",
    "\n",
    "  Returns:\n",
    "      positions: np.array, the estimated positions corresponding to the input center-of-mass values.\n",
    "  \"\"\"\n",
    "  smoothed_CoM = gaussian_filter1d(tuning_curve_CoM_coordinates, sigma)\n",
    "  positions = np.full_like(tuning_curve_CoM_coordinates, np.nan)\n",
    "  for i, CoM in enumerate(smoothed_CoM):\n",
    "    # Find interpolation bin and check validity\n",
    "    bin_idx = (xbin_edge_labels <= CoM).sum() - 1\n",
    "    if bin_idx < len(xbin) - 1:\n",
    "      # Perform linear interpolation between neighbors\n",
    "      interpolator = interp1d(xbin_edge_labels[bin_idx:bin_idx+2], xbin[bin_idx:bin_idx+2], kind=\"linear\")\n",
    "      positions[i] = interpolator(CoM)\n",
    "\n",
    "  return positions\n",
    "\n",
    "\n",
    "xbin_edge_labels = np.arange(len(ratemap.xbin)) # the index range spanning all x-bins\n",
    "xbin = deepcopy(ratemap.xbin)\n",
    "\n",
    "tuning_curve_CoM_coordinates = deepcopy(ratemap.peak_tuning_curve_center_of_masses)\n",
    "\n",
    "assert np.all(np.diff(xbin) > 0)\n",
    "\n",
    "\n",
    "# map_CoM_to_position_interp_array(xbin, tuning_curve_CoM_coordinates, xbin_edge_labels)\n",
    "\n",
    "test_CoM_df = pd.DataFrame({'simple_interp': np.interp(tuning_curve_CoM_coordinates, xp=xbin_edge_labels, fp=xbin),\n",
    "\t\t\t\t\t\t\t'interp': map_CoM_to_position_interp_array(xbin, tuning_curve_CoM_coordinates, xbin_edge_labels),\n",
    "                        #    'inv_weighted_avg': map_CoM_to_position_weighted_average(xbin, tuning_curve_CoM_coordinates, xbin_edge_labels),\n",
    "                           'smooth_interp': map_CoM_to_position_smooth_interp(xbin, tuning_curve_CoM_coordinates, xbin_edge_labels, sigma=0.1)})\n",
    "test_CoM_df\n",
    "# xbin: np.array([36.5862, 40.3916, 44.197, 48.0025, 51.8079, 55.6133, 59.4187, 63.2241, 67.0295, 70.835, 74.6404, 78.4458, 82.2512, 86.0566, 89.862, 93.6675, 97.4729, 101.278, 105.084, 108.889, 112.695, 116.5, 120.305, 124.111, 127.916, 131.722, 135.527, 139.332, 143.138, 146.943, 150.749, 154.554, 158.36, 162.165, 165.97, 169.776, 173.581, 177.387, 181.192, 184.997, 188.803, 192.608, 196.414, 200.219, 204.025, 207.83, 211.635, 215.441, 219.246, 223.052, 226.857, 230.662, 234.468, 238.273, 242.079, 245.884, 249.69])\n",
    "# tuning_curve_CoM_coordinates: np.array([22.3504, 33.5163, 36.219, 32.3855, 23.8594, 24.6407, 10.0726, 37.7695, 5.75679, 32.0758, 9.31427, 51.8735, 34.7201, 18.7215, 24.9917, 4.85872, 5.76916, 21.6332, 39.7348, 39.1924, 16.6196, 12.1798, 21.817, 15.8822, 7.94376, 46.0885, 24.9744, 18.708, 32.783, 46.3567, 40.9617, 30.5369, 47.7288, 19.5929, 37.7068, 50.7682, 18.0797, 30.9646, 18.4351, 26.3567, 46.3524, 9.01655, 42.9213, 33.1472, 7.45477, 11.561, 14.1151, 26.0069, 20.4058, 47.9163, 28.6427])\n",
    "# xbin_edge_labels: np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e45e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratemap.peak_tuning_curve_center_of_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_epoch_spikes_df.aclu.map(short_LR_peak_x_map) # 2023-12-20 - The core issue seems to involve missing values. For example, when an aclu is missing from this template, a NaN is returned.\n",
    "\n",
    "## ERROR HERE: \n",
    "# 280: aclu=79, result=122.208089\n",
    "# 281: aclu=48, result=122.208089\n",
    "# WHY WOULD THEY BE THE SAME?\n",
    "short_LR_peak_x_map[79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_LR_peak_x_map[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrwith\n",
    "scipy.stats.spearmanr(active_epoch_aclu_short_ranks, actually_included_epoch_ranks) # Both arrays need to have the same length in the axis dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710deb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ac600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98109834",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epochs_df = deepcopy(ripple_result_tuple.active_epochs)\n",
    "active_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43543ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "active_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1963fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(rank_order_raster_debugger.global_spikes_df.columns)) # ['start', 'stop', 'label', 'duration', 'end', 'LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson', 'LR_Long_Old_Spearman', 'RL_Long_Old_Spearman', 'LR_Short_Old_Spearman', 'RL_Short_Old_Spearman', 'LR_Long_ActuallyIncludedAclus', 'LR_Long_rel_num_cells', 'RL_Long_ActuallyIncludedAclus', 'RL_Long_rel_num_cells', 'LR_Long_Z', 'RL_Long_Z', 'LR_Short_Z', 'RL_Short_Z']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537629d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(active_selected_spikes_df.columns)) # ['start', 'stop', 'label', 'duration', 'end', 'LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson', 'LR_Long_Old_Spearman', 'RL_Long_Old_Spearman', 'LR_Short_Old_Spearman', 'RL_Short_Old_Spearman', 'LR_Long_ActuallyIncludedAclus', 'LR_Long_rel_num_cells', 'RL_Long_ActuallyIncludedAclus', 'RL_Long_rel_num_cells', 'LR_Long_Z', 'RL_Long_Z', 'LR_Short_Z', 'RL_Short_Z']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(active_epochs_df.columns)) # ['start', 'stop', 'label', 'duration', 'end', 'LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson', 'LR_Long_Old_Spearman', 'RL_Long_Old_Spearman', 'LR_Short_Old_Spearman', 'RL_Short_Old_Spearman', 'LR_Long_ActuallyIncludedAclus', 'LR_Long_rel_num_cells', 'RL_Long_ActuallyIncludedAclus', 'RL_Long_rel_num_cells', 'LR_Long_Z', 'RL_Long_Z', 'LR_Short_Z', 'RL_Short_Z']\n",
    "\n",
    "\n",
    "a_row, a_label = get_epoch_label_row(188)\n",
    "a_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_row = list(a_row.itertuples())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd51cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rank_order_raster_debugger.plots_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_update_plot_titles(rank_order_raster_debugger, an_idx=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_debug_callback_fn(rank_order_raster_debugger, an_idx=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rank_order_raster_debugger.plots.root_plots.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_likelihoods_tuple: DirectionalRankOrderLikelihoods = ripple_result_tuple.directional_likelihoods_tuple\n",
    "directional_likelihoods_tuple.long_best_direction_indices\n",
    "ripple_result_tuple.rank_order_z_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.rank_order_z_score_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c16275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _display_directional_laps_overview:\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "session_description: str = curr_active_pipeline.get_session_context().get_description()\n",
    "print(f'session_description: {session_description}')\n",
    "pg.setConfigOptions(imageAxisOrder='row-major')\n",
    "_out_directional_laps_overview = curr_active_pipeline.display(DirectionalPlacefieldGlobalDisplayFunctions._display_directional_laps_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b438c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "source": [
    "### Independent DirectionalTemplatePFsDebugger for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7ac4e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    },
    "tags": [
     "figure"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "# _display_directional_laps_overview:\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "session_description: str = curr_active_pipeline.get_session_context().get_description()\n",
    "print(f'session_description: {session_description}')\n",
    "pg.setConfigOptions(imageAxisOrder='row-major')\n",
    "# _out = curr_active_pipeline.display(DirectionalPlacefieldGlobalDisplayFunctions._display_directional_laps_overview)\n",
    "_out_all_cells_directional_template_pfs_debugger = curr_active_pipeline.display(DirectionalPlacefieldGlobalDisplayFunctions._display_directional_template_debugger, included_any_context_neuron_ids=None, figure_name='All Cells (Independent)', \n",
    "                                                                                debug_print=True, debug_draw=True)\n",
    "all_cells_directional_template_pfs_debugger: TemplateDebugger = _out_all_cells_directional_template_pfs_debugger['obj']\n",
    "plots = all_cells_directional_template_pfs_debugger.plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5816a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data = all_cells_directional_template_pfs_debugger.plots_data\n",
    "plots_data.all_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extent=(0, 200, 0, 100)\n",
    "a_decoder_name = 'long_LR'\n",
    "a_decoder =  all_cells_directional_template_pfs_debugger.decoders_dict[a_decoder_name]\n",
    "a_decoder.pf.ratemap.xbin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83656a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_extents(xmin, xmax, ymin, ymax):\n",
    "    # (x, y, w, h)\n",
    "    return (xmin, ymin, (xmax-xmin), (ymax-xmin))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(v) for v in plots_data.sorted_neuron_IDs_lists]\n",
    "\n",
    "# img_extents_dict = {a_decoder_name:[a_decoder.pf.ratemap.xbin[0], a_decoder.pf.ratemap.xbin[-1], 0, float(len(plots_data.sorted_neuron_IDs_lists[i]))] for i, (a_decoder_name, a_decoder) in enumerate(all_cells_directional_template_pfs_debugger.decoders_dict.items()) } # these extents are  (xmin, xmax, ymin, ymax) \n",
    "img_extents_dict = {a_decoder_name:[a_decoder.pf.ratemap.xbin[0], 0, (a_decoder.pf.ratemap.xbin[-1]-a_decoder.pf.ratemap.xbin[0]), (float(len(plots_data.sorted_neuron_IDs_lists[i]))-0.0)] for i, (a_decoder_name, a_decoder) in enumerate(all_cells_directional_template_pfs_debugger.decoders_dict.items()) } # these extents are  (x, y, w, h)\n",
    "\n",
    "for a_decoder_name, (curr_win, curr_img) in plots.pf1D_heatmaps.items():\n",
    "    print(f'a_decoder_name: {a_decoder_name}, img_extents_dict[a_decoder_name]: {img_extents_dict[a_decoder_name]}')\n",
    "    curr_win.showAxes(True)\n",
    "    # Set the extent to map pixels to x-locations\n",
    "    curr_img.setRect(img_extents_dict[a_decoder_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ratemap: Ratemap = a_decoder.pf.ratemap\n",
    "active_n_cells = 3\n",
    "y_max = float(active_n_cells) + 0.5\n",
    "img_extents = [a_ratemap.xbin[0], a_ratemap.xbin[-1], 0, y_max] # (xmin, xmax, ymin, ymax)\n",
    "img_extents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_win, curr_img = plots.pf1D_heatmaps[a_decoder_name] # win, img\n",
    "curr_win.showAxes(True)\n",
    "# curr_img.ImageItem\n",
    "# all_cells_directional_template_pfs_debugger.plots.\n",
    "# a_view_box = an_image_item.getViewBox()\n",
    "# a_plot_widget\n",
    "\n",
    "# Set the extent to map pixels to x-locations\n",
    "img_item.setAttr(extent=(0, 200, 0, 100))\n",
    "\n",
    "plots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b547db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_cells_directional_template_pfs_debugger.plots_data.sorted_pf_peak_location_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import PyQtGraphCrosshairs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    ".data.sorted_pf_peak_location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just compute line point locations... EZ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd514e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d589de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefield_occupancy', long_LR_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d6049",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = []\n",
    "rois.append(pg.RectROI([20, 20], [20, 20], pen=(0,9)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to handle hover event\n",
    "def hoverEvent(pos):\n",
    "    print(f'hoverEvent')\n",
    "    index = int(pos.x())\n",
    "    region.setRegion([index, index + 1])  # Highlight the entire row\n",
    "\n",
    "# Add LinearRegionItem for highlighting\n",
    "region = pg.LinearRegionItem(brush='#FFFFFF', hoverBrush='#FFFFFF', movable=False, clipItem=an_image_item)\n",
    "region.setZValue(10)\n",
    "\n",
    "a_plot_widget.addItem(region, ignoreBounds=True)\n",
    "\n",
    "# Connect hover event\n",
    "a_plot_widget.scene().sigMouseMoved.connect(hoverEvent)\n",
    "\n",
    "region.setRegion([0.0, 10.0 + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "## Main\n",
    "ripple_result_tuple, laps_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline, decoding_time_bin_size=0.003)\n",
    "ripple_result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296afed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_result_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].laps_most_likely_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline, decoding_time_bin_size=0.006) # 6ms bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c19f9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# Plot the z-scores differences and their raw-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_rank_order_epoch_inst_fr_result_tuples\n",
    "\n",
    "ripple_outputs = plot_rank_order_epoch_inst_fr_result_tuples(curr_active_pipeline, ripple_result_tuple, 'Ripple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of the function for Lap\n",
    "lap_outputs = plot_rank_order_epoch_inst_fr_result_tuples(curr_active_pipeline, laps_result_tuple, 'Lap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba64ce",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# result_tuple.plot_histograms()\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_rank_order_histograms\n",
    "\n",
    "# Plot histograms:\n",
    "post_title_info: str = f'{minimum_inclusion_fr_Hz} Hz\\n{curr_active_pipeline.get_session_context().get_description()}'\n",
    "_out_z_score, _out_real, _out_most_likely_z, _out_most_likely_raw = plot_rank_order_histograms(rank_order_results, post_title_info=post_title_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the raw spearman_rho values for the best-direction for both Long/Short:\n",
    "# Adds ['Long_BestDir_spearman', 'Short_BestDir_spearman']\n",
    "\n",
    "## Ripples:\n",
    "long_best_direction_indicies = deepcopy(rank_order_results.ripple_most_likely_result_tuple.directional_likelihoods_tuple.long_best_direction_indices)\n",
    "short_best_direction_indicies = deepcopy(rank_order_results.ripple_most_likely_result_tuple.directional_likelihoods_tuple.short_best_direction_indices)\n",
    "assert np.shape(long_best_direction_indicies) == np.shape(short_best_direction_indicies)\n",
    "\n",
    "ripple_evts_long_best_dir_raw_stats_values = np.where(long_best_direction_indicies, rank_order_results.ripple_combined_epoch_stats_df['LR_Long_spearman'].to_numpy(), rank_order_results.ripple_combined_epoch_stats_df['RL_Long_spearman'].to_numpy())\n",
    "ripple_evts_short_best_dir_raw_stats_values = np.where(short_best_direction_indicies, rank_order_results.ripple_combined_epoch_stats_df['LR_Short_spearman'].to_numpy(), rank_order_results.ripple_combined_epoch_stats_df['RL_Short_spearman'].to_numpy())\n",
    "assert np.shape(ripple_evts_long_best_dir_raw_stats_values) == np.shape(ripple_evts_short_best_dir_raw_stats_values)\n",
    "rank_order_results.ripple_combined_epoch_stats_df['Long_BestDir_spearman'] = ripple_evts_long_best_dir_raw_stats_values\n",
    "rank_order_results.ripple_combined_epoch_stats_df['Short_BestDir_spearman'] = ripple_evts_short_best_dir_raw_stats_values\n",
    "\n",
    "## Laps\n",
    "long_best_direction_indicies = deepcopy(laps_result_tuple.directional_likelihoods_tuple.long_best_direction_indices)\n",
    "short_best_direction_indicies = deepcopy(laps_result_tuple.directional_likelihoods_tuple.short_best_direction_indices)\n",
    "assert np.shape(long_best_direction_indicies) == np.shape(short_best_direction_indicies)\n",
    "\n",
    "laps_evts_long_best_dir_raw_stats_values = np.where(long_best_direction_indicies, rank_order_results.laps_combined_epoch_stats_df['LR_Long_spearman'].to_numpy(), rank_order_results.laps_combined_epoch_stats_df['RL_Long_spearman'].to_numpy())\n",
    "laps_evts_short_best_dir_raw_stats_values = np.where(short_best_direction_indicies, rank_order_results.laps_combined_epoch_stats_df['LR_Short_spearman'].to_numpy(), rank_order_results.laps_combined_epoch_stats_df['RL_Short_spearman'].to_numpy())\n",
    "assert np.shape(laps_evts_long_best_dir_raw_stats_values) == np.shape(laps_evts_short_best_dir_raw_stats_values)\n",
    "rank_order_results.laps_combined_epoch_stats_df['Long_BestDir_spearman'] = laps_evts_long_best_dir_raw_stats_values\n",
    "rank_order_results.laps_combined_epoch_stats_df['Short_BestDir_spearman'] = laps_evts_short_best_dir_raw_stats_values\n",
    "\n",
    "rank_order_results.ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143279df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_LR_ripple_long_z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in track_templates.decoder_neuron_IDs_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_likelihoods_df.plot.bar(y=['long_relative_direction_likelihoods', 'short_relative_direction_likelihoods'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76430f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_idx: int = 154\n",
    "# a_label = lookup_label_from_index(an_idx)\n",
    "# a_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_selected_spikes_df[active_selected_spikes_df['Probe_Epoch_id'] == an_idx]['aclu'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7eeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_selected_spikes_df[active_selected_spikes_df['Probe_Epoch_id'] == an_idx].plot.scatter(x='t_rel_seconds', y='LR_Long_pf_peak_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter events by (|z| > 1.0)\n",
    "ripple_result_tuple.active_epochs\n",
    "ripple_result_tuple.long_best_dir_z_score_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5402b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripple_result_tuple.directional_likelihoods_tuple\n",
    "ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices[105] # 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple.short_best_direction_indices[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.long_best_dir_z_score_values[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.short_best_dir_z_score_values[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.active_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(ripple_result_tuple.long_best_dir_z_score_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(ripple_result_tuple.short_best_dir_z_score_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_replays.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_replays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d468a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratemap = long_pf1D.ratemap\n",
    "ratemap.tuning_curve_unsmoothed_peak_firing_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bebb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rank_order_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ae69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple.long_relative_direction_likelihoods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be485e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple.short_relative_direction_likelihoods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd965be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.short_best_dir_z_score_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213cd164073dbb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:24:06.810860500Z",
     "start_time": "2023-11-16T23:24:06.644858700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2023-11-16_LapsRankOrderHistogram Figure:\n",
    "pd.DataFrame({'long_z_scores': laps_result_tuple.long_best_dir_z_score_values, 'short_z_scores': laps_result_tuple.short_best_dir_z_score_values}).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2a9a6",
   "metadata": {},
   "source": [
    "# 🎨🎯 Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc87fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "# active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_spike_rasters_pyqtplot_2D', 'maze_any') # 'maze_any'\n",
    "assert isinstance(_out_graphics_dict, dict)\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = _out_graphics_dict['spike_raster_plt_2d'], _out_graphics_dict['spike_raster_plt_3d'], _out_graphics_dict['spike_raster_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffa08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_raster_window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_2d_plot.clear_all_matplotlib_plots()\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig, CustomCyclicColorsDockDisplayConfig\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.NestedDockAreaWidget import NestedDockAreaWidget\n",
    "\n",
    "dynamic_docked_widget_container: NestedDockAreaWidget = active_2d_plot.ui.dynamic_docked_widget_container\n",
    "dynamic_docked_widget_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_docked_widget_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262dc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dockitems_list = dynamic_docked_widget_container.get_flat_dockitems_list()\n",
    "max_height = 65\n",
    "for a_dockitem in _dockitems_list:\n",
    "\ta_dockitem.setMaximumHeight(max_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dockitem = _dockitems_list[0]\n",
    "a_dockitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dockitem.size() # PyQt5.QtCore.QSize(1835, 121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6961e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_widgets_list = dynamic_docked_widget_container.get_flat_widgets_list()\n",
    "a_widget = _widgets_list[0]\n",
    "a_widget.size() # PyQt5.QtCore.QSize(1835, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a54dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_dockitem.resize\n",
    "a_dockitem.setMaximumHeight(50)\n",
    "a_dockitem.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957459e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730e4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_widget.height() # 132\n",
    "a_widget.width() # 1835\n",
    "a_widget.setMaximumHeight(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_widget.setMaximumHeight(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e277ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.dock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_docked_widget_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c08eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Dock Widgets:\n",
    "from pyphoplacecellanalysis.GUI.Qt.Menus.BaseMenuProviderMixin import BaseMenuCommand\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig, CustomCyclicColorsDockDisplayConfig\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand\n",
    "\n",
    "# decoder_names_list = ('long_LR', 'long_RL', 'short_LR', 'short_RL')\n",
    "# _out_dock_widgets = {}\n",
    "\n",
    "# dock_add_locations = (['left'], ['left'], ['right'], ['right'])\n",
    "# dock_add_locations = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (['right'], ['right'], ['right'], ['right'])))\n",
    "\n",
    "# for i, (a_decoder_name, a_heatmap) in enumerate(_out_pf1D_heatmaps.items()):\n",
    "# \t_out_dock_widgets[a_decoder_name] = root_dockAreaWindow.add_display_dock(identifier=a_decoder_name, widget=a_heatmap[0], dockSize=(300,200), dockAddLocationOpts=dock_add_locations[a_decoder_name], display_config=dock_configs[a_decoder_name])\n",
    "\n",
    "\n",
    "# @define(slots=False)\n",
    "# class AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand(BaseMenuCommand):\n",
    "# \t\"\"\" 2024-01-17 \"\"\"\n",
    "# \t_spike_raster_window = field()\n",
    "# \t_active_pipeline = field(alias='curr_active_pipeline')\n",
    "# \t_active_config_name = field(default=None)\n",
    "# \t_context = field(default=None, alias=\"active_context\")\n",
    "# \t_display_output = field(default=Factory(dict))\n",
    "\n",
    "# \t@classmethod\n",
    "# \tdef _perform_add_new_decoded_row(cls, curr_active_pipeline, active_2d_plot, a_decoder_name: str, a_decoder, a_dock_config):\n",
    "# \t\t\"\"\" captures dock_configs \n",
    "\t\t\n",
    "# \t\t\"\"\"\n",
    "# \t\tfrom pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "\t\t\n",
    "# \t\t## ✅ Add a new row for each of the four 1D directional decoders:\n",
    "# \t\tidentifier_name: str = f'{a_decoder_name}_ContinuousDecode'\n",
    "# \t\tprint(f'identifier_name: {identifier_name}')\n",
    "# \t\twidget, matplotlib_fig, matplotlib_fig_axes = active_2d_plot.add_new_matplotlib_render_plot_widget(name=identifier_name, dockSize=(300, 20), display_config=a_dock_config)\n",
    "# \t\tan_ax = matplotlib_fig_axes[0]\n",
    "\n",
    "# \t\t# all_directional_decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "# \t\t# all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = dict(zip(all_directional_decoder_names, [deepcopy(long_LR_pf1D_Decoder), deepcopy(long_RL_pf1D_Decoder), deepcopy(short_LR_pf1D_Decoder), deepcopy(short_RL_pf1D_Decoder)]))\n",
    "\n",
    "# \t\t# a_decoder_name: str = \"long_LR\"\n",
    "# \t\t# _active_config_name = None\n",
    "# \t\tvariable_name: str = a_decoder_name\n",
    "# \t\t# active_decoder = deepcopy(all_directional_pf1D_Decoder_dict[a_decoder_name]) # computation_result.computed_data['pf2D_Decoder']\n",
    "# \t\tactive_decoder = deepcopy(a_decoder)\n",
    "# \t\t# active_result = deepcopy(_out_continuously_decoded_dict[a_decoder_name]) # already decoded\n",
    "# \t\tactive_marginals = active_decoder.marginal.x\n",
    "# \t\tactive_bins = active_decoder.xbin\n",
    "\n",
    "# \t\t# active_most_likely_positions = active_marginals.most_likely_positions_1D # Raw decoded positions\n",
    "# \t\tactive_most_likely_positions = None\n",
    "# \t\tactive_posterior = active_marginals.p_x_given_n\n",
    "\n",
    "# \t\t# most_likely_positions_mode: 'standard'|'corrected'\n",
    "# \t\t# fig, curr_ax = curr_active_pipeline.display('_display_plot_marginal_1D_most_likely_position_comparisons', _active_config_name, variable_name='x', most_likely_positions_mode='corrected', ax=an_ax) # ax=active_2d_plot.ui.matplotlib_view_widget.ax\n",
    "# \t\t## Actual plotting portion:\n",
    "# \t\tfig, curr_ax = plot_1D_most_likely_position_comparsions(None, time_window_centers=active_decoder.time_window_centers, xbin=active_bins,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tposterior=active_posterior,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactive_most_likely_positions_1D=active_most_likely_positions,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tax=an_ax, variable_name=variable_name, debug_print=True, enable_flat_line_drawing=False)\n",
    "\n",
    "# \t\t# print(f'\\t AddNewDecodedPosition_MatplotlibPlotCommand.execute(...) finished with the display call...')\n",
    "# \t\t# active_2d_plot.ui.matplotlib_view_widget.draw()\n",
    "# \t\twidget.draw() # alternative to accessing through full path?\n",
    "# \t\tactive_2d_plot.sync_matplotlib_render_plot_widget(identifier_name) # Sync it with the active window:\n",
    "# \t\treturn identifier_name, widget, matplotlib_fig, matplotlib_fig_axes\n",
    "\n",
    "\n",
    "\n",
    "# \t@classmethod\n",
    "# \tdef add_directional_decoder_decoded_epochs(cls, curr_active_pipeline, active_2d_plot, debug_print=False):\n",
    "# \t\t\"\"\" adds the decoded epochs for the long/short decoder from the global_computation_results as new matplotlib plot rows. \"\"\"\n",
    "# \t\t# from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions # Actual most general\n",
    "# \t\t# from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_slices_1D_most_likely_position_comparsions\n",
    "# \t\tfrom pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum\n",
    "# \t\tfrom pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig, CustomCyclicColorsDockDisplayConfig\n",
    "\t\t\n",
    "# \t\tshowCloseButton = False\n",
    "# \t\tdock_configs = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_LR_dock_colors, showCloseButton=showCloseButton), CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_RL_dock_colors, showCloseButton=showCloseButton),\n",
    "# \t\t\t\t\t\tCustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_LR_dock_colors, showCloseButton=showCloseButton), CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_RL_dock_colors, showCloseButton=showCloseButton))))\n",
    "\n",
    "\n",
    "# \t\t# Unpack all directional variables:\n",
    "# \t\t## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "# \t\tlong_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# \t\t# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "# \t\t(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "# \t\t# long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "# \t\t# (long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "# \t\t(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "# \t\t# (long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "# \t\t# (long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "# \t\t# (long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "# \t\t(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "# \t\tall_directional_decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "# \t\tall_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = dict(zip(all_directional_decoder_names, [deepcopy(long_LR_pf1D_Decoder), deepcopy(long_RL_pf1D_Decoder), deepcopy(short_LR_pf1D_Decoder), deepcopy(short_RL_pf1D_Decoder)]))\n",
    "\n",
    "# \t\t# Need all_directional_pf1D_Decoder_dict\n",
    "# \t\toutput_dict = {}\n",
    "\n",
    "# \t\tfor a_decoder_name, a_decoder in all_directional_pf1D_Decoder_dict.items():\n",
    "# \t\t\ta_dock_config = dock_configs[a_decoder_name]\n",
    "# \t\t\t_out_tuple = cls._perform_add_new_decoded_row(curr_active_pipeline=curr_active_pipeline, active_2d_plot=active_2d_plot, a_decoder_name=a_decoder_name, a_decoder=a_decoder, a_dock_config=a_dock_config)\n",
    "# \t\t\t# identifier_name, widget, matplotlib_fig, matplotlib_fig_axes = _out_tuple\n",
    "# \t\t\toutput_dict[a_decoder_name] = _out_tuple\n",
    "\n",
    "# \t\treturn output_dict\n",
    "\n",
    "\n",
    "# \tdef execute(self, *args, **kwargs) -> None:\n",
    "# \t\t## To begin, the destination plot must have a matplotlib widget plot to render to:\n",
    "# \t\t# print(f'AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand.execute(...)')\n",
    "# \t\tactive_2d_plot = self._spike_raster_window.spike_raster_plt_2d\n",
    "# \t\t# If no plot to render on, do this:\n",
    "# \t\toutput_dict = self.add_directional_decoder_decoded_epochs(self._active_pipeline, active_2d_plot) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "# \t\t# Update display output dict:\n",
    "# \t\tfor a_decoder_name, an_output_tuple in output_dict.items():\n",
    "# \t\t\tidentifier_name, widget, matplotlib_fig, matplotlib_fig_axes = an_output_tuple\n",
    "# \t\t\tself._display_output[identifier_name] = an_output_tuple\n",
    "\n",
    "# \t\t# self._display_output['long_decoded_replay_tuple'] = long_decoded_replay_tuple\n",
    "# \t\t# self._display_output['short_decoded_replay_tuple'] = short_decoded_replay_tuple\n",
    "\n",
    "# \t\t# widget, matplotlib_fig, matplotlib_fig_ax = active_2d_plot.add_new_matplotlib_render_plot_widget(name='MenuCommand_display_plot_marginal_1D_most_likely_position_comparisons')\n",
    "# \t\t# # most_likely_positions_mode: 'standard'|'corrected'\n",
    "# \t\t# fig, curr_ax = self._curr_active_pipeline.display('_display_plot_marginal_1D_most_likely_position_comparisons', self._active_config_name, variable_name='x', most_likely_positions_mode='corrected', ax=matplotlib_fig_ax) # ax=active_2d_plot.ui.matplotlib_view_widget.ax\n",
    "# \t\t# # print(f'\\t AddNewDecodedPosition_MatplotlibPlotCommand.execute(...) finished with the display call...')\n",
    "# \t\t# # active_2d_plot.ui.matplotlib_view_widget.draw()\n",
    "# \t\t# widget.draw() # alternative to accessing through full path?\n",
    "# \t\t# active_2d_plot.sync_matplotlib_render_plot_widget('MenuCommand_display_plot_marginal_1D_most_likely_position_comparisons') # Sync it with the active window:\n",
    "# \t\tprint(f'\\t AddNewDirectionalDecodedEpochs_MatplotlibPlotCommand.execute() is done.')\n",
    "\t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c300e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "\n",
    "\n",
    "## ✅ Add a new row for each of the four 1D directional decoders:\n",
    "widget, matplotlib_fig, matplotlib_fig_axes = active_2d_plot.add_new_matplotlib_render_plot_widget(row=2, col=0, name='PhoManualTest')\n",
    "an_ax = matplotlib_fig_axes[0]\n",
    "\n",
    "# all_directional_decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "# all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = dict(zip(all_directional_decoder_names, [deepcopy(long_LR_pf1D_Decoder), deepcopy(long_RL_pf1D_Decoder), deepcopy(short_LR_pf1D_Decoder), deepcopy(short_RL_pf1D_Decoder)]))\n",
    "\n",
    "a_decoder_name: str = \"long_LR\"\n",
    "\n",
    "\n",
    "_active_config_name = None\n",
    "variable_name: str = a_decoder_name\n",
    "active_decoder = deepcopy(all_directional_pf1D_Decoder_dict[a_decoder_name]) # computation_result.computed_data['pf2D_Decoder']\n",
    "# active_result = deepcopy(_out_continuously_decoded_dict[a_decoder_name]) # already decoded\n",
    "active_marginals = active_decoder.marginal.x\n",
    "active_bins = active_decoder.xbin\n",
    "\n",
    "# active_most_likely_positions = active_marginals.most_likely_positions_1D # Raw decoded positions\n",
    "active_most_likely_positions = None\n",
    "\n",
    "active_posterior = active_marginals.p_x_given_n\n",
    "\n",
    "# most_likely_positions_mode: 'standard'|'corrected'\n",
    "# fig, curr_ax = curr_active_pipeline.display('_display_plot_marginal_1D_most_likely_position_comparisons', _active_config_name, variable_name='x', most_likely_positions_mode='corrected', ax=an_ax) # ax=active_2d_plot.ui.matplotlib_view_widget.ax\n",
    " ## Actual plotting portion:\n",
    "fig, curr_ax = plot_1D_most_likely_position_comparsions(None, time_window_centers=active_decoder.time_window_centers, xbin=active_bins,\n",
    "                                                        posterior=active_posterior,\n",
    "                                                        active_most_likely_positions_1D=active_most_likely_positions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tax=an_ax, variable_name=variable_name, debug_print=True, enable_flat_line_drawing=False)\n",
    "\n",
    "                                                        # **overriding_dict_with(lhs_dict={'ax':None, 'variable_name':variable_name, 'enable_flat_line_drawing':False, 'debug_print': False}, **kwargs))\n",
    "\n",
    "# out_plot_tuple = plot_decoded_epoch_slices(active_filter_epochs, filter_epochs_decoder_result, global_pos_df=computation_result.sess.position.to_dataframe(), xbin=active_decoder.xbin, included_epoch_indicies=included_epoch_indicies,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t**overriding_dict_with(lhs_dict={'name':default_figure_name, 'debug_test_max_num_slices':256, 'enable_flat_line_drawing':False, 'debug_print': False}, **kwargs))\n",
    "# params, plots_data, plots, ui = out_plot_tuple\n",
    "\n",
    "\n",
    "# `self._curr_active_pipeline` -> `self._active_pipeline``\n",
    "# print(f'\\t AddNewDecodedPosition_MatplotlibPlotCommand.execute(...) finished with the display call...')\n",
    "# active_2d_plot.ui.matplotlib_view_widget.draw()\n",
    "widget.draw() # alternative to accessing through full path?\n",
    "active_2d_plot.sync_matplotlib_render_plot_widget('PhoManualTest') # Sync it with the active window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_result: DecodedFilterEpochsResult = deepcopy(pseudo2D_decoder_continuously_decoded_result) # already decoded\n",
    "active_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(active_result.p_x_given_n_list) == 1, f\"expected len(active_result.p_x_given_n_list)==1 but len(active_result.p_x_given_n_list): {len(active_result.p_x_given_n_list)}\"\n",
    "p_x_given_n = active_result.p_x_given_n_list[0]\n",
    "marginal_x = active_result.marginal_x_list[0]\n",
    "time_bin_container =  active_result.time_bin_containers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87316d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_centers = time_bin_container.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ✅ Add a row for the pseudo2D decoder `pseudo2D_decoder`:\n",
    "widget, matplotlib_fig, matplotlib_fig_axes = active_2d_plot.add_new_matplotlib_render_plot_widget(row=2, col=0, name='pseudo2D_decoder')\n",
    "an_ax = matplotlib_fig_axes[0]\n",
    "\n",
    "# pseudo2D_decoder_continuously_decoded_result = pseudo2D_decoder.decode_specific_epochs(spikes_df=spikes_df, filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "_active_config_name = None\n",
    "variable_name: str = 'pseudo2D_decoder'\n",
    "active_decoder = deepcopy(pseudo2D_decoder) # computation_result.computed_data['pf2D_Decoder']\n",
    "active_result = deepcopy(pseudo2D_decoder_continuously_decoded_result) # already decoded\n",
    "\n",
    "# active_marginals = active_decoder.marginal.x\n",
    "active_marginals = deepcopy(marginal_x)\n",
    "active_bins = active_decoder.xbin\n",
    "\n",
    "# active_most_likely_positions = active_marginals.most_likely_positions_1D # Raw decoded positions\n",
    "active_most_likely_positions = None\n",
    "active_posterior = active_marginals.p_x_given_n\n",
    "\n",
    "# most_likely_positions_mode: 'standard'|'corrected'\n",
    "# fig, curr_ax = curr_active_pipeline.display('_display_plot_marginal_1D_most_likely_position_comparisons', _active_config_name, variable_name='x', most_likely_positions_mode='corrected', ax=an_ax) # ax=active_2d_plot.ui.matplotlib_view_widget.ax\n",
    " ## Actual plotting portion:\n",
    "fig, curr_ax = plot_1D_most_likely_position_comparsions(None, time_window_centers=time_window_centers, xbin=active_bins,\n",
    "                                                        posterior=active_posterior,\n",
    "                                                        active_most_likely_positions_1D=active_most_likely_positions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tax=an_ax, variable_name=variable_name, debug_print=True, enable_flat_line_drawing=False)\n",
    "\n",
    "\n",
    "widget.draw() # alternative to accessing through full path?\n",
    "active_2d_plot.sync_matplotlib_render_plot_widget('pseudo2D_decoder') # Sync it with the active window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0402ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_docked_widget_container.add_display_dock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Dock Widgets:\n",
    "# decoder_names_list = ('long_LR', 'long_RL', 'short_LR', 'short_RL')\n",
    "_out_dock_widgets = {}\n",
    "dock_configs = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_LR_dock_colors, showCloseButton=False), CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_RL_dock_colors, showCloseButton=False),\n",
    "\t\t\t\tCustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_LR_dock_colors, showCloseButton=False), CustomDockDisplayConfig(custom_get_colors_callback_fn=DisplayColorsEnum.Laps.get_RL_dock_colors, showCloseButton=False))))\n",
    "# dock_add_locations = (['left'], ['left'], ['right'], ['right'])\n",
    "dock_add_locations = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (['right'], ['right'], ['right'], ['right'])))\n",
    "\n",
    "for i, (a_decoder_name, a_heatmap) in enumerate(_out_pf1D_heatmaps.items()):\n",
    "\t_out_dock_widgets[a_decoder_name] = root_dockAreaWindow.add_display_dock(identifier=a_decoder_name, widget=a_heatmap[0], dockSize=(300,200), dockAddLocationOpts=dock_add_locations[a_decoder_name], display_config=dock_configs[a_decoder_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_additional_window_menus\n",
    "\n",
    "## Finally, add the display function to the active context\n",
    "active_identifying_context = global_epoch_context\n",
    "active_display_fn_identifying_ctx = active_identifying_context.adding_context('display_fn', display_fn_name='display_spike_rasters_window')\n",
    "active_display_fn_identifying_ctx_string = active_display_fn_identifying_ctx.get_description(separator='|') # Get final discription string:\n",
    "\n",
    "computation_result = deepcopy(curr_active_pipeline.computation_results[global_epoch_name])\n",
    "\n",
    "## Build the additional menus:\n",
    "output_references = _build_additional_window_menus(spike_raster_window, curr_active_pipeline, computation_result, active_display_fn_identifying_ctx)\n",
    "output_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ad15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot.ui.menus #.global_window_menus.docked_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockedWidgets_menuProvider = spike_raster_window.main_menu_window.ui.menus.global_window_menus.docked_widgets.menu_provider_obj\n",
    "actions_dict = dockedWidgets_menuProvider.activeMenuReference.actions_dict #['actionMenuDockedWidgets']\n",
    "actions_dict\n",
    "# DockedWidgets_MenuProvider_actionsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actions_dict['actionNewDockedMatplotlibView'].activate(pg.QtGui.QAction.Trigger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05dbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions_dict['actionAddDockedWidget'].activate(pg.QtGui.QAction.Trigger)\n",
    "actions_dict['actionNewDockedContextNested'].activate(pg.QtGui.QAction.Trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa129f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8ba57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6abb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spike_raster_window # Spike3DRasterWindowWidget\n",
    "spike_raster_window.ui #.menus.global_window_menus.docked_widgets # <pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Uic_AUTOGEN_Spike3DRasterWindowBase.Ui_RootWidget at 0x199fc0c0490>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69746542",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_3d_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot, active_3d_plot, spike_raster_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39840338",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75eb376",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_raster_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_window = spike_raster_window.spikes_window # SpikesDataframeWindow\n",
    "\n",
    "bottomPlaybackControlBarWidget = spike_raster_window.ui.bottomPlaybackControlBarWidget # Spike3DRasterBottomPlaybackControlBar \n",
    "\n",
    "doubleSpinBox_ActiveWindowStartTime = bottomPlaybackControlBarWidget.ui.doubleSpinBox_ActiveWindowStartTime\n",
    "doubleSpinBox_ActiveWindowEndTime = bottomPlaybackControlBarWidget.ui.doubleSpinBox_ActiveWindowEndTime\n",
    "\n",
    "\n",
    "# spikes_window.timeWindow.start\n",
    "# spikes_window.active_window_start_time\n",
    "# spikes_window.update_window_start_end(451.8908457518555, 451.9895490613999) ## Works but does not trigger refresh/update of the window. The changes are reflected as soon as you try to scroll at all though.\n",
    "# spikes_window.active_window_end_time\n",
    "\n",
    "print(f'spikes_window.active_window_start_time: {spikes_window.active_window_start_time}, spikes_window.active_window_end_time: {spikes_window.active_window_end_time}')\n",
    "# need to block signals:\n",
    "# doubleSpinBox_ActiveWindowStartTime.blockSignals(True)\n",
    "# doubleSpinBox_ActiveWindowEndTime.blockSignals(True)\n",
    "doubleSpinBox_ActiveWindowStartTime.setValue(spikes_window.active_window_start_time)\n",
    "doubleSpinBox_ActiveWindowEndTime.setValue(spikes_window.active_window_end_time)\n",
    "# doubleSpinBox_ActiveWindowStartTime.blockSignals(False) # unblock the signals when done\n",
    "# doubleSpinBox_ActiveWindowEndTime.blockSignals(False)\n",
    "\n",
    "\n",
    "# @pyqtExceptionPrintingSlot(float, float)\n",
    "def on_active_window_changed(start_t, end_t, _obj):\n",
    "\t# need to block signals:\n",
    "\t# doubleSpinBox_ActiveWindowStartTime.blockSignals(True)\n",
    "\t# doubleSpinBox_ActiveWindowEndTime.blockSignals(True)\n",
    "\tif start_t is not None:\n",
    "\t\tdoubleSpinBox_ActiveWindowStartTime.setValue(start_t)\n",
    "\tif end_t is not None:\n",
    "\t\tdoubleSpinBox_ActiveWindowEndTime.setValue(end_t)\n",
    "\t# doubleSpinBox_ActiveWindowStartTime.blockSignals(False) # unblock the signals when done\n",
    "\t# doubleSpinBox_ActiveWindowEndTime.blockSignals(False)\n",
    "\n",
    "curr_window_ctrls_connection = spikes_window.windowed_data_window_updated_signal.connect(on_active_window_changed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doubleSpinBox_ActiveWindowStartTime.setReadOnly(True)\n",
    "doubleSpinBox_ActiveWindowEndTime.setReadOnly(True)\n",
    "\n",
    "spikes_window.on_window_changed.connect("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "doubleSpinBox_ActiveWindowStartTime.setVisible(False)\n",
    "bottomPlaybackControlBarWidget.setVisible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d043d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_epoch_context\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "# curr_active_pipeline.prepare_for_display()\n",
    "curr_active_pipeline.clear_display_outputs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb657697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.laps import plot_lap_trajectories_2d\n",
    "# Complete Version:\n",
    "# fig, axs, laps_pages = plot_lap_trajectories_2d(curr_active_pipeline.sess, curr_num_subplots=len(curr_active_pipeline.sess.laps.lap_id), active_page_index=0)\n",
    "# Paginated Version:\n",
    "fig, axs, laps_pages = plot_lap_trajectories_2d(curr_active_pipeline.sess, curr_num_subplots=22, active_page_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs, laps_pages = plot_lap_trajectories_2d(curr_active_pipeline.sess, curr_num_subplots=22, active_page_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg # Used to get the app for TopLevelWindowHelper.top_level_windows\n",
    "## For searching with `TopLevelWindowHelper.all_widgets(...)`:\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike3DRaster import Spike3DRaster\n",
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n",
    "\n",
    "found_spike_raster_windows = TopLevelWindowHelper.all_widgets(pg.mkQApp(), searchType=Spike3DRasterWindowWidget)\n",
    "\n",
    "if len(found_spike_raster_windows) < 1:\n",
    "\t# no existing spike_raster_windows. Make a new one\n",
    "\tprint(f'no existing SpikeRasterWindow. Creating a new one.')\n",
    "\t# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "\t# active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "\tactive_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "else:\n",
    "\tprint(f'found {len(found_spike_raster_windows)} existing Spike3DRasterWindowWidget windows using TopLevelWindowHelper.all_widgets(...). Will use the most recent.')\n",
    "\t# assert len(found_spike_raster_windows) == 1, f\"found {len(found_spike_raster_windows)} Spike3DRasterWindowWidget windows using TopLevelWindowHelper.all_widgets(...) but require exactly one.\"\n",
    "\t# Get the most recent existing one and reuse that:\n",
    "\tspike_raster_window = found_spike_raster_windows[0]\n",
    "\n",
    "\n",
    "# Extras:\n",
    "active_2d_plot = spike_raster_window.spike_raster_plt_2d # <pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster.Spike2DRaster at 0x196c7244280>\n",
    "active_3d_plot = spike_raster_window.spike_raster_plt_3d # <pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster.Spike2DRaster at 0x196c7244280>\n",
    "main_graphics_layout_widget = active_2d_plot.ui.main_graphics_layout_widget # GraphicsLayoutWidget\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "background_static_scroll_plot_widget = active_2d_plot.plots.background_static_scroll_window_plot # PlotItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6338da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_raster_window.isVisible() # False\n",
    "# spike_raster_window.show()\n",
    "spike_raster_window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafe4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_raster_window.connection_man.active_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a801d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_any_window = TopLevelWindowHelper.top_level_windows(pg.mkQApp())\n",
    "found_any_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d815bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print windows:\n",
    "[print_widget_hierarchy(v) for k, v in found_any_window.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "_display_out = curr_active_pipeline.last_added_display_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ea13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer = _display_out['ipspikesDataExplorer']\n",
    "pActiveSpikesBehaviorPlotter = _display_out['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    " = curr_active_pipeline.last_added_display_output\n",
    "_display_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer = self._display_output['ipspikesDataExplorer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd461fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjusting Spike Emphasis:\n",
    "#### Usage Examples:\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "from neuropy.core.neuron_identities import NeuronType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Example 1: De-emphasize spikes excluded from the placefield calculations:\n",
    "is_spike_included_in_pf = np.isin(spike_raster_window.spike_raster_plt_2d.spikes_df.index, active_pf_2D.filtered_spikes_df.index)\n",
    "spike_raster_window.spike_raster_plt_2d.update_spike_emphasis(np.logical_not(is_spike_included_in_pf), SpikeEmphasisState.Deemphasized)\n",
    "\n",
    "## Example 2: De-emphasize spikes that don't have their 'aclu' from a given set of indicies:\n",
    "is_spike_included = spike_raster_window.spike_raster_plt_2d.spikes_df.aclu.to_numpy() == 2\n",
    "spike_raster_window.spike_raster_plt_2d.update_spike_emphasis(np.logical_not(is_spike_included), SpikeEmphasisState.Deemphasized)\n",
    "\n",
    "## Example 3: De-emphasize all spikes \n",
    "active_2d_plot.update_spike_emphasis(new_emphasis_state=SpikeEmphasisState.Deemphasized)\n",
    "\n",
    "## Example 4: Hide all spikes entirely\n",
    "active_2d_plot.update_spike_emphasis(new_emphasis_state=SpikeEmphasisState.Hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup: Hide all non-pyramidal spikes entirely\n",
    "spikes_df = spike_raster_window.spikes_df\n",
    "spike_raster_window.spike_raster_plt_2d.update_spike_emphasis(np.logical_not((spikes_df.neuron_type == NeuronType.from_string('pyr'))), SpikeEmphasisState.Hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3feb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_window = spike_raster_window.spikes_window # SpikesDataframeWindow\n",
    "# spikes_window.update_window_start_end(451.8908457518555, 451.9895490613999) ## Works but does not trigger refresh/update of the window. The changes are reflected as soon as you try to scroll at all though.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae85b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20*60.0 + 50.0 +  0.218 = 1250.218\n",
    "\n",
    "spikes_window.update_window_start_end(1250.218, (1250.218 + 3.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325548b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_window.window_duration # Prints the current window's duration. The win. dur. label control in the left bar is not updated.\n",
    "\n",
    "desired_window_fraction: float = 0.1 # 10% of the window is the default jump size\n",
    "relevant_jump_duration: float = spikes_window.window_duration * desired_window_fraction\n",
    "relevant_jump_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.spikes_mixins import SpikeRenderingPyVistaMixin\n",
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractivePlaceCellTuningCurvesDataExplorer import InteractivePlaceCellTuningCurvesDataExplorer\n",
    "# from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractivePlaceCellTuningCurvesDataExplorer import InteractivePlaceCellTuningCurvesDataExplorer\n",
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractivePlaceCellDataExplorer import InteractivePlaceCellDataExplorer\n",
    "\n",
    "found_windows_of_type = TopLevelWindowHelper.all_widgets(pg.mkQApp(), searchType=InteractivePlaceCellDataExplorer)\n",
    "found_windows_of_type\n",
    "TopLevelWindowHelper.top_level_windows(pg.mkQApp(), only_visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(451.8908457518555, 451.9895490613999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c418968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import BatchPhoJonathanFiguresHelper\n",
    "\n",
    "fig_1c_figures_out_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, included_unit_neuron_IDs=XOR_subset.track_exclusive_aclus, n_max_page_rows=20, write_vector_format=False, write_png=True, disable_top_row=True) # active_out_figures_dict: {IdentifyingContext<('kdiba', 'gor01', 'two', '2006-6-07_16-40-19', 'BatchPhoJonathanReplayFRC', 'long_only', '(12,21,48)')>: <Figure size 1920x660 with 12 Axes>, IdentifyingContext<('kdiba', 'gor01', 'two', '2006-6-07_16-40-19', 'BatchPhoJonathanReplayFRC', 'short_only', '(18,19,65)')>: <Figure size 1920x660 with 12 Axes>}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2f309",
   "metadata": {},
   "source": [
    "# PhoKamran2023Paper Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.setConfigOptions(background='white', foreground='black') # black on white background (more traditional) color scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d813d",
   "metadata": {},
   "source": [
    "## Figure 1) pf1D Ratemaps, Active set, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273696fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_1_add_replay_epoch_rasters\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "pf1d_compare_graphics, (example_epoch_rasters_L, example_epoch_rasters_S), example_stacked_epoch_graphics, fig_1c_figures_out_dict = PAPER_FIGURE_figure_1_full(curr_active_pipeline) # did not display the pf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdf = jonathan_firing_rate_analysis_result.rdf.rdf\n",
    "# rdf\n",
    "# ==================================================================================================================== #\n",
    "# Fig 1c) 2023-07-14 - LxC and SxC PhoJonathanSession plots                                                            #\n",
    "# ==================================================================================================================== #\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import BatchPhoJonathanFiguresHelper\n",
    "\n",
    "## Get global 'jonathan_firing_rate_analysis' results:\n",
    "curr_jonathan_firing_rate_analysis = curr_active_pipeline.global_computation_results.computed_data['jonathan_firing_rate_analysis']\n",
    "neuron_replay_stats_df, rdf, aclu_to_idx, irdf = curr_jonathan_firing_rate_analysis.neuron_replay_stats_df, curr_jonathan_firing_rate_analysis.rdf.rdf, curr_jonathan_firing_rate_analysis.rdf.aclu_to_idx, curr_jonathan_firing_rate_analysis.irdf.irdf\n",
    "\n",
    "fig_1c_figures_out_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, included_unit_neuron_IDs=XOR_subset.track_exclusive_aclus, n_max_page_rows=20, write_vector_format=False, write_png=True, disable_top_row=True) # active_out_figures_dict: {IdentifyingContext<('kdiba', 'gor01', 'two', '2006-6-07_16-40-19', 'BatchPhoJonathanReplayFRC', 'long_only', '(12,21,48)')>: <Figure size 1920x660 with 12 Axes>, IdentifyingContext<('kdiba', 'gor01', 'two', '2006-6-07_16-40-19', 'BatchPhoJonathanReplayFRC', 'short_only', '(18,19,65)')>: <Figure size 1920x660 with 12 Axes>}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba3c7a",
   "metadata": {},
   "source": [
    "## Figure 2) `PaperFigureTwo`: LxC/SxC Analyses\n",
    "Note: this fails when SxC or LxC are empty for this session (as it's not meaningful to produce a comparison bar plot). In this case, aggregate across multiple sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PaperFigureTwo\n",
    "\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # 10ms\n",
    "_out_fig_2.compute(curr_active_pipeline=curr_active_pipeline)\n",
    "_out_fig_2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a52142",
   "metadata": {},
   "source": [
    "## Figure 3) `PAPER_FIGURE_figure_3`: Firing Rate Index and Long/Short Firing Rate Replays v. Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f765ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.LongShortTrackComparingDisplayFunctions import _plot_long_short_firing_rate_indicies\n",
    "# curr_active_pipeline.reload_default_display_functions()\n",
    "\n",
    "_out, _out2 = PAPER_FIGURE_figure_3(curr_active_pipeline, defer_render=False, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909c676",
   "metadata": {},
   "source": [
    "##  All Programmatic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bcb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "\n",
    "batch_perform_all_plots(curr_active_pipeline, enable_neptune=False, neptuner=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87521d",
   "metadata": {},
   "source": [
    "# 2023-12-22 - Validating Direction Detection Via Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac454d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "## Main\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].laps_most_likely_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline)\n",
    "\n",
    "\n",
    "# laps_result_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ff425",
   "metadata": {},
   "source": [
    "# 2023-12-22 - Final Things to do\n",
    "- [X] Use `pearson_r` values instead of `spearman_rho` values in the quantile calculation\n",
    "- [ ] Throw out bad events\n",
    "- [X] Shuffle only amongst the cells that are active in a given epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laps_result_tuple.directional_likelihoods_df\n",
    "# laps_result_tuple.active_epochs\n",
    "\n",
    "\n",
    "# rank_order_results.LR_ripple.ranked_aclus_stats_dict\n",
    "# output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles = rank_order_results.laps_new_output_tuple\n",
    "\n",
    "\n",
    "assert len(rank_order_results.laps_new_output_tuple) == 5\n",
    "\n",
    "# laps_merged_complete_epoch_stats_df\n",
    "\n",
    "laps_merged_complete_epoch_stats_df[['Long_best_direction_indicies', 'Short_best_direction_indicies', 'combined_best_direction_indicies', 'long_best_direction_indices', 'short_best_direction_indices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a003e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rank_order_results.laps_new_output_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need: valid_stacked_arrays, real_stacked_arrays, combined_variable_names\n",
    "combined_epoch_stats_df: pd.DataFrame = pd.DataFrame(real_stacked_arrays, columns=combined_variable_names)\n",
    "combined_variable_z_score_column_names = [f\"{a_name}_Z\" for a_name in combined_variable_names] # combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']\n",
    "\n",
    "combined_variable_z_score_column_names = ['LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']\n",
    "combined_variable_names = ['LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson'] # list(set(real_stats_df.columns) - set(['label'])) # ['RL_Short_spearman', 'RL_Long_pearson', 'RL_Short_pearson', 'LR_Long_spearman', 'LR_Short_pearson', 'LR_Long_pearson', 'LR_Short_spearman', 'RL_Long_spearman']\n",
    "\n",
    "\n",
    "## Extract the stats values for each shuffle from `valid_stacked_arrays`:\n",
    "n_epochs = np.shape(real_stacked_arrays)[0]\n",
    "n_variables = np.shape(real_stacked_arrays)[1]\n",
    "\n",
    "assert n_epochs == np.shape(valid_stacked_arrays)[-2]\n",
    "assert n_variables == np.shape(valid_stacked_arrays)[-1]\n",
    "\n",
    "for variable_IDX, a_column_name in enumerate(combined_variable_z_score_column_names):\n",
    "\tstats_corr_values = np.squeeze(valid_stacked_arrays[:, :, variable_IDX])\n",
    "\tz_scorer_list = [Zscorer.init_from_values(stats_corr_values=stats_corr_values, real_value=real_stacked_arrays[epoch_IDX, variable_IDX]) for epoch_IDX in np.arange(n_epochs)]\n",
    "\tz_score_values = np.array([a_zscorer.z_score_value for a_zscorer in z_scorer_list])\n",
    "\tcombined_epoch_stats_df[a_column_name] = z_score_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be33928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "laps_merged_complete_epoch_stats_df[['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence', 'Long_LR_product_evidence', 'Long_RL_product_evidence', 'Short_LR_product_evidence', 'Short_RL_product_evidence', 'Long_normed_LR_evidence', 'Long_normed_RL_evidence', 'Long_normed_product_LR_evidence', 'Long_normed_product_RL_evidence', 'Long_best_direction_indicies', 'Short_normed_LR_evidence', 'Short_normed_RL_evidence', 'Short_normed_product_LR_evidence', 'Short_normed_product_RL_evidence']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence', 'Long_LR_product_evidence', 'Long_RL_product_evidence', 'Short_LR_product_evidence', 'Short_RL_product_evidence', 'Long_normed_LR_evidence', 'Long_normed_RL_evidence', 'Long_normed_product_LR_evidence', 'Long_normed_product_RL_evidence', 'Long_best_direction_indicies', 'Short_normed_LR_evidence', 'Short_normed_RL_evidence', 'Short_normed_product_LR_evidence', 'Short_normed_product_RL_evidence',\n",
    "'Short_best_direction_indicies', 'combined_best_direction_indicies', 'long_relative_direction_likelihoods', 'short_relative_direction_likelihoods', 'long_best_direction_indices', 'short_best_direction_indices',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809003ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import _validate_estimated_lap_dirs\n",
    "\n",
    "['long_best_direction_indices']\n",
    "\n",
    "rank_order_results.laps_merged_complete_epoch_stats_df\n",
    "\n",
    "_validate_estimated_lap_dirs(rank_order_results, global_any_laps_epochs_obj)\n",
    "# LR_laps_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['combined_best_direction_indicies', 'Short_best_direction_indicies', 'Long_best_direction_indicies', 'short_best_direction_indices', 'long_best_direction_indices'\n",
    "np.all(rank_order_results.laps_merged_complete_epoch_stats_df['Long_best_direction_indicies'] == rank_order_results.laps_merged_complete_epoch_stats_df['long_best_direction_indices'])\n",
    "np.all(rank_order_results.laps_merged_complete_epoch_stats_df['Short_best_direction_indicies'] == rank_order_results.laps_merged_complete_epoch_stats_df['short_best_direction_indices'])\n",
    "(rank_order_results.laps_merged_complete_epoch_stats_df['Long_best_direction_indicies'].values == rank_order_results.laps_merged_complete_epoch_stats_df['Long_best_direction_indicies'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.laps_most_likely_result_tuple.directional_likelihoods_tuple.short_best_direction_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e67f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "long_best_direction_indicies = np.argmax(np.vstack([np.abs(active_LR_long_z_score), np.abs(active_RL_long_z_score)]), axis=0).astype(int)\n",
    "short_best_direction_indicies = np.argmax(np.vstack([np.abs(active_LR_short_z_score), np.abs(active_RL_short_z_score)]), axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24712591",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_lap_epochs = rank_order_results.laps_most_likely_result_tuple.active_epochs\n",
    "estimated_directions = deepcopy(rank_order_results.laps_most_likely_result_tuple.directional_likelihoods_tuple.long_best_direction_indices)\n",
    "assert len(recovered_lap_epochs) == len(estimated_directions), f\"recovered_lap_epochs should be same length as estimated_directions\"\n",
    "estimated_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a374f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_lap_epochs['estimated_direction_index'] = estimated_directions\n",
    "recovered_lap_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_lap_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7807cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_laps.epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.laps_most_likely_result_tuple.plot_histograms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.laps_most_likely_result_tuple.directional_likelihoods_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeab6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.laps_most_likely_result_tuple.directional_likelihoods_tuple\n",
    ", \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c12d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_laps_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852508a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_L.plot.scatter(x='start', y='normed_LR_evidence') # 'normed_product_LR_evidence'\n",
    "epochs_df_L.plot.scatter(x='start', y='normed_product_LR_evidence') # 'normed_product_LR_evidence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeeb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_laps.epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_laps.epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b51573",
   "metadata": {},
   "outputs": [],
   "source": [
    "ActuallyIncludedAclusColumnNames = ['LR_Long_ActuallyIncludedAclus', 'RL_Long_ActuallyIncludedAclus', 'LR_Short_ActuallyIncludedAclus', 'RL_Short_ActuallyIncludedAclus']\n",
    "LR_laps_epochs_df[ActuallyIncludedAclusColumnNames]\n",
    "\n",
    "## find uniques in each one:\n",
    "\n",
    "def uniques(*aclus_lists):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e603ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_aclus_lists = LR_laps_epochs_df.loc[0, ActuallyIncludedAclusColumnNames].to_dict()\n",
    "# {'LR_Long_ActuallyIncludedAclus': array([  5,  17,  25,  31,  32,  34,  36,  41,  45,  48,  50,  53,  54,  55,  57,  58,  59,  61,  62,  63,  64,  66,  68,  69,  74,  75,  76,  78,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,  93,  96,  98, 100, 102, 108]),\n",
    "#  'RL_Long_ActuallyIncludedAclus': array([  3,   5,  10,  11,  14,  15,  16,  21,  31,  32,  33,  35,  36,  37,  41,  45,  48,  49,  50,  53,  55,  57,  59,  60,  61,  62,  63,  64,  69,  70,  71,  73,  75,  76,  78,  81,  83,  84,  86,  88,  89,  90,  92,  93,  98, 100, 102, 107, 108]),\n",
    "#  'LR_Short_ActuallyIncludedAclus': array([  5,  17,  25,  31,  32,  34,  36,  41,  45,  48,  50,  53,  54,  55,  57,  58,  59,  61,  62,  63,  64,  66,  68,  69,  74,  75,  76,  78,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,  93,  96,  98, 100, 102, 108]),\n",
    "#  'RL_Short_ActuallyIncludedAclus': array([  3,   5,  10,  11,  14,  15,  16,  21,  31,  32,  33,  35,  36,  37,  41,  45,  48,  49,  50,  53,  55,  57,  59,  60,  61,  62,  63,  64,  69,  70,  71,  73,  75,  76,  78,  81,  83,  84,  86,  88,  89,  90,  92,  93,  98, 100, 102, 107, 108])}\n",
    "# an_aclus_lists['LR_Long_ActuallyIncludedAclus']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LR_only, RL_only = bidirectional_setdiff1d(an_aclus_lists['LR_Long_ActuallyIncludedAclus'], an_aclus_lists['RL_Long_ActuallyIncludedAclus'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[bidirectional_setdiff1d(a_LR_aclus, a_RL_aclus) for a_LR_aclus, a_RL_aclus in zip(LR_laps_epochs_df['LR_Long_ActuallyIncludedAclus'].to_list(), LR_laps_epochs_df['RL_Long_ActuallyIncludedAclus'].to_list())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_laps_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs_df_L.plot(x='start', y=['LR_evidence', 'LR_product_evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e10b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_L.plot(x='start', y=['normed_LR_evidence', 'normed_product_LR_evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48afb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_laps.epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "decoders_dict = track_templates.get_decoders_dict()\n",
    "active_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "epoch_accumulated_evidence, epoch_rate_dfs, epochs_df_L = RankOrderAnalyses.epoch_directionality_active_set_evidence(decoders_dict, active_epochs_df)\n",
    "epochs_df_L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_best_direction_indicies = np.argmax(np.vstack([np.abs(epochs_df_L['Long_normed_LR_evidence'].to_numpy()), np.abs(epochs_df_L['Long_normed_RL_evidence'].to_numpy())]), axis=0).astype(int)\n",
    "short_best_direction_indicies = np.argmax(np.vstack([np.abs(epochs_df_L['Short_normed_LR_evidence'].to_numpy()), np.abs(epochs_df_L['Short_normed_RL_evidence'].to_numpy())]), axis=0).astype(int)\n",
    "long_best_direction_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(epochs_df_L.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e70134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LR_Long_ActuallyIncludedAclus', 'LR_Long_rel_num_cells', 'RL_Long_ActuallyIncludedAclus', 'RL_Long_rel_num_cells', 'LR_Short_ActuallyIncludedAclus', 'LR_Short_rel_num_cells', 'RL_Short_ActuallyIncludedAclus', 'RL_Short_rel_num_cells'\n",
    "# \n",
    "# epoch_rate_dfs\n",
    "\n",
    "['normed_LR_evidence','normed_RL_evidence','normed_product_LR_evidence','normed_product_RL_evidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bedea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_laps.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_rate_df = epoch_rate_dfs[410]\n",
    "epoch_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c314a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accumulated_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7bc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_L['normed_LR_evidence'].hist()\n",
    "\n",
    "# epochs_df_L[['LR_evidence', 'RL_evidence']]/epochs_df_L[['LR_evidence', 'RL_evidence']].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_L['normed_product_LR_evidence'].hist()\n",
    "epochs_df_L['normed_product_RL_evidence'].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_rate_dfs[188]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df_L.plot(x='start', y='normed_LR_evidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1120474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which cells are active in the given epoch:\n",
    "\n",
    "# Get these cells' firing rates, using 0.0 if it isn't included in the map for one of the directions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835130b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f467b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.index).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fe07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(rank_order_results.ripple_combined_epoch_stats_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(rank_order_results.ripple_combined_epoch_stats_df.label.to_numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0961819",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df97f15",
   "metadata": {},
   "source": [
    "# 2023-12-18 - Simpily detect bimodal cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9795e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_ratemap = deepcopy(long_pf1D.ratemap)\n",
    "active_ratemap = deepcopy(long_LR_pf1D.ratemap)\n",
    "peaks_dict, aclu_n_peaks_dict, unimodal_peaks_dict = active_ratemap.compute_tuning_curve_modes()\n",
    "print(aclu_n_peaks_dict) # {2: 4, 5: 4, 7: 2, 8: 2, 9: 2, 10: 5, 17: 2, 24: 2, 25: 3, 26: 1, 31: 3, 32: 5, 34: 2, 35: 1, 36: 2, 37: 2, 41: 4, 45: 3, 48: 4, 49: 4, 50: 4, 51: 3, 53: 5, 54: 3, 55: 5, 56: 4, 57: 4, 58: 5, 59: 3, 61: 4, 62: 3, 63: 4, 64: 4, 66: 3, 67: 4, 68: 2, 69: 2, 71: 3, 73: 3, 74: 3, 75: 5, 76: 5, 78: 3, 81: 3, 82: 1, 83: 4, 84: 4, 86: 3, 87: 3, 88: 4, 89: 3, 90: 3, 92: 4, 93: 4, 96: 2, 97: 4, 98: 5, 100: 4, 102: 7, 107: 1, 108: 5, 109: 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_n_peaks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d106dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unimodal_only_aclus = np.array(list(unimodal_peaks_dict.keys()))\n",
    "unimodal_only_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=unimodal_only_aclus, sortby=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be99ada",
   "metadata": {},
   "source": [
    "## 2023-12-19\n",
    "\n",
    "5 epochs where we have a higher Z for short than we do for long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4773eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df[rank_order_results.ripple_combined_epoch_stats_df.label == 148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.epochs_df[rank_order_results.LR_ripple.epochs_df.label == 148]\n",
    "# rank_order_results.ripple_combined_epoch_stats_df[rank_order_results.ripple_combined_epoch_stats_df.label == 148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bdbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df[rank_order_results.ripple_combined_epoch_stats_df['LR_Short_spearman'].abs() > rank_order_results.ripple_combined_epoch_stats_df['LR_Long_spearman'].abs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c25c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df[(rank_order_results.ripple_combined_epoch_stats_df['LR_Short_spearman_Z'].abs() > rank_order_results.ripple_combined_epoch_stats_df['LR_Long_spearman_Z'].abs()) & (rank_order_results.ripple_combined_epoch_stats_df['LR_Short_spearman_Z'].abs() > 1.98)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29413126",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get old LR_ripple/RL_ripple results\n",
    "LR_active_result: RankOrderResult = rank_order_results.LR_ripple\n",
    "LR_active_result_epochs = LR_active_result.epochs_df\n",
    "LR_active_result_epoch_labels = LR_active_result.epochs_df.label.to_numpy()\n",
    "LR_active_result_n_epochs = np.shape(LR_active_result.epochs_df)[0]\n",
    "LR_z_variable_shapes = [np.shape(x) for x in (LR_active_result.long_z_score, LR_active_result.short_z_score, LR_active_result.long_short_z_score_diff)]\n",
    "assert np.all([LR_active_result_n_epochs == s[0] for s in LR_z_variable_shapes]), f\"z-variable shapes should match the number of epochs\"\n",
    "\n",
    "\n",
    "RL_active_result: RankOrderResult = rank_order_results.RL_ripple\n",
    "RL_active_result_epochs = RL_active_result.epochs_df\n",
    "RL_active_result_epoch_labels = RL_active_result.epochs_df.label.to_numpy()\n",
    "print(f'active_result_epoch_labels: {RL_active_result_epoch_labels}')\n",
    "RL_active_result_n_epochs = np.shape(RL_active_result.epochs_df)[0]\n",
    "RL_z_variable_shapes = [np.shape(x) for x in (RL_active_result.long_z_score, RL_active_result.short_z_score, RL_active_result.long_short_z_score_diff)]\n",
    "print(f'RL_z_variable_shapes: {RL_z_variable_shapes}')\n",
    "assert np.all([RL_active_result_n_epochs == s[0] for s in RL_z_variable_shapes]), f\"z-variable shapes should match the number of epochs\"\n",
    "\n",
    "## LR and RL epoch labels must be equal to each other!\n",
    "assert (np.array_equal(rank_order_results.LR_ripple.epochs_df.label.to_numpy(), rank_order_results.RL_ripple.epochs_df.label.to_numpy())), f\"Epoch labels must be equal\"\n",
    "new_result_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "new_result_epochs_df.reset_index(drop=True)\n",
    "new_result_df = pd.DataFrame({\n",
    "'label': deepcopy(RL_active_result.epochs_df.label.to_numpy()),\n",
    "'LR_Long_Z': rank_order_results.LR_ripple.long_z_score, \n",
    "'LR_Short_Z': rank_order_results.LR_ripple.short_z_score,\n",
    "'LR_LongShort_Zdiff': rank_order_results.LR_ripple.long_short_z_score_diff,\n",
    "'RL_Long_Z': rank_order_results.RL_ripple.long_z_score, \n",
    "'RL_Short_Z': rank_order_results.RL_ripple.short_z_score,\n",
    "'RL_LongShort_Zdiff': rank_order_results.RL_ripple.long_short_z_score_diff,\n",
    " }, index=new_result_epochs_df.index).reset_index(drop=True)\n",
    "\n",
    "# new_result_epochs_df.update(new_result_df, errors='raise')\n",
    "# assert np.shape(new_result_epochs_df)[0] == np.shape(rank_order_results.LR_ripple.epochs_df)[0], f\"number of rows must not have changed\"\n",
    "# new_result_epochs_df\n",
    "new_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74529ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_labels = [4, 148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c61e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result_df[new_result_df.label == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result_df[new_result_df['LR_Long_Z'].abs() > 1.97]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.ripple_combined_epoch_stats_df[rank_order_results.ripple_combined_epoch_stats_df.label == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8653a1",
   "metadata": {},
   "source": [
    "# 2023-12-19 - Transition Matricies Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import TransitionMatrixComputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf1D = deepcopy(global_pf1D)\n",
    "# pf1D = deepcopy(short_pf1D)\n",
    "# pf1D = deepcopy(long_pf1D)\n",
    "\n",
    "\n",
    "decoders_dict = track_templates.get_decoders_dict()\n",
    "binned_x_transition_matrix_higher_order_list_dict = {}\n",
    "\n",
    "for a_decoder_name, a_decoder in decoders_dict.items():\n",
    "\ta_pf1D = deepcopy(a_decoder.pf)\n",
    "\tbinned_x_transition_matrix_higher_order_list_dict[a_decoder_name] = TransitionMatrixComputations._compute_position_transition_matrix(a_pf1D.xbin_labels, a_pf1D.filtered_pos_df['binned_x'].to_numpy())\n",
    "\n",
    "binned_x_transition_matrix_higher_order_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_decoder.dec\n",
    "\n",
    "long_LR_pf1D_Decoder.active_time_window_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization ______________________________________________________________________________________________________ #\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow, LayoutScrollability\n",
    "out = BasicBinnedImageRenderingWindow(binned_x_transition_matrix_higher_order_list[2], a_pf1D.xbin_labels, a_pf1D.xbin_labels, name='binned_x_transition_matrix', title=\"Transition Matrix for binned x (from, to)\", variable_label='Transition Matrix', scrollability_mode=LayoutScrollability.NON_SCROLLABLE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "rank_order_results.LR_laps.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _time_transition_matrix(df, bin_size=0.25): # \"0.25S\"\n",
    "\t# df[\"bin\"] = pd.cut(df[\"t_rel_seconds\"], bins=pd.timedelta_range(df[\"t_rel_seconds\"].min(), df[\"t_rel_seconds\"].max(), freq=bin_size))\n",
    "\t# df[\"bin\"] = pd.cut(\n",
    "\t# \tpd.to_timedelta(df[\"t_rel_seconds\"], unit=\"s\"),\n",
    "\t# \tbins=pd.timedelta_range(df[\"t_rel_seconds\"].min(), df[\"t_rel_seconds\"].max(), freq=bin_size),\n",
    "\t# )\n",
    "\tdef get_bin(seconds):\n",
    "\t\tbin_number = int(seconds // bin_size)\n",
    "\t\treturn bin_number\n",
    "\n",
    "\tdf[\"bin\"] = df[\"t_rel_seconds\"].apply(get_bin)\n",
    "\n",
    "\ttransition_matrix = csr_matrix((\n",
    "\t\tcounts, (i, j)\n",
    "\t) for i, (bin, group) in df.groupby([\"bin\", \"aclu\"]).agg(counts=(\"aclu\", \"count\")).iterrows()\n",
    "\t\tfor j, counts in group.iteritems()\n",
    "\t)\n",
    "\ttransition_matrix /= transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "\treturn transition_matrix\n",
    "\n",
    "\n",
    "test_spikes_df = deepcopy(rank_order_results.LR_laps.spikes_df)\n",
    "\n",
    "test_spikes_df = test_spikes_df.spikes.add_binned_time_column()\n",
    "\n",
    "# [['t_rel_seconds', 'aclu', 'flat_spike_idx', 'Probe_Epoch_id']]]\n",
    "\n",
    "\n",
    "test_spikes_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = _time_transition_matrix(df=test_spikes_df) #  bin_size=\"0.25S\"\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63242f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_LR_pf1D_Decoder.time_window_centers\n",
    "# long_LR_pf1D_Decoder.decode_specific_epochs\n",
    "long_LR_pf1D_Decoder.p_x_given_n\n",
    "\n",
    "# time_bin_size = 0.25\n",
    "time_bin_size = 0.1\n",
    "manual_time_bin_start_t = curr_active_pipeline.sess.t_start\n",
    "manual_time_bin_end_t = curr_active_pipeline.sess.t_stop\n",
    "\n",
    "print(f'time_bin_size: {time_bin_size}, manual_time_bins: (start_t: {manual_time_bin_start_t}, end_t: {manual_time_bin_end_t})')\n",
    "manual_time_window_edges, manual_time_window_edges_binning_info = compute_spanning_bins(None, bin_size=time_bin_size, variable_start_value=manual_time_bin_start_t, variable_end_value=manual_time_bin_end_t) # np.shape(out_digitized_variable_bins)[0] == np.shape(spikes_df)[0]\n",
    "# debug_print_1D_bin_infos(manual_time_window_edges, 'manual_time_window_edges')\n",
    "\n",
    "## Build the new decoder with custom params:\n",
    "new_decoder_pf_params = deepcopy(active_computation_config.pf_params) # should be a PlacefieldComputationParameters\n",
    "# override some settings before computation:\n",
    "new_decoder_pf_params.time_bin_size = time_bin_size\n",
    "\n",
    "## 1D Decoder\n",
    "new_decoder_pf1D = active_pf_1D\n",
    "new_1D_decoder_spikes_df = new_decoder_pf1D.filtered_spikes_df.copy()\n",
    "new_1D_decoder_spikes_df = new_1D_decoder_spikes_df.spikes.add_binned_time_column(manual_time_window_edges, manual_time_window_edges_binning_info, debug_print=False)\n",
    "new_1D_decoder = BayesianPlacemapPositionDecoder(new_decoder_pf_params.time_bin_size, new_decoder_pf1D, new_1D_decoder_spikes_df, manual_time_window_edges=manual_time_window_edges, manual_time_window_edges_binning_info=manual_time_window_edges_binning_info, debug_print=False)\n",
    "new_1D_decoder.compute_all() #  --> n = self.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d17566",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_LR_pf1D_Decoder.unit_specific_time_binned_spike_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05936088",
   "metadata": {},
   "source": [
    "# 2023-12-21 - Inversion Count Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa459b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import InversionCount\n",
    "\n",
    "# Example usage\n",
    "# list1 = [3, 1, 5, 2, 4]\n",
    "list1 = [1, 2, 4, 3, 5] # 1\n",
    "list1 = [1, 3, 4, 5, 2] # 3\n",
    "num_swaps = InversionCount.count_swaps_to_sort(list1)\n",
    "print(\"Number of swaps required:\", num_swaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list = [...]  # Your list of items\n",
    "sorted_list = sorted(original_list)\n",
    "\n",
    "# Count the number of swaps needed to sort the list\n",
    "swaps_needed = sum(1 for i, j in zip(original_list, sorted_list) if i != j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_results_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b6473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54aa4837",
   "metadata": {},
   "source": [
    "# ❇️🆕 READY/NEXT: 2023-11-10 - All directional pf1D works for merging all four 1D templates!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76086ac5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "31"
    }
   },
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "1. Builds a combined pseudo-2D decoder out of the four 1D directional decoders.\n",
    "\ty-bins (centers) of the pseduo-decoder are: {'long_LR': 0, 'long_RL': 1, 'short_LR': 2, 'short_RL': 3}\n",
    "\n",
    "2. \n",
    "\"\"\"\n",
    "\n",
    "# Use the four epochs to make to a pseudo-y:\n",
    "all_directional_decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "all_directional_decoder_dict = dict(zip(all_directional_decoder_names, [deepcopy(long_LR_pf1D), deepcopy(long_RL_pf1D), deepcopy(short_LR_pf1D), deepcopy(short_RL_pf1D)]))\n",
    "all_directional_pf1D = PfND.build_merged_directional_placefields(all_directional_decoder_dict, debug_print=False)\n",
    "all_directional_pf1D_Decoder = BasePositionDecoder(all_directional_pf1D, setup_on_init=True, post_load_on_init=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all_directional_pf1D_Decoder to decode all time across session:\n",
    "all_directional_pf1D_Decoder.decode_specific_epochs(\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b0e02",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "# curr_active_pipeline.perform_specific_computation(\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "\n",
    "batch_extended_computations(curr_active_pipeline, include_includelist=['merged_directional_placefields'], include_global_functions=True, fail_on_exception=True, force_recompute=True, force_rem)\n",
    "# ## Get the result after computation:\n",
    "# directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "# laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "# laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "# ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "# ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7fcab",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "31"
    }
   },
   "outputs": [],
   "source": [
    "active_context = curr_active_pipeline.get_session_context()\n",
    "\n",
    "collected_output_path = Path('output/collected_outputs').resolve()\n",
    "collected_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "(laps_marginals_df, laps_out_path), (ripple_marginals_df, ripple_out_path) = directional_merged_decoders_result.compute_and_export_marginals_df_csvs(parent_output_path=collected_output_path, active_context=active_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uri_from_path(laps_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f780ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "t_start, t_end = global_epoch.start_end_times\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "## Get the result after computation:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "collector = plot_all_epoch_bins_marginal_predictions(directional_merged_decoders_result, t_start=t_start, t_split=split_time_t, t_end=t_end, active_context=active_context, perform_write_to_file_callback=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223159f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_display_directional_track_template_pf1Ds\n",
    "\n",
    "_display_directional_merged_pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325fb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "# Fully unpack the `'DirectionalMergedDecoders'` result:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "# directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "# directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "\n",
    "laps_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs).to_dataframe()\n",
    "laps_directional_marginals_tuple = DirectionalMergedDecodersResult.determine_directional_likelihoods(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals = DirectionalMergedDecodersResult.determine_long_short_likelihoods(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "track_identity_marginals, track_identity_all_epoch_bins_marginal, most_likely_track_identity_from_decoder, is_most_likely_track_identity_Long = laps_track_identity_marginals\n",
    "\n",
    "## Decode Ripples:\n",
    "ripple_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs)\n",
    "all_directional_ripple_filter_epochs_decoder_result: DecodedFilterEpochsResult = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "ripple_marginals = DirectionalMergedDecodersResult.determine_directional_likelihoods(all_directional_ripple_filter_epochs_decoder_result)\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = ripple_marginals\n",
    "ripple_track_identity_marginals = DirectionalMergedDecodersResult.determine_long_short_likelihoods(all_directional_ripple_filter_epochs_decoder_result)\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = ripple_track_identity_marginals\n",
    "\n",
    "# directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\n",
    "directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\n",
    "\n",
    "# ripple_marginals = DirectionalMergedDecodersResult.determine_directional_likelihoods(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "# ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = ripple_marginals\n",
    "\n",
    "type(ripple_marginals)\n",
    "type(ripple_track_identity_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b392fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_directional_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaae3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71baf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive-mode parameters:\n",
    "_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=True, scrollable_figure=True, defer_render=False)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_curr_interaction_mode_kwargs = _interactive_mode_kwargs # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc756e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interactive:\n",
    "_non_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=False, scrollable_figure=False, defer_render=True)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=False, backend='AGG')\n",
    "_curr_interaction_mode_kwargs = _non_interactive_mode_kwargs # non-interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', curr_active_pipeline.get_session_context(),\n",
    "\tmax_num_lap_epochs = 240, max_num_ripple_epochs = 500,\n",
    "\trender_directional_marginal_laps=True, render_directional_marginal_ripples=True, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=True,\n",
    "\t# render_directional_marginal_laps=True, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=False, render_track_identity_marginal_ripples=False,\n",
    "\t# constrained_layout=True, # layout='none',\n",
    "\t# build_fn='basic_view', constrained_layout=True, \n",
    "\tbuild_fn='insets_view', constrained_layout=True, #constrained_layout=None, layout='none', # , constrained_layout=False constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast\n",
    "\t**_curr_interaction_mode_kwargs, # interactive mode\n",
    "\tskip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=True, \n",
    "\t# directional_merged_decoders_result=directional_merged_de?coders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t)\n",
    "collector = _out['collector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ed671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-display_dir_merged_pf_decoded_epochs.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# Here\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', max_num_lap_epochs = 85, max_num_ripple_epochs = 120,\n",
    "\t# render_directional_marginal_laps=True, render_directional_marginal_ripples=True, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=True,\n",
    "\trender_directional_marginal_laps=True, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=False, render_track_identity_marginal_ripples=False,\n",
    "\t# constrained_layout=True, # layout='none',\n",
    " \t# build_fn='basic_view', constrained_layout=True,\n",
    "\tbuild_fn='insets_view', constrained_layout=False, # constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast\n",
    "\t**_curr_interaction_mode_kwargs, # interactive mode\n",
    "\tskip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=True) # , size=(5,12), dpi=96 size=(15,7), dpi=72, constrained_layout=True\n",
    "collector = _out['collector']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(num=plots.figure_id, ncols=1, nrows=1, dpi=dpi, clear=True, sharex=False, sharey=False, constrained_layout=constrained_layout, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = _out['collector']\n",
    "laps_plot_tuple = _out['directional_laps_plot_tuple']\n",
    "params, plots_data, plots, ui = laps_plot_tuple\n",
    "# mw = ui.mw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.get('skip_plotting_most_likely_positions', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e218368",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "fig = mw.getFigure()\n",
    "sub_context = collector.contexts[0]\n",
    "\n",
    "\n",
    "# Recover the proper title:\n",
    "title = mw.params.name\n",
    "\n",
    "# `flexitext` version:\n",
    "text_formatter = FormattedFigureText()\n",
    "fig.suptitle('')\n",
    "text_formatter.setup_margins(fig) # , top_margin=0.740\n",
    "title_text_obj = flexitext(text_formatter.left_margin, text_formatter.top_margin, title, va=\"bottom\", xycoords=\"figure fraction\")\n",
    "footer_text_obj = flexitext((text_formatter.left_margin * 0.1), (text_formatter.bottom_margin * 0.25),\n",
    "\t\t\t\t\t\t\ttext_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t\t\t\tva=\"top\", xycoords=\"figure fraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_display_context = curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs')\n",
    "active_display_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7280e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _main_context = [{'decoded_epochs': 'Laps', 'Marginal': 'Direction'}, {'decoded_epochs': 'Laps', 'Marginal': 'Direction'}, {'decoded_epochs': 'Laps', 'Marginal': 'Direction'}, {'decoded_epochs': 'Laps', 'Marginal': 'Direction'}]\n",
    "\n",
    "# Safe seperator characters\n",
    "safe_seperators_list = ['-','.','_'] # for dates I frequently use '2006-6-09_1-22-43' format, meaning both dashes and underscores are ruled out as info separators\n",
    "\n",
    "\n",
    "'_'.join(['Laps', 'Direction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d43925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_params_kwargs = {'t_bin_size': directional_merged_decoders_result.laps_decoding_time_bin_size} # Parameters:\n",
    "\n",
    "_merged_context = _main_context | _params_kwargs\n",
    "_merged_context\n",
    "\n",
    "# {'decoded_epochs': 'Laps', 'Marginal': 'Direction', 't_bin_size': 0.075}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaffae7",
   "metadata": {},
   "source": [
    "# 2024-01-06 - Decoded Epoch Posterior Marginal Figures outputs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b94d27",
   "metadata": {},
   "source": [
    "### `BatchPhoJonathanFiguresHelper._perform_batch_plot` approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3473b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From `pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing.BatchPhoJonathanFiguresHelper._perform_batch_plot`\n",
    "fig_man = curr_active_pipeline.get_output_manager()\n",
    "\n",
    "active_out_figures_dict = {} # empty dict to hold figures\n",
    "num_pages = len(active_kwarg_list)\n",
    "\n",
    "# Each figure:\n",
    "for i, curr_batch_plot_kwargs in enumerate(active_kwarg_list):\n",
    "\tcurr_active_identifying_ctx = curr_batch_plot_kwargs['active_identifying_ctx'] # this context is good\n",
    "\t\n",
    "\t## 2023-06-14 - New way using `fig_man.get_figure_save_file_path(...)` - this is correct\n",
    "\tfinal_figure_file_path = fig_man.get_figure_save_file_path(curr_active_identifying_ctx, make_folder_if_needed=True) # complete file path without extension: e.g. '/ProgrammaticDisplayFunctionTesting/2023-06-15/kdiba_pin01_one_11-03_12-3-25_BatchPhoJonathanReplayFRC_short_only_[13, 22, 28]'\n",
    "\n",
    "\t# Perform the plotting:\n",
    "\ta_fig = cls._subfn_batch_plot_automated(curr_active_pipeline, **curr_batch_plot_kwargs)\n",
    "\tactive_out_figures_dict[curr_active_identifying_ctx] = a_fig\n",
    "\n",
    "\t# One plot at a time to PDF:\n",
    "\tif write_vector_format:\n",
    "\t\tactive_pdf_metadata, _UNUSED_pdf_save_filename = build_pdf_metadata_from_display_context(curr_active_identifying_ctx, subset_includelist=subset_includelist, subset_excludelist=subset_excludelist)\n",
    "\t\tcurr_pdf_save_path = final_figure_file_path.with_suffix('.pdf')\n",
    "\t\twith backend_pdf.PdfPages(curr_pdf_save_path, keep_empty=False, metadata=active_pdf_metadata) as pdf:\n",
    "\t\t\t# Save out PDF page:\n",
    "\t\t\tpdf.savefig(a_fig)\n",
    "\t\t\tcurr_active_pipeline.register_output_file(output_path=curr_pdf_save_path, output_metadata={'context': curr_active_identifying_ctx, 'fig': (a_fig), 'pdf_metadata': active_pdf_metadata})\n",
    "\t\t\tif progress_print:\n",
    "\t\t\t\tprint(f'\\t saved {curr_pdf_save_path}')\n",
    "\n",
    "\n",
    "\t# Also save .png versions:\n",
    "\tif write_png:\n",
    "\t\t# curr_page_str = f'pg{i+1}of{num_pages}'\n",
    "\t\tfig_png_out_path = final_figure_file_path.with_suffix('.png')\n",
    "\t\t# fig_png_out_path = fig_png_out_path.with_stem(f'{curr_pdf_save_path.stem}_{curr_page_str}') # note this replaces the current .pdf extension with .png, resulting in a good filename for a .png\n",
    "\t\ta_fig.savefig(fig_png_out_path)\n",
    "\t\tcurr_active_pipeline.register_output_file(output_path=fig_png_out_path, output_metadata={'context': curr_active_identifying_ctx, 'fig': (a_fig)})\n",
    "\t\tif progress_print:\n",
    "\t\t\tprint(f'\\t saved {fig_png_out_path}')\n",
    "\n",
    "\n",
    "return active_out_figures_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23f62b",
   "metadata": {},
   "source": [
    "### `FigureCollector`-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa607283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "\n",
    "if active_context is not None:\n",
    "\tdisplay_context = active_context.adding_context('display_fn', display_fn_name='plot_quantile_diffs')\n",
    "\t\n",
    "with mpl.rc_context({'figure.figsize': (12.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "\t# Create a FigureCollector instance\n",
    "\twith FigureCollector(name='plot_quantile_diffs', base_context=display_context) as collector:\n",
    "\n",
    "\t\t## Define common operations to do after making the figure:\n",
    "\t\tdef setup_common_after_creation(a_collector, fig, axes, sub_context, title=f'<size:22> Sig. (>0.95) <weight:bold>Best</> <weight:bold>Quantile Diff</></>'):\n",
    "\t\t\t\"\"\" Captures:\n",
    "\n",
    "\t\t\tt_split\n",
    "\t\t\t\"\"\"\n",
    "\t\t\ta_collector.contexts.append(sub_context)\n",
    "\t\t\t\n",
    "\t\t\t# Add epoch indicators\n",
    "\t\t\tfor ax in (axes if isinstance(axes, Iterable) else [axes]):\n",
    "\t\t\t\tPlottingHelpers.helper_matplotlib_add_long_short_epoch_indicator_regions(ax=ax, t_split=t_split)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# `flexitext` version:\n",
    "\t\t\t\ttext_formatter = FormattedFigureText()\n",
    "\t\t\t\tax.set_title('')\n",
    "\t\t\t\tfig.suptitle('')\n",
    "\t\t\t\ttext_formatter.setup_margins(fig)\n",
    "\t\t\t\ttitle_text_obj = flexitext(text_formatter.left_margin, text_formatter.top_margin,\n",
    "\t\t\t\t\t\t\t\t\t\ttitle,\n",
    "\t\t\t\t\t\t\t\t\t\tva=\"bottom\", xycoords=\"figure fraction\")\n",
    "\t\t\t\tfooter_text_obj = flexitext((text_formatter.left_margin * 0.1), (text_formatter.bottom_margin * 0.25),\n",
    "\t\t\t\t\t\t\t\t\t\t\ttext_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t\t\t\t\t\t\t\tva=\"top\", xycoords=\"figure fraction\")\n",
    "\t\t\n",
    "\t\t\tif ((perform_write_to_file_callback is not None) and (sub_context is not None)):\n",
    "\t\t\t\tperform_write_to_file_callback(sub_context, fig)\n",
    "\t\t\t\n",
    "\n",
    "\t\t# Plot for BestDir\n",
    "\t\tfig, ax = collector.subplots(num='LongShort_BestDir_quantile_diff', clear=True)\n",
    "\t\t_out_BestDir = sns.scatterplot(\n",
    "\t\t\tax=ax,\n",
    "\t\t\tdata=significant_BestDir_quantile_stats_df,\n",
    "\t\t\tx='start',\n",
    "\t\t\ty='LongShort_BestDir_quantile_diff',\n",
    "\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t)\n",
    "\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=display_context.adding_context('subplot', subplot_name='BestDir'), \n",
    "\t\t\t\t\t\t\t\t\ttitle=f'<size:22> Sig. (>0.95) <weight:bold>Best</> Quantile Diff</>')\n",
    "\t\t\n",
    "\n",
    "\t\tif include_LR_LR_plot:\n",
    "\t\t\t# Create the scatter plot with Seaborn, using 'size' to set marker sizes\n",
    "\t\t\tfig, ax = collector.subplots(num='LR-LR_LongShort_LR_quantile_diff', clear=True)\n",
    "\t\t\t_out_LR = sns.scatterplot(\n",
    "\t\t\t\tax=ax,\n",
    "\t\t\t\tdata=LR_likely_active_df,\n",
    "\t\t\t\tx='start',\n",
    "\t\t\t\ty='LongShort_LR_quantile_diff',\n",
    "\t\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t\t)\n",
    "\t\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=display_context.adding_context('subplot', subplot_name='LR-Likely'), \n",
    "\t\t\t\t\t\t\t\t\t\ttitle=f'<size:22> Sig. (>0.95) <weight:bold>LR-LR (LR-Likely)</> Quantile Diff</>')\n",
    "\t\t\n",
    "\n",
    "\t\tif include_RL_RL_plot:\n",
    "\t\t\tfig, ax = collector.subplots(num='RL-RL_LongShort_RL_quantile_diff', clear=True)\n",
    "\t\t\t_out_RL = sns.scatterplot(\n",
    "\t\t\t\tax=ax,\n",
    "\t\t\t\tdata=RL_likely_active_df[RL_likely_active_df['RL_Long_rel_num_cells']>10],\n",
    "\t\t\t\tx='start',\n",
    "\t\t\t\ty='LongShort_RL_quantile_diff',\n",
    "\t\t\t\t# size='RL_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t\t)\n",
    "\t\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=display_context.adding_context('subplot', subplot_name='RL-Likely'), \n",
    "\t\t\t\t\t\t\t\t\t\ttitle=f'<size:22> Sig. (>0.95) <weight:bold>RL-RL (RL-Likely)</> Quantile Diff</>')\n",
    "\t\t\n",
    "\n",
    "\n",
    "# Access the collected figures outside the context manager\n",
    "# result = tuple(collector.created_figures)\n",
    "\n",
    "return collector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if active_context is not None:\n",
    "\tdisplay_context = active_context.adding_context('display_fn', display_fn_name='plot_rank_order_histograms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00267bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\treturn curr_active_pipeline.output_figure(final_context, fig)\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context, perform_write_to_file_callback=_perform_write_to_file_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb65cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display('_display_directional_merged_pfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "laps_filter_epochs_decoder_result\n",
    "\n",
    "render_merged_pseudo2D_decoder_laps=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cfe1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validate Laps:\n",
    "# requires `laps_is_most_likely_direction_LR_dir` from `laps_marginals`\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_session = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name]) # used for validate_lap_dir_estimations(...) \n",
    "global_any_laps_epochs_obj = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any'\n",
    "percent_laps_estimated_correctly = DirectionalMergedDecodersResult.validate_lap_dir_estimations(global_session, active_global_laps_df=global_any_laps_epochs_obj.to_dataframe(), laps_is_most_likely_direction_LR_dir=laps_is_most_likely_direction_LR_dir)\n",
    "print(f'percent_laps_estimated_correctly: {percent_laps_estimated_correctly}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_marginals, directional_all_epoch_bins_marginal, most_likely_direction_from_decode, is_most_likely_direction_LR_dir = DirectionalMergedDecodersResult.determine_directional_likelihoods(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "\n",
    "laps_track_identity_marginals = DirectionalMergedDecodersResult.determine_long_short_likelihoods(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "track_identity_marginals, track_identity_all_epoch_bins_marginal, most_likely_track_identity_from_decoder, is_most_likely_track_identity_Long = laps_track_identity_marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e271eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\n",
    "all_directional_pf1D_Decoder_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_decoder = all_directional_pf1D_Decoder_value\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='TEST NEW LAPS',\n",
    "                                            # active_marginal_fn = lambda filter_epochs_decoder_result: filter_epochs_decoder_result.marginal_y_list,\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result:  DirectionalMergedDecodersResult.build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "\n",
    "active_decoder = all_directional_pf1D_Decoder_value\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='TEST NEW LAPS',\n",
    "                                            # active_marginal_fn = lambda filter_epochs_decoder_result: filter_epochs_decoder_result.marginal_y_list,\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result:  DirectionalMergedDecodersResult.build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "owning_pipeline_reference = curr_active_pipeline\n",
    "\n",
    "# Direction (LR/RL) Marginal:\n",
    "global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any'\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "directional_laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='Directional_Marginal_LAPS',\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t# single_plot_fixed_height=single_plot_fixed_height, debug_test_max_num_slices=max_num_lap_epochs, size=size, dpi=dpi, constrained_layout=constrained_layout, scrollable_figure=scrollable_figure,\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "# Track-identity (Long/Short) Marginal:\n",
    "global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any'\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "track_identity_marginal_laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='TrackIdentity_Marginal_LAPS',\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_long_short(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t# single_plot_fixed_height=single_plot_fixed_height, debug_test_max_num_slices=max_num_lap_epochs, size=size, dpi=dpi, constrained_layout=constrained_layout, scrollable_figure=scrollable_figure,\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "# Replays: ___________________________________________________________________________________________________________ #\n",
    "\n",
    "# Direction (LR/RL) Marginal:\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(global_session.replay))\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "directional_ripples_plot_tuple = plot_decoded_epoch_slices(global_replays,  directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='Directional_Marginal_Ripples',\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t# single_plot_fixed_height=single_plot_fixed_height, debug_test_max_num_slices=max_num_ripple_epochs, size=size, dpi=dpi, constrained_layout=constrained_layout, scrollable_figure=scrollable_figure,\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "# Track-identity (Long/Short) Marginal:\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(global_session.replay))\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "track_identity_marginal_ripples_plot_tuple = plot_decoded_epoch_slices(global_replays,  directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='TrackIdentity_Marginal_Ripples',\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_long_short(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t# single_plot_fixed_height=single_plot_fixed_height, debug_test_max_num_slices=max_num_ripple_epochs, size=size, dpi=dpi, constrained_layout=constrained_layout, scrollable_figure=scrollable_figure,\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global_any_laps_epochs_obj = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any'\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "implemented_laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='Directional_Marginal_LAPS',\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t# single_plot_fixed_height=single_plot_fixed_height, debug_test_max_num_slices=max_num_lap_epochs, size=size, dpi=dpi, constrained_layout=constrained_layout, scrollable_figure=scrollable_figure,\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2670344",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_decoder = all_directional_pf1D_Decoder_value\n",
    "ripples_plot_tuple = plot_decoded_epoch_slices(global_replays, all_directional_ripples_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='stacked_epoch_slices_matplotlib_subplots_Ripples',\n",
    "                                            # active_marginal_fn = lambda filter_epochs_decoder_result: filter_epochs_decoder_result.marginal_y_list,\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "active_decoder = all_directional_pf1D_Decoder_value\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, long_only_laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='long_only_lstacked_epoch_slices_matplotlib_subplots_LAPS',\n",
    "                                            # active_marginal_fn = lambda filter_epochs_decoder_result: filter_epochs_decoder_result.marginal_y_list,\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "                                            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_merged_decoders_result = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "# requires `laps_is_most_likely_direction_LR_dir` from `laps_marginals`\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = owning_pipeline_reference.find_LongShortGlobal_epoch_names()\n",
    "global_session = deepcopy(owning_pipeline_reference.filtered_sessions[global_epoch_name]) # used for validate_lap_dir_estimations(...) \n",
    "\n",
    "# Direction (LR/RL) Marginal:\n",
    "global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any'\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='stacked_epoch_slices_matplotlib_subplots_LAPS',\n",
    "\t\t\t\t\t\t\t\t\t\t\t# active_marginal_fn = lambda filter_epochs_decoder_result: filter_epochs_decoder_result.marginal_y_list,\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\tdebug_test_max_num_slices=max_num_lap_epochs\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "# Track-identity (Long/Short) Marginal:\n",
    "global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any'\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "track_identity_marginal_laps_plot_tuple = plot_decoded_epoch_slices(global_any_laps_epochs_obj, directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='TrackIdentity_Marginal_LAPS',\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_long_short(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\tdebug_test_max_num_slices=max_num_lap_epochs\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\n",
    "\n",
    "## Replays:\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(global_session.replay))\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "ripples_plot_tuple = plot_decoded_epoch_slices(global_replays,  directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result, global_pos_df=global_session.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "\t\t\t\t\t\t\t\t\t\t\tname='stacked_epoch_slices_matplotlib_subplots_Ripples',\n",
    "\t\t\t\t\t\t\t\t\t\t\t# active_marginal_fn = lambda filter_epochs_decoder_result: filter_epochs_decoder_result.marginal_y_list,\n",
    "\t\t\t\t\t\t\t\t\t\t\tactive_marginal_fn = lambda filter_epochs_decoder_result: DirectionalMergedDecodersResult.build_custom_marginal_over_direction(filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291de005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode using long_directional_decoder\n",
    "global_spikes_df, (odd_shuffle_helper, even_shuffle_helper) = RankOrderAnalyses.common_analysis_helper(curr_active_pipeline=curr_active_pipeline, num_shuffles=1000)\n",
    "spikes_df = deepcopy(global_spikes_df) #.spikes.sliced_by_neuron_id(track_templates.shared_aclus_only_neuron_IDs)\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# long_directional_decoding_result: DecodedFilterEpochsResult = long_directional_pf1D_Decoder.decode_specific_epochs(spikes_df, global_replays, decoding_time_bin_size=0.01)\n",
    "all_directional_decoding_result: DecodedFilterEpochsResult = all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df, global_replays, decoding_time_bin_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spikes = len(all_directional_pf1D.spikes_df)\n",
    "print(f'num_spikes: {num_spikes}')\n",
    "num_unique_spikes = len(all_directional_pf1D.spikes_df[['t_rel_seconds']].unique())\n",
    "print(f'num_unique_spikes: {num_unique_spikes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ff61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post 2022-10-22 display_all_pf_2D_pyqtgraph_binned_image_rendering-based method:\n",
    "\n",
    "# Visualization:\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap, visualize_heatmap_pyqtgraph\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow, LayoutScrollability\n",
    "\n",
    "# active_context = curr_active_pipeline.build_display_context_for_session(track_config='All-Directions', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')\n",
    "active_context = curr_active_pipeline.build_display_context_for_session(track_config='Long-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')\n",
    "assert active_context is not None\n",
    "active_pf_2D = long_directional_pf1D_Decoder.pf # computation_result.computed_data['pf2D']\n",
    "# active_pf_2D = all_directional_pf1D_Decoder.pf # computation_result.computed_data['pf2D']\n",
    "# active_pf_2D = all_directions_merged_pf\n",
    "# active_pf_2D = long_directional_manual_merged_pf\n",
    "\n",
    "# figure_format_config = {} # empty dict for config\n",
    "figure_format_config = {} # kwargs # kwargs as default figure_format_config\n",
    "out_all_pf_2D_pyqtgraph_binned_image_fig = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow\n",
    "\n",
    "# Set the window title from the context\n",
    "out_all_pf_2D_pyqtgraph_binned_image_fig.setWindowTitle(f'{active_context.get_description()}')\n",
    "\n",
    "out_all_pf_2D_pyqtgraph_binned_image_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43305707",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_directional_pf1D_Decoder.ratemap.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319bd93",
   "metadata": {},
   "source": [
    "# 🔶 2023-12-23 - All Plots - Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121e192",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_rank_order_histograms\n",
    "\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "\n",
    "save_figure = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e9fec",
   "metadata": {},
   "source": [
    "## 2024-01-08 - Pulled from NonInteractiveProcessing to see plots easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c373e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# _display_directional_merged_pf_decoded_epochs_marginals ________________________________________________________________________________ #\n",
    "try:\n",
    "\t_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs_marginals', curr_active_pipeline.get_session_context(), defer_render=True, save_figure=save_figure)\n",
    "except Exception as e:\n",
    "\tprint(f'batch_extended_programmatic_figures(...): \"_display_directional_merged_pf_decoded_epochs_marginals\" failed with error: {e}\\n skipping.')\n",
    "\traise\n",
    "\n",
    "# # _display_rank_order_z_stats_results ________________________________________________________________________________ #\n",
    "# try:\n",
    "# \t_out = curr_active_pipeline.display('_display_rank_order_z_stats_results', curr_active_pipeline.get_session_context(), defer_render=True, save_figure=save_figure)\n",
    "# except Exception as e:\n",
    "# \tprint(f'batch_extended_programmatic_figures(...): \"_display_rank_order_z_stats_results\" failed with error: {e}\\n skipping.')\n",
    "# \traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow\n",
    "\n",
    "# curr_active_pipeline.reload_default_display_functions()\n",
    "\n",
    "# _out = curr_active_pipeline.display('_display_directional_track_template_pf1Ds', curr_active_pipeline.get_session_context(), defer_render=False, save_figure=save_figure)\n",
    "\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pfs', curr_active_pipeline.get_session_context(), defer_render=False, save_figure=save_figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir_outputs = list(_out.values())[0] # BasicBinnedImageRenderingWindow \n",
    "# list(all_dir_outputs.keys())\n",
    "names_list = [v for v in list(all_dir_outputs.plots.keys()) if v not in ('name', 'context')]\n",
    "names_list\n",
    "\n",
    "out_figs_dict = {}\n",
    "# active_context = curr_active_pipeline.build_display_context_for_session(display_fn_name='directional_merged_pfs')\n",
    "# for a_name, a_plot in all_dir_outputs.plots.items():\n",
    "for a_name in names_list:\n",
    "\t# Adjust the size of the text for the item by passing formatted text\n",
    "\ta_plot: pg.PlotItem = all_dir_outputs.plots[a_name].mainPlotItem # PlotItem \n",
    "\t# if (a_plot is not None) and (not isinstance(a_plot, str)):\n",
    "\t# a_plot.setTitle(f\"<span style = 'font-size : 12px;' >{a_name}</span>\")\n",
    "\t# a_plo\n",
    "\t# active_context , epochs='replays', decoder='long_results_obj'\t\n",
    "\tfinal_context = curr_active_pipeline.build_display_context_for_session(display_fn_name='directional_merged_pfs', track_config='All-Directions', cell=a_name)\n",
    "\tout_figs_dict[a_name] = curr_active_pipeline.output_figure(final_context, a_plot.getViewBox())\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(out_figs_dict.values())[0][0][0]\n",
    "\n",
    "out_figs_paths = [v[0][0] for v in list(out_figs_dict.values())]\n",
    "out_figs_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the individual cell's pf export figures and composite them into a single stack\n",
    "\n",
    "from PIL import Image\n",
    "from pyphocorehelpers.plotting.filesystem_figure_operations import render_image_stack\n",
    "\n",
    "output_img, output_path = render_image_stack(out_figs_paths, offset=55, single_image_alpha_level=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context = curr_active_pipeline.build_display_context_for_session(display_fn_name='directional_merged_pfs', track_config='All-Directions', cell=a_name)\n",
    "curr_active_pipeline.output_figure(final_context, a_plot.getViewBox(), write_vector_format=True, write_png=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dedb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_context\n",
    "\n",
    "curr_active_pipeline.get_output_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plot.titleLabel.setContentsMargins(50, 0, 0, 0)\n",
    "a_plot.titleLabel.\n",
    "\n",
    "for i in range(4):\n",
    "\ta_plot.layout.setRowPreferredHeight(i, 0)\n",
    "\ta_plot.layout.setRowMinimumHeight(i, 0)\n",
    "\ta_plot.layout.setRowSpacing(i, 0)\n",
    "\ta_plot.layout.setRowStretchFactor(i, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plot.layout.setRowMinimumHeight(0, 500) # idk...\n",
    "a_plot.layout.setRowPreferredHeight(1, 0) # nothing\n",
    "a_plot.layout.setRowPreferredHeight(2, 30) # the plot item\n",
    "a_plot.layout.setRowPreferredHeight(3, 0) \n",
    "a_plot.layout.setRowPreferredHeight(4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ca405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no clue why 2 is a good value for this...\n",
    "a_plot.titleLabel.setMaximumHeight(2)\n",
    "a_plot.layout.setRowFixedHeight(0, 2)\n",
    "\n",
    "## Could be the plot item size that should be changed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets rid of the annoying white bounding box in the ViewBox of the plot\n",
    "# a_plot.hideAxis('left') # Hide left border\n",
    "a_plot.hideAxis('right') # Hide right border\n",
    "# a_plot.hideAxis('top') # Hide top border\n",
    "a_plot.hideAxis('bottom') # Hide bottom border\n",
    "\n",
    "a_plot.showAxes('left')\n",
    "a_plot.showAxes('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91970a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vb: pg.ViewBox = a_plot.getViewBox()\n",
    "# vb.background = pg.mkColor('red')\n",
    "# vb.border\n",
    "# vb.setBorder(None)\n",
    "vb.setBackgroundColor(pg.mkColor('red')) # works\n",
    "vb.setBackgroundColor(None)\n",
    "vb.setBorder(pg.mkPen('blue'))\n",
    "# vb.setXRange(\n",
    "vb.setYRange(0, 0.2, 0.0, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "laps_filter_epochs_decoder_result: DecodedFilterEpochsResult = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\n",
    "# laps_filter_epochs_decoder_result.p_x_given_n_list[0].shape # (n_x_bins, 4, n_curr_epoch_time_bins) - (63, 4, 120)\n",
    "\n",
    "# len(laps_filter_epochs_decoder_result.p_x_given_n_list) # n_epochs = 72\n",
    "\n",
    "# laps_filter_epochs_decoder_result.nbins\n",
    "\n",
    "# directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "# laps_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "# laps_directional_marginals # a list just like `laps_filter_epochs_decoder_result.p_x_given_n_list`   # .shape (2, n_curr_epoch_time_bins) - (2, 120)\n",
    "\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "\n",
    "# .shape: (n_x_bins, 4, n_curr_epoch_time_bins) - (63, 4, 120)\n",
    "# laps_directional_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_posterior_laps_marginals = DirectionalMergedDecodersResult.build_non_marginalized_raw_posteriors(laps_filter_epochs_decoder_result)\n",
    "raw_posterior_laps_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf31210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_id = 0\n",
    "a_raw_posterior_marginal_p_x_given_n = raw_posterior_laps_marginals[epoch_id]['p_x_given_n'] # .shape: (4, n_curr_epoch_time_bins) - (63, 4, 120)\n",
    "print(f'a_raw_posterior_marginal_p_x_given_n: {np.shape(a_raw_posterior_marginal_p_x_given_n)}') # .shape: (4, n_curr_epoch_time_bins) - (4, 120)\n",
    "# a_raw_posterior_marginal_p_x_given_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_id = 0\n",
    "a_marginal_dir_p_x_given_n = laps_directional_marginals[epoch_id]['p_x_given_n'] # .shape: (n_x_bins, 4, n_curr_epoch_time_bins) - (63, 4, 120)\n",
    "print(f'a_marginal_dir_p_x_given_n: {np.shape(a_marginal_dir_p_x_given_n)}') # .shape: (2, n_curr_epoch_time_bins) - (2, 120)\n",
    "a_marginal_dir_p_x_given_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a_p_x_given_n = laps_filter_epochs_decoder_result.p_x_given_n_list[epoch_id] # .shape: (n_x_bins, 4, n_curr_epoch_time_bins) - (63, 4, 120)\n",
    "a_p_x_given_n\n",
    "\n",
    "raw_posterior_laps_marginals[epoch_id]\n",
    "\n",
    "a_time_bin = 0\n",
    "a_p_x_given_n[:,:,a_time_bin]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01985e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "# img_data = a_p_x_given_n.astype(float).transpose(2, 0, 1)\n",
    "# img_data = a_p_x_given_n.astype(float)\n",
    "img_data = a_marginal_dir_p_x_given_n.astype(float)\n",
    "# img_data = a_raw_posterior_marginal_p_x_given_n.astype(float)\n",
    "\n",
    "print(f'np.shape(img_data): {np.shape(img_data)}')\n",
    "# out = napari.gui_qt()\n",
    "# viewer = napari.view_image(data.astronaut(), rgb=True)\n",
    "viewer = napari.view_image(img_data) # rgb=True\n",
    "\n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5101e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13800650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pyphocorehelpers.plotting.media_output_helpers import save_array_as_image\n",
    "\n",
    "## Outputs the three decoded posteriors from the marginal decoders\n",
    "\n",
    "epoch_id = 9\n",
    "epoch_id_str = f\"lap[{epoch_id}]\"\n",
    "_img_path = Path(f'output/raw_marginal_{epoch_id_str}.png').resolve()\n",
    "img_data = raw_posterior_laps_marginals[epoch_id]['p_x_given_n'].astype(float)  # .shape: (4, n_curr_epoch_time_bins) - (63, 4, 120)\n",
    "image_raw, out_path = save_array_as_image(img_data, desired_height=100, desired_width=None, skip_img_normalization=True, out_path=_img_path)\n",
    "image_raw\n",
    "\n",
    "_img_path = Path(f'output/marginal_dir_{epoch_id_str}.png').resolve()\n",
    "img_data = laps_directional_marginals[epoch_id]['p_x_given_n'].astype(float)\n",
    "image_marginal_dir, out_path = save_array_as_image(img_data, desired_height=50, desired_width=None, skip_img_normalization=True, out_path=_img_path)\n",
    "image_marginal_dir\n",
    "\n",
    "_img_path = Path(f'output/marginal_track_identity_{epoch_id_str}.png').resolve()\n",
    "img_data = laps_track_identity_marginals[epoch_id]['p_x_given_n'].astype(float)\n",
    "image_marginal_track_identity, out_path = save_array_as_image(img_data, desired_height=50, desired_width=None, skip_img_normalization=True, out_path=_img_path)\n",
    "image_marginal_track_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in active_relative_entropy_results['historical_snapshots'].values()])\n",
    "\n",
    "\n",
    "image_layer_dict = {}\n",
    "layer_properties_dict = {\n",
    "\t'snapshot_occupancy_weighted_tuning_maps': dict(blending='additive', colormap='viridis', name='pf1D_dt'),\n",
    "#  'flat_jensen_shannon_distance_results': dict(blending='additive', colormap='gray'),\n",
    "\t'long_short_rel_entr_curves_frames': dict(blending='additive', colormap='bop blue'),\n",
    "\t'short_long_rel_entr_curves_frames': dict(blending='additive', colormap='red'),\n",
    "\t\n",
    "}\n",
    "\n",
    "for a_name, layer_properties in layer_properties_dict.items():\n",
    "\t# image_layer_dict[a_name] = viewer.add_image(active_relative_entropy_results_xr_dict[a_name].to_numpy().astype(float), name=a_name)\n",
    "\timage_layer_dict[a_name] = viewer.add_image(active_relative_entropy_results[a_name].astype(float), **(dict(name=a_name)|layer_properties))\n",
    "\n",
    "assert viewer.dims.ndim == 3\n",
    "## Set the dimensions appropriately\n",
    "viewer.dims.axis_labels = ('t', 'neuron_id', 'xbin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Valid use of Unicode in a variable name\n",
    "my_variable_δ = 42\n",
    "# This is a comment with Unicode characters: 🐍\n",
    "# Parentheses: ()\n",
    "# Square brackets: []\n",
    "# Curly brackets: {}\n",
    "# Angle brackets: <>\n",
    "# Double angle brackets: « »\n",
    "# Double square brackets: ⟦ ⟧\n",
    "# Mathematical brackets: 〈〉\n",
    "# Tortoise shell brackets: 〔〕\n",
    "# White square brackets: ⟦ ⟧\n",
    "# Brackets with quills: ⸤⸥\n",
    "\n",
    "@define(slots=True)\n",
    "class BracketGroup:\n",
    "\topen: str = field()\n",
    "\tclose: str = field()\n",
    "\t\n",
    "\n",
    "native_python_list_brackets = BracketGroup(\"[\", \"]\")\n",
    "numpy_array_brackets = BracketGroup(\"⟦\", \"⟧\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c527c6",
   "metadata": {
    "tags": [
     "figure",
     "marginal",
     "active"
    ]
   },
   "outputs": [],
   "source": [
    "# _display_directional_merged_pf_decoded_epochs ______________________________________________________________________ #\n",
    "try:\n",
    "\t# Interactive-mode parameters:\n",
    "\t_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=True, scrollable_figure=True, defer_render=False)\n",
    "\t_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\t_curr_interaction_mode_kwargs = _interactive_mode_kwargs # interactive mode\n",
    "\n",
    "\t# Non-interactive:\n",
    "\t# _non_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=False, scrollable_figure=False, defer_render=True)\n",
    "\t# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=False, backend='AGG')\n",
    "\t# _curr_interaction_mode_kwargs = _non_interactive_mode_kwargs # non-interactive mode\n",
    "\n",
    "\t_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', curr_active_pipeline.get_session_context(),\n",
    "\t\t\t\tmax_num_lap_epochs = 100, max_num_ripple_epochs = 10,\n",
    "\t\t\t\trender_merged_pseudo2D_decoder_laps=True, \n",
    "\t\t\t\t# render_directional_marginal_laps=False, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=False, render_track_identity_marginal_ripples=False,\n",
    "\t\t\t\trender_directional_marginal_laps=True, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=False,\n",
    "\t\t\t\t# constrained_layout=True, # layout='none',\n",
    "\t\t\t\tbuild_fn='basic_view', constrained_layout=True, \n",
    "\t\t\t\t# build_fn='insets_view', constrained_layout=None, layout='none', # , constrained_layout=False constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast\n",
    "\t\t\t\t**_curr_interaction_mode_kwargs, # interactive mode\n",
    "\t\t\t\tskip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=save_figure)\n",
    "\t\n",
    "except Exception as e:\n",
    "\tprint(f'batch_extended_programmatic_figures(...): \"_display_directional_merged_pf_decoded_epochs\" failed with error: {e}\\n skipping.')\n",
    "\traise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_any_laps_epochs_obj\n",
    "\n",
    "\n",
    "\n",
    "curr_active_pipeline.filtered_sessions[long_any_name].laps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974d23f",
   "metadata": {},
   "source": [
    "### Plot the z-scores differences and their raw-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PyQt5.QtWidgets import QGraphicsTextItem\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum, LongShortDisplayConfigManager\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, plot_rank_order_epoch_inst_fr_result_tuples\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "\n",
    "# histogram_display_context = active_context.adding_context('display_fn', display_fn_name='plot_rank_order_epoch_inst_fr_result_tuples')\n",
    "ripple_outputs = plot_rank_order_epoch_inst_fr_result_tuples(curr_active_pipeline, ripple_result_tuple, 'Ripple', show=False)\n",
    "# _out_ripple_result_tuple_histograms.context = histogram_display_context.adding_context('subplot', subplot_name='ripple_result_tuple')\n",
    "diff_app, diff_win, diff_p1, diff_out_plot_1D, diff_label_tuple, raw_app, raw_win, raw_p1, raw_out_plot_1D, raw_label_tuple = ripple_outputs\n",
    "diff_header_label, diff_footer_label = diff_label_tuple\n",
    "raw_header_label, raw_footer_label = raw_label_tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732dd0e9",
   "metadata": {},
   "source": [
    "## 2024-01-02 - Almost working for building footer/header strings in pyqtgraph plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ccf0c3",
   "metadata": {
    "tags": [
     "display",
     "_display_rank_order_z_stats_results"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalDisplayFunctions\n",
    "\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out = curr_active_pipeline.display('_display_rank_order_z_stats_results', defer_show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_rank_order_epoch_inst_fr_result_tuples\n",
    "\n",
    "ripple_outputs = plot_rank_order_epoch_inst_fr_result_tuples(curr_active_pipeline, ripple_result_tuple, 'Ripple')\n",
    "ripple_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ecf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of the function for Lap\n",
    "lap_outputs = plot_rank_order_epoch_inst_fr_result_tuples(curr_active_pipeline, laps_result_tuple, 'Lap')\n",
    "lap_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_tuple.plot_histograms()\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_rank_order_histograms\n",
    "\n",
    "# Plot histograms:\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\treturn curr_active_pipeline.output_figure(final_context, fig)\n",
    "\n",
    "post_title_info: str = f'{minimum_inclusion_fr_Hz} Hz'\n",
    "collector_histograms = plot_rank_order_histograms(rank_order_results, post_title_info=post_title_info, active_context=active_context, perform_write_to_file_callback=_perform_write_to_file_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351ca5f",
   "metadata": {},
   "source": [
    "## 2023-12-23 - Good for lap direction debugging:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_order_results.laps_most_likely_result_tuple.directional_likelihoods_df.plot.bar(y=['long_relative_direction_likelihoods', 'short_relative_direction_likelihoods'])\n",
    "\n",
    "# _temp_dir_like_df = rank_order_results.laps_most_likely_result_tuple.directional_likelihoods_df.copy()\n",
    "_temp_dir_like_df = rank_order_results.ripple_most_likely_result_tuple.directional_likelihoods_df.copy()\n",
    "_temp_dir_like_df[['long_relative_direction_likelihoods', 'short_relative_direction_likelihoods']] -= 0.5 # Subtract 0.5 so y is centered on zero, above zero showing LR favor below RL\n",
    "_temp_dir_like_df.plot.bar(y=['long_relative_direction_likelihoods'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45831aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# directional_likelihoods_df = pd.DataFrame({\n",
    "#   \"long_relative_direction_likelihoods\": [0.41, 0.48, 0.27, 0.33, 0.69, 0.50],\n",
    "#   \"short_relative_direction_likelihoods\": [0.58, 0.51, 0.72, 0.66, 0.30, 0.49],\n",
    "#   \"long_best_direction_indices\": [0, 1, 1, 1, 0, 0]\n",
    "# })\n",
    "\n",
    "\n",
    "fig = plt.figure(num='directional_likelihoods_df figure')\n",
    "sns.scatterplot(x=directional_likelihoods_df.index, y=directional_likelihoods_df[\"long_relative_direction_likelihoods\"], hue=directional_likelihoods_df[\"long_best_direction_indices\"], palette=\"hls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359244bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\treturn curr_active_pipeline.output_figure(final_context, fig)\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context, perform_write_to_file_callback=_perform_write_to_file_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 2023-11-16 22:23: - [ ] The other display functions using matplotlib do things like this:\n",
    "# final_context = active_context\n",
    "# graphics_output_dict['context'] = final_context\n",
    "# graphics_output_dict['plot_data'] |= {'df': neuron_replay_stats_df, 'rdf':rdf, 'aclu_to_idx':aclu_to_idx, 'irdf':irdf, 'time_binned_unit_specific_spike_rate': global_computation_results.computed_data['jonathan_firing_rate_analysis'].time_binned_unit_specific_spike_rate,\n",
    "#     'time_variable_name':time_variable_name, 'fignum':curr_fig_num}\n",
    "\n",
    "# def _perform_write_to_file_callback():\n",
    "#     ## 2023-05-31 - Reference Output of matplotlib figure to file, along with building appropriate context.\n",
    "#     return owning_pipeline_reference.output_figure(final_context, graphics_output_dict.figures[0])\n",
    "\n",
    "# if save_figure:\n",
    "#     active_out_figure_paths = _perform_write_to_file_callback()\n",
    "# else:\n",
    "#     active_out_figure_paths = []\n",
    "\n",
    "# graphics_output_dict['saved_figures'] = active_out_figure_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c475c31",
   "metadata": {},
   "source": [
    "## Weighted Correlation can only be applied to decoded posteriors, not spikes themselves.\n",
    "### It works by assessing the degree to which a change in position corresponds to a change in time. For a simple diagonally increasing trajectory across the track at early timebins position will start at the bottom of the track, and as time increases the position also increases. The \"weighted\" part just corresponds to making use of the confidence probabilities of the decoded posterior: instead of relying on only the most-likely position we can include all information returned. Naturally will emphasize sharp decoded positions and de-emphasize diffuse ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import add_weighted_correlation_result, compute_epoch_weighted_correlation\n",
    "\n",
    "# add_weighted_correlation_result(xbin_centers, a_long_decoder_result: DecodedFilterEpochsResult, a_short_decoder_result: DecodedFilterEpochsResult, method=('pearson', 'spearman'), debug_print = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get decoded posteriors for each replay epoch:\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from PendingNotebookCode import add_weighted_correlation_result, _add_maze_id_to_epochs\n",
    "\n",
    "## 2023-10-19 - Weighted Correlation:\n",
    "a_long_decoder_result: DecodedFilterEpochsResult = long_results_obj.all_included_filter_epochs_decoder_result\n",
    "a_short_decoder_result: DecodedFilterEpochsResult = short_results_obj.all_included_filter_epochs_decoder_result\n",
    "# Get the xbin_centers which are the same for long/short:\n",
    "xbin_centers = long_results_obj.original_1D_decoder.xbin_centers.copy()\n",
    "# Compute the weighte correlation:\n",
    "epoch_long_weighted_corr_results, epoch_short_weighted_corr_results = add_weighted_correlation_result(xbin_centers, a_long_decoder_result, a_short_decoder_result, debug_print=False)\n",
    "epoch_long_weighted_corr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-19 - Weighted Correlation:\n",
    "\n",
    "directional_merged_decoders_result: DirectionalMergedDecodersResult  = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "laps_filter_epochs_decoder_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31208041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# laps_filter_epochs_decoder_result\n",
    "\n",
    "\n",
    "## 2023-10-19 - Weighted Correlation:\n",
    "a_long_decoder_result: DecodedFilterEpochsResult = long_results_obj.all_included_filter_epochs_decoder_result\n",
    "a_short_decoder_result: DecodedFilterEpochsResult = short_results_obj.all_included_filter_epochs_decoder_result\n",
    "\n",
    "# Get the xbin_centers which are the same for long/short:\n",
    "xbin_centers = directional_merged_decoders_result.all_directional_pf1D_Decoder.xbin_centers.copy()\n",
    "# Compute the weighte correlation:\n",
    "epoch_long_weighted_corr_results, epoch_short_weighted_corr_results = add_weighted_correlation_result(xbin_centers, a_long_decoder_result, a_short_decoder_result, debug_print=False)\n",
    "epoch_long_weighted_corr_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90c0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_print = True\n",
    "method = ('pearson', 'spearman')\n",
    "epoch_long_weighted_corr_results = []\n",
    "\n",
    "# Get the xbin_centers which are the same for long/short:\n",
    "xbin_centers = directional_merged_decoders_result.all_directional_pf1D_Decoder.xbin_centers.copy()\n",
    "a_decoder_result: DecodedFilterEpochsResult = deepcopy(laps_filter_epochs_decoder_result)\n",
    "\n",
    "\n",
    "for decoded_epoch_idx in np.arange(a_decoder_result.num_filter_epochs):\n",
    "\t# decoded_epoch_idx:int = 0\n",
    "\tcurr_epoch_time_bin_container = a_decoder_result.time_bin_containers[decoded_epoch_idx]\n",
    "\tcurr_time_bins = curr_epoch_time_bin_container.centers\n",
    "\tcurr_n_time_bins = len(curr_time_bins)\n",
    "\tif debug_print:\n",
    "\t\tprint(f'curr_n_time_bins: {curr_n_time_bins}')\n",
    "\n",
    "\t## Long Decoding:\n",
    "\tcurr_long_epoch_p_x_given_n = a_decoder_result.p_x_given_n_list[decoded_epoch_idx] # .shape: (239, 5) - (n_x_bins, n_epoch_time_bins) - np.shape(curr_long_epoch_p_x_given_n): (63, 4, 120)\n",
    "\tprint(f'np.shape(curr_long_epoch_p_x_given_n): {np.shape(curr_long_epoch_p_x_given_n)}')\n",
    "\tweighted_corr_result = compute_epoch_weighted_correlation(xbin_centers, curr_time_bins, curr_long_epoch_p_x_given_n, method=method)\n",
    "\tepoch_long_weighted_corr_results.append(weighted_corr_result)\n",
    "\n",
    "\t# ## Short Decoding:\n",
    "\t# curr_short_epoch_p_x_given_n = a_short_decoder_result.p_x_given_n_list[decoded_epoch_idx] # .shape: (239, 5) - (n_x_bins, n_epoch_time_bins)\n",
    "\t# weighted_corr_result = compute_epoch_weighted_correlation(xbin_centers, curr_time_bins, curr_short_epoch_p_x_given_n, method=method)\n",
    "\t# epoch_short_weighted_corr_results.append(weighted_corr_result)\n",
    "\n",
    "# ## Build separate result dataframe:\n",
    "# epoch_weighted_corr_results_df = pd.DataFrame({'weighted_corr_LONG': np.array(epoch_long_weighted_corr_results), 'weighted_corr_SHORT': np.array(epoch_short_weighted_corr_results)})\n",
    "# epoch_weighted_corr_results_df\n",
    "\n",
    "epoch_long_weighted_corr_results = np.array(epoch_long_weighted_corr_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a_long_decoder_result: DecodedFilterEpochsResult = long_results_obj.all_included_filter_epochs_decoder_result\n",
    "a_short_decoder_result: DecodedFilterEpochsResult = short_results_obj.all_included_filter_epochs_decoder_result\n",
    "# Get the xbin_centers which are the same for long/short:\n",
    "xbin_centers = long_results_obj.original_1D_decoder.xbin_centers.copy()\n",
    "# Compute the weighte correlation:\n",
    "epoch_long_weighted_corr_results, epoch_short_weighted_corr_results = add_weighted_correlation_result(xbin_centers, a_long_decoder_result, a_short_decoder_result, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342effc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_long_weighted_corr_results.shape # (151, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Add new weighted correlation results as new columns in existing filter_epochs df:\n",
    "active_filter_epochs = long_results_obj.active_filter_epochs\n",
    "# Add the maze_id to the active_filter_epochs so we can see how properties change as a function of which track the replay event occured on:\n",
    "active_filter_epochs = _add_maze_id_to_epochs(active_filter_epochs, short_session.t_start)\n",
    "active_filter_epochs._df['weighted_corr_LONG'] = epoch_long_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_SHORT'] = epoch_short_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_spearman_LONG'] = epoch_long_weighted_corr_results[:,1]\n",
    "active_filter_epochs._df['weighted_corr_spearman_SHORT'] = epoch_short_weighted_corr_results[:,1]\n",
    "\n",
    "\n",
    "active_filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ed106",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_filter_epochs.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the `weighted_corr_LONG` over time\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=1, nrows=active_num_rows, sharex=True, sharey=sharey, figsize=figsize)\n",
    "\n",
    "## Weighted Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_LONG', title='weighted_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a498345",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weighted Spearman Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_LONG', title='weighted_spearman_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Spearman Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='score_LONG', title='Radon Transform Score during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='score_SHORT', xlabel='Replay Epoch Time', ylabel='Replay Radon Transform Score', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd68cf0",
   "metadata": {},
   "source": [
    "# 2023-01-16 - Continuously applied Pseduo2D decoder across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691320ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to I get time-parcilated intervals, similar to epochs, from the raw timezz? I know there are a lot of decoders in the past that did this. I think that more involved decoder even does it automatically by taking a time-bin size.\n",
    "\n",
    "## Each lap was labeled LR_Long, RL_Long, LR_Short, or RL_Short. \n",
    "\n",
    "## From this four 1D non-directional decoders were built independently from the data obtained from each of the four running directions. This resulted in four independent sets of firing rmaps, a set consisting of all participating cells, each of which mapped a position bin on the track to an average firing rate. Minimum peak activity thresholds were applied independently to each, meaning some cells were only participating in one of the four configurations. \n",
    "\n",
    "## To determine the correct configuration for each time bin these four 1D decoders were vertically concatenated to form a \"pseudo-2D\" ratemap for each cell, where the artficial y-direction mapped to four possible  \n",
    "## The deta was vertically concatenated to form \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch\n",
    "from pyphocorehelpers.indexing_helpers import BinningContainer, BinningInfo\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, BayesianPlacemapPositionDecoder\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an Epoch object containing a single epoch, corresponding to the global epoch for the entire session:\n",
    "single_global_epoch_df: pd.DataFrame = pd.DataFrame({'start': [t_start], 'stop': [t_end], 'label': [0]})\n",
    "# single_global_epoch_df['label'] = single_global_epoch_df.index.to_numpy()\n",
    "single_global_epoch: Epoch = Epoch(single_global_epoch_df)\n",
    "single_global_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc056fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Epoch object across whole sessions:\n",
    "time_bin_size = long_LR_pf1D_Decoder.time_bin_size\n",
    "# time_binning_container: BinningContainer = deepcopy(long_LR_pf1D_Decoder.time_binning_container)\n",
    "# time_binning_container\n",
    "# time_binning_container.edges # array([31.8648, 31.8978, 31.9308, ..., 1203.56, 1203.6, 1203.63])\n",
    "# time_binning_container.centers # array([31.8813, 31.9143, 31.9473, ..., 1203.55, 1203.58, 1203.61])\n",
    "print(f'time_bin_size: {time_bin_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc498488",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_spikes_df, _, _ = RankOrderAnalyses.common_analysis_helper(curr_active_pipeline=curr_active_pipeline, num_shuffles=0) # does not do shuffling\n",
    "\n",
    "# # Get proper global_spikes_df:\n",
    "# rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "# minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "# included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "# directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "# track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "# any_list_neuron_IDs = track_templates.any_decoder_neuron_IDs # neuron_IDs as they appear in any list\n",
    "# global_spikes_df = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].spikes_df).spikes.sliced_by_neuron_id(any_list_neuron_IDs) # Cut spikes_df down to only the neuron_IDs that appear at least in one decoder:\n",
    "\n",
    "spikes_df = deepcopy(global_spikes_df) #.spikes.sliced_by_neuron_id(track_templates.shared_aclus_only_neuron_IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Already come in with long_LR_pf1D_Decoder, long_LR_pf1D_Decoder\n",
    "long_LR_pf1D_Decoder # type(long_LR_pf1D_Decoder) # pyphoplacecellanalysis.Analysis.Decoder.reconstruction.BayesianPlacemapPositionDecoder\n",
    "long_RL_pf1D_Decoder\n",
    "short_LR_pf1D_Decoder\n",
    "short_RL_pf1D_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo2D_decoder: BasePositionDecoder = all_directional_pf1D_Decoder_value\n",
    "pseudo2D_decoder_continuously_decoded_result = pseudo2D_decoder.decode_specific_epochs(spikes_df=spikes_df, filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "# 16.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a014c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode continuously for the four 1D directional decoders:\n",
    "# all_directional_decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "# all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = dict(zip(all_directional_decoder_names, [deepcopy(long_LR_pf1D_Decoder), deepcopy(long_RL_pf1D_Decoder), deepcopy(short_LR_pf1D_Decoder), deepcopy(short_RL_pf1D_Decoder)]))\n",
    "\n",
    "all_directional_continuously_decoded_dict: Dict[str, DecodedFilterEpochsResult] = {k:v.decode_specific_epochs(spikes_df=spikes_df, filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False) for k,v in all_directional_pf1D_Decoder_dict.items()}\n",
    "# _out_continuously_decoded_dict \n",
    "# 32.7s\n",
    "all_directional_continuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2993591",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_decode_result: DirectionalDecodersDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "# _out_continuously_decoded_dict['long_LR']\n",
    "a_decoder_name = 'long_LR'\n",
    "active_decoder: BasePositionDecoder = deepcopy(all_directional_pf1D_Decoder_dict[a_decoder_name])\n",
    "active_result: DecodedFilterEpochsResult = deepcopy(all_directional_continuously_decoded_dict[a_decoder_name]) # already decoded\n",
    "assert active_result.num_filter_epochs == 1\n",
    "active_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_marginals = active_result.marginal_x_list[0]\n",
    "active_posterior = active_marginals.p_x_given_n\n",
    "# active_marginals\n",
    "active_posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67140f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_marginals = active_decoder.marginal.x\n",
    "active_posterior = active_marginals.p_x_given_n\n",
    "active_posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d211130",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_bins = active_decoder.xbin\n",
    "\n",
    "# active_most_likely_positions = active_marginals.most_likely_positions_1D # Raw decoded positions\n",
    "active_most_likely_positions = None\n",
    "active_posterior = active_marginals.p_x_given_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e71c2",
   "metadata": {},
   "source": [
    "# 2024-01-17 - Explore the effect of time_bin_size of decoding performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n",
    "from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "from PendingNotebookCode import _perform_variable_time_bin_lap_groud_truth_performance_testing \n",
    "\n",
    "# active_decoder = long_LR_pf1D_Decoder\n",
    "# long_only_laps_filter_epochs_decoder_result: DecodedFilterEpochsResult = active_decoder.decode_specific_epochs(spikes_df=spikes_df, filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=True)\n",
    "\n",
    "# _out = long_LR_pf1D_Decoder.decode_specific_epochs(spikes_df, filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=True) # 8.3 seconds\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "\n",
    "## Copy the default result:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "alt_directional_merged_decoders_result = deepcopy(directional_merged_decoders_result)\n",
    "alt_directional_merged_decoders_result\n",
    "\n",
    "# _perform_variable_time_bin_lap_groud_truth_performance_testing(curr_active_pipeline, desired_laps_decoding_time_bin_size=0.5, desired_ripple_decoding_time_bin_size=0.1)\n",
    "alt_directional_merged_decoders_result, result_laps_epochs_df = _perform_variable_time_bin_lap_groud_truth_performance_testing(curr_active_pipeline,\n",
    "                                                                    desired_laps_decoding_time_bin_size=0.1, desired_ripple_decoding_time_bin_size=0.1)\n",
    "\n",
    "# laps_decoding_time_bin_size: 1.668\n",
    "# percent_laps_track_identity_estimated_correctly: 0.9875\n",
    "# percent_laps_direction_estimated_correctly: 1.0\n",
    "# percent_laps_estimated_correctly: 0.9875\n",
    "\n",
    "\n",
    "# laps_decoding_time_bin_size: 0.25\n",
    "# percent_laps_track_identity_estimated_correctly: 0.9875\n",
    "# percent_laps_direction_estimated_correctly: 1.0\n",
    "# percent_laps_estimated_correctly: 0.9875\n",
    "\n",
    "laps_decoding_time_bin_size: float = alt_directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "ripple_decoding_time_bin_size: float = alt_directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size, ripple_decoding_time_bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unpack the result:\n",
    "all_directional_laps_filter_epochs_decoder_result_value = alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = alt_directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_epochs_df = alt_directional_merged_decoders_result.laps_epochs_df\n",
    "ripple_epochs_df = alt_directional_merged_decoders_result.ripple_epochs_df\n",
    "\n",
    "# laps_epochs_df\n",
    "all_directional_laps_filter_epochs_decoder_result_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d02ad",
   "metadata": {},
   "source": [
    "## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848fc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_context = owning_pipeline_reference.sess.get_context()\n",
    "# Build the active context directly:\n",
    "active_display_context = curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs', laps_t_bin=laps_decoding_time_bin_size)\n",
    "active_display_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab95d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dabb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive-mode parameters:\n",
    "_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=True, scrollable_figure=True, defer_render=False)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_curr_interaction_mode_kwargs = _interactive_mode_kwargs # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interactive:\n",
    "_non_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=False, scrollable_figure=False, defer_render=True)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=False, backend='AGG')\n",
    "_curr_interaction_mode_kwargs = _non_interactive_mode_kwargs # non-interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dff9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the decoded epoch bins of the custom result:\n",
    "\n",
    "_out_decoded_epochs = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', curr_active_pipeline.get_session_context(), #active_display_context,\n",
    "\tmax_num_lap_epochs = 240, max_num_ripple_epochs = 500,\n",
    "\t# render_directional_marginal_laps=True, render_directional_marginal_ripples=True, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=True,\n",
    "\trender_directional_marginal_laps=True, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=False,\n",
    "\t# constrained_layout=True, # layout='none',\n",
    "\tbuild_fn='basic_view', constrained_layout=True, \n",
    "\t# build_fn='insets_view', constrained_layout=True, #constrained_layout=None, layout='none', # , constrained_layout=False constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast\n",
    "\t**_curr_interaction_mode_kwargs, # interactive mode\n",
    "\tskip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=True, \n",
    "\tdirectional_merged_decoders_result=alt_directional_merged_decoders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t)\n",
    "collector_decoded_epochs = _out_decoded_epochs['collector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5699b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _try_validate_laps(global_session, a_directional_merged_decoders_result: DirectionalMergedDecodersResult):\n",
    "\t# Validate Laps:\n",
    "\ttry:\n",
    "\t\tall_directional_laps_filter_epochs_decoder_result_value: DecodedFilterEpochsResult = a_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\t\tlaps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = a_directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "\t\tlaps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = a_directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "\t\tpercent_laps_estimated_correctly = DirectionalMergedDecodersResult.validate_lap_dir_estimations(global_session, active_global_laps_df=all_directional_laps_filter_epochs_decoder_result_value.filter_epochs.to_dataframe(), laps_is_most_likely_direction_LR_dir=laps_is_most_likely_direction_LR_dir)\n",
    "\t\t# Check 'maze_id' decoding accuracy\n",
    "\t\tground_truth_lap_is_Long_track = laps_df['maze_id'].to_numpy()\n",
    "\t\tn_laps = np.shape(laps_df)[0]\n",
    "\t\tassert len(laps_is_most_likely_track_identity_Long) == n_laps\n",
    "\t\tpercent_laps_track_identity_estimated_correctly = (np.sum(ground_truth_lap_is_Long_track == laps_is_most_likely_track_identity_Long) / n_laps)\n",
    "\t\tprint(f'percent_laps_track_identity_estimated_correctly: {percent_laps_track_identity_estimated_correctly}')\n",
    "\t\tprint(f'percent_laps_estimated_correctly: {percent_laps_estimated_correctly}')\n",
    "\t\treturn percent_laps_estimated_correctly\n",
    "\texcept (AssertionError, ValueError) as err:\n",
    "\t\tprint(F'fails due to some types thing?')\n",
    "\t\tprint(f'\\terr: {err}')\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "_try_validate_laps(global_session, a_directional_merged_decoders_result=alt_directional_merged_decoders_result)\n",
    "\t\n",
    "# _try_validate_laps(curr_active_pipeline.sess, a_directional_merged_decoders_result=alt_directional_merged_decoders_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac01221",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(alt_directional_merged_decoders_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cbcf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_directional_merged_decoders_result.laps_directional_marginals_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f880f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_directional_laps_filter_epochs_decoder_result_value.filter_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3800d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_directional_merged_decoders_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_yellow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
