{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from benedict import benedict\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from benedict import benedict # https://github.com/fabiocaccamo/python-benedict#usage\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "# from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "# set_pho_preferences_concise()\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.general_helpers import CodeConversion\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import print_keys_if_possible, print_value_overview_only, document_active_variables, objsize, print_object_memory_usage, debug_dump_object_member_shapes, TypePrintMode\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_time_str, get_now_time_precise_str\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveWrapper import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "\n",
    "session_batch_status = {}\n",
    "session_batch_errors = {}\n",
    "enable_saving_to_disk = False\n",
    "\n",
    "global_data_root_parent_path = find_first_extant_path([Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data')])\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1538e2a-4e39-4d11-90b5-a9fef9258058",
   "metadata": {
    "tags": [
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f917bad9-8fe7-4882-b83b-71cf878fffd2",
   "metadata": {
    "tags": [
     "load",
     "REQUIRED"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_session_names_list: ['2006-6-07_16-40-19', '2006-6-08_15-46-47', '2006-6-08_21-16-25', '2006-6-09_22-24-40', '2006-6-12_16-53-46', '2006-6-13_15-22-3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{WindowsPath('W:/Data/KDIBA/gor01/two/2006-6-07_16-40-19'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/two/2006-6-08_15-46-47'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/two/2006-6-08_21-16-25'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/two/2006-6-09_22-24-40'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/two/2006-6-12_16-53-46'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>,\n",
       " WindowsPath('W:/Data/KDIBA/gor01/two/2006-6-13_15-22-3'): <SessionBatchProgress.NOT_STARTED: 'NOT_STARTED'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "\n",
    "## Data must be pre-processed using the MATLAB script located here: \n",
    "#     neuropy/data_session_pre_processing_scripts/KDIBA/IIDataMat_Export_ToPython_2022_08_01.m\n",
    "# From pre-computed .mat files:\n",
    "\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=self.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# Animal `gor01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='one') # IdentifyingContext<('kdiba', 'gor01', 'one')>\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # 'gor01', 'one'\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['PhoHelpers', 'Spike3D-Minimal-Test', 'Unused'])\n",
    "\n",
    "local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='gor01', exper_name='two')\n",
    "local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ## Animal `vvp01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='vvp01', exper_name='two')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name)\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=[])\n",
    "\n",
    "# ### Animal `pin01`:\n",
    "# local_session_parent_context = local_session_root_parent_context.adding_context(collision_prefix='animal', animal='pin01', exper_name='one')\n",
    "# local_session_parent_path = local_session_root_parent_path.joinpath(local_session_parent_context.animal, local_session_parent_context.exper_name) # no exper_name ('one' or 'two') folders for this animal.\n",
    "# local_session_paths_list, local_session_names_list =  find_local_session_paths(local_session_parent_path, blacklist=['redundant','showclus','sleep','tmaze'])\n",
    "\n",
    "## Build session contexts list:\n",
    "local_session_contexts_list = [local_session_parent_context.adding_context(collision_prefix='sess', session_name=a_name) for a_name in local_session_names_list] # [IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-07_11-26-53')>, ..., IdentifyingContext<('kdiba', 'gor01', 'one', '2006-6-13_14-42-6')>]\n",
    "\n",
    "## Initialize `session_batch_status` with the NOT_STARTED status if it doesn't already have a different status\n",
    "for curr_session_basedir in local_session_paths_list:\n",
    "    curr_session_status = session_batch_status.get(curr_session_basedir, None)\n",
    "    if curr_session_status is None:\n",
    "        session_batch_status[curr_session_basedir] = SessionBatchProgress.NOT_STARTED # set to not started if not present\n",
    "        # session_batch_status[curr_session_basedir] = SessionBatchProgress.COMPLETED # set to not started if not present\n",
    "\n",
    "session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13e68b4-03e0-4388-a35c-87a352a6e6b3",
   "metadata": {
    "tags": [
     "load",
     "single_session",
     "REQUIRED"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\loadedSessPickle.pkl.\n",
      "property already present in pickled version. No need to save.\n",
      "using provided computation_functions_name_whitelist: ['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_time_dependent_pf_sequential_surprise_computation_perform_two_step_position_decoding_computation']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the blacklist/whitelist or computation function definitions change. Rework so that this is smarter.\n",
      "updating computation_results...\n",
      "done.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "# %%viztracer\n",
    "basedir = local_session_paths_list[3] # NOT 3\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "## Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# epoch_name_whitelist = ['maze']\n",
    "epoch_name_whitelist = None\n",
    "active_computation_functions_name_whitelist=['_perform_baseline_placefield_computation', '_perform_time_dependent_placefield_computation', '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        # '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_whitelist=epoch_name_whitelist,\n",
    "                                          computation_functions_name_whitelist=active_computation_functions_name_whitelist,\n",
    "                                          saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                          skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e403d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sane_midpoint_x: 121.69345040890211, hardcoded_track_midpoint_x: 150.0, track_min_max_x: (14.134952144165558, 257.5218529619698)\n",
      "desc_crossings_x: (26,), asc_crossings_x: (27,)\n",
      "WARNING: must drop last asc_crossing_midpoints.\n"
     ]
    }
   ],
   "source": [
    "from neuropy.analyses.placefields import PfND\n",
    "from PendingNotebookCode import constrain_to_laps\n",
    "from PendingNotebookCode import compute_short_long_constrained_decoders\n",
    "\n",
    "\n",
    "curr_active_pipeline = constrain_to_laps(curr_active_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda48261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (23644,) should be less than time_window_edges: (26338,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (23644,) should be less than time_window_edges: (26338,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "self will be re-binned to match target_pf...\n",
      "done.\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (23644,) should be less than time_window_edges: (26338,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (23644,) should be less than time_window_edges: (26338,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (37041,) should be less than time_window_edges: (49832,)!\n"
     ]
    }
   ],
   "source": [
    "(long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "# 3m 40.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a896c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalized_loaded_sess_pickle_path: W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\loadedSessPickle.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\20230414212853-loadedSessPickle.pkl... done.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PfND_TimeDependent' object has no attribute '_included_thresh_neurons_indx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m curr_active_pipeline\u001b[39m.\u001b[39;49msave_pipeline(saving_mode\u001b[39m=\u001b[39;49mPipelineSavingScheme\u001b[39m.\u001b[39;49mTEMP_THEN_OVERWRITE)\n\u001b[0;32m      3\u001b[0m \u001b[39m# TypeError: cannot pickle 'MplMultiTab' object\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\NeuropyPipeline.py:605\u001b[0m, in \u001b[0;36mNeuropyPipeline.save_pipeline\u001b[1;34m(self, saving_mode, active_pickle_filename)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWARNING: saving_mode is OVERWRITE_IN_PLACE so \u001b[39m\u001b[39m{\u001b[39;00mfinalized_loaded_sess_pickle_path\u001b[39m}\u001b[39;00m\u001b[39m will be overwritten even though exists.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    604\u001b[0m \u001b[39m# Save reloaded pipeline out to pickle for future loading\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m saveData(finalized_loaded_sess_pickle_path, db\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m) \u001b[39m# Save the pipeline out to pickle.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[39m# If we saved to a temporary name, now see if we should overwrite or backup and then replace:\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[39mif\u001b[39;00m saving_mode\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m PipelineSavingScheme\u001b[39m.\u001b[39mTEMP_THEN_OVERWRITE\u001b[39m.\u001b[39mname:\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Loading.py:30\u001b[0m, in \u001b[0;36msaveData\u001b[1;34m(pkl_path, db, should_append)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m ProgressMessagePrinter(pkl_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSaving (file mode \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile_mode\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msaved session pickle file\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     28\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(pkl_path, file_mode) \u001b[39mas\u001b[39;00m dbfile: \n\u001b[0;32m     29\u001b[0m         \u001b[39m# source, destination\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m         pickle\u001b[39m.\u001b[39;49mdump(db, dbfile)\n\u001b[0;32m     31\u001b[0m         dbfile\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\pho\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-WtdfU5rp-py3.9\\lib\\site-packages\\dill\\_dill.py:336\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[0;32m    334\u001b[0m _kwds \u001b[39m=\u001b[39m kwds\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m _kwds\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(byref\u001b[39m=\u001b[39mbyref, fmode\u001b[39m=\u001b[39mfmode, recurse\u001b[39m=\u001b[39mrecurse))\n\u001b[1;32m--> 336\u001b[0m Pickler(file, protocol, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwds)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m    337\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pho\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-WtdfU5rp-py3.9\\lib\\site-packages\\dill\\_dill.py:620\u001b[0m, in \u001b[0;36mPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m    619\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m     StockPickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[0;32m    621\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[0;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pho\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-WtdfU5rp-py3.9\\lib\\site-packages\\dill\\_dill.py:1251\u001b[0m, in \u001b[0;36msave_module_dict\u001b[1;34m(pickler, obj)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[0;32m   1249\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[0;32m   1252\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1253\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[0;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m    996\u001b[0m         save(k)\n\u001b[1;32m--> 997\u001b[0m         save(v)\n\u001b[0;32m    998\u001b[0m     write(SETITEMS)\n\u001b[0;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pho\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-WtdfU5rp-py3.9\\lib\\site-packages\\dill\\_dill.py:1251\u001b[0m, in \u001b[0;36msave_module_dict\u001b[1;34m(pickler, obj)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[0;32m   1249\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[0;32m   1252\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1253\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[0;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m    996\u001b[0m         save(k)\n\u001b[1;32m--> 997\u001b[0m         save(v)\n\u001b[0;32m    998\u001b[0m     write(SETITEMS)\n\u001b[0;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "    \u001b[1;31m[... skipping similar frames: _Pickler._batch_setitems at line 997 (3 times), _Pickler.save at line 560 (3 times), _Pickler.save_dict at line 971 (3 times), save_module_dict at line 1251 (3 times), _Pickler.save at line 603 (1 times), _Pickler.save_reduce at line 717 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: _Pickler._batch_setitems at line 997 (1 times), _Pickler.save at line 560 (1 times), _Pickler.save_dict at line 971 (1 times), save_module_dict at line 1251 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pho\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-WtdfU5rp-py3.9\\lib\\site-packages\\dill\\_dill.py:1251\u001b[0m, in \u001b[0;36msave_module_dict\u001b[1;34m(pickler, obj)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[0;32m   1249\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[0;32m   1252\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1253\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[0;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m    996\u001b[0m         save(k)\n\u001b[1;32m--> 997\u001b[0m         save(v)\n\u001b[0;32m    998\u001b[0m     write(SETITEMS)\n\u001b[0;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pickle.py:578\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    576\u001b[0m reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 578\u001b[0m     rv \u001b[39m=\u001b[39m reduce(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproto)\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\analyses\\time_dependent_placefields.py:489\u001b[0m, in \u001b[0;36mPfND_TimeDependent.__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getstate__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_dict()\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\analyses\\time_dependent_placefields.py:484\u001b[0m, in \u001b[0;36mPfND_TimeDependent.to_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39m# self._setup_time_varying() # reset completely before saving. Throw out everything\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[39m# Excluded from serialization: ['_included_thresh_neurons_indx', '_peak_frate_filter_function']\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[39m# filter_fn = filters.exclude(fields(PfND)._included_thresh_neurons_indx, int)\u001b[39;00m\n\u001b[0;32m    483\u001b[0m filter_fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m attr, value: attr\u001b[39m.\u001b[39mname \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m_included_thresh_neurons_indx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_peak_frate_filter_function\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_ratemap\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_ratemap_spiketrains\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_ratemap_spiketrains_pos\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 484\u001b[0m \u001b[39mreturn\u001b[39;00m asdict(\u001b[39mself\u001b[39;49m, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49mfilter_fn)\n",
      "File \u001b[1;32mc:\\Users\\pho\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-WtdfU5rp-py3.9\\lib\\site-packages\\attr\\_next_gen.py:208\u001b[0m, in \u001b[0;36masdict\u001b[1;34m(inst, recurse, filter, value_serializer)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39masdict\u001b[39m(inst, \u001b[39m*\u001b[39m, recurse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mfilter\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, value_serializer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m    Same as `attr.asdict`, except that collections types are always retained\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m    and dict is always used as *dict_factory*.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[0;32m    206\u001b[0m \u001b[39m    .. versionadded:: 21.3.0\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m _asdict(\n\u001b[0;32m    209\u001b[0m         inst\u001b[39m=\u001b[39;49minst,\n\u001b[0;32m    210\u001b[0m         recurse\u001b[39m=\u001b[39;49mrecurse,\n\u001b[0;32m    211\u001b[0m         \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m,\n\u001b[0;32m    212\u001b[0m         value_serializer\u001b[39m=\u001b[39;49mvalue_serializer,\n\u001b[0;32m    213\u001b[0m         retain_collection_types\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    214\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pho\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\spike3d-WtdfU5rp-py3.9\\lib\\site-packages\\attr\\_funcs.py:55\u001b[0m, in \u001b[0;36masdict\u001b[1;34m(inst, recurse, filter, dict_factory, retain_collection_types, value_serializer)\u001b[0m\n\u001b[0;32m     53\u001b[0m rv \u001b[39m=\u001b[39m dict_factory()\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m attrs:\n\u001b[1;32m---> 55\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(inst, a\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mfilter\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mfilter\u001b[39m(a, v):\n\u001b[0;32m     57\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PfND_TimeDependent' object has no attribute '_included_thresh_neurons_indx'"
     ]
    }
   ],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE) # AttributeError: 'PfND_TimeDependent' object has no attribute '_included_thresh_neurons_indx'\n",
    "\n",
    "# TypeError: cannot pickle 'MplMultiTab' object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a92c8",
   "metadata": {},
   "source": [
    "# 2023-03-16 - Explore passing in long/short decoders specifically to `perform_full_session_leave_one_out_decoding_analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f22b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neurons = 56, neurons_colors_array.shape =(4, 56)\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder, BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_full_session_leave_one_out_decoding_analysis\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import SurpriseAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import build_neurons_color_map # for plot_short_v_long_pf1D_comparison\n",
    "\n",
    "# Get existing long/short decoders from the cell under \"# 2023-02-24 Decoders\"\n",
    "long_decoder, short_decoder = deepcopy(long_one_step_decoder_1D), deepcopy(short_one_step_decoder_1D)\n",
    "assert np.all(long_decoder.xbin == short_decoder.xbin)\n",
    "\n",
    "## backup existing replay objects\n",
    "# long_session.replay_backup, short_session.replay_backup, global_session.replay_backup = [deepcopy(a_session.replay) for a_session in [long_session, short_session, global_session]]\n",
    "# null-out the replay objects\n",
    "# long_session.replay, short_session.replay, global_session.replay = [None, None, None]\n",
    "\n",
    "# Compute/estimate replays if missing from session:\n",
    "if not global_session.has_replays:\n",
    "    print(f'Replays missing from sessions. Computing replays...')\n",
    "    long_session.replay, short_session.replay, global_session.replay = [a_session.estimate_replay_epochs(min_epoch_included_duration=0.06, max_epoch_included_duration=None, maximum_speed_thresh=None, min_inclusion_fr_active_thresh=0.01, min_num_unique_aclu_inclusions=3).to_dataframe() for a_session in [long_session, short_session, global_session]]\n",
    "\n",
    "### Need to prune to only the cells active in both epochs ahead of time:\n",
    "# Prune to the shared aclus in both epochs (short/long):\n",
    "long_shared_aclus_only_decoder, short_shared_aclus_only_decoder = [BasePositionDecoder.init_from_stateful_decoder(a_decoder) for a_decoder in (long_decoder, short_decoder)]\n",
    "shared_aclus, (long_shared_aclus_only_decoder, short_shared_aclus_only_decoder), long_short_pf_neurons_diff = BasePositionDecoder.prune_to_shared_aclus_only(long_shared_aclus_only_decoder, short_shared_aclus_only_decoder)\n",
    "n_neurons = len(shared_aclus)\n",
    "# for plotting purposes, build colors only for the common (present in both, the intersection) neurons:\n",
    "neurons_colors_array = build_neurons_color_map(n_neurons, sortby=None, cmap=None)\n",
    "print(f'{n_neurons = }, {neurons_colors_array.shape =}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b360bc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 56, n_all_epoch_timebins = 2917)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 56, n_all_epoch_timebins = 2917)\n"
     ]
    }
   ],
   "source": [
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "long_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=long_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_long', perform_cache_load=True) # , perform_cache_load=False\n",
    "short_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=short_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_short', perform_cache_load=True) # , perform_cache_load=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69eed8a",
   "metadata": {},
   "source": [
    "## 2023-04-13 - Shuffled Surprise\n",
    "\"\"\" \n",
    "Relevant Functions:\n",
    "`perform_full_session_leave_one_out_decoding_analysis`:\n",
    "\t`perform_leave_one_aclu_out_decoding_analysis`:\tfrom pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_leave_one_aclu_out_decoding_analysis\n",
    "\t`_analyze_leave_one_out_decoding_results`: from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions import _analyze_leave_one_out_decoding_results\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aac42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import _new_compute_surprise, TimebinnedNeuronActivity\n",
    "\n",
    "# Distance metrics used by `_new_compute_surprise`\n",
    "from scipy.spatial import distance # for Jensen-Shannon distance in `_subfn_compute_leave_one_out_analysis`\n",
    "import random # for random.choice(mylist)\n",
    "# from PendingNotebookCode import _scramble_curve\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: distance.jensenshannon(pf, p_x_given_n)\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: distance.correlation(pf, p_x_given_n)\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: distance.sqeuclidean(pf, p_x_given_n)\n",
    "# active_surprise_metric_fn = lambda pf, p_x_given_n: wasserstein_distance(pf, p_x_given_n) # Figure out the correct function for this, it's in my old notebooks\n",
    "active_surprise_metric_fn = lambda pf, p_x_given_n: pearsonr(pf, p_x_given_n)[0] # this returns just the correlation coefficient (R), not the p-value due to the [0]\n",
    "result, result_df, result_df_grouped = _new_compute_surprise(long_results_obj, active_surprise_metric_fn=active_surprise_metric_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940bc02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PendingNotebookCode.TimebinnedNeuronActivity at 0x2746f8a94c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_results_obj.timebinned_neuron_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c107570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first valid index: 0\n",
      "first valid index: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c384e597fc4105975fdc164f78384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Slider:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "## Create a diagnostic plot that plots a stack of the three curves used for computations in the given epoch:\n",
    "\n",
    "debug_print = False\n",
    "timebinned_neuron_info = long_results_obj.timebinned_neuron_info\n",
    "\n",
    "\n",
    "def _get_updated_plot_data(index):\n",
    "\thardcoded_sub_epoch_item_idx = 0\n",
    "\tcurr_random_not_firing_cell_pf_curve = result.random_noise_curves[index]\n",
    "\tcurr_decoded_timebins_p_x_given_n = result.decoded_timebins_p_x_given_n[index]\n",
    "\tneuron_IDX, aclu = timebinned_neuron_info.active_IDXs[index], timebinned_neuron_info.active_aclus[index]\n",
    "\tif len(neuron_IDX) > 0:\n",
    "\t\t# Get first index\n",
    "\t\tis_valid = True\n",
    "\t\tneuron_IDX = neuron_IDX[hardcoded_sub_epoch_item_idx]\n",
    "\t\taclu = aclu[hardcoded_sub_epoch_item_idx]\n",
    "\t\t# curr_cell_pf_curve = long_results_obj.original_1D_decoder.pf.ratemap.tuning_curves[neuron_IDX]\n",
    "\t\tcurr_cell_pf_curve = long_results_obj.original_1D_decoder.pf.ratemap.unit_max_tuning_curves[neuron_IDX]\n",
    "\n",
    "\t\tif curr_random_not_firing_cell_pf_curve.ndim > 1:\n",
    "\t\t\tcurr_random_not_firing_cell_pf_curve = curr_random_not_firing_cell_pf_curve[hardcoded_sub_epoch_item_idx]\n",
    "\n",
    "\t\tif curr_decoded_timebins_p_x_given_n.ndim > 1:\n",
    "\t\t\tcurr_decoded_timebins_p_x_given_n = curr_decoded_timebins_p_x_given_n[hardcoded_sub_epoch_item_idx]\n",
    "\n",
    "\t\t# curr_timebin_p_x_given_n = curr_timebins_p_x_given_n[:, index]\n",
    "\t\tcurr_timebin_p_x_given_n = curr_decoded_timebins_p_x_given_n\n",
    "\t\tnormal_surprise, random_surprise = result.one_left_out_posterior_to_pf_surprises[index][hardcoded_sub_epoch_item_idx], result.one_left_out_posterior_to_scrambled_pf_surprises[index][hardcoded_sub_epoch_item_idx]\n",
    "\t\tupdated_plot_data = {'curr_cell_pf_curve': curr_cell_pf_curve, 'curr_random_not_firing_cell_pf_curve': curr_random_not_firing_cell_pf_curve, 'curr_timebin_p_x_given_n': curr_timebin_p_x_given_n}\n",
    "\t\t\n",
    "\telse:\n",
    "\t\t# Invalid period\n",
    "\t\tis_valid = False\n",
    "\t\tnormal_surprise, random_surprise = None, None\n",
    "\t\tupdated_plot_data = {'curr_cell_pf_curve': None, 'curr_random_not_firing_cell_pf_curve': None, 'curr_timebin_p_x_given_n': None}\n",
    "\n",
    "\treturn updated_plot_data, is_valid, (normal_surprise, random_surprise)\n",
    "\n",
    "\n",
    "def _add_plot(win: pg.GraphicsLayoutWidget, data, name:str):\n",
    "\tplot = win.addPlot() # PlotItem has to be built first?\n",
    "\tcurve = plot.plot(data, name=name, label=name)\n",
    "\tplot.setLabel('top', name)\n",
    "\treturn plot, curve\n",
    "\n",
    "win = pg.GraphicsLayoutWidget(show=True, title='diagnostic_plot')\n",
    "# plot_data = {'curr_cell_pf_curve': curr_cell_pf_curve, 'curr_random_not_firing_cell_pf_curve': curr_random_not_firing_cell_pf_curve, 'curr_timebin_p_x_given_n': curr_timebin_p_x_given_n}\n",
    "# plot_data = {'curr_cell_pf_curve': None, 'curr_random_not_firing_cell_pf_curve': None, 'curr_timebin_p_x_given_n': None}\n",
    "\n",
    "is_valid = False\n",
    "\n",
    "for index in np.arange(timebinned_neuron_info.n_timebins):\n",
    "\t# find the first valid index\n",
    "\tif not is_valid:\n",
    "\t\tplot_data, is_valid, (normal_surprise, random_surprise) = _get_updated_plot_data(index)\n",
    "\t\tprint(f'first valid index: {index}')\n",
    "\n",
    "\n",
    "plot_dict = {}\n",
    "\n",
    "## Many capture `plot_dict`\n",
    "\n",
    "def _initialize_plots(plot_data):\n",
    "\tfor i, (name, data) in enumerate(plot_data.items()):\n",
    "\t\tplot_item, curve = _add_plot(win, data=data, name=name)\n",
    "\t\tplot_dict[name] = {'plot_item':plot_item,'curve':curve}\n",
    "\t\tif i == 0:\n",
    "\t\t\tfirst_curve_name = name\n",
    "\t\telse:\n",
    "\t\t\tplot_dict[name]['plot_item'].setYLink(first_curve_name)  ## test linking by name\n",
    "\t\twin.nextRow()\n",
    "\treturn plot_dict\n",
    "\n",
    "\n",
    "def _update_plots(plot_dict, updated_plot_data):\n",
    "\t\"\"\" updates the plots created with `_initialize_plots`\"\"\"\n",
    "\tfor i, (name, data) in enumerate(updated_plot_data.items()):\n",
    "\t\tcurr_plot = plot_dict[name]['plot_item']\n",
    "\t\tcurr_curve = plot_dict[name]['curve']\n",
    "\t\tif data is not None:\n",
    "\t\t\tcurr_curve.setData(data)\n",
    "\t\telse:\n",
    "\t\t\t# curr_plot.clear() # will this mess up the plot by perminantly removing the curve? \n",
    "\t\t\t# curr_curve.clear()\n",
    "\t\t\tcurr_curve.setData([])\n",
    "\n",
    "def update_function(index):\n",
    "\t\"\"\" Define an update function that will be called with the current slider index \n",
    "\tCaptures plot_dict, and all data variables\n",
    "\t\"\"\"\n",
    "    # print(f'Slider index: {index}')\n",
    "\thardcoded_sub_epoch_item_idx = 0\n",
    "\tupdated_plot_data, is_valid, (normal_surprise, random_surprise) = _get_updated_plot_data(index)\n",
    "\tif is_valid:\n",
    "\t\t_update_plots(plot_dict, updated_plot_data)\n",
    "\t\tplot_dict['curr_cell_pf_curve']['plot_item'].setLabel('bottom', f\"{normal_surprise}\")\n",
    "\t\tplot_dict['curr_random_not_firing_cell_pf_curve']['plot_item'].setLabel('bottom', f\"{random_surprise}\")\n",
    "\telse:\n",
    "\t\t# Invalid period\n",
    "\t\tplot_dict['curr_cell_pf_curve']['plot_item'].setLabel('bottom', f\"NO ACTIVITY\")\n",
    "\t\tplot_dict['curr_random_not_firing_cell_pf_curve']['plot_item'].setLabel('bottom', f\"NO ACTIVITY\")\n",
    "\t\t_update_plots(plot_dict, updated_plot_data)\n",
    "\n",
    "\t# curr_random_not_firing_cell_pf_curve = result.random_noise_curves[index]\n",
    "\t# curr_decoded_timebins_p_x_given_n = result.decoded_timebins_p_x_given_n[index]\n",
    "\t# neuron_IDX, aclu = timebinned_neuron_info.active_IDXs[index], timebinned_neuron_info.active_aclus[index]\n",
    "\t# if debug_print:\n",
    "\t# \tprint(f'{neuron_IDX = }, {aclu = }')\n",
    "\t# if len(neuron_IDX) > 0:\n",
    "\t# \t# Get first index\n",
    "\t# \tneuron_IDX = neuron_IDX[hardcoded_sub_epoch_item_idx]\n",
    "\t# \taclu = aclu[hardcoded_sub_epoch_item_idx]\n",
    "\t# \t# curr_cell_pf_curve = long_results_obj.original_1D_decoder.pf.ratemap.tuning_curves[neuron_IDX]\n",
    "\t# \tcurr_cell_pf_curve = long_results_obj.original_1D_decoder.pf.ratemap.unit_max_tuning_curves[neuron_IDX]\n",
    "\n",
    "\t# \tif curr_random_not_firing_cell_pf_curve.ndim > 1:\n",
    "\t# \t\tcurr_random_not_firing_cell_pf_curve = curr_random_not_firing_cell_pf_curve[hardcoded_sub_epoch_item_idx]\n",
    "\n",
    "\t# \tif curr_decoded_timebins_p_x_given_n.ndim > 1:\n",
    "\t# \t\tcurr_decoded_timebins_p_x_given_n = curr_decoded_timebins_p_x_given_n[hardcoded_sub_epoch_item_idx]\n",
    "\n",
    "\n",
    "\t# \t# curr_timebin_p_x_given_n = curr_timebins_p_x_given_n[:, index]\n",
    "\t# \tcurr_timebin_p_x_given_n = curr_decoded_timebins_p_x_given_n\n",
    "\t# \tif debug_print:\n",
    "\t# \t\tprint(f'{curr_random_not_firing_cell_pf_curve.shape = }, {curr_cell_pf_curve.shape = }, {curr_timebin_p_x_given_n.shape = }')\n",
    "\t# \t# result.one_left_out_posterior_to_pf_surprises[index]\n",
    "\t# \t# result.one_left_out_posterior_to_scrambled_pf_surprises[index]\n",
    "\t# \tnormal_surprise, random_surprise = result.one_left_out_posterior_to_pf_surprises[index][hardcoded_sub_epoch_item_idx], result.one_left_out_posterior_to_scrambled_pf_surprises[index][hardcoded_sub_epoch_item_idx]\n",
    "\n",
    "\t# \tupdated_plot_data = {'curr_cell_pf_curve': curr_cell_pf_curve, 'curr_random_not_firing_cell_pf_curve': curr_random_not_firing_cell_pf_curve, 'curr_timebin_p_x_given_n': curr_timebin_p_x_given_n}\n",
    "\t# \t_update_plots(plot_dict, updated_plot_data)\n",
    "\n",
    "\t# \tplot_dict['curr_cell_pf_curve']['plot_item'].setLabel('bottom', f\"{normal_surprise}\")\n",
    "\t# \tplot_dict['curr_random_not_firing_cell_pf_curve']['plot_item'].setLabel('bottom', f\"{random_surprise}\")\n",
    "\t# else:\n",
    "\t# \t# Invalid period\n",
    "\t# \tplot_dict['curr_cell_pf_curve']['plot_item'].setLabel('bottom', f\"NO ACTIVITY\")\n",
    "\t# \tplot_dict['curr_random_not_firing_cell_pf_curve']['plot_item'].setLabel('bottom', f\"NO ACTIVITY\")\n",
    "\t# \tupdated_plot_data = {'curr_cell_pf_curve': None, 'curr_random_not_firing_cell_pf_curve': None, 'curr_timebin_p_x_given_n': None}\n",
    "\t# \t_update_plots(plot_dict, updated_plot_data)\n",
    "\n",
    "plot_dict = _initialize_plots(plot_data=plot_data)\n",
    "\n",
    "# win.graphicsItem().setLabel(axis='left', text='Short v. Long - Expected vs. Observed # Spikes')\n",
    "# win.graphicsItem().setLabel(axis='bottom', text='time')\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def integer_slider(update_func):\n",
    "    \"\"\" 2023-04-13 - WORKS!!!\n",
    "    Displays an integer slider that the user can adjust.\n",
    "\n",
    "    Args:\n",
    "        update_func (function): A user-provided update function that will be called with the current slider index.\n",
    "    \"\"\"\n",
    "    slider = widgets.IntSlider(description='Slider:', min=0, max=100, value=0)\n",
    "\n",
    "    def on_slider_change(change):\n",
    "        \"\"\"Callback function for slider value change.\"\"\"\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            # Call the user-provided update function with the current slider index\n",
    "            update_func(change['new'])\n",
    "\n",
    "    slider.observe(on_slider_change)\n",
    "    display(slider)\n",
    "\n",
    "# Define an update function that will be called with the current slider index\n",
    "# def update_function(index):\n",
    "#     print(f'Slider index: {index}')\n",
    "\n",
    "# Call the integer_slider function with the update function\n",
    "integer_slider(update_function)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54b239b9",
   "metadata": {},
   "source": [
    "## 2023-04-13 - Desired Plotting Interface Idea:\n",
    "```python\n",
    "# Rows of plots can be constructed trivially through lists:\n",
    "row_of_plots = [pg.plot(curr_cell_pf_curve, label='curr_cell_pf_curve'), pg.plot(curr_random_not_firing_cell_pf_curve, label='curr_random_not_firing_cell_pf_curve'), ...] \n",
    "\t# I'd guess behind the scenes they would be converted into a helper.row([...]) object\n",
    "\n",
    "# If you want a column instead, use helper.column\n",
    "column_of_plots = helper.column([pg.plot(curr_cell_pf_curve, label='curr_cell_pf_curve'), pg.plot(curr_random_not_firing_cell_pf_curve, label='curr_random_not_firing_cell_pf_curve'), ...])\n",
    "\n",
    "# The returned objects are composable:\n",
    "row_of_layouts = [row_of_plots, column_of_plots] # stacks the layout objects just like they were plot objects\n",
    "\n",
    "# Showing the result is easy, as is combining separate results in a new place:\n",
    "whole_figure_window = [row_of_layouts] \n",
    "whole_figure_window.show()\n",
    "```\n",
    "\n",
    "\"\"\" \n",
    "Relevant Functions:\n",
    "`perform_full_session_leave_one_out_decoding_analysis`:\n",
    "\t`perform_leave_one_aclu_out_decoding_analysis`:\tfrom pyphoplacecellanalysis.Analysis.Decoder.decoder_result import perform_leave_one_aclu_out_decoding_analysis\n",
    "\t`_analyze_leave_one_out_decoding_results`: from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions import _analyze_leave_one_out_decoding_results\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Get set of cells active in a given time bin, for each compute the surprise of its placefield with the leave-one-out decoded posterior.\n",
    "\n",
    "# 2. From the remainder of cells (those not active), randomly choose one to grab the placefield of and compute the surprise with that and the same posterior.\n",
    "\n",
    "\n",
    "# Expectation: The cells that are included in the time bin are expected to have a lower surprise (be less correlated with) the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "# 'time_bin_indices': valid_time_bin_indicies, 'posterior_to_pf_mean_surprise': one_left_out_posterior_to_pf_surprises_mean, 'posterior_to_scrambled_pf_mean_surprise': one_left_out_posterior_to_scrambled_pf_surprises_mean}\n",
    "\n",
    "# make a separate symbol_brush color for each cell:\n",
    "# cell_color_symbol_brush = [pg.intColor(i,hues=9, values=3, alpha=180) for i, aclu in enumerate(long_results_obj.original_1D_decoder.neuron_IDs)] # maxValue=128\n",
    "# All properties in common:\n",
    "win = pg.plot()\n",
    "win.setWindowTitle('Long Sanity Check - Leave-one-out Custom Surprise Plot')\n",
    "# legend_size = (80,60) # fixed size legend\n",
    "legend_size = None # auto-sizing legend to contents\n",
    "legend = pg.LegendItem(legend_size, offset=(-1,0)) # do this instead of # .addLegend\n",
    "legend.setParentItem(win.graphicsItem())\n",
    "\n",
    "plots = {}\n",
    "label_prefix_list = ['normal', 'scrambled']\n",
    "long_short_symbol_list = ['t', 'o'] # note: 's' is a square. 'o', 't1': triangle pointing upwards0\n",
    "\n",
    "\n",
    "# Use mean time_bin and surprise for each epoch\n",
    "# plots['normal'] = win.plot(x=valid_time_bin_indicies, y=one_left_out_posterior_to_pf_surprises_mean, pen=None, symbol='t', symbolBrush=pg.intColor(1,6,maxValue=128), name=f'normal', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "# plots['scrambled'] = win.plot(x=valid_time_bin_indicies, y=one_left_out_posterior_to_scrambled_pf_surprises_mean, pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), name=f'scrambled', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "curr_surprise_difference = one_left_out_posterior_to_scrambled_pf_surprises_mean - one_left_out_posterior_to_pf_surprises_mean\n",
    "\n",
    "\n",
    "# x=valid_time_bin_indicies\n",
    "# y=curr_surprise_difference\n",
    "x=result_df_grouped.time_bin_indices.to_numpy()\n",
    "y=result_df_grouped['surprise_diff'].to_numpy()\n",
    "\n",
    "plots['difference'] = win.plot(x=x, y=y, pen=None, symbol='t', symbolBrush=pg.intColor(2,6,maxValue=128), name=f'difference', alpha=0.5) #  symbolBrush=pg.intColor(i,6,maxValue=128) , symbol=curr_symbol, symbolBrush=cell_color_symbol_brush[unit_IDX]\n",
    "\n",
    "\n",
    "for k, v in plots.items():\n",
    "\tlegend.addItem(v, f'{k}')\n",
    "\n",
    "win.graphicsItem().setLabel(axis='left', text='Normal v. Random - Surprise (Custom)')\n",
    "win.graphicsItem().setLabel(axis='bottom', text='time')\n",
    "# return win, plots_tuple, legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_time_bin_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f539736",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_left_out_posterior_to_pf_surprises_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f996fd9",
   "metadata": {},
   "source": [
    "## Pre 2023-04-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Fresh (don't load from cache)\n",
    "long_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=long_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_long', perform_cache_load=False, skip_cache_save=False)\n",
    "short_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=short_shared_aclus_only_decoder, decoding_time_bin_size = 0.025, cache_suffix = '_short', perform_cache_load=False, skip_cache_save=False)\n",
    "# # (time_bins, neurons), (epochs, neurons), (epochs)\n",
    "# all_epochs_computed_one_left_out_posterior_to_pf_surprises, all_epochs_computed_cell_one_left_out_posterior_to_pf_surprises_mean, all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.decoder_result import plot_kourosh_activity_style_figure\n",
    "\n",
    "from neuropy.core.neurons import NeuronType\n",
    "# # Include only pyramidal aclus:\n",
    "# print(f'all shared_aclus: {len(shared_aclus)}\\nshared_aclus: {shared_aclus}')\n",
    "# shared_aclu_neuron_type = long_session.neurons.neuron_type[np.isin(long_session.neurons.neuron_ids, shared_aclus)]\n",
    "# assert len(shared_aclu_neuron_type) == len(shared_aclus)\n",
    "# # Find only the aclus that are pyramidal:\n",
    "# is_shared_aclu_pyramidal = (shared_aclu_neuron_type == NeuronType.PYRAMIDAL)\n",
    "# pyramidal_only_shared_aclus = shared_aclus[is_shared_aclu_pyramidal]\n",
    "# print(f'num pyramidal_only_shared_aclus: {len(pyramidal_only_shared_aclus)}\\npyramidal_only_shared_aclus: {pyramidal_only_shared_aclus}')\n",
    "\n",
    "\n",
    "## Drop Pyramidal but don't use only shared aclus:\n",
    "all_aclus = deepcopy(long_session.neurons.neuron_ids)\n",
    "neuron_type = long_session.neurons.neuron_type\n",
    "assert len(neuron_type) == len(all_aclus)\n",
    "# Find only the aclus that are pyramidal:\n",
    "is_aclu_pyramidal = (neuron_type == NeuronType.PYRAMIDAL)\n",
    "pyramidal_only_all_aclus = all_aclus[is_aclu_pyramidal]\n",
    "print(f'num pyramidal_only_all_aclus: {len(pyramidal_only_all_aclus)}\\npyramidal_only_all_aclus: {pyramidal_only_all_aclus}')\n",
    "\n",
    "# app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, shared_aclus, epoch_idx=5, callout_epoch_IDXs=[0,1,2,3], skip_rendering_callouts=True)\n",
    "# app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_shared_aclus, epoch_idx=2, callout_epoch_IDXs=[0,4], skip_rendering_callouts=False)\n",
    "app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_all_aclus, epoch_idx=3, callout_epoch_IDXs=[2,4,6], skip_rendering_callouts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea983fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "app, win, plots, plots_data = plot_kourosh_activity_style_figure(long_results_obj, long_session, pyramidal_only_all_aclus, epoch_idx=11, callout_epoch_IDXs=[0,1,2, 3, 4, 5], skip_rendering_callouts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3584cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f519e392",
   "metadata": {},
   "source": [
    "# 2023-04-13 Show Surprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from PendingNotebookCode import plot_long_short, plot_long_short_any_values, plot_long_short_expected_vs_observed_firing_rates\n",
    "\n",
    "# plot_long_short(long_results_obj, short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_long_short_any_values(long_results_obj=long_results_obj, short_results_obj=short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_long_short_expected_vs_observed_firing_rates(long_results_obj=long_results_obj, short_results_obj=short_results_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fn = lambda a_results_obj: a_results_obj.all_epochs_decoded_epoch_time_bins_mean[:,0]\n",
    "# y_fn = lambda a_results_obj: a_results_obj.all_epochs_all_cells_one_left_out_posterior_to_scrambled_pf_surprises_mean\n",
    "y_fn = lambda a_results_obj: a_results_obj.all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean\n",
    "# y_fn = lambda a_results_obj: a_results_obj.all_epochs_computed_one_left_out_posterior_to_pf_surprises\n",
    "\n",
    "# (time_bins, neurons), (epochs, neurons), (epochs)\n",
    "# all_epochs_computed_one_left_out_posterior_to_pf_surprises, all_epochs_computed_cell_one_left_out_posterior_to_pf_surprises_mean, all_epochs_all_cells_one_left_out_posterior_to_pf_surprises_mean\n",
    "win, plots_tuple, legend = plot_long_short_any_values(long_results_obj, short_results_obj, x=x_fn, y=y_fn, limit_aclus=[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe80559",
   "metadata": {},
   "source": [
    "# 2023-04-13 - Find Good looking epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(long_results_obj.active_filter_epochs, long_results_obj.all_included_filter_epochs_decoder_result, global_pos_df=global_session.position.df, variable_name='lin_pos', xbin=long_results_obj.original_1D_decoder.xbin,\n",
    "                                                        name='stacked_epoch_slices_long_results_obj', debug_print=True, debug_test_max_num_slices=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[23, 27, 29, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pagination support for `plot_decoded_epoch_slices`\n",
    "from pyphocorehelpers.indexing_helpers import compute_paginated_grid_config\n",
    "\n",
    "n_epochs = long_results_obj.active_filter_epochs.n_epochs\n",
    "subplot_no_pagination_configuration, included_combined_indicies_pages, page_grid_sizes = compute_paginated_grid_config(n_epochs, max_num_columns=1, max_subplots_per_page=32, data_indicies=result_df_grouped.index.to_numpy(), last_figure_subplots_same_layout=True)\n",
    "num_pages = len(included_combined_indicies_pages)\n",
    "num_pages\n",
    "included_epoch_indicies_pages = [[curr_included_epoch_index for (a_linear_index, curr_row, curr_col, curr_included_epoch_index) in v] for page_idx, v in enumerate(included_combined_indicies_pages)] # a list of length `num_pages` containing up to 10 items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
