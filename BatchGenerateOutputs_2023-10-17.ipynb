{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionTables, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult, BatchSessionCompletionHandler\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_file_metadata\n",
    "from pyphocorehelpers.Filesystem.path_helpers import discover_data_files, generate_copydict, copy_movedict, copy_file\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import get_file_str_if_file_exists\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import check_output_h5_files, copy_files_in_filelist_to_dest\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import ConcreteSessionFolder, BackupMethods\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots, BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import build_merged_neuron_firing_rate_indicies\n",
    "\n",
    "BATCH_DATE_TO_USE = '2023-10-17_Lab' # used for filenames throught the notebook\n",
    "# BATCH_DATE_TO_USE = '2023-10-07_Apogee' # used for filenames throught the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : /media/MAX/Data/global_batch_result_2023-10-17_Lab.pkl... done.\n",
      "Failure loading /media/MAX/Data/global_batch_result_2023-10-17_Lab.pkl.\n",
      "creating new batch_run\n",
      "local_session_names_list: ['2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_1-22-43', '2006-6-09_3-23-37', '2006-6-12_15-55-31', '2006-6-13_14-42-6']\n",
      "local_session_names_list: ['2006-6-07_16-40-19', '2006-6-08_15-46-47', '2006-6-08_21-16-25', '2006-6-09_22-24-40', '2006-6-12_16-53-46', '2006-6-13_15-22-3']\n",
      "local_session_names_list: ['2006-4-09_17-29-30', '2006-4-10_12-25-50', '2006-4-10_21-2-40', '2006-4-11_15-16-59', '2006-4-12_14-39-31', '2006-4-12_17-53-55', '2006-4-16_15-12-23', '2006-4-17_12-33-47', '2006-4-18_13-6-1', '2006-4-18_15-23-32', '2006-4-19_13-34-40', '2006-4-19_16-48-9', '2006-4-21_10-24-35', '2006-4-25_14-28-51', '2006-4-25_17-17-6', '2006-4-26_13-22-13', '2006-4-27_14-43-12', '2006-4-28_12-17-27', '2006-4-28_16-48-29']\n",
      "local_session_names_list: ['2006-4-09_16-40-54', '2006-4-10_12-58-3', '2006-4-10_19-11-57', '2006-4-11_12-48-38', '2006-4-11_16-2-46', '2006-4-12_14-59-23', '2006-4-12_15-25-59', '2006-4-16_14-49-24', '2006-4-16_18-47-52', '2006-4-17_12-52-15', '2006-4-18_13-28-57', '2006-4-18_15-38-2', '2006-4-19_13-50-7', '2006-4-19_16-37-40', '2006-4-21_11-19-2', '2006-4-25_13-20-55', '2006-4-25_17-33-28', '2006-4-26_13-51-50', '2006-4-27_18-21-57', '2006-4-28_12-38-13', '2006-4-28_17-6-14']\n",
      "local_session_names_list: ['11-02_17-46-44', '11-02_19-28-0', '11-03_12-3-25', '11-03_21-26-8', '11-05_19-26-43', '11-09_11-43-50', '11-09_12-15-3', '11-09_21-17-16', '11-09_22-4-5', '11-19_12-35-59', '11-19_13-2-0', '11-19_13-55-7', 'fet11-01_12-58-54', 'fet11-03_11-0-53', 'fet11-03_20-28-3', 'fet11-04_21-20-3', 'redundant', 'showclus', 'sleep', 'tmaze']\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-07_11-26-53</td>\n",
       "      <td>kdiba_gor01_one_2006-6-07_11-26-53</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 11:26:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_3-23-37</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_3-23-37</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 03:23:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-04_21-20-3</td>\n",
       "      <td>kdiba_pin01_one_fet11-04_21-20-3</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20-3</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-04 21:20:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>redundant</td>\n",
       "      <td>kdiba_pin01_one_redundant</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/redundant</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>showclus</td>\n",
       "      <td>kdiba_pin01_one_showclus</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/showclus</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>sleep</td>\n",
       "      <td>kdiba_pin01_one_sleep</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/sleep</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>tmaze</td>\n",
       "      <td>kdiba_pin01_one_tmaze</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/tmaze</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "0        kdiba  gor01        one  2006-6-07_11-26-53   \n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "3        kdiba  gor01        one   2006-6-09_3-23-37   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "..         ...    ...        ...                 ...   \n",
       "67       kdiba  pin01        one    fet11-04_21-20-3   \n",
       "68       kdiba  pin01        one           redundant   \n",
       "69       kdiba  pin01        one            showclus   \n",
       "70       kdiba  pin01        one               sleep   \n",
       "71       kdiba  pin01        one               tmaze   \n",
       "\n",
       "                               context  \\\n",
       "0   kdiba_gor01_one_2006-6-07_11-26-53   \n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "3    kdiba_gor01_one_2006-6-09_3-23-37   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "..                                 ...   \n",
       "67    kdiba_pin01_one_fet11-04_21-20-3   \n",
       "68           kdiba_pin01_one_redundant   \n",
       "69            kdiba_pin01_one_showclus   \n",
       "70               kdiba_pin01_one_sleep   \n",
       "71               kdiba_pin01_one_tmaze   \n",
       "\n",
       "                                             basedirs  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43   \n",
       "3   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23-37   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67   /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20-3   \n",
       "68          /media/MAX/Data/KDIBA/pin01/one/redundant   \n",
       "69           /media/MAX/Data/KDIBA/pin01/one/showclus   \n",
       "70              /media/MAX/Data/KDIBA/pin01/one/sleep   \n",
       "71              /media/MAX/Data/KDIBA/pin01/one/tmaze   \n",
       "\n",
       "                              status errors    session_datetime  n_long_laps  \\\n",
       "0   SessionBatchProgress.NOT_STARTED   None 2006-06-07 11:26:53            0   \n",
       "1   SessionBatchProgress.NOT_STARTED   None 2006-06-08 14:26:15            0   \n",
       "2   SessionBatchProgress.NOT_STARTED   None 2006-06-09 01:22:43            0   \n",
       "3   SessionBatchProgress.NOT_STARTED   None 2006-06-09 03:23:37            0   \n",
       "4   SessionBatchProgress.NOT_STARTED   None 2006-06-12 15:55:31            0   \n",
       "..                               ...    ...                 ...          ...   \n",
       "67  SessionBatchProgress.NOT_STARTED   None 2009-11-04 21:20:03            0   \n",
       "68  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "69  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "70  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "71  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "0                0             0                0     False   \n",
       "1                0             0                0     False   \n",
       "2                0             0                0     False   \n",
       "3                0             0                0     False   \n",
       "4                0             0                0     False   \n",
       "..             ...           ...              ...       ...   \n",
       "67               0             0                0     False   \n",
       "68               0             0                0     False   \n",
       "69               0             0                0     False   \n",
       "70               0             0                0     False   \n",
       "71               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3                                                       \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3                                                       \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "0   /media/MAX/Data/KDIBA/gor01/one/2006-6-07_11-2...   \n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "3   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_3-23...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "..                                                ...   \n",
       "67  /media/MAX/Data/KDIBA/pin01/one/fet11-04_21-20...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "0                         False                                  True  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "3                         False                                  True  \n",
       "4                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "67                        False                                  True  \n",
       "68                        False                                 False  \n",
       "69                        False                                 False  \n",
       "70                        False                                 False  \n",
       "71                        False                                 False  \n",
       "\n",
       "[72 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/nfs/turbo/umms-kdiba/Data')] # , Path(r'/home/halechr/FastData')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "# try to load an existing batch result:\n",
    "global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    # display(batch_progress_df)\n",
    "    # display(good_only_batch_progress_df)\n",
    "    display(batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019afbbd-70d2-4e75-9548-b6f22d2e31ca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>kdiba_gor01_two_2006-6-07_16-40-19</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>kdiba_gor01_two_2006-6-08_21-16-25</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-10_12-58-3</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>kdiba_pin01_one_11-02_17-46-44</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>kdiba_pin01_one_11-02_19-28-0</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>kdiba_pin01_one_11-03_12-3-25</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>kdiba_pin01_one_fet11-01_12-58-54</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "6        kdiba  gor01        two  2006-6-07_16-40-19   \n",
       "8        kdiba  gor01        two  2006-6-08_21-16-25   \n",
       "..         ...    ...        ...                 ...   \n",
       "32       kdiba  vvp01        two   2006-4-10_12-58-3   \n",
       "52       kdiba  pin01        one      11-02_17-46-44   \n",
       "53       kdiba  pin01        one       11-02_19-28-0   \n",
       "54       kdiba  pin01        one       11-03_12-3-25   \n",
       "64       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "\n",
       "                               context  \\\n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "6   kdiba_gor01_two_2006-6-07_16-40-19   \n",
       "8   kdiba_gor01_two_2006-6-08_21-16-25   \n",
       "..                                 ...   \n",
       "32   kdiba_vvp01_two_2006-4-10_12-58-3   \n",
       "52      kdiba_pin01_one_11-02_17-46-44   \n",
       "53       kdiba_pin01_one_11-02_19-28-0   \n",
       "54       kdiba_pin01_one_11-03_12-3-25   \n",
       "64   kdiba_pin01_one_fet11-01_12-58-54   \n",
       "\n",
       "                                             basedirs  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "..                                                ...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3   \n",
       "52     /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44   \n",
       "53      /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0   \n",
       "54      /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54   \n",
       "\n",
       "                              status errors    session_datetime  n_long_laps  \\\n",
       "1   SessionBatchProgress.NOT_STARTED   None 2006-06-08 14:26:15            0   \n",
       "2   SessionBatchProgress.NOT_STARTED   None 2006-06-09 01:22:43            0   \n",
       "4   SessionBatchProgress.NOT_STARTED   None 2006-06-12 15:55:31            0   \n",
       "6   SessionBatchProgress.NOT_STARTED   None 2006-06-07 16:40:19            0   \n",
       "8   SessionBatchProgress.NOT_STARTED   None 2006-06-08 21:16:25            0   \n",
       "..                               ...    ...                 ...          ...   \n",
       "32  SessionBatchProgress.NOT_STARTED   None 2006-04-10 12:58:03            0   \n",
       "52  SessionBatchProgress.NOT_STARTED   None 2009-11-02 17:46:44            0   \n",
       "53  SessionBatchProgress.NOT_STARTED   None 2009-11-02 19:28:00            0   \n",
       "54  SessionBatchProgress.NOT_STARTED   None 2009-11-03 12:03:25            0   \n",
       "64  SessionBatchProgress.NOT_STARTED   None 2009-11-01 12:58:54            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "1                0             0                0     False   \n",
       "2                0             0                0     False   \n",
       "4                0             0                0     False   \n",
       "6                0             0                0     False   \n",
       "8                0             0                0     False   \n",
       "..             ...           ...              ...       ...   \n",
       "32               0             0                0     False   \n",
       "52               0             0                0     False   \n",
       "53               0             0                0     False   \n",
       "54               0             0                0     False   \n",
       "64               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "..                                                ...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...   \n",
       "52  /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...   \n",
       "53  /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...   \n",
       "54  /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...   \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "..                                                ...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...   \n",
       "52  /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...   \n",
       "53  /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...   \n",
       "54  /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...   \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "..                                                ...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...   \n",
       "52  /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...   \n",
       "53  /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...   \n",
       "54  /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...   \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "4                          True                                  True  \n",
       "6                          True                                  True  \n",
       "8                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "32                         True                                  True  \n",
       "52                         True                                  True  \n",
       "53                         True                                  True  \n",
       "54                         True                                  True  \n",
       "64                         True                                  True  \n",
       "\n",
       "[15 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "included_session_batch_progress_df = batch_progress_df[np.isin(batch_progress_df['context'].values, included_session_contexts)]\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(included_session_batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71584edc",
   "metadata": {},
   "source": [
    "# Execute Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6ae279",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(included_session_contexts): 15\n",
      "Beginning processing with len(included_session_contexts): 15\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.one.2006-6-09_1-22-43\"):build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.one.2006-6-08_14-26-15\"):\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-09_1-22-43 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-09_1-22-43.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-08_14-26-15 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-08_14-26-15.log\n",
      "\n",
      "========================== runBatch STARTING ==================================================== runBatch STARTING ==========================\n",
      "\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\n",
      "\tsession_context: kdiba_gor01_one_2006-6-08_14-26-15\tsession_context: kdiba_gor01_one_2006-6-09_1-22-43\n",
      "\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15\tsession_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43\n",
      "\n",
      "__________________________________________________________________\n",
      "__________________________________________________________________basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15\n",
      "\n",
      "active_data_mode_name: kdibabasedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43\n",
      "\n",
      "active_data_mode_name: kdibaSkipping loading from pickled file because force_reload == True.\n",
      "\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.epochs_info.mat... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.epochs_info.mat... done.done.\n",
      "\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.position_info.mat...Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.position_info.mat...  done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.spikes.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.spikes.mat... done.\n",
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Saving updated position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.position.npy... 2006-6-09_1-22-43.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.interpolated_spike_positions.npy... Failure loading .position.npy. Must recompute.\n",
      "\n",
      "2006-6-09_1-22-43.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.mua.npy... 2006-6-09_1-22-43.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving updated position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.position.npy... 2006-6-08_14-26-15.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.interpolated_spike_positions.npy... Saving pbe results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.pbe.npy... 2006-6-09_1-22-43.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... 2006-6-08_14-26-15.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "done.\n",
      "Computing added spike scISI column results : spikes_df... Saving mua results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.mua.npy... 2006-6-08_14-26-15.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.pbe.npy... 2006-6-08_14-26-15.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 415\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1029.316608761903)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1029.316608761903, end: 1737.1968310000375)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1737.1968310000375)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t curr_replays: 626\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1211.5580800310709)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1211.5580800310709, end: 2093.8978568242164)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2093.8978568242164)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (524075,) should be less than time_window_edges: (2060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (2031,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (596822,) should be less than time_window_edges: (2425,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (27562,) should be less than time_window_edges: (2394,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (344561,) should be less than time_window_edges: (1417,)!using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (1236,)!using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (410073,) should be less than time_window_edges: (1766,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19981,) should be less than time_window_edges: (1658,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (868636,) should be less than time_window_edges: (3476,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (49509,) should be less than time_window_edges: (3310,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/20231017200710-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl'\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (2912.6540336608887 MB) > new_temporary_file_size_MB (2912.6540307998657 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/backup-20231017200738-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/20231017200731-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl'\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1006895,) should be less than time_window_edges: (4189,)!\n",
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47543,) should be less than time_window_edges: (4125,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\tmoving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/20231017200820-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl'\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-08_14-26-15, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "done.\n",
      "WARNING: prev_extant_file_size_MB (4156.6062297821045 MB) > new_temporary_file_size_MB (4156.6062269210815 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/backup-20231017200901-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/20231017200852-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl'\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (524075,) should be less than time_window_edges: (2060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (2031,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (344561,) should be less than time_window_edges: (1417,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (1236,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (868636,) should be less than time_window_edges: (3476,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (49509,) should be less than time_window_edges: (3310,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 14, 16, 296235)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 14, 35, 793230)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 15, 11, 194370)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 15, 30, 584066)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21408,) should be less than time_window_edges: (18527,)!\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (596822,) should be less than time_window_edges: (2425,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (27562,) should be less than time_window_edges: (2394,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (410073,) should be less than time_window_edges: (1766,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19981,) should be less than time_window_edges: (1658,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1006895,) should be less than time_window_edges: (4189,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47543,) should be less than time_window_edges: (4125,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 71, n_all_epoch_timebins = 1652)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 18, 27, 562318)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 18, 59, 342059)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 19, 53, 202993)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 20, 26, 386943)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 71, n_all_epoch_timebins = 1652)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2501: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 67, n_all_epoch_timebins = 2237)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/20231017202556-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:53.841937\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_1-22-43'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "(n_neurons = 67, n_all_epoch_timebins = 2237)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-09_1-22-43...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2501: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.one.2006-6-12_15-55-31\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-12_15-55-31 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.one.2006-6-12_15-55-31.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_one_2006-6-12_15-55-31\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.spikes.mat... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 3 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.position.npy... 2006-6-12_15-55-31.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.interpolated_spike_positions.npy... 2006-6-12_15-55-31.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.mua.npy... 2006-6-12_15-55-31.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.pbe.npy... 2006-6-12_15-55-31.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "\t curr_replays: 134\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 656.0648088779999)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 656.0648088779999, end: 1122.1864874939201)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1122.1864874939201)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (204660,) should be less than time_window_edges: (1314,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (1220,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... _generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (141254,) should be less than time_window_edges: (934,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (856,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (345914,) should be less than time_window_edges: (2246,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20211,) should be less than time_window_edges: (2132,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/20231017202916-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl'\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (2252.77986907959 MB) > new_temporary_file_size_MB (2252.779866218567 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/backup-20231017202937-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/20231017202934-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl'\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (204660,) should be less than time_window_edges: (1314,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (1220,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (141254,) should be less than time_window_edges: (934,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (856,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (345914,) should be less than time_window_edges: (2246,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20211,) should be less than time_window_edges: (2132,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 32, 47, 616869)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 32, 56, 310237)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 33, 10, 877494)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 33, 19, 895208)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 36, n_all_epoch_timebins = 612)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 36, n_all_epoch_timebins = 612)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "\tmoving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/20231017203338-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl'\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:01:01.994902\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1545: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1545: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_14-26-15'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\tmoving new output at '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/20231017203506-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:15.826904\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "ERROR: encountered exception !! HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5' ::::: (<class 'tables.exceptions.HDF5ExtError'>, HDF5ExtError(\"Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'\"), <traceback object at 0x7ff229094c00>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! HDF5 error back trace\n",
      "\n",
      "  File \"H5F.c\", line 620, in H5Fopen\n",
      "    unable to open file\n",
      "  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n",
      "    failed to iterate over available VOL connector plugins\n",
      "  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n",
      "    can't iterate over plugins in plugin path '(null)'\n",
      "  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n",
      "    can't open directory: /usr/local/hdf5/lib/plugin\n",
      "  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n",
      "    open failed\n",
      "  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n",
      "    unable to open file\n",
      "  File \"H5Fint.c\", line 1990, in H5F_open\n",
      "    unable to read superblock\n",
      "  File \"H5Fsuper.c\", line 614, in H5F__super_read\n",
      "    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n",
      "\n",
      "End of HDF5 error back trace\n",
      "\n",
      "Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5' ::::: (<class 'tables.exceptions.HDF5ExtError'>, HDF5ExtError(\"Unable to open/create file '/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5'\"), <traceback object at 0x7ff20191ebc0>) while trying to build the session HDF output for kdiba_gor01_one_2006-6-12_15-55-31\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-07_16-40-19\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-07_16-40-19 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-07_16-40-19.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-07_16-40-19\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-12_15-55-31.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.spikes.mat... done.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.position.npy... 2006-6-07_16-40-19.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.interpolated_spike_positions.npy... 2006-6-07_16-40-19.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.mua.npy... 2006-6-07_16-40-19.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-08_14-26-15...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.pbe.npy... 2006-6-07_16-40-19.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 733\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1236.2662453636294)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1236.2662453636294, end: 2587.801681999932)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2587.801681999932)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-08_21-16-25\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-08_21-16-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-08_21-16-25.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-08_21-16-25\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_14-26-15.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Saving updated position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.position.npy... 2006-6-08_21-16-25.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.interpolated_spike_positions.npy... 2006-6-08_21-16-25.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.mua.npy... 2006-6-08_21-16-25.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.pbe.npy... 2006-6-08_21-16-25.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 141\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 722.653951405664)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 722.653951405664, end: 1201.0839364149142)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1201.0839364149142)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\t spikes_df[time_variable_name]: (488487,) should be less than time_window_edges: (2474,)!\n",
      "\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18768,) should be less than time_window_edges: (2387,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (372221,) should be less than time_window_edges: (1447,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (1339,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (471751,) should be less than time_window_edges: (2705,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15458,) should be less than time_window_edges: (2552,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (246159,) should be less than time_window_edges: (958,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (853,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D...\n",
      "Recomputing active_epoch_time_dependent_placefields2D...  using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))WARNING: PREVIOUSLY ASSERT: \n",
      "\n",
      "\t spikes_df[time_variable_name]: (44903,) should be less than time_window_edges: (34338,)!\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44903,) should be less than time_window_edges: (34338,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (618380,) should be less than time_window_edges: (2404,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44903,) should be less than time_window_edges: (2290,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/20231017203750-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl'\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (2106.930214881897 MB) > new_temporary_file_size_MB (2106.930212020874 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/backup-20231017203814-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/20231017203809-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl'\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (960238,) should be less than time_window_edges: (5177,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (34226,) should be less than time_window_edges: (5105,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl... _perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/20231017203820-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl'\n",
      "using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44903,) should be less than time_window_edges: (34338,)!\n",
      "done.\n",
      "WARNING: prev_extant_file_size_MB (4840.323866844177 MB) > new_temporary_file_size_MB (4840.323863983154 MB)! A backup will be made!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44903,) should be less than time_window_edges: (34338,)!\n",
      "'/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/backup-20231017203911-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/20231017203902-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl'\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (372221,) should be less than time_window_edges: (1447,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (1339,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (246159,) should be less than time_window_edges: (958,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (853,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (618380,) should be less than time_window_edges: (2404,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44903,) should be less than time_window_edges: (2290,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 42, 26, 483341)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 42, 38, 937709)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (26737,) should be less than time_window_edges: (20071,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 43, 1, 784910)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 43, 14, 154866)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18166,) should be less than time_window_edges: (12769,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 64, n_all_epoch_timebins = 616)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 64, n_all_epoch_timebins = 616)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2501: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1545: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1545: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (488487,) should be less than time_window_edges: (2474,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18768,) should be less than time_window_edges: (2387,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (471751,) should be less than time_window_edges: (2705,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15458,) should be less than time_window_edges: (2552,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (960238,) should be less than time_window_edges: (5177,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (34226,) should be less than time_window_edges: (5105,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 47, 21, 379110)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/20231017204727-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl'\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 47, 49, 210710)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:24.444432\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_21-16-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 48, 16, 449563)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-08_21-16-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-09_22-24-40\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-09_22-24-40 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-09_22-24-40.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-09_22-24-40\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-08_21-16-25.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-08_14-26-15.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Saving updated position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.position.npy... 2006-6-09_22-24-40.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.interpolated_spike_positions.npy... 2006-6-09_22-24-40.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.laps_info.mat... _execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 20, 48, 44, 526817)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.mua.npy... 2006-6-09_22-24-40.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.pbe.npy... 2006-6-09_22-24-40.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 723\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 911.6011600069469)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 911.6011600069469, end: 2573.457162000006)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2573.457162000006)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... reusing extant decoder.\n",
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (32341,) should be less than time_window_edges: (26888,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (32341,) should be less than time_window_edges: (26888,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (489077,) should be less than time_window_edges: (1825,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (32341,) should be less than time_window_edges: (1794,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (686401,) should be less than time_window_edges: (3325,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14699,) should be less than time_window_edges: (3295,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 41, n_all_epoch_timebins = 3163)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1175478,) should be less than time_window_edges: (5148,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47040,) should be less than time_window_edges: (5125,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/20231017205200-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl'\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (4154.2933559417725 MB) > new_temporary_file_size_MB (4154.2933530807495 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/backup-20231017205241-loadedSessPickle.pkl.bak'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/20231017205231-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl'\n",
      "(n_neurons = 41, n_all_epoch_timebins = 3163)\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2501: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (32341,) should be less than time_window_edges: (26888,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (32341,) should be less than time_window_edges: (26888,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/20231017205727-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:40.603189\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-07_16-40-19'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff129f79a80>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1f93377c0>) while trying to build the session HDF output for kdiba_gor01_two_2006-6-07_16-40-19\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-07_16-40-19...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.gor01.two.2006-6-12_16-53-46\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-12_16-53-46 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.gor01.two.2006-6-12_16-53-46.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-12_16-53-46\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-07_16-40-19.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_15-55-31.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.position.npy... 2006-6-12_16-53-46.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.interpolated_spike_positions.npy... 2006-6-12_16-53-46.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.mua.npy... 2006-6-12_16-53-46.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.pbe.npy... 2006-6-12_16-53-46.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 132\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 471.0674003356835)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 471.0674003356835, end: 785.4513262689579)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 785.4513262689579)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16001,) should be less than time_window_edges: (12999,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16001,) should be less than time_window_edges: (12999,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (174940,) should be less than time_window_edges: (943,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16001,) should be less than time_window_edges: (868,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110870,) should be less than time_window_edges: (630,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (609,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28077,) should be less than time_window_edges: (22715,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28077,) should be less than time_window_edges: (22715,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (285810,) should be less than time_window_edges: (1572,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28077,) should be less than time_window_edges: (1516,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/20231017205944-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl'\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (1536.7434711456299 MB) > new_temporary_file_size_MB (1536.743468284607 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/backup-20231017210000-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/20231017205957-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl'\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16001,) should be less than time_window_edges: (12999,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16001,) should be less than time_window_edges: (12999,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28077,) should be less than time_window_edges: (22715,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28077,) should be less than time_window_edges: (22715,)!\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (489077,) should be less than time_window_edges: (1825,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (32341,) should be less than time_window_edges: (1794,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (686401,) should be less than time_window_edges: (3325,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14699,) should be less than time_window_edges: (3295,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1175478,) should be less than time_window_edges: (5148,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47040,) should be less than time_window_edges: (5125,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (31742,) should be less than time_window_edges: (26319,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (31742,) should be less than time_window_edges: (26319,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 1, 45, 658494)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (174940,) should be less than time_window_edges: (943,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16001,) should be less than time_window_edges: (868,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110870,) should be less than time_window_edges: (630,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (609,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (285810,) should be less than time_window_edges: (1572,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28077,) should be less than time_window_edges: (1516,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15672,) should be less than time_window_edges: (12618,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15672,) should be less than time_window_edges: (12618,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 2, 22, 314549)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 2, 24, 266908)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (31742,) should be less than time_window_edges: (26319,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 2, 30, 332471)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (31742,) should be less than time_window_edges: (26319,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15672,) should be less than time_window_edges: (12618,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15672,) should be less than time_window_edges: (12618,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 2, 39, 987015)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 2, 46, 7271)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (12076,) should be less than time_window_edges: (9109,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 2, 47, 538176)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 38, n_all_epoch_timebins = 413)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 3, 24, 967285)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 38, n_all_epoch_timebins = 413)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/20231017210430-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:12.509166\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_16-53-46'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not output time-dependent placefields to .h5. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not output time-dependent placefields to .h5. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.one.2006-4-09_17-29-30\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-09_17-29-30 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-09_17-29-30.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-09_17-29-30\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-12_16-53-46.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-07_16-40-19.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-12_15-55-31.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.spikes.mat... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.position.npy... 2006-4-09_17-29-30.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.interpolated_spike_positions.npy... 2006-4-09_17-29-30.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "Loading success: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.mua.npy... 2006-4-09_17-29-30.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.pbe.npy... 2006-4-09_17-29-30.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t curr_replays: 201\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 873.6244981772179)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 873.6244981772179, end: 1391.655627853339)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1391.655627853339)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (187994,) should be less than time_window_edges: (1749,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11886,) should be less than time_window_edges: (1653,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110977,) should be less than time_window_edges: (1038,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8749,) should be less than time_window_edges: (930,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (298971,) should be less than time_window_edges: (2785,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20635,) should be less than time_window_edges: (2700,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/20231017210659-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl'\n",
      "(n_neurons = 58, n_all_epoch_timebins = 3783)\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (3798.7737560272217 MB) > new_temporary_file_size_MB (3798.7737531661987 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/backup-20231017210727-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/20231017210722-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl'\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (187994,) should be less than time_window_edges: (1749,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11886,) should be less than time_window_edges: (1653,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (110977,) should be less than time_window_edges: (1038,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8749,) should be less than time_window_edges: (930,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (298971,) should be less than time_window_edges: (2785,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20635,) should be less than time_window_edges: (2700,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "(n_neurons = 58, n_all_epoch_timebins = 3783)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 12, 11, 473089)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 12, 24, 226398)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 12, 46, 75005)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 12, 58, 872762)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "(n_neurons = 27, n_all_epoch_timebins = 766)\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 27, n_all_epoch_timebins = 766)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/20231017211444-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:11.512146\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_17-29-30'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff22add0340>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1fe97a3c0>) while trying to build the session HDF output for kdiba_vvp01_one_2006-4-09_17-29-30\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-09_17-29-30...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.one.2006-4-10_12-25-50\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-10_12-25-50 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.one.2006-4-10_12-25-50.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-10_12-25-50\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-09_17-29-30.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-09_17-29-30.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-12_16-53-46.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-07_16-40-19.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-12_15-55-31.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Saving updated position results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.position.npy... 2006-4-10_12-25-50.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.interpolated_spike_positions.npy... 2006-4-10_12-25-50.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "Loading success: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.mua.npy... 2006-4-10_12-25-50.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.pbe.npy... 2006-4-10_12-25-50.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t curr_replays: 60\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 883.897131568141)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 883.897131568141, end: 1413.3991723447689)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1413.3991723447689)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (160181,) should be less than time_window_edges: (1769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11410,) should be less than time_window_edges: (1738,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (88598,) should be less than time_window_edges: (1060,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8854,) should be less than time_window_edges: (882,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (248779,) should be less than time_window_edges: (2828,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20264,) should be less than time_window_edges: (2795,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/20231017211637-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl'\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (3915.1977853775024 MB) > new_temporary_file_size_MB (3915.1977825164795 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/backup-20231017211711-loadedSessPickle.pkl.bak'\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "moving new output at '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/20231017211706-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl'\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl... _perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\tmoving new output at '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/20231017211727-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:01:06.953724\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_22-24-40'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1b7c2f300>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1f14fa800>) while trying to build the session HDF output for kdiba_gor01_two_2006-6-09_22-24-40\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-09_22-24-40...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.two.2006-4-09_16-40-54\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-09_16-40-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-09_16-40-54.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-09_16-40-54\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-6-09_22-24-40.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-6-09_22-24-40.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-6-08_21-16-25.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-6-08_14-26-15.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Saving updated position results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.position.npy... 2006-4-09_16-40-54.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.interpolated_spike_positions.npy... 2006-4-09_16-40-54.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "Loading success: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.mua.npy... 2006-4-09_16-40-54.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.pbe.npy... 2006-4-09_16-40-54.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t curr_replays: 70\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1155.7064689620165)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1155.7064689620165, end: 1724.0331400701689)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1724.0331400701689)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (218208,) should be less than time_window_edges: (2313,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9650,) should be less than time_window_edges: (1729,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (100496,) should be less than time_window_edges: (1138,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6829,) should be less than time_window_edges: (983,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (318704,) should be less than time_window_edges: (3449,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16479,) should be less than time_window_edges: (2840,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/20231017212037-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl'\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (160181,) should be less than time_window_edges: (1769,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11410,) should be less than time_window_edges: (1738,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (88598,) should be less than time_window_edges: (1060,)!\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8854,) should be less than time_window_edges: (882,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (248779,) should be less than time_window_edges: (2828,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20264,) should be less than time_window_edges: (2795,)!\n",
      "Saving (file mode 'None') saved session pickle file results : None... \t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "done.\n",
      "WARNING: prev_extant_file_size_MB (3843.9136123657227 MB) > new_temporary_file_size_MB (3843.9136095046997 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/backup-20231017212105-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/20231017212100-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl'\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 21, 25, 854264)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... _execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 21, 35, 82635)}using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 21, 59, 672576)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 22, 8, 641174)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 26, n_all_epoch_timebins = 321)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 26, n_all_epoch_timebins = 321)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2501: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1545: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1545: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/20231017212312-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:08.925482\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-25-50'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-10_12-25-50...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.vvp01.two.2006-4-10_12-58-3\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-10_12-58-3 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.vvp01.two.2006-4-10_12-58-3.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-10_12-58-3\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-25-50.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-25-50.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-09_17-29-30.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-09_17-29-30.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-12_16-53-46.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-07_16-40-19.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-12_15-55-31.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.epochs_info.mat...done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 3 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.position.npy... 2006-4-10_12-58-3.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.interpolated_spike_positions.npy... 2006-4-10_12-58-3.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "Loading success: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.mua.npy... 2006-4-10_12-58-3.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.pbe.npy... 2006-4-10_12-58-3.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t curr_replays: 109\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 932.8262060632987)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 932.8262060632987, end: 1458.5390641578997)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1458.5390641578997)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (127947,) should be less than time_window_edges: (1867,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9325,) should be less than time_window_edges: (1785,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (77409,) should be less than time_window_edges: (1053,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7744,) should be less than time_window_edges: (939,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (218208,) should be less than time_window_edges: (2313,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9650,) should be less than time_window_edges: (1729,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (100496,) should be less than time_window_edges: (1138,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6829,) should be less than time_window_edges: (983,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (318704,) should be less than time_window_edges: (3449,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16479,) should be less than time_window_edges: (2840,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 25, 15, 382689)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 25, 24, 9067)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (205356,) should be less than time_window_edges: (2919,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (17069,) should be less than time_window_edges: (2843,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/20231017212527-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl'\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 25, 42, 939153)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 25, 51, 393543)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (3619.2732486724854 MB) > new_temporary_file_size_MB (3619.2732458114624 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/backup-20231017212556-loadedSessPickle.pkl.bak'\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 23, n_all_epoch_timebins = 317)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 23, n_all_epoch_timebins = 317)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving new output at '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/20231017212551-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmoving new output at '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/20231017212632-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:07.547777\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_16-40-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1f1a1bcc0>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1f73a2200>) while trying to build the session HDF output for kdiba_vvp01_two_2006-4-09_16-40-54\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-09_16-40-54...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.11-02_17-46-44\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_17-46-44 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_17-46-44.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_17-46-44\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-4-09_16-40-54.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-4-09_16-40-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-6-09_22-24-40.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-6-09_22-24-40.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-6-08_21-16-25.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-6-08_14-26-15.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.position.npy... 11-02_17-46-44.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.interpolated_spike_positions.npy... 11-02_17-46-44.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "externally computed ripple_df.pkl not found. Falling back to .ripple.npy...\n",
      "Loading success: .ripple.npy.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.mua.npy... 11-02_17-46-44.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.pbe.npy... 11-02_17-46-44.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t curr_replays: 293\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1144.228403621622)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1144.228403621622, end: 1941.4332220000001)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1941.4332220000001)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (172300,) should be less than time_window_edges: (2290,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7381,) should be less than time_window_edges: (1936,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (148480,) should be less than time_window_edges: (1596,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8102,) should be less than time_window_edges: (1460,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (320780,) should be less than time_window_edges: (3884,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15483,) should be less than time_window_edges: (3742,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/20231017212819-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl'\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (3148.973147392273 MB) > new_temporary_file_size_MB (3148.97314453125 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/backup-20231017212846-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/20231017212842-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl'\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (127947,) should be less than time_window_edges: (1867,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9325,) should be less than time_window_edges: (1785,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (77409,) should be less than time_window_edges: (1053,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7744,) should be less than time_window_edges: (939,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (205356,) should be less than time_window_edges: (2919,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (17069,) should be less than time_window_edges: (2843,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 29, 56, 526077)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 30, 7, 233585)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 30, 34, 134078)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 30, 46, 336329)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 28, n_all_epoch_timebins = 629)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 28, n_all_epoch_timebins = 629)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/20231017213228-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:10.163145\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-58-3'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1d7e39380>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1db868bc0>) while trying to build the session HDF output for kdiba_vvp01_two_2006-4-10_12-58-3\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-10_12-58-3...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.11-02_19-28-0\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_19-28-0 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-02_19-28-0.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_19-28-0\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-4-10_12-58-3.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-4-10_12-58-3.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-4-10_12-25-50.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-4-10_12-25-50.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-4-09_17-29-30.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-4-09_17-29-30.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-12_16-53-46.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-07_16-40-19.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-12_15-55-31.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.position.npy... 11-02_19-28-0.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.interpolated_spike_positions.npy... 11-02_19-28-0.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "externally computed ripple_df.pkl not found. Falling back to .ripple.npy...\n",
      "Loading success: .ripple.npy.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.mua.npy... 11-02_19-28-0.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.pbe.npy... 11-02_19-28-0.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 161\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 787.5080421553757)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 787.5080421553757, end: 1177.7605639999983)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1177.7605639999983)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t done.using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (172300,) should be less than time_window_edges: (2290,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\t spikes_df[time_variable_name]: (7381,) should be less than time_window_edges: (1936,)!\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (148480,) should be less than time_window_edges: (1596,)!\n",
      "WARNING: PREVIOUSLY ASSERT: using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t spikes_df[time_variable_name]: (8102,) should be less than time_window_edges: (1460,)!\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (320780,) should be less than time_window_edges: (3884,)!\n",
      "WARNING: PREVIOUSLY ASSERT: using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t spikes_df[time_variable_name]: (15483,) should be less than time_window_edges: (3742,)!\n",
      "\n",
      "\t done.\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (155661,) should be less than time_window_edges: (1577,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11393,) should be less than time_window_edges: (1486,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 33, 17, 130007)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (78036,) should be less than time_window_edges: (782,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10793,) should be less than time_window_edges: (746,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 33, 28, 180154)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (233697,) should be less than time_window_edges: (2357,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (22186,) should be less than time_window_edges: (2262,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl... _execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 33, 43, 429693)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/20231017213341-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl'\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... _execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 33, 53, 409766)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "done.\n",
      "WARNING: prev_extant_file_size_MB (1813.6466655731201 MB) > new_temporary_file_size_MB (1813.6466627120972 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/backup-20231017213354-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/20231017213351-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl'\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 26, n_all_epoch_timebins = 2254)\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 26, n_all_epoch_timebins = 2254)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (155661,) should be less than time_window_edges: (1577,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11393,) should be less than time_window_edges: (1486,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (78036,) should be less than time_window_edges: (782,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10793,) should be less than time_window_edges: (746,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (233697,) should be less than time_window_edges: (2357,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (22186,) should be less than time_window_edges: (2262,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 36, 47, 308925)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl... _execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 36, 51, 632288)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 37, 1, 353279)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/20231017213649-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:14.761201\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 37, 5, 404751)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_17-46-44'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not output time-dependent placefields to .h5. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not output time-dependent placefields to .h5. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not output time-dependent placefields to .h5. Skipping.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_17-46-44...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 26, n_all_epoch_timebins = 1218)\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.11-03_12-3-25\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-03_12-3-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.11-03_12-3-25.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_11-03_12-3-25\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-02_17-46-44.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-02_17-46-44.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-4-09_16-40-54.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-4-09_16-40-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-6-09_22-24-40.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-6-09_22-24-40.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-6-08_21-16-25.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-6-08_14-26-15.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.spikes.mat... done.\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 3 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.position.npy... 11-03_12-3-25.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.interpolated_spike_positions.npy... 11-03_12-3-25.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "externally computed ripple_df.pkl not found. Falling back to .ripple.npy...\n",
      "Loading success: .ripple.npy.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.mua.npy... 11-03_12-3-25.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.pbe.npy... 11-03_12-3-25.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t curr_replays: 50\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 669.0602578192211)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 669.0602578192211, end: 1005.4446971784892)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1005.4446971784892)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (89683,) should be less than time_window_edges: (1340,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9259,) should be less than time_window_edges: (1250,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44874,) should be less than time_window_edges: (674,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6186,) should be less than time_window_edges: (609,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... (n_neurons = 26, n_all_epoch_timebins = 1218)\n",
      "using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "\t done.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2501: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (134557,) should be less than time_window_edges: (2012,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15445,) should be less than time_window_edges: (1877,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl... _generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "\tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/20231017213810-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl'\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n",
      "WARNING: prev_extant_file_size_MB (1429.8324136734009 MB) > new_temporary_file_size_MB (1429.832410812378 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/backup-20231017213820-loadedSessPickle.pkl.bak'\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "moving new output at '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/20231017213818-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl'\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl... _perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... \tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/20231017213827-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl'\n",
      "using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:13.904633\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_19-28-0'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff22d9bf140>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1db2a7380>) while trying to build the session HDF output for kdiba_pin01_one_11-02_19-28-0\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_19-28-0...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"LNX00052.kdiba.pin01.one.fet11-01_12-58-54\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.fet11-01_12-58-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.LNX00052.kdiba.pin01.one.fet11-01_12-58-54.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /media/MAX/Data\n",
      "\tsession_context: kdiba_pin01_one_fet11-01_12-58-54\n",
      "\tsession_basedir: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "__________________________________________________________________\n",
      "basedir: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/11-02_19-28-0.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/11-02_19-28-0.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-4-10_12-58-3.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-4-10_12-58-3.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-4-10_12-25-50.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-4-10_12-25-50.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-4-09_17-29-30.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-4-09_17-29-30.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-12_16-53-46.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-07_16-40-19.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-12_15-55-31.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-09_1-22-43.eeg does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.epochs_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.position_info.mat... done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.spikes.mat... done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.position.npy... fet11-01_12-58-54.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.interpolated_spike_positions.npy... fet11-01_12-58-54.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.replay_info.mat... done.\n",
      "session.replays could not be loaded from .replay_info.mat due to error Reader needs file name or open file-like object. Skipping (will be unavailable)\n",
      "externally computed ripple_df.pkl not found. Falling back to .ripple.npy...\n",
      "Loading success: .ripple.npy.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.mua.npy... fet11-01_12-58-54.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.pbe.npy... fet11-01_12-58-54.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "computing estimated replay epochs for session...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 863\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2057.2259484970764)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 2057.2259484970764, end: 3031.7272470000007)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 3031.7272470000007)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (120546,) should be less than time_window_edges: (4115,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11619,) should be less than time_window_edges: (3522,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (61833,) should be less than time_window_edges: (1950,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6257,) should be less than time_window_edges: (1891,)!\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (89683,) should be less than time_window_edges: (1340,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9259,) should be less than time_window_edges: (1250,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (44874,) should be less than time_window_edges: (674,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6186,) should be less than time_window_edges: (609,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (134557,) should be less than time_window_edges: (2012,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15445,) should be less than time_window_edges: (1877,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 40, 28, 946540)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 40, 32, 36040)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 40, 38, 459700)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (182379,) should be less than time_window_edges: (6063,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (17876,) should be less than time_window_edges: (5997,)!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl..._execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 40, 41, 546916)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      " DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/20231017214041-loadedSessPickle.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 22, n_all_epoch_timebins = 447)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 22, n_all_epoch_timebins = 447)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2501: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('pf_computation', 'maze'), ('pfdt_computation', 'maze'), ('position_decoding', 'maze'), ('pf_dt_sequential_surprise', 'maze'), ('firing_rate_trends', 'maze'), ('long_short_decoding_analyses', 'maze'), ('short_long_pf_overlap_analyses', 'maze'), ('long_short_fr_indicies_analyses', 'maze'), ('jonathan_firing_rate_analysis', 'maze'), ('long_short_post_decoding', 'maze'), ('long_short_inst_spike_rate_groups', 'maze')]. Saving global results...\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl... done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "WARNING: basic pipleine was updated by post_compute_validate and needs to be saved to be correct.Overriding self.save_mode to ensure pipeline is saved!\n",
      "finalized_loaded_sess_pickle_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... \tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/20231017214104-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:05.196290\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-03_12-3-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1d7e265c0>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x7ff1f9740cc0>) while trying to build the session HDF output for kdiba_pin01_one_11-03_12-3-25\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-03_12-3-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "done.\n",
      "WARNING: prev_extant_file_size_MB (4297.963575363159 MB) > new_temporary_file_size_MB (4297.963572502136 MB)! A backup will be made!\n",
      "'/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl' backing up -> to_file: '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/backup-20231017214113-loadedSessPickle.pkl.bak'\n",
      "moving new output at '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/20231017214106-loadedSessPickle.pkl' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl'\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'pf_dt_sequential_surprise', 'position_decoding', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "pf_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pf_computation missing.\n",
      "\t Recomputing pf_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "\t done.\n",
      "pfdt_computation, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "\t done.\n",
      "position_decoding, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "position_decoding missing.\n",
      "\t Recomputing position_decoding...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "pf_dt_sequential_surprise missing.\n",
      "\t Recomputing pf_dt_sequential_surprise...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze already computed.\n",
      "\tforce_recompute is true so recomputing anyway\n",
      "firing_rate_trends missing.\n",
      "\t Recomputing firing_rate_trends...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (120546,) should be less than time_window_edges: (4115,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11619,) should be less than time_window_edges: (3522,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (61833,) should be less than time_window_edges: (1950,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6257,) should be less than time_window_edges: (1891,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (182379,) should be less than time_window_edges: (6063,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (17876,) should be less than time_window_edges: (5997,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 45, 45, 141061)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 45, 55, 56075)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 46, 17, 776203)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x7ff2688b0310>: datetime.datetime(2023, 10, 17, 21, 46, 27, 682089)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 25, n_all_epoch_timebins = 5484)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 25, n_all_epoch_timebins = 5484)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:596: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_pin01_one_fet11-01_12-58-54. error: !! ERROR: time_bin_size_seconds (0.01) > (t_stop - t_start) (0.006266876793233678). Reduce the bin size or exclude this epoch. ::::: (<class 'AssertionError'>, AssertionError('ERROR: time_bin_size_seconds (0.01) > (t_stop - t_start) (0.006266876793233678). Reduce the bin size or exclude this epoch.'), <traceback object at 0x7ff2ddfb6e00>)\n",
      "no changes in global results.\n",
      "Saving mode == ALWAYS so trying to save despite no changes.\n",
      "global_computation_results_pickle_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl\n",
      "Saving (file mode '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl') saved session pickle file results : /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl... \tmoving new output at '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/20231017214950-global_computation_results.pkltmp' -> to desired location: '/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:47.580080\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'fet11-01_12-58-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x7ff227dac2c0>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x7ff12a068a80>) while trying to build the session HDF output for kdiba_pin01_one_fet11-01_12-58-54\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_fet11-01_12-58-54...\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'jonathan_firing_rate_analysis' ::::: (<class 'KeyError'>, KeyError('jonathan_firing_rate_analysis'), <traceback object at 0x7ff1fe330480>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_fet11-01_12-58-54]\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "\n",
    "# multiprocessing_kwargs = dict(use_multiprocessing=False, num_processes=1)\n",
    "multiprocessing_kwargs = dict(use_multiprocessing=True, num_processes=2)\n",
    "  \n",
    "# Whether to output figures:\n",
    "should_perform_figure_generation_to_file=False\n",
    "# should_perform_figure_generation_to_file=True\n",
    "\n",
    "## Included Session Contexts:\n",
    "# included_session_contexts = batch_progress_df[np.logical_and(batch_progress_df['has_user_replay_annotations'], batch_progress_df['is_ready'])]['context'].to_numpy().tolist()\n",
    "\n",
    "# Only require sessions to have replay annotations:\n",
    "# included_session_contexts = batch_progress_df[batch_progress_df['has_user_replay_annotations']]['context'].to_numpy().tolist()\n",
    "\n",
    "# included_session_contexts = batch_progress_df['context'].to_numpy().tolist()[:4] # Only get the first 6\n",
    "# Limit the contexts to run to the last N:\n",
    "# included_session_contexts = included_session_contexts[3:5]\n",
    "\n",
    "# included_session_contexts = [included_session_contexts[3]]\n",
    "\n",
    "# ALL\n",
    "included_session_contexts = included_session_contexts\n",
    "\n",
    "# ## No filtering the sessions:\n",
    "# included_session_contexts = None\n",
    "\n",
    "if included_session_contexts is not None:\n",
    "    print(f'len(included_session_contexts): {len(included_session_contexts)}')\n",
    "else:\n",
    "    print(f'included_session_contexts is None so all session contexts will be included.')\n",
    "\n",
    "# included_session_contexts\n",
    "\n",
    "# # No recomputing at all:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "# # No Reloading\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "# Forced Reloading:\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=True,\n",
    "                                                session_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "                                                global_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "                                                should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE, force_global_recompute=True,\n",
    "                                                **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "active_post_run_callback_fn = result_handler.on_complete_success_execution_session\n",
    "# active_post_run_callback_fn = _temp_on_complete_success_execution_session\n",
    "\n",
    "\n",
    "# @result_handler.completion_decorator\n",
    "# def a_test_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
    "#     # print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#     print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "#     print(f'a_test_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...,across_session_results_extended_dict: {across_session_results_extended_dict})')\n",
    "#     long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "#     # long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "#     # long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "#     # Get existing laps from session:\n",
    "#     long_laps, short_laps, global_laps = [curr_active_pipeline.filtered_sessions[an_epoch_name].laps.as_epoch_obj() for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "#     long_replays, short_replays, global_replays = [Epoch(curr_active_pipeline.filtered_sessions[an_epoch_name].replay.epochs.get_valid_df()) for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "#     long_PBEs, short_PBEs, global_PBEs = [curr_active_pipeline.filtered_sessions[an_epoch_name].pbe for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "\n",
    "#     # jonathan_firing_rate_analysis_result = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "#     # (epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "#     # neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "#     # ## Output the BatchPhoJonathanFiguresHelper\n",
    "#     # fig_1c_figures_all_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df.sort_values('custom_frs_index', ascending=True, inplace=False), included_unit_neuron_IDs=None,\n",
    "# \t# n_max_page_rows=20, write_vector_format=False, write_png=True,\n",
    "# \t# show_only_refined_cells=False, disable_top_row=False, split_by_short_long_shared=False)\n",
    "\n",
    "#     output_file_prefix = curr_session_context.get_description(separator=\"|\", include_property_names=False)\n",
    "#     # global_replays.filename = Path(f\"output/{output_file_prefix}_global_replays\").resolve()\n",
    "#     # print(f'global_replays.filename: {global_replays.filename}')\n",
    "#     # global_replays.to_neuroscope()\n",
    "\n",
    "#     # global_PBEs.filename = Path(f\"output/{output_file_prefix}_global_PBEs\").resolve()\n",
    "#     # print(f'global_PBEs.filename: {global_PBEs.filename}')\n",
    "#     # global_PBEs.to_neuroscope('PBE')\n",
    "\n",
    "#     joined_neruon_fri_df = build_merged_neuron_firing_rate_indicies(curr_active_pipeline, enable_display_intermediate_results=False)\n",
    "#     AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df')\n",
    "#     print(f'>>\\t done with {output_file_prefix}')\n",
    "#     print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#     print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "#     return across_session_results_extended_dict\n",
    "\n",
    "\n",
    "# result_handler.completion_functions.append(a_test_completion_function)\n",
    "\n",
    "## Specific Setup for 2023-09-28 Changes to LxC/SxC \"refinements\"\n",
    "result_handler.extended_computations_include_includelist = ['pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "                                                'pf_dt_sequential_surprise',\n",
    "                                                # 'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                #'position_decoding_two_step', \n",
    "                                                'long_short_decoding_analyses',\n",
    "                                                'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                                'long_short_inst_spike_rate_groups',\n",
    "                                                # 'long_short_endcap_analysis',\n",
    "                                                ]\n",
    "\n",
    "\n",
    "basic_local_computations = ['pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "#                                                 'pf_dt_sequential_surprise',\n",
    "                                                # 'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                #'position_decoding_two_step', \n",
    "                                                ]\n",
    " \n",
    "# result_handler.extended_computations_include_includelist = ['long_short_inst_spike_rate_groups']\n",
    "\n",
    "\n",
    "result_handler.enable_hdf5_output = True # output the HDF5 when done.\n",
    "# result_handler.override_existing_frs_index_values = True\n",
    "# result_handler.frs_index_inclusion_magnitude = 0.1\n",
    "\n",
    "# result_handler.enable_hdf5_output = False\n",
    "result_handler.override_existing_frs_index_values = False\n",
    "\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "global_batch_run.execute_all(force_reload=result_handler.force_reload_all, saving_mode=result_handler.saving_mode, skip_extended_batch_computations=True, post_run_callback_fn=active_post_run_callback_fn,\n",
    "                             fail_on_exception=False, included_session_contexts=included_session_contexts,\n",
    "                                                                                        **{'computation_functions_name_includelist': basic_local_computations,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}, **multiprocessing_kwargs) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f130e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving (file mode '/media/MAX/Data/global_batch_result_2023-10-17_Lab.pkl') saved session pickle file results : /media/MAX/Data/global_batch_result_2023-10-17_Lab.pkl... \tmoving new output at '/media/MAX/Data/20231017215021-global_batch_result_2023-10-17_Lab.pkltmp' -> to desired location: '/media/MAX/Data/global_batch_result_2023-10-17_Lab.pkl'\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_14-26-15'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_1-22-43'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_15-55-31'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-07_16-40-19'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_21-16-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_22-24-40'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_16-53-46'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_17-29-30'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-25-50'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_16-40-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-58-3'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_17-46-44'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_19-28-0'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-03_12-3-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done outputting HDF file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'fet11-01_12-58-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "# Save to pickle:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0981cde1",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>40</td>\n",
       "      <td>354</td>\n",
       "      <td>40</td>\n",
       "      <td>272</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>46</td>\n",
       "      <td>235</td>\n",
       "      <td>40</td>\n",
       "      <td>180</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>76</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>kdiba_gor01_two_2006-6-07_16-40-19</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>42</td>\n",
       "      <td>287</td>\n",
       "      <td>40</td>\n",
       "      <td>446</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>kdiba_gor01_two_2006-6-08_21-16-25</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>40</td>\n",
       "      <td>62</td>\n",
       "      <td>40</td>\n",
       "      <td>77</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>kdiba_gor01_two_2006-6-09_22-24-40</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 22:24:40</td>\n",
       "      <td>51</td>\n",
       "      <td>155</td>\n",
       "      <td>43</td>\n",
       "      <td>568</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>kdiba_gor01_two_2006-6-12_16-53-46</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 16:53:46</td>\n",
       "      <td>41</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>58</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-09_17-29-30</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-09 17:29:30</td>\n",
       "      <td>51</td>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>80</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-10_12-25-50</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-10 12:25:50</td>\n",
       "      <td>50</td>\n",
       "      <td>34</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-09_16-40-54</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-09 16:40:54</td>\n",
       "      <td>48</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-10_12-58-3</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>40</td>\n",
       "      <td>75</td>\n",
       "      <td>42</td>\n",
       "      <td>31</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>kdiba_pin01_one_11-02_17-46-44</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>54</td>\n",
       "      <td>137</td>\n",
       "      <td>66</td>\n",
       "      <td>156</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>kdiba_pin01_one_11-02_19-28-0</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>56</td>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>kdiba_pin01_one_11-03_12-3-25</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>kdiba_pin01_one_fet11-01_12-58-54</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>315</td>\n",
       "      <td>584</td>\n",
       "      <td>163</td>\n",
       "      <td>278</td>\n",
       "      <td>True</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...</td>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "6        kdiba  gor01        two  2006-6-07_16-40-19   \n",
       "8        kdiba  gor01        two  2006-6-08_21-16-25   \n",
       "9        kdiba  gor01        two  2006-6-09_22-24-40   \n",
       "10       kdiba  gor01        two  2006-6-12_16-53-46   \n",
       "12       kdiba  vvp01        one  2006-4-09_17-29-30   \n",
       "13       kdiba  vvp01        one  2006-4-10_12-25-50   \n",
       "31       kdiba  vvp01        two  2006-4-09_16-40-54   \n",
       "32       kdiba  vvp01        two   2006-4-10_12-58-3   \n",
       "52       kdiba  pin01        one      11-02_17-46-44   \n",
       "53       kdiba  pin01        one       11-02_19-28-0   \n",
       "54       kdiba  pin01        one       11-03_12-3-25   \n",
       "64       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "\n",
       "                               context  \\\n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "6   kdiba_gor01_two_2006-6-07_16-40-19   \n",
       "8   kdiba_gor01_two_2006-6-08_21-16-25   \n",
       "9   kdiba_gor01_two_2006-6-09_22-24-40   \n",
       "10  kdiba_gor01_two_2006-6-12_16-53-46   \n",
       "12  kdiba_vvp01_one_2006-4-09_17-29-30   \n",
       "13  kdiba_vvp01_one_2006-4-10_12-25-50   \n",
       "31  kdiba_vvp01_two_2006-4-09_16-40-54   \n",
       "32   kdiba_vvp01_two_2006-4-10_12-58-3   \n",
       "52      kdiba_pin01_one_11-02_17-46-44   \n",
       "53       kdiba_pin01_one_11-02_19-28-0   \n",
       "54       kdiba_pin01_one_11-03_12-3-25   \n",
       "64   kdiba_pin01_one_fet11-01_12-58-54   \n",
       "\n",
       "                                             basedirs  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "9   /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...   \n",
       "10  /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...   \n",
       "12  /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...   \n",
       "13  /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...   \n",
       "31  /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3   \n",
       "52     /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44   \n",
       "53      /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0   \n",
       "54      /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54   \n",
       "\n",
       "                            status errors    session_datetime  n_long_laps  \\\n",
       "1   SessionBatchProgress.COMPLETED   None 2006-06-08 14:26:15           40   \n",
       "2   SessionBatchProgress.COMPLETED   None 2006-06-09 01:22:43           46   \n",
       "4   SessionBatchProgress.COMPLETED   None 2006-06-12 15:55:31           40   \n",
       "6   SessionBatchProgress.COMPLETED   None 2006-06-07 16:40:19           42   \n",
       "8   SessionBatchProgress.COMPLETED   None 2006-06-08 21:16:25           40   \n",
       "9   SessionBatchProgress.COMPLETED   None 2006-06-09 22:24:40           51   \n",
       "10  SessionBatchProgress.COMPLETED   None 2006-06-12 16:53:46           41   \n",
       "12  SessionBatchProgress.COMPLETED   None 2006-04-09 17:29:30           51   \n",
       "13  SessionBatchProgress.COMPLETED   None 2006-04-10 12:25:50           50   \n",
       "31  SessionBatchProgress.COMPLETED   None 2006-04-09 16:40:54           48   \n",
       "32  SessionBatchProgress.COMPLETED   None 2006-04-10 12:58:03           40   \n",
       "52  SessionBatchProgress.COMPLETED   None 2009-11-02 17:46:44           54   \n",
       "53  SessionBatchProgress.COMPLETED   None 2009-11-02 19:28:00           56   \n",
       "54  SessionBatchProgress.COMPLETED   None 2009-11-03 12:03:25           50   \n",
       "64  SessionBatchProgress.COMPLETED   None 2009-11-01 12:58:54          315   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "1              354            40              272      True   \n",
       "2              235            40              180      True   \n",
       "4               57            34               76      True   \n",
       "6              287            40              446      True   \n",
       "8               62            40               77      True   \n",
       "9              155            43              568      True   \n",
       "10              69            40               58      True   \n",
       "12              78            42               80      True   \n",
       "13              34            42               26      True   \n",
       "31              44            50               26      True   \n",
       "32              75            42               31      True   \n",
       "52             137            66              156      True   \n",
       "53             110            50               51      True   \n",
       "54              32            46               18      True   \n",
       "64             584           163              278      True   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "9   /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...   \n",
       "10  /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...   \n",
       "12  /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...   \n",
       "13  /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...   \n",
       "31  /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...   \n",
       "52  /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...   \n",
       "53  /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...   \n",
       "54  /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...   \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "9   /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...   \n",
       "10  /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...   \n",
       "12  /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...   \n",
       "13  /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...   \n",
       "31  /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...   \n",
       "52  /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...   \n",
       "53  /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...   \n",
       "54  /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...   \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "1   /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "2   /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "4   /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-5...   \n",
       "6   /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-4...   \n",
       "8   /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "9   /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-2...   \n",
       "10  /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...   \n",
       "12  /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-2...   \n",
       "13  /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...   \n",
       "31  /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-4...   \n",
       "32  /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-5...   \n",
       "52  /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...   \n",
       "53  /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/...   \n",
       "54  /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/...   \n",
       "64  /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58...   \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "4                          True                                  True  \n",
       "6                          True                                  True  \n",
       "8                          True                                  True  \n",
       "9                          True                                  True  \n",
       "10                         True                                  True  \n",
       "12                         True                                  True  \n",
       "13                         True                                  True  \n",
       "31                         True                                  True  \n",
       "32                         True                                  True  \n",
       "52                         True                                  True  \n",
       "53                         True                                  True  \n",
       "54                         True                                  True  \n",
       "64                         True                                  True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Across Sessions After Batching Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dce885b-5b99-4a7a-9f72-2eed2e45ae18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_batch_progress_df = included_session_batch_progress_df.copy()\n",
    "\n",
    "good_session_concrete_folders = [ConcreteSessionFolder(a_context, a_basedir) for a_context, a_basedir in zip(list(a_batch_progress_df.context.values), list(a_batch_progress_df.basedirs.values))]\n",
    "\n",
    "# good_only_batch_progress_df.batch_results\n",
    "# included_h5_paths = [get_file_str_if_file_exists(v.joinpath('output','pipeline_results.h5').resolve()) for v in list(good_only_batch_progress_df.basedirs.values)]\n",
    "# included_h5_paths = [a_dir.joinpath('output','pipeline_results.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']]\n",
    "included_h5_paths = [get_file_str_if_file_exists(v.pipeline_results_h5) for v in good_session_concrete_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39738fa-f8f3-46d2-a937-0fb08c0ccb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_dir = Path('output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-10-03').resolve()\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, target_dir=target_dir)\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-05', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-06', only_include_file_types=['local_pkl', 'global_pkl'])\n",
    "copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-07', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "copy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cb2d8-65b3-405b-a41b-22b2fa7cb28e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_dict_h5_files = copy_movedict(copy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243e3954-15a4-449c-b927-56d5d79153c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /media/MAX/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>modification_time</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>file_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...</td>\n",
       "      <td>2023-10-17 20:35:39.254519</td>\n",
       "      <td>2023-10-17 20:35:39.254519</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...</td>\n",
       "      <td>2023-10-17 20:27:45.640954</td>\n",
       "      <td>2023-10-17 20:27:45.640954</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...</td>\n",
       "      <td>2023-10-17 20:48:24.275403</td>\n",
       "      <td>2023-10-17 20:48:24.275403</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...</td>\n",
       "      <td>2023-10-17 21:05:00.868580</td>\n",
       "      <td>2023-10-17 21:05:00.868580</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...</td>\n",
       "      <td>2023-10-17 21:23:38.433948</td>\n",
       "      <td>2023-10-17 21:23:38.433948</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...</td>\n",
       "      <td>2023-10-17 21:37:24.787964</td>\n",
       "      <td>2023-10-17 21:37:24.787964</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  /media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-2...   \n",
       "1  /media/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22...   \n",
       "2  /media/MAX/Data/KDIBA/gor01/two/2006-6-08_21-1...   \n",
       "3  /media/MAX/Data/KDIBA/gor01/two/2006-6-12_16-5...   \n",
       "4  /media/MAX/Data/KDIBA/vvp01/one/2006-4-10_12-2...   \n",
       "5  /media/MAX/Data/KDIBA/pin01/one/11-02_17-46-44...   \n",
       "\n",
       "           modification_time              creation_time  file_size  \n",
       "0 2023-10-17 20:35:39.254519 2023-10-17 20:35:39.254519       1.55  \n",
       "1 2023-10-17 20:27:45.640954 2023-10-17 20:27:45.640954       1.26  \n",
       "2 2023-10-17 20:48:24.275403 2023-10-17 20:48:24.275403       0.90  \n",
       "3 2023-10-17 21:05:00.868580 2023-10-17 21:05:00.868580       0.42  \n",
       "4 2023-10-17 21:23:38.433948 2023-10-17 21:23:38.433948       0.41  \n",
       "5 2023-10-17 21:37:24.787964 2023-10-17 21:37:24.787964       0.59  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_output_h5_files(included_h5_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0953",
   "metadata": {},
   "source": [
    "## Extract `across_sessions_instantaneous_fr_dict` from the computation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39691fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sessions: 14\n"
     ]
    }
   ],
   "source": [
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "across_sessions_recomputed_instantaneous_fr_dict = {}\n",
    "\n",
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "        a_good_recomp_result = a_result.across_session_results.get('recomputed_inst_fr_comps', None)\n",
    "        if a_good_recomp_result is not None:\n",
    "            across_sessions_recomputed_instantaneous_fr_dict[a_ctxt] = a_good_recomp_result\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_sessions_recomputed_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09136799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_batch_result_inst_fr_file_path: /media/MAX/Data/across_session_result_long_short_recomputed_inst_firing_rate_2023-10-17_Lab.pkl\n",
      "Saving (file mode 'None') saved session pickle file results : None... done.\n"
     ]
    }
   ],
   "source": [
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "# AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "#                                                  inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_recomputed_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "                                                 inst_fr_output_filename=f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "# neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True, )\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_and_save_all_combined_tables(included_session_contexts, included_h5_paths, override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'{BATCH_DATE_TO_USE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a859bff-8cdb-4281-a64f-251d24db7cb9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True)\n",
    "# neuron_replay_stats_table['is_refined_LxC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b2430-c9ec-4adb-81a8-1ffbf8a0cb3c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95013ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_identities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(neuron_replay_stats_table['is_refined_LxC'])\n",
    "# np.isnan(neuron_replay_stats_table['is_refined_LxC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "# _out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "# _out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership','is_refined_LxC', 'is_refined_SxC'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'is_refined_LxC', 'is_refined_SxC', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0cd9-3e60-4425-9351-dfc903f3f067",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import convert_filelist_to_new_parent\n",
    "\n",
    "def save_filelist_to_text_file(hdf5_output_paths, filelist_path: Path):\n",
    "    _out_string = '\\n'.join([str(a_file) for a_file in hdf5_output_paths])\n",
    "    print(f'{_out_string}')\n",
    "    print(f'saving out to \"{filelist_path}\"...')\n",
    "    with open(filelist_path, 'w') as f:\n",
    "        f.write(_out_string)\n",
    "    return _out_string, filelist_path\n",
    "\n",
    "# Save output filelist:\n",
    "\n",
    "# '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'\n",
    "\n",
    "# kdiba_vvp01_two_2006-4-10_12-58-3\n",
    "# \toutputs_local ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl')}\n",
    "# \toutputs_global ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'), 'hdf5': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5')}\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths = global_batch_run.build_output_files_lists()\n",
    "\n",
    "h5_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_HDF5_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_HDF5_savepath = save_filelist_to_text_file(hdf5_output_paths, h5_filelist_path)\n",
    "\n",
    "pkls_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_pkls_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_pkls_savepath = save_filelist_to_text_file(pkl_output_paths, pkls_filelist_path)\n",
    "\n",
    "# source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "source_parent_path = Path(r'/nfs/turbo/umms-kdiba/Data')\n",
    "dest_parent_path = Path(r'/~/W/Data/')\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_source = hdf5_output_paths\n",
    "filelist_dest_paths = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest_paths\n",
    "\n",
    "dest_Apogee_h5_filelist_path = global_data_root_parent_path.joinpath(f'dest_fileList_Apogee_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, dest_filelist_savepath = save_filelist_to_text_file(filelist_dest_paths, dest_Apogee_h5_filelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e69a3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "file_path\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ce774-98fb-4e69-a7e2-e94cbff1b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "\n",
    "# list(global_batch_run.session_batch_outputs.keys())\n",
    "\n",
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "# good_session_batch_outputs = global_batch_run.session_batch_outputs\n",
    "\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d19a7-5a89-43f1-aa33-3a7450d1f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178426c-54df-47ac-8103-a66f114c77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[a_ctxt.get_initialization_code_string() for a_ctxt in sessions_with_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28828512",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d9a7",
   "metadata": {},
   "source": [
    "# 2023-10-06 - `joined_neruon_fri_df` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE = '2023-10-05_NewParameters'\n",
    "BATCH_DATE_TO_USE = '2023-10-07'\n",
    "all_sessions_joined_neruon_fri_df, out_path = build_and_merge_all_sessions_joined_neruon_fri_df(global_data_root_parent_path, BATCH_DATE_TO_USE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_neruon_fri_df_basename = f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df'\n",
    "AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=joined_neruon_fri_df_basename, include_csv=False)\n",
    "print(f'>>\\t done with {output_file_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c4c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-10-04 - Load Saved across-sessions-data and testing Batch-computed inst_firing_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PaperFigureTwo, InstantaneousSpikeRateGroupsComputation\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import list_of_dicts_to_dict_of_lists\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34549c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved across-session results:\n",
    "# inst_fr_output_filename = 'long_short_inst_firing_rate_result_handlers_2023-07-12.pkl'\n",
    "# inst_fr_output_filename = 'across_session_result_long_short_inst_firing_rate.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-07-21.pkl'\n",
    "# inst_fr_output_filename=f'across_session_result_handler_{BATCH_DATE_TO_USE}.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-08-09_Test.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-10-04-GL.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-04-GL-Recomp.pkl'\n",
    "inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-07.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abea5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionTables\n",
    " \n",
    "## Load all across-session tables from the pickles:\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.load_all_combined_tables(override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'2023-10-07') # output_path_suffix=f'2023-10-04-GL-Recomp'\n",
    "num_sessions = len(neuron_replay_stats_table.session_uid.unique().to_numpy())\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager, SessionCellExclusivityRecord\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "# for a_ctx, a_val in annotation_man.get_hardcoded_specific_session_override_dict().items():\n",
    "# \tannotation_man.annotations[a_ctx] = a_val\n",
    "\n",
    "# for a_ctx, a_val in UserAnnotationsManager.get_user_annotations().items():\n",
    "# \tannotation_man.annotations[a_ctx] = a_val\n",
    "\n",
    "# for a_ctx, a_val in session_cell_exclusivity_annotations.items():\n",
    "# \t# Not ideal. Adds a key 'session_cell_exclusivity' to the extant session context instead of being indexable by an entirely new context\n",
    "# \tannotation_man.annotations[a_ctx] = annotation_man.annotations.get(a_ctx, {}) | dict(session_cell_exclusivity=a_val)\n",
    "# \t# annotation_man.annotations[a_ctx.overwriting_context(user_annotation='session_cell_exclusivity')] = a_val\n",
    "\n",
    "annotation_man = UserAnnotationsManager()\n",
    "\n",
    "LxC_uids = []\n",
    "SxC_uids = []\n",
    "\n",
    "for a_ctxt in included_session_contexts:\n",
    "\tsession_uid = a_ctxt.get_description(separator=\"|\", include_property_names=False)\n",
    "\tsession_uid\n",
    "\tsession_cell_exclusivity: SessionCellExclusivityRecord = annotation_man.annotations[a_ctxt].get('session_cell_exclusivity', None)\n",
    "\tLxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.LxC])\n",
    "\tSxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.SxC])\n",
    "\t\n",
    "# [a_ctxt.get_description(separator=\"|\", include_property_names=False) for a_ctxt in included_session_contexts]\n",
    "\n",
    "long_short_fr_indicies_analysis_table['XxC_status'] = 'Shared'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, LxC_uids), 'XxC_status'] = 'LxC'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, SxC_uids), 'XxC_status'] = 'SxC'\n",
    "\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Get the long peak location\n",
    "\n",
    "long_short_fr_indicies_analysis_table['long_pf_peak_x'] = neuron_replay_stats_table['long_pf_peak_x']\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1641aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='x_frs_index', title='Pf Peak position vs. LapsFRI', ylabel='Lap FRI')\n",
    "\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='y_frs_index', title='Pf Peak position vs. ReplayFRI', ylabel='Replay FRI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9987dc",
   "metadata": {},
   "source": [
    " #TODO 2023-10-05 11:40: - [ ] Extract the \"contrarian cells\", the ones that have a strong exclusivity on the laps but the opposite tendency on the replays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_short_fr_indicies_analysis_table.to_csv('output/2023-10-07_long_short_fr_indicies_analysis_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427597f",
   "metadata": {},
   "source": [
    "# 2023-10-10 - Statistics for `across_sessions_bar_graphs`, analysing `across_session_inst_fr_computation` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33422d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import pho_stats_perform_diagonal_line_binomial_test, pho_stats_bar_graph_t_tests\n",
    "\n",
    "binom_test_chance_result = pho_stats_perform_diagonal_line_binomial_test(long_short_fr_indicies_analysis_table)\n",
    "print(f'binom_test_chance_result: {binom_test_chance_result}')\n",
    "\n",
    "LxC_Laps_T_result, SxC_Laps_T_result, LxC_Replay_T_result, SxC_Replay_T_result = pho_stats_bar_graph_t_tests(across_session_inst_fr_computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea5a5",
   "metadata": {},
   "source": [
    "## 2023-10-04 - Run `AcrossSessionsVisualizations` corresponding to the PhoDibaPaper2023 figures for all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1152c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \n",
    "global_multi_session_context, _out_aggregate_fig_2 = AcrossSessionsVisualizations.across_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels=False, enable_hover_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_long_and_short_firing_rate_replays_v_laps_figure(neuron_replay_stats_table=neuron_replay_stats_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_man = UserAnnotationsManager()\n",
    "included_annotations = {ctxt:ann_man.annotations[ctxt].get('session_cell_exclusivity', None) for ctxt in included_session_contexts}\n",
    "\n",
    "all_LxCs = []\n",
    "all_SxCs = []\n",
    "\n",
    "for ctxt, an_ann in included_annotations.items():\n",
    "\tsession_ctxt_key:str = ctxt.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "\tall_LxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.LxC])\n",
    "\tall_SxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.SxC])\n",
    "\t\n",
    "all_LxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1222a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_SxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381fcf5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "across_session_inst_fr_computation.LxC_scatter_props\n",
    "across_session_inst_fr_computation.SxC_scatter_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate across all of the sessions to build a new combined `InstantaneousSpikeRateGroupsComputation`, which can be used to plot the \"PaperFigureTwo\", bar plots for many sessions.\n",
    "global_multi_session_context = IdentifyingContext(format_name='kdiba', num_sessions=num_sessions) # some global context across all of the sessions, not sure what to put here.\n",
    "\n",
    "# To correctly aggregate results across sessions, it only makes sense to combine entries at the `.cell_agg_inst_fr_list` variable and lower (as the number of cells can be added across sessions, treated as unique for each session).\n",
    "\n",
    "## Display the aggregate across sessions:\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # WARNING: we didn't save this info\n",
    "_out_fig_2.computation_result = across_session_inst_fr_computation # the result loaded from the file\n",
    "_out_fig_2.active_identifying_session_ctx = across_session_inst_fr_computation.active_identifying_session_ctx\n",
    "# Set callback, the only self-specific property\n",
    "# _out_fig_2._pipeline_file_callback_fn = curr_active_pipeline.output_figure # lambda args, kwargs: self.write_to_file(args, kwargs, curr_active_pipeline)\n",
    "_out_fig_2.scatter_props_fn = _return_scatter_props_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LxC_aclus = _out_fig_2.computation_result.LxC_aclus\n",
    "SxC_aclus = _out_fig_2.computation_result.SxC_aclus\n",
    "\n",
    "LxC_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureOutputManager, FigureOutputLocation, ContextToPathMode\n",
    "\n",
    "registered_output_files = {}\n",
    "\n",
    "def output_figure(final_context: IdentifyingContext, fig, write_vector_format:bool=False, write_png:bool=True, debug_print=True):\n",
    "    \"\"\" outputs the figure using the provided context. \"\"\"\n",
    "    from pyphoplacecellanalysis.General.Mixins.ExportHelpers import build_and_write_to_file\n",
    "    def register_output_file(output_path, output_metadata=None):\n",
    "        \"\"\" registers a new output file for the pipeline \"\"\"\n",
    "        print(f'register_output_file(output_path: {output_path}, ...)')\n",
    "        registered_output_files[output_path] = output_metadata or {}\n",
    "\n",
    "    fig_out_man = FigureOutputManager(figure_output_location=FigureOutputLocation.DAILY_PROGRAMMATIC_OUTPUT_FOLDER, context_to_path_mode=ContextToPathMode.HIERARCHY_UNIQUE)\n",
    "    active_out_figure_paths = build_and_write_to_file(fig, final_context, fig_out_man, write_vector_format=write_vector_format, write_png=write_png, register_output_file_fn=register_output_file)\n",
    "    return active_out_figure_paths, final_context\n",
    "\n",
    "\n",
    "# Set callback, the only self-specific property\n",
    "_out_fig_2._pipeline_file_callback_fn = output_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing\n",
    "restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# Perform interactive Matplotlib operations with 'Qt5Agg' backend\n",
    "_fig_2_theta_out, _fig_2_replay_out = _out_fig_2.display(active_context=global_multi_session_context, title_modifier_fn=lambda original_title: f\"{original_title} ({num_sessions} sessions)\", save_figure=True)\n",
    "\t\n",
    "_out_fig_2.perform_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Surprise Shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d829b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
