{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:20.608442900Z",
     "start_time": "2023-11-16T23:21:20.217442100Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function pyphocorehelpers.pprint.wide_pprint_jupyter(obj, p, cycle, **kwargs)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_output_parent_folder: /home/halechr/repos/Spike3D/EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation\n",
      "DAY_DATE_STR: 2024-06-26, DAY_DATE_TO_USE: 2024-06-26\n",
      "NOW_DATETIME: 2024-06-26_0753PM, NOW_DATETIME_TO_USE: 2024-06-26_0753PM\n",
      "global_data_root_parent_path changed to /media/halechr/MAX/Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b06d403260943999ee15a349261908f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Data Root:', layout=Layout(width='auto'), options=(PosixPath('/media/halechr/MAX/Data'), PosixPath('/home/halechr/FastData'), PosixPath('/home/halechr/cloud/turbo/Data')), style=ToggleButtonsStyle(button_width='max-content'), tooltip='global_data_root_parent_path', value=PosixPath('/media/halechr/MAX/Data'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "# !pip install viztracer\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory, make_class\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import array_repr_with_graphical_shape, dataframe_show_more_button\n",
    "\n",
    "ip = get_ipython()\n",
    "\n",
    "# Register the custom display function for NumPy arrays\n",
    "# ip.display_formatter.formatters['text/html'].for_type(np.ndarray, lambda arr: array_preview_with_graphical_shape_repr_html(arr))\n",
    "ip = array_repr_with_graphical_shape(ip=ip)\n",
    "# ip = dataframe_show_more_button(ip=ip)\n",
    "\n",
    "text_formatter: PlainTextFormatter = ip.display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "import pyphocorehelpers.programming_helpers as programming_helpers\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns, parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_evaluate_required_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions,  RankOrderComputationsContainer, RankOrderResult, RankOrderAnalyses\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ComputationFunctionRegistryHolder import ComputationFunctionRegistryHolder, computation_precidence_specifying_function, global_function\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path('/Volumes/SwapSSD/Data'), Path('/Users/pho/data'), Path(r'/media/halechr/MAX/Data'), Path(r'/home/halechr/FastData'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/Users/pho/cloud/turbo/Data')] # Path('/Volumes/FedoraSSD/FastData'), \n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "\tglobal global_data_root_parent_path\n",
    "\tnew_global_data_root_parent_path = new_path.resolve()\n",
    "\tglobal_data_root_parent_path = new_global_data_root_parent_path\n",
    "\tprint(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "\tassert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\t\t\t\n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {},
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basedir: /media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context: IdentifyingContext = good_contexts_list[1] # select the session from all of the good sessions here.\n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # 2024-04-30 - Completely cleaned.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # Working wcorr_shuffle and trial_by_trial - DONE, might be the BEST SESSION, good example session with lots of place cells, clean replays, and clear bar graphs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # DONE, Good Pfs but no good replays ---- VERY weird effect of the replays, a sharp drop to strongly negative values more than 3/4 through the experiment.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6') # BAD, 2023-07-14, unsure why still.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # DONE, GREAT, both good Pfs and replays! Interesting see-saw!\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25') # DONE, Added replay selections. Very \"jumpy\" between the starts and ends of the track.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40') # 2024-05-28 __ DEAD # 2024-01-10 new RANKORDER APOGEE | DONE, Added replay selections. A TON of putative replays in general, most bad, but some good. LOOKIN GOOD!\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='twolong_LR_pf1Dsession_name='2006-4-12_15-25-59') # BAD, No Epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-16_18-47-52')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-17_12-52-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-25_13-20-55')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-28_12-38-13')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44') # BAD: Confirmed frequent jumping off of the track in this session. DONE, good. Many good pfs, many good replays. Noticed very strange jumping off the track in the 3D behavior/spikes viewer. Is there something wrong with this session?\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0') # DONE, good?, replays selected, few  # BAD: Seems like in 3D view there's also jumping off of the track in this session.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # DONE, very few replays\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_12-15-3') ### KeyError: 'maze1_odd'\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_22-4-5') ### \n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # NEWDONE, replays selected, quite a few replays but few are very good.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# \n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21478b32",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'extended_stats',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    # 'rank_order_shuffle_analysis',\n",
    "    # 'directional_train_test_split',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "    # 'directional_decoders_evaluate_epochs',\n",
    "    # 'directional_decoders_epoch_heuristic_scoring',\n",
    "    # 'perform_wcorr_shuffle_analysis',\n",
    "    # 'trial_by_trial_metrics',\n",
    "    # # 'extended_pf_peak_information',\n",
    "] # do only specified\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'] # , 'directional_decoders_decode_continuous'\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_decode_continuous'] # \n",
    "# force_recompute_override_computations_includelist = ['trial_by_trial_metrics']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'] # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70141a66",
   "metadata": {},
   "source": [
    "## 2024-06-25 - Load from saved custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679473aa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Loads custom pipeline pickles that were saved out via `custom_save_filepaths['pipeline_pkl'] = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename=custom_save_filenames['pipeline_pkl'])`\n",
    "\n",
    "## INPUTS: global_data_root_parent_path, active_data_mode_name, basedir, saving_mode, force_reload, custom_save_filenames\n",
    "# custom_suffix: str = '_withNewComputedReplays'\n",
    "custom_suffix: str = '_withNewKamranExportedReplays'\n",
    "custom_save_filenames = {\n",
    "    'pipeline_pkl':f'loadedSessPickle{custom_suffix}.pkl',\n",
    "    'global_computation_pkl':f\"global_computation_results{custom_suffix}.pkl\",\n",
    "    'pipeline_h5':f'pipeline{custom_suffix}.h5',\n",
    "}\n",
    "print(f'custom_save_filenames: {custom_save_filenames}')\n",
    "custom_save_filepaths = {k:v for k, v in custom_save_filenames.items()}\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# PIPELINE LOADING                                                                                                     #\n",
    "# ==================================================================================================================== #\n",
    "# load the custom saved outputs\n",
    "active_pickle_filename = custom_save_filenames['pipeline_pkl'] # 'loadedSessPickle_withParameters.pkl'\n",
    "print(f'active_pickle_filename: \"{active_pickle_filename}\"')\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "epoch_name_includelist=None\n",
    "active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation','firing_rate_trends', 'position_decoding']\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True, active_pickle_filename=active_pickle_filename) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        if saving_mode == PipelineSavingScheme.SKIP_SAVING:\n",
    "            print(f'WARNING: PipelineSavingScheme.SKIP_SAVING but need to save post_compute_validate changes!!')\n",
    "        else:\n",
    "            curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except BaseException as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n",
    "\n",
    "print(f'Pipeline loaded from custom pickle!!')\n",
    "## OUTPUT: curr_active_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88868964",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Global computations loading:                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "# Loads saved global computations that were saved out via: `custom_save_filepaths['global_computation_pkl'] = curr_active_pipeline.save_global_computation_results(override_global_pickle_filename=custom_save_filenames['global_computation_pkl'])`\n",
    "## INPUTS: custom_save_filenames\n",
    "## INPUTS: curr_active_pipeline, override_global_computation_results_pickle_path, extended_computations_include_includelist\n",
    "\n",
    "override_global_computation_results_pickle_path = custom_save_filenames['global_computation_pkl']\n",
    "print(f'override_global_computation_results_pickle_path: \"{override_global_computation_results_pickle_path}\"')\n",
    "\n",
    "# Pre-load ___________________________________________________________________________________________________________ #\n",
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list\n",
    "\n",
    "# Try Unpickling Global Computations to update pipeline ______________________________________________________________ #\n",
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # INPUTS: override_global_computation_results_pickle_path\n",
    "        sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(override_global_computation_results_pickle_path=override_global_computation_results_pickle_path,\n",
    "                                                                                        allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist, ) # is new\n",
    "        print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except BaseException as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n",
    "\n",
    "# Post-Load __________________________________________________________________________________________________________ #\n",
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "\n",
    "# Compute ____________________________________________________________________________________________________________ #\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# Post-compute _______________________________________________________________________________________________________ #\n",
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=False, force_recompute_override_computations_includelist=[], debug_print=True)\n",
    "print(f'Post-compute validation: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98170b",
   "metadata": {},
   "source": [
    "## Normal Pipeline Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8167df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loaded session pickle file results : \"/media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:==========================================================================================\n",
      "========== Logger INIT \"2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15\" ==============================\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:NeuropyPipeline.__setstate__(state=\"{'pipeline_name': 'kdiba_pipeline', 'session_data_type': 'kdiba', '_stage': <pyphoplacecellanalysis.General.Pipeline.Stages.Display.DisplayPipelineStage object at 0x7f8fe0ffe310>}\")\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:select_filters(...) with: []\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_logger(full_logger_string=\"2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15\", file_logging_dir: None):\n",
      "done.\n",
      "Loading pickled pipeline success: /media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing global computations...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:select_filters(...) with: []\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing global computations...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:select_filters(...) with: []\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:Performing global computations...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:NeuropyPipeline.on_stage_changed(new_stage=\"PipelineStage.Displayed\")\n",
      "WARNING:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation',\n",
    "                                            #    'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step', \n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            # 'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba46b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.574268400Z",
     "start_time": "2023-11-16T23:21:35.966373700Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "#### GLOBAL COMPUTE via `batch_extended_computations(...)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb47e03a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields'], so only performing these extended computations.\n",
      "Running batch_evaluate_required_computations(...) with global_epoch_name: \"maze_any\"\n",
      "done with all batch_evaluate_required_computations(...).\n",
      "Pre-load global computations: needs_computation_output_dict: ['split_to_directional_laps', 'merged_directional_placefields', 'long_short_decoding_analyses', 'short_long_pf_overlap_analyses', 'long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'extended_stats', 'pfdt_computation']\n"
     ]
    }
   ],
   "source": [
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28cf10d2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loaded session pickle file results : \"/media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl\"... done.\n",
      "sucessfully_updated_keys: ['DirectionalLaps', 'DirectionalMergedDecoders', 'long_short_leave_one_out_decoding_analysis', 'short_long_pf_overlap_analyses', 'long_short_fr_indicies_analysis', 'jonathan_firing_rate_analysis', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'RankOrder', 'DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations', 'TrainTestSplit', 'TrialByTrialActivity', 'long_short_endcap', 'SequenceBased']\n",
      "successfully_loaded_keys: ['DirectionalLaps', 'DirectionalMergedDecoders', 'long_short_leave_one_out_decoding_analysis', 'short_long_pf_overlap_analyses', 'long_short_fr_indicies_analysis', 'jonathan_firing_rate_analysis', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'RankOrder', 'DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations', 'TrainTestSplit', 'TrialByTrialActivity', 'long_short_endcap', 'SequenceBased']\n"
     ]
    }
   ],
   "source": [
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # curr_active_pipeline.load_pickled_global_computation_results()\n",
    "        sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist) # is new\n",
    "        print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except BaseException as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0d5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].laps_most_likely_result_tuple = RankOrderAnalyses.most_likely_directional_rank_order_shuffling(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cdcca0",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields'], so only performing these extended computations.\n",
      "Running batch_evaluate_required_computations(...) with global_epoch_name: \"maze_any\"\n",
      "done with all batch_evaluate_required_computations(...).\n",
      "Post-load global computations: needs_computation_output_dict: ['extended_stats', 'pfdt_computation']\n"
     ]
    }
   ],
   "source": [
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0fe66c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "===>|> for filtered_session with filter named \"maze1_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze2_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze1_even\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze2_even\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze_even\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze1_any\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze2_any\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze_any\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_extended_statistics_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function ExtendedStatsComputations._perform_extended_statistics_computation at 0x7f9041122ee0>\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "\t done.\n",
      "pfdt_computation missing.\n",
      "\t Recomputing pfdt_computation...\n",
      "===>|> for filtered_session with filter named \"maze1_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze2_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze1_even\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze2_even\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze_even\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze1_any\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze2_any\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze_any\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['_perform_time_dependent_placefield_computation'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x7f9041112550>\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "\t done.\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any'), ('pfdt_computation', 'maze_any')].\n",
      "\n",
      "\n",
      "!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"\n",
      "\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b4793f3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields'], so only performing these extended computations.\n",
      "Running batch_evaluate_required_computations(...) with global_epoch_name: \"maze_any\"\n",
      "done with all batch_evaluate_required_computations(...).\n",
      "Post-compute validation: needs_computation_output_dict: []\n"
     ]
    }
   ],
   "source": [
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=False, force_recompute_override_computations_includelist=[], debug_print=True)\n",
    "print(f'Post-compute validation: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f755b",
   "metadata": {},
   "source": [
    "## Shared Post-Pipeline load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_suffix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_rMBP' # TODO: Change this as needed, templating isn't actually doing anything rn.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee'\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m BATCH_DATE_TO_USE: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDAY_DATE_TO_USE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Lab_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m known_collected_output_paths \u001b[38;5;241m=\u001b[39m [Path(v)\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/nfs/turbo/umms-kdiba/Data/Output/collected_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/halechr/FastData/collected_outputs/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/halechr/cloud/turbo/Data/Output/collected_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                                            \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpho\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrepos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpike3DWorkEnv\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpike3D\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcollected_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m                                                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/pho/data/collected_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/gen_scripts/\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     11\u001b[0m collected_outputs_path \u001b[38;5;241m=\u001b[39m find_first_extant_path(known_collected_output_paths)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_suffix' is not defined"
     ]
    }
   ],
   "source": [
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_rMBP' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee'\n",
    "BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Lab_{custom_suffix}'\n",
    "\n",
    "known_collected_output_paths = [Path(v).resolve() for v in ['/nfs/turbo/umms-kdiba/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/',\n",
    "                                                           '/home/halechr/cloud/turbo/Data/Output/collected_outputs',\n",
    "                                                           r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs',\n",
    "                                                           '/Users/pho/data/collected_outputs',\n",
    "                                                          'output/gen_scripts/']]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_output_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: {collected_outputs_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# collected_outputs_path.mkdir(exist_ok=True)\n",
    "# assert collected_outputs_path.exists()\n",
    "\n",
    "## Build the output prefix from the session context:\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: \"{CURR_BATCH_OUTPUT_PREFIX}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e0148",
   "metadata": {},
   "source": [
    "## Specific Recomputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_most_recent_computation_time, each_epoch_latest_computation_time, each_epoch_each_result_computation_completion_times, (global_computations_latest_computation_time, global_computation_completion_times) = curr_active_pipeline.get_computation_times(debug_print=False)\n",
    "# each_epoch_latest_computation_time\n",
    "each_epoch_each_result_computation_completion_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config.instantaneous_time_bin_size_seconds = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47820977",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "4    'extended_stats',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    # 'rank_order_shuffle_analysis',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "    # 'directional_decoders_evaluate_epochs',\n",
    "    # 'directional_decoders_epoch_heuristic_scoring',\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_override_computations_includelist = [\n",
    "    # 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring',\n",
    "    # 'lap_direction_determination', 'DirectionalLaps',\n",
    "    # 'merged_directional_placefields',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "]\n",
    "# force_recompute_override_computations_includelist = None\n",
    "\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "newly_computed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recompute_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# force_recompute_override_computations_includelist = ['_decode_continuous_using_directional_decoders']\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(extended_computations_include_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.02}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.20}, {'time_bin_size': 0.20}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "# 2024-04-20 - HACK -- FIXME: Invert the 'is_LR_dir' column since it is clearly reversed. No clue why.\n",
    "# fails due to some types thing?\n",
    "# \terr: Length of values (82) does not match length of index (80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_evaluate_epochs'], computation_kwargs_list=[{'should_skip_radon_transform': False}], enabled_filter_names=None, fail_on_exception=True, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe380f10",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_epoch_heuristic_scoring'], enabled_filter_names=None, fail_on_exception=True, debug_print=False) # OK FOR PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372737e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39417243",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['lap_direction_determination'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89757a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['extended_stats'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['trial_by_trial_metrics'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da92d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['perform_wcorr_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 50}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.2}, {'time_bin_size': 0.025}, {'should_skip_radon_transform': False}, {}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62493eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=[\n",
    "    'merged_directional_placefields', \n",
    "    'long_short_decoding_analyses', #'pipeline_complete_compute_long_short_fr_indicies',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    ], enabled_filter_names=None, fail_on_exception=True, debug_print=False) # , computation_kwargs_list=[{'should_skip_radon_transform': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c267dae5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "NEW_REPLAY"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/DirectionalPlacefieldGlobalComputationFunctions.py:1886: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  a_pearson_v = pearsonr(_an_arr[0], _an_arr[1]).statistic\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/DirectionalPlacefieldGlobalComputationFunctions.py:1886: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  a_pearson_v = pearsonr(_an_arr[0], _an_arr[1]).statistic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: hit NaN a_pearson_v: nan for a_peak_x_col_name: long_LR_pf_peak_x, a_decoder_neuron_IDs: [  3   5   7   9  10  11  14  15  16  19  31  32  33  35  36  37  41  45  48  49  50  53  55  59  60  61  62  64  71  73  75  76  78  83  84  85  86  88  90  92  93  98 102 107 108], min_num_required_unique_aclus: 14\n",
      "WARNING: hit NaN a_pearson_v: nan for a_peak_x_col_name: short_LR_pf_peak_x, a_decoder_neuron_IDs: [  3   5   7   9  10  11  14  15  16  19  31  32  33  35  36  37  41  45  48  49  50  53  55  59  60  61  62  64  71  73  75  76  78  83  84  85  86  88  90  92  93  98 102 107 108], min_num_required_unique_aclus: 14\n"
     ]
    }
   ],
   "source": [
    "# directional_merged_decoders_result = deepcopy(directional_decoders_epochs_decode_result)\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "\n",
    "spikes_df = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "\n",
    "global_computation_results = curr_active_pipeline.global_computation_results\n",
    "\n",
    " # spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results = global_computation_results.computed_data['RankOrder'] # : \"RankOrderComputationsContainer\"\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "# included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "directional_laps_results: DirectionalLapsResult = global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "# print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "# print(f'included_qclu_values: {included_qclu_values}')\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "# pos_bin_size = _recover_position_bin_size(track_templates.get_decoders()[0]) # 3.793023081021702\n",
    "# print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "# pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "\n",
    "## Simple Pearson Correlation\n",
    "assert spikes_df is not None\n",
    "(laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df), corr_column_names = directional_merged_decoders_result.compute_simple_spike_time_v_pf_peak_x_by_epoch(track_templates=track_templates, spikes_df=deepcopy(spikes_df))\n",
    "## OUTPUTS: (laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df), corr_column_names\n",
    "## Computes the highest-valued decoder for this score:\n",
    "best_decoder_index_col_name: str = 'best_decoder_index'\n",
    "laps_simple_pf_pearson_merged_df[best_decoder_index_col_name] = laps_simple_pf_pearson_merged_df[corr_column_names].abs().apply(lambda row: np.argmax(row.values), axis=1)\n",
    "ripple_simple_pf_pearson_merged_df[best_decoder_index_col_name] = ripple_simple_pf_pearson_merged_df[corr_column_names].abs().apply(lambda row: np.argmax(row.values), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a452a66",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(active_epochs_df): 611\n",
      "min_num_unique_aclu_inclusions: 15\n",
      "len(active_epochs_df): 345\n",
      "pos_bin_size: 3.793023081021702\n",
      "ripple_decoding_time_bin_size: 0.025\n",
      "laps_decoding_time_bin_size: 0.25\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "print(f'pos_bin_size: {pos_bin_size}')\n",
    "print(f'ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}')\n",
    "\n",
    "# Radon Transforms:\n",
    "decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "# Weighted correlations:\n",
    "laps_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "ripple_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "decoder_laps_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "decoder_ripple_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "# Pearson's correlations:\n",
    "laps_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "ripple_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "# laps_simple_pf_pearson_merged_df\n",
    "# ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## Drop rows where all are missing\n",
    "corr_column_names = ['long_LR_pf_peak_x_pearsonr', 'long_RL_pf_peak_x_pearsonr', 'short_LR_pf_peak_x_pearsonr', 'short_RL_pf_peak_x_pearsonr']\n",
    "# ripple_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='all') # 350/412 rows\n",
    "filtered_laps_simple_pf_pearson_merged_df: pd.DataFrame = laps_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='any') # 320/412 rows\n",
    "filtered_ripple_simple_pf_pearson_merged_df: pd.DataFrame = ripple_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='any') # 320/412 rows\n",
    "\n",
    "## Update the `decoder_ripple_filter_epochs_decoder_result_dict` with the included epochs:\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epochs(filtered_ripple_simple_pf_pearson_merged_df.index) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epochs(filtered_laps_simple_pf_pearson_merged_df.index) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_ripple_simple_pf_pearson_merged_df[['start', 'stop']].to_numpy()) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_laps_simple_pf_pearson_merged_df[['start', 'stop']].to_numpy()) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_ripple_simple_pf_pearson_merged_df['start'].to_numpy()) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_laps_simple_pf_pearson_merged_df['start'].to_numpy()) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed38171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData\n",
    "\n",
    "# directional_decoders_epochs_decode_result\n",
    "# save_path = Path(\"/Users/pho/data/KDIBA/gor01/one/2006-6-09_1-22-43/output/2024-04-25_CustomDecodingResults.pkl\").resolve()\n",
    "# save_path = curr_active_pipeline.get_output_path().joinpath(\"2024-04-28_CustomDecodingResults.pkl\").resolve()\n",
    "save_path = curr_active_pipeline.get_output_path().joinpath(f\"{DAY_DATE_TO_USE}_CustomDecodingResults.pkl\").resolve()\n",
    "\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "xbin_centers = deepcopy(long_pf2D.xbin_centers)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "ybin_centers = deepcopy(long_pf2D.ybin_centers)\n",
    "\n",
    "print(xbin_centers)\n",
    "save_dict = {\n",
    "'directional_decoders_epochs_decode_result': directional_decoders_epochs_decode_result.__getstate__(),\n",
    "'xbin': xbin, 'xbin_centers': xbin_centers}\n",
    "\n",
    "saveData(save_path, save_dict)\n",
    "print(f'save_path: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "464b4531",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAll scores df CSV exporting...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'collected_outputs_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m t_start, t_delta, t_end \u001b[38;5;241m=\u001b[39m curr_active_pipeline\u001b[38;5;241m.\u001b[39mfind_LongShortDelta_times()\n\u001b[1;32m     11\u001b[0m export_df_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mripple_all_scores_merged_df\u001b[39m\u001b[38;5;124m'\u001b[39m: extracted_merged_scores_df}\n\u001b[0;32m---> 12\u001b[0m _csv_export_paths \u001b[38;5;241m=\u001b[39m directional_decoders_epochs_decode_result\u001b[38;5;241m.\u001b[39mperform_export_dfs_dict_to_csvs(extracted_dfs_dict\u001b[38;5;241m=\u001b[39mexport_df_dict, parent_output_path\u001b[38;5;241m=\u001b[39m\u001b[43mcollected_outputs_path\u001b[49m\u001b[38;5;241m.\u001b[39mresolve(), active_context\u001b[38;5;241m=\u001b[39mactive_context, session_name\u001b[38;5;241m=\u001b[39mcurr_session_name, curr_session_t_delta\u001b[38;5;241m=\u001b[39mt_delta,\n\u001b[1;32m     13\u001b[0m                                                                             \u001b[38;5;66;03m#   user_annotation_selections={'ripple': any_good_selected_epoch_times},\u001b[39;00m\n\u001b[1;32m     14\u001b[0m                                                                             \u001b[38;5;66;03m#   valid_epochs_selections={'ripple': filtered_valid_epoch_times},\u001b[39;00m\n\u001b[1;32m     15\u001b[0m                                                                             )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124msuccessfully exported ripple_all_scores_merged_df to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollected_outputs_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m _output_csv_paths_info_str: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_uri_from_path(a_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a_name, a_path \u001b[38;5;129;01min\u001b[39;00m _csv_export_paths\u001b[38;5;241m.\u001b[39mitems()])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collected_outputs_path' is not defined"
     ]
    }
   ],
   "source": [
    "# 💾 Export CSVs: \n",
    "## INPUTS: directional_decoders_epochs_decode_result,\n",
    "\n",
    "extracted_merged_scores_df = directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df()\n",
    "# extracted_merged_scores_df\n",
    "\n",
    "print(f'\\tAll scores df CSV exporting...')\n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "export_df_dict = {'ripple_all_scores_merged_df': extracted_merged_scores_df}\n",
    "_csv_export_paths = directional_decoders_epochs_decode_result.perform_export_dfs_dict_to_csvs(extracted_dfs_dict=export_df_dict, parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                            #   user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                            #   valid_epochs_selections={'ripple': filtered_valid_epoch_times},\n",
    "                                                                            )\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported ripple_all_scores_merged_df to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _csv_export_paths.items()])\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extracted_merged_scores_df.to_csv('test_(ripple_all_scores_merged_df).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a860a1",
   "metadata": {},
   "outputs": [],
   "source": [
    " decoder_ripple_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_extras_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbf34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ripple_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_extras_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_laps_simple_pf_pearson_merged_df\n",
    "# filtered_ripple_simple_pf_pearson_merged_df\n",
    "# decoder_ripple_weighted_corr_df_dict\n",
    "ripple_weighted_corr_merged_df['ripple_start_t']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a95450",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorr_column_names = ['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL']\n",
    "filtered_ripple_simple_pf_pearson_merged_df.label = filtered_ripple_simple_pf_pearson_merged_df.label.astype('int64')\n",
    "ripple_weighted_corr_merged_df['label'] = ripple_weighted_corr_merged_df['ripple_idx'].astype('int64')\n",
    "\n",
    "filtered_ripple_simple_pf_pearson_merged_df.join(ripple_weighted_corr_merged_df[wcorr_column_names], on='start') # , on='label'\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ripple_weighted_corr_merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_decoded_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3599f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window.save_selections()\n",
    "\n",
    "a_decoded_filter_epochs_decoder_result_dict.epochs.find_data_indicies_from_epoch_times([380.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77babf98",
   "metadata": {},
   "source": [
    "## 💾 Continue Saving/Exporting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results() # newly_computed_values: [('pfdt_computation', 'maze_any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7af96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = curr_active_pipeline.save_split_global_computation_results(debug_print=True) # encountered issue with pickling `long_short_post_decoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a869b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n",
    "curr_active_pipeline.clear_registered_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE) ## #TODO 2024-02-16 14:25: - [ ] PicklingError: Can't pickle <function make_set_closure_cell.<locals>.set_closure_cell at 0x7fd35e66b700>: it's not found as attr._compat.make_set_closure_cell.<locals>.set_closure_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline._persistance_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10989b4",
   "metadata": {},
   "source": [
    "#### Get computation times/info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_most_recent_computation_time, each_epoch_latest_computation_time, each_epoch_each_result_computation_completion_times, (global_computations_latest_computation_time, global_computation_completion_times) = curr_active_pipeline.get_computation_times(debug_print=False)\n",
    "# each_epoch_latest_computation_time\n",
    "# each_epoch_each_result_computation_completion_times\n",
    "# global_computation_completion_times\n",
    "\n",
    "# curr_active_pipeline.get_merged_computation_function_validators()\n",
    "# Get the names of the global and non-global computations:\n",
    "all_validators_dict = curr_active_pipeline.get_merged_computation_function_validators()\n",
    "global_only_validators_dict = {k:v for k, v in all_validators_dict.items() if v.is_global}\n",
    "non_global_only_validators_dict = {k:v for k, v in all_validators_dict.items() if (not v.is_global)}\n",
    "non_global_comp_names: List[str] = [v.short_name for k, v in non_global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))] # ['firing_rate_trends', 'spike_burst_detection', 'pf_dt_sequential_surprise', 'extended_stats', 'placefield_overlap', 'ratemap_peaks_prominence2d', 'velocity_vs_pf_simplified_count_density', 'EloyAnalysis', '_perform_specific_epochs_decoding', 'recursive_latent_pf_decoding', 'position_decoding_two_step', 'position_decoding', 'lap_direction_determination', 'pfdt_computation', 'pf_computation']\n",
    "global_comp_names: List[str] = [v.short_name for k, v in global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))] # ['long_short_endcap_analysis', 'long_short_inst_spike_rate_groups', 'long_short_post_decoding', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_decoding_analyses', 'PBE_stats', 'rank_order_shuffle_analysis', 'directional_decoders_epoch_heuristic_scoring', 'directional_decoders_evaluate_epochs', 'directional_decoders_decode_continuous', 'merged_directional_placefields', 'split_to_directional_laps']\n",
    "\n",
    "# mappings between the long computation function names and their short names:\n",
    "non_global_comp_names_map: Dict[str, str] = {v.computation_fn_name:v.short_name for k, v in non_global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))}\n",
    "global_comp_names_map: Dict[str, str] = {v.computation_fn_name:v.short_name for k, v in global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))} # '_perform_long_short_endcap_analysis': 'long_short_endcap_analysis', '_perform_long_short_instantaneous_spike_rate_groups_analysis': 'long_short_inst_spike_rate_groups', ...}\n",
    "\n",
    "# convert long function names to short-names:\n",
    "each_epoch_each_result_computation_completion_times = {an_epoch:{non_global_comp_names_map.get(k, k):v for k,v in a_results_dict.items()} for an_epoch, a_results_dict in each_epoch_each_result_computation_completion_times.items()}\n",
    "global_computation_completion_times = {global_comp_names_map.get(k, k):v for k,v in global_computation_completion_times.items()}\n",
    "\n",
    "each_epoch_each_result_computation_completion_times\n",
    "global_computation_completion_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967369f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_evaluate_required_computations\n",
    "\n",
    "# force_recompute_global = force_reload\n",
    "force_recompute_global = True\n",
    "active_probe_includelist = extended_computations_include_includelist\n",
    "# active_probe_includelist = ['lap_direction_determination']\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=active_probe_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "needs_computation_output_dict\n",
    "# valid_computed_results_output_list\n",
    "# remaining_include_function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.SpecificComputationValidation import SpecificComputationValidator, SpecificComputationResultsSpecification, DependencyGraph\n",
    "\n",
    "_comp_specifiers_dict: Dict[str, SpecificComputationValidator] = curr_active_pipeline.get_merged_computation_function_validators()\n",
    "_comp_specifiers_graph: DependencyGraph = DependencyGraph(_comp_specifiers_dict)\n",
    "_comp_specifiers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "_comp_specifiers_graph.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream_dependents = graph.get_downstream_dependents('_build_merged_directional_placefields')\n",
    "# downstream_dependents = graph.get_downstream_dependents('_perform_jonathan_replay_firing_rate_analyses')\n",
    "# downstream_dependents = graph.get_downstream_dependents('jonathan_firing_rate_analysis')\n",
    "downstream_dependents = _comp_specifiers_graph.get_downstream_dependents('perform_wcorr_shuffle_analysis')\n",
    "# 'wcorr_shuffle_analysis'\n",
    "print(downstream_dependents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f56132",
   "metadata": {},
   "outputs": [],
   "source": [
    "upstream_requirements = _comp_specifiers_graph.get_upstream_requirements('perform_wcorr_shuffle_analysis')\n",
    "print(upstream_requirements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_downstream_dependents = []\n",
    "requirements_list = ['perform_wcorr_shuffle_analysis',  'perform_rank_order_shuffle_analysis',  'jonathan_firing_rate_analysis']\n",
    "for a_requirement in requirements_list:\n",
    "    downstream_dependents = graph.get_downstream_dependents(a_requirement)\n",
    "    all_downstream_dependents.extend(downstream_dependents)\n",
    "    \n",
    "# '_split_to_directional_laps'\n",
    "print(set(all_downstream_dependents))\n",
    "# print(downstream_dependents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_computed_results_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aeeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "['merged_directional_placefields', ]\n",
    "\n",
    "['long_short_decoding_analyses', 'long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'extended_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe_fn_name = 'long_short_decoding_analyses'\n",
    "# probe_fn_name = 'long_short_fr_indicies_analyses'\n",
    "\n",
    "# found_validator = None\n",
    "# provided_global_keys = []\n",
    "# remaining_comp_specifiers_dict = deepcopy(_comp_specifiers_dict)\n",
    "\n",
    "# for a_name, a_validator in remaining_comp_specifiers_dict.items():\n",
    "#     if a_validator.does_name_match(probe_fn_name):\n",
    "#         found_validator = a_validator\n",
    "#         print(f'found matching validator: {found_validator}')\n",
    "#     else:\n",
    "#         remaining_comp_specifiers_dict[a_name] = a_validator\n",
    "\n",
    "# if found_validator is not None:\n",
    "#     provided_global_keys = found_validator.results_specification.provides_global_keys\n",
    "#     print(f'provided_global_keys: {provided_global_keys}')\n",
    "# else:\n",
    "#     provided_global_keys = []\n",
    "\n",
    "# provided_global_keys\n",
    "## OUTPUTS: remaining_comp_specifiers_dict, found_validator, provided_global_keys\n",
    "\n",
    "\n",
    "remaining_comp_specifiers_dict = deepcopy(_comp_specifiers_dict)\n",
    "remaining_comp_specifiers_dict, found_matching_validators, provided_global_keys = SpecificComputationValidator.find_matching_validators(remaining_comp_specifiers_dict=remaining_comp_specifiers_dict,\n",
    "                                                                                    probe_fn_names=['long_short_decoding_analyses','long_short_fr_indicies_analyses'])\n",
    "\n",
    "provided_global_keys\n",
    "\n",
    "## INPUTS: remaining_comp_specifiers_dict, found_validator, provided_global_keys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "remaining_comp_specifiers_dict, dependent_validators, provided_global_keys = SpecificComputationValidator.find_immediate_dependencies(remaining_comp_specifiers_dict=remaining_comp_specifiers_dict, provided_global_keys=provided_global_keys)\n",
    "provided_global_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_comp_specifiers_dict, dependent_validators, provided_global_keys = SpecificComputationValidator.find_immediate_dependencies(remaining_comp_specifiers_dict=remaining_comp_specifiers_dict, provided_global_keys=provided_global_keys)\n",
    "provided_global_keys # ['long_short_leave_one_out_decoding_analysis', 'long_short_post_decoding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59374622",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85822a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.replays\n",
    "assert replay_estimation_parameters is not None\n",
    "replay_estimation_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recompute_earlier_than_date = datetime(2024, 4, 1, 0, 0, 0)\n",
    "recompute_earlier_than_date\n",
    "\n",
    "each_epoch_needing_recompute = [an_epoch for an_epoch, last_computed_datetime in each_epoch_latest_computation_time.items() if (last_computed_datetime < recompute_earlier_than_date)]\n",
    "each_epoch_needing_recompute\n",
    "each_epoch_each_result_needing_recompute = {an_epoch:{a_computation_name:last_computed_datetime for a_computation_name, last_computed_datetime in last_computed_datetimes_dict.items() if (last_computed_datetime < recompute_earlier_than_date)} for an_epoch, last_computed_datetimes_dict in each_epoch_each_result_computation_completion_times.items()}\n",
    "each_epoch_each_result_needing_recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ffa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_times\n",
    "curr_active_pipeline.global_computation_results\n",
    "# curr_active_pipeline.try_load_split_pickled_global_computation_results\n",
    "\n",
    "global_computation_times = deepcopy(curr_active_pipeline.global_computation_results.computation_times.to_dict()) # DynamicParameters({'perform_rank_order_shuffle_analysis': datetime.datetime(2024, 4, 3, 5, 41, 31, 287680), '_decode_continuous_using_directional_decoders': datetime.datetime(2024, 4, 3, 5, 12, 7, 337326), '_perform_long_short_decoding_analyses': datetime.datetime(2024, 4, 3, 5, 43, 10, 361685), '_perform_long_short_pf_overlap_analyses': datetime.datetime(2024, 4, 3, 5, 43, 10, 489296), '_perform_long_short_firing_rate_analyses': datetime.datetime(2024, 4, 3, 5, 45, 3, 73472), '_perform_jonathan_replay_firing_rate_analyses': datetime.datetime(2024, 4, 3, 5, 45, 5, 168790), '_perform_long_short_post_decoding_analysis': datetime.datetime(2024, 2, 16, 18, 13, 4, 734621), '_perform_long_short_endcap_analysis': datetime.datetime(2024, 4, 3, 5, 45, 24, 274261), '_decode_and_evaluate_epochs_using_directional_decoders': datetime.datetime(2024, 4, 3, 5, 14, 37, 935482), '_perform_long_short_instantaneous_spike_rate_groups_analysis': datetime.datetime(2024, 4, 3, 5, 45, 24, 131955), '_split_to_directional_laps': datetime.datetime(2024, 4, 3, 5, 11, 22, 627789), '_build_merged_directional_placefields': datetime.datetime(2024, 4, 3, 5, 11, 28, 376078)})\n",
    "global_computation_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {},
   "source": [
    "# Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8055110221f740fda22164a8a392ba7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Box(children=(Label(value='session path:', layout=Layout(width='auto')), HTML(value=\"<b style='font-size: larger;'>/media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output</b>\", layout=Layout(flex='1 1 auto', margin='2px', width='auto')), Button(button_style='info', description='Copy', icon='clipboard', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Copy to Clipboard'), Button(button_style='info', description='Reveal', icon='folder-open-o', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Reveal in System Explorer')), layout=Layout(align_items='center', display='flex', flex_flow='row nowrap', justify_content='flex-start', width='70%')), HBox(children=(Button(description='Output Folder', style=ButtonStyle()), Button(description='global pickle', style=ButtonStyle()), Button(description='pipeline pickle', style=ButtonStyle()), Button(description='.h5 export', style=ButtonStyle()), Button(description='TEST - Dialog', style=ButtonStyle()), Button(description='Save Pipeline', style=ButtonStyle()))), HBox(children=(Button(description='Reload display functions...', style=ButtonStyle()), Button(description='Reload computation functions...', style=ButtonStyle()))), ToggleButton(value=True, description='Figures Displaying')))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {},
   "source": [
    "# End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a533ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.700275900Z",
     "start_time": "2023-11-16T23:21:40.584273Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1211.5580800310709, 2093.8978568242164)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "t_start, t_delta, t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have several python variables I want to print: t_start, t_delta, t_end\n",
    "# I want to generate a print statement that explicitly lists the variable name prior to its value like `print(f't_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')`\n",
    "# Currently I have to t_start, t_delta, t_end\n",
    "curr_active_pipeline.get_session_context()\n",
    "\n",
    "print(f'{curr_active_pipeline.session_name}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9071e94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:43.601382Z",
     "start_time": "2023-11-16T23:21:40.702275600Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "WARNING: PAPER_FIGURE_figure_1_add_replay_epoch_rasters(...): no user-assigned manually labeled replay epochs. Reeturning all epochs.\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n"
     ]
    }
   ],
   "source": [
    "## long_short_decoding_analyses:\n",
    "from attrs import astuple\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import LeaveOneOutDecodingAnalysis\n",
    "\n",
    "curr_long_short_decoding_analyses: LeaveOneOutDecodingAnalysis = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c49f5d4f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7104fc37",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult, DirectionalLapsResult, DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']   \n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: float = rank_order_results.included_qclu_values\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93346114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.BatchCompletionHandler import BatchSessionCompletionHandler\n",
    "\n",
    "BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(directional_laps_results.directional_lap_specific_configs.keys()) # ['maze1_odd', 'maze1_even', 'maze2_odd', 'maze2_even']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "912656a7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(active_epochs_df): 611\n",
      "min_num_unique_aclu_inclusions: 15\n",
      "len(active_epochs_df): 345\n",
      "pos_bin_size = 3.793023081021702, ripple_decoding_time_bin_size = 0.025, laps_decoding_time_bin_size = 0.25\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "\n",
    "if ('DirectionalDecodersEpochsEvaluations' in curr_active_pipeline.global_computation_results.computed_data) and (curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations'] is not None):\n",
    "    directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "    directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "    ## UNPACK HERE via direct property access:\n",
    "    pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "    ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "    laps_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "    print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }, {laps_decoding_time_bin_size = }') # pos_bin_size = 3.8054171165052444, ripple_decoding_time_bin_size = 0.025, laps_decoding_time_bin_size = 0.2\n",
    "    decoder_laps_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "    decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "    decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "    decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "\n",
    "    # New items:\n",
    "    decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "    decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "    # Weighted correlations:\n",
    "    laps_weighted_corr_merged_df = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "    ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "    decoder_laps_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "    decoder_ripple_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "    # Pearson's correlations:\n",
    "    laps_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "    ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "881402df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>|> for filtered_session with filter named \"maze1_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['ratemap_peaks_prominence2d'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldDensityAnalysisComputationFunctions._perform_pf_find_ratemap_peaks_peak_prominence2d_computation at 0x7f9041122820>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldDensityAnalysisComputationFunctions._perform_pf_find_ratemap_peaks_peak_prominence2d_computation at 0x7f9041122820>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f90923d4160>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:528: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = -1 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing lower extrema, adding ybin_outer_extrema[0] = {ybin_outer_extrema[0]} to matching_vertical_scan_y_idxs')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:522: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = (-1, 5)\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 0: setting matching_vertical_scan_y_idxs = {ybin_outer_extrema}')\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:533: UserWarning: \tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = 5 to matching_vertical_scan_y_idxs\n",
      "  warn(f'\\tWARNING: len(matching_vertical_scan_y_idxs) == 1: missing upper extrema, adding ybin_outer_extrema[1] = {ybin_outer_extrema[1]} to matching_vertical_scan_y_idxs')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "===>|> for filtered_session with filter named \"maze2_odd\": Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['ratemap_peaks_prominence2d'])...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function PlacefieldDensityAnalysisComputationFunctions._perform_pf_find_ratemap_peaks_peak_prominence2d_computation at 0x7f9041122820>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function PlacefieldDensityAnalysisComputationFunctions._perform_pf_find_ratemap_peaks_peak_prominence2d_computation at 0x7f9041122820>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m active_peak_prominence_2d_results \u001b[38;5;241m=\u001b[39m curr_active_pipeline\u001b[38;5;241m.\u001b[39mcomputation_results[active_config_name]\u001b[38;5;241m.\u001b[39mcomputed_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRatemapPeaksAnalysis\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeakProminence2D\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active_peak_prominence_2d_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mcurr_active_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_specific_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mratemap_peaks_prominence2d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_filter_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     active_peak_prominence_2d_results \u001b[38;5;241m=\u001b[39m curr_active_pipeline\u001b[38;5;241m.\u001b[39mcomputation_results[active_config_name]\u001b[38;5;241m.\u001b[39mcomputed_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRatemapPeaksAnalysis\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeakProminence2D\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m active_peak_prominence_2d_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad even after computation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:1123\u001b[0m, in \u001b[0;36mPipelineWithComputedPipelineStageMixin.perform_specific_computation\u001b[0;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, debug_print)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" perform a specific computation (specified in computation_functions_name_includelist) in a minimally destructive manner using the previously recomputed results:\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03mPassthrough wrapper to self.stage.perform_specific_computation(...) with the same arguments.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03mUpdates:\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m    curr_active_pipeline.computation_results\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.stage is of type ComputedPipelineStage\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_specific_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_computation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_computation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_filter_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_filter_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:719\u001b[0m, in \u001b[0;36mComputedPipelineStage.perform_specific_computation\u001b[0;34m(self, active_computation_params, enabled_filter_names, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, debug_print, progress_logger_callback)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m## Here is an issue, we need to get the appropriate computation result depending on whether it's global or not \u001b[39;00m\n\u001b[1;32m    718\u001b[0m previous_computation_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_results[a_select_config_name]\n\u001b[0;32m--> 719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_results[a_select_config_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_specific_computations_single_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious_computation_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_functions_name_includelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mare_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:325\u001b[0m, in \u001b[0;36mComputedPipelineStage.run_specific_computations_single_context\u001b[0;34m(self, previous_computation_result, computation_functions_name_includelist, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[1;32m    323\u001b[0m     progress_logger_callback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mrun_specific_computations_single_context(including only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_computation_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregistered_computation_function_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registered computation functions): active_computation_functions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_computation_functions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Perform the computations:\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mComputedPipelineStage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_computation_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_computation_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_computation_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprevious_computation_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_logger_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mare_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mare_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_print\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py:832\u001b[0m, in \u001b[0;36mComputedPipelineStage._execute_computation_functions\u001b[0;34m(active_computation_functions, previous_computation_result, computation_kwargs_list, fail_on_exception, progress_logger_callback, are_global, debug_print)\u001b[0m\n\u001b[1;32m    829\u001b[0m     progress_logger_callback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecuting [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_num_funcs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;66;03m# evaluate the function 'f' using the result provided from the previous output or the initial input\u001b[39;00m\n\u001b[0;32m--> 832\u001b[0m     temp_result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious_computation_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomputation_kwargs_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mNameError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    834\u001b[0m     exception_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/PlacefieldDensityAnalysisComputationFunctions.py:697\u001b[0m, in \u001b[0;36mPlacefieldDensityAnalysisComputationFunctions._perform_pf_find_ratemap_peaks_peak_prominence2d_computation\u001b[0;34m(computation_result, step, peak_height_multiplier_probe_levels, minimum_included_peak_height, uniform_blur_size, gaussian_blur_sigma, debug_print)\u001b[0m\n\u001b[1;32m    695\u001b[0m neuron_tuning_curve_peak_firing_rate \u001b[38;5;241m=\u001b[39m tuning_curve_peak_firing_rates[neuron_idx]\n\u001b[1;32m    696\u001b[0m slab \u001b[38;5;241m=\u001b[39m active_tuning_curves[neuron_idx]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 697\u001b[0m _, _, slab, cell_peaks_dict, id_map, prominence_map, parent_map \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_prominence_contours\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxbin_centers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_pf_2D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxbin_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mybin_centers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_pf_2D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mybin_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_edge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m#\"\"\" Analyze all peaks of a given cell/ratemap \"\"\"\u001b[39;00m\n\u001b[1;32m    699\u001b[0m n_peaks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cell_peaks_dict)\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/External/peak_prominence2d.py:515\u001b[0m, in \u001b[0;36mcompute_prominence_contours\u001b[0;34m(xbin_centers, ybin_centers, slab, step, min_area, min_depth, include_edge, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_prominence_contours\u001b[39m(xbin_centers, ybin_centers, slab, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, min_area\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, min_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, include_edge\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Simple wrapper around the getProminence function by Pho Hale\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m    xbin_centers and ybin_centers should be like *bin_labels not *bin\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m    slab should usually be transposed: tuning_curves[i].T\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     peaks_dict, id_map, prominence_map, parent_map \u001b[38;5;241m=\u001b[39m \u001b[43mgetProminence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mybin_centers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mybin_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxbin_centers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxbin_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_area\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_edge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_edge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xbin_centers, ybin_centers, slab, peaks_dict, id_map, prominence_map, parent_map\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/External/peak_prominence2d.py:283\u001b[0m, in \u001b[0;36mgetProminence\u001b[0;34m(var, step, ybin_centers, xbin_centers, min_depth, include_edge, min_area, max_area, area_func, centroid_num_to_center, allow_hole, max_hole_area, verbose)\u001b[0m\n\u001b[1;32m    281\u001b[0m     csii\u001b[38;5;241m=\u001b[39max\u001b[38;5;241m.\u001b[39mcontourf(xbin_centers,ybin_centers,var,[levii,vmax\u001b[38;5;241m+\u001b[39mstep]) \u001b[38;5;66;03m## Heavy-lifting code here. levii is the level\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     csii\u001b[38;5;241m=\u001b[39mcsii\u001b[38;5;241m.\u001b[39mcollections[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 283\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcla\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     csii\u001b[38;5;241m=\u001b[39mcontours[ii]\n",
      "File \u001b[0;32m~/repos/Spike3D/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:1397\u001b[0m, in \u001b[0;36m_AxesBase.cla\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__clear()\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1397\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/Spike3D/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:1388\u001b[0m, in \u001b[0;36m_AxesBase.clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcla()\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1388\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/Spike3D/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:1271\u001b[0m, in \u001b[0;36m_AxesBase.__clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1268\u001b[0m yaxis_visible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mget_visible()\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_axis_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 1271\u001b[0m     \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Also resets the scale to linear.\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spine \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspines\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1273\u001b[0m     spine\u001b[38;5;241m.\u001b[39m_clear()  \u001b[38;5;66;03m# Use _clear to not clear Axis again\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/Spike3D/.venv/lib/python3.9/site-packages/matplotlib/axis.py:895\u001b[0m, in \u001b[0;36mAxis.clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclear\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;124;03m    Clear the axis.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m    - registered callbacks\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_visual_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# The above resets the label formatting using text rcParams,\u001b[39;00m\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;66;03m# so we then update the formatting using axes rcParams\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mset_color(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.labelcolor\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/repos/Spike3D/.venv/lib/python3.9/site-packages/matplotlib/text.py:189\u001b[0m, in \u001b[0;36mText._reset_visual_defaults\u001b[0;34m(self, text, color, fontproperties, usetex, parse_math, wrap, verticalalignment, horizontalalignment, multialignment, rotation, transform_rotates_text, linespacing, rotation_mode, antialiased)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m linespacing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     linespacing \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.2\u001b[39m  \u001b[38;5;66;03m# Maybe use rcParam later.\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_linespacing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinespacing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_rotation_mode(rotation_mode)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_antialiased(antialiased \u001b[38;5;28;01mif\u001b[39;00m antialiased \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    192\u001b[0m                      mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext.antialiased\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/repos/Spike3D/.venv/lib/python3.9/site-packages/matplotlib/text.py:1037\u001b[0m, in \u001b[0;36mText.set_linespacing\u001b[0;34m(self, spacing)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_linespacing\u001b[39m(\u001b[38;5;28mself\u001b[39m, spacing):\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m    Set the line spacing as a multiple of the font size.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    spacing : float (multiple of font size)\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     \u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_isinstance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mReal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspacing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linespacing \u001b[38;5;241m=\u001b[39m spacing\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# active_config_name: str = 'maze_any'\n",
    "active_config_name: str = global_epoch_name\n",
    "## INPUTS: curr_active_pipeline, active_config_name\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "if active_peak_prominence_2d_results is None:\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=None, fail_on_exception=False, debug_print=False)\n",
    "    active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "    assert active_peak_prominence_2d_results is not None, f\"bad even after computation\"\n",
    "\n",
    "active_peak_prominence_2d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f141f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=[short_LR_name, short_RL_name, long_any_name, short_any_name], fail_on_exception=False, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f67cb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "if 'DirectionalDecodersDecoded' in curr_active_pipeline.global_computation_results.computed_data:\n",
    "    directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "    all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "    pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "    spikes_df = directional_decoders_decode_result.spikes_df\n",
    "    continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "    previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "    print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4e959",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "\n",
    "wcorr_shuffle_results: SequenceBasedComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('SequenceBased', None)\n",
    "if wcorr_shuffle_results is not None:    \n",
    "    wcorr_ripple_shuffle: WCorrShuffle = wcorr_shuffle_results.wcorr_ripple_shuffle\n",
    "    print(f'wcorr_ripple_shuffle.n_completed_shuffles: {wcorr_ripple_shuffle.n_completed_shuffles}')\n",
    "else:\n",
    "    print(f'SequenceBased is not computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba865e4",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "\n",
    "directional_trial_by_trial_activity_result: TrialByTrialActivityResult = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None)\n",
    "\n",
    "if directional_trial_by_trial_activity_result is not None:\n",
    "    any_decoder_neuron_IDs = directional_trial_by_trial_activity_result.any_decoder_neuron_IDs\n",
    "    active_pf_dt: PfND_TimeDependent = directional_trial_by_trial_activity_result.active_pf_dt\n",
    "    directional_lap_epochs_dict: Dict[str, Epoch] = directional_trial_by_trial_activity_result.directional_lap_epochs_dict\n",
    "    directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts\n",
    "    ## OUTPUTS: directional_trial_by_trial_activity_result, directional_active_lap_pf_results_dicts\n",
    "else:\n",
    "    print(f'TrialByTrialActivity is not computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "most_recent_time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# most_recent_time_bin_size\n",
    "most_recent_continuously_decoded_dict = deepcopy(directional_decoders_decode_result.most_recent_continuously_decoded_dict)\n",
    "# most_recent_continuously_decoded_dict\n",
    "\n",
    "## Adds in the 'pseudo2D' decoder in:\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# time_bin_size: float = 0.01\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = continuously_decoded_result_cache_dict[time_bin_size]\n",
    "pseudo2D_decoder_continuously_decoded_result = continuously_decoded_dict.get('pseudo2D', None)\n",
    "if pseudo2D_decoder_continuously_decoded_result is None:\n",
    "\t# compute here...\n",
    "\t## Currently used for both cases to decode:\n",
    "\tt_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\tsingle_global_epoch_df: pd.DataFrame = pd.DataFrame({'start': [t_start], 'stop': [t_end], 'label': [0]}) # Build an Epoch object containing a single epoch, corresponding to the global epoch for the entire session:\n",
    "\tsingle_global_epoch: Epoch = Epoch(single_global_epoch_df)\n",
    "\tspikes_df = directional_decoders_decode_result.spikes_df\n",
    "\tpseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = pseudo2D_decoder.decode_specific_epochs(spikes_df=deepcopy(spikes_df), filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "\tcontinuously_decoded_dict['pseudo2D'] = pseudo2D_decoder_continuously_decoded_result\n",
    "\tcontinuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dc5fe",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "# `LongShortStatsItem` form (2024-01-02):\n",
    "# LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "# RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260739a4f36c662",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestSplitResult\n",
    "\n",
    "directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data.get('TrainTestSplit', None)\n",
    "training_data_portion: float = directional_train_test_split_result.training_data_portion\n",
    "test_data_portion: float = directional_train_test_split_result.test_data_portion\n",
    "test_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.test_epochs_dict\n",
    "train_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.train_epochs_dict\n",
    "train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_train_test_split_result.train_lap_specific_pf1D_Decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "# active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_extended_stats = global_results['extended_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1c6006aba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Entropy/Surprise Results:\n",
    "active_extended_stats = global_results['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['pf_dt_sequential_surprise'] # DynamicParameters\n",
    "historical_snapshots = active_relative_entropy_results['historical_snapshots']\n",
    "post_update_times: np.ndarray = active_relative_entropy_results['post_update_times'] # (4152,) = (n_post_update_times,)\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals: np.ndarray = active_relative_entropy_results['time_intervals']\n",
    "surprise_time_bin_duration = (post_update_times[2]-post_update_times[1])\n",
    "long_short_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['long_short_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "short_long_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['short_long_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "flat_relative_entropy_results: np.ndarray = active_relative_entropy_results['flat_relative_entropy_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_results: np.ndarray = active_relative_entropy_results['flat_jensen_shannon_distance_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_across_all_positions: np.ndarray = np.sum(np.abs(flat_jensen_shannon_distance_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "flat_surprise_across_all_positions: np.ndarray = np.sum(np.abs(flat_relative_entropy_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "\n",
    "## Get the placefield dt matrix:\n",
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\t## Compute it if missing:\n",
    "\toccupancy_weighted_tuning_maps_over_time = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in historical_snapshots.values()])\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = occupancy_weighted_tuning_maps_over_time\n",
    "else:\n",
    "\toccupancy_weighted_tuning_maps_over_time = active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] # (n_post_update_times, n_neurons, n_xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d3bf5955d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c62d5c18c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "appearing_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec24467335a760",
   "metadata": {},
   "source": [
    "# POST-Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "728c46e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": [
     "unwrap"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n",
      "laps_decoding_time_bin_size: 0.25, ripple_decoding_time_bin_size: 0.025\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multi_sort_raster_browser\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons, paired_incremental_sort_neurons # _display_directional_template_debugger\n",
    "from neuropy.utils.indexing_helpers import paired_incremental_sorting, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ScrollBarWithSpinBox.ScrollBarWithSpinBox import ScrollBarWithSpinBox\n",
    "\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "\n",
    "## Display Testing\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph import QtGui\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import pyqtplot_build_image_bounds_extent, pyqtplot_plot_image\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "# ripple_result_tuple\n",
    "\n",
    "## Unpacks `rank_order_results`: \n",
    "# global_replays = Epoch(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# active_replay_epochs, active_epochs_df, active_selected_spikes_df = combine_rank_order_results(rank_order_results, global_replays, track_templates=track_templates)\n",
    "# active_epochs_df\n",
    "\n",
    "# ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices\n",
    "dir_index_to_direction_name_map: Dict[int, str] = {0:'LR', 1:\"RL\"}\n",
    "\n",
    "\n",
    "## All three DataFrames are the same number of rows, each with one row corresponding to an Epoch:\n",
    "active_replay_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "# active_replay_epochs_df\n",
    "\n",
    "# Change column type to int8 for columns: 'long_best_direction_indices', 'short_best_direction_indices'\n",
    "# directional_likelihoods_df = pd.DataFrame.from_dict(ripple_result_tuple.directional_likelihoods_tuple._asdict()).astype({'long_best_direction_indices': 'int8', 'short_best_direction_indices': 'int8'})\n",
    "directional_likelihoods_df = ripple_result_tuple.directional_likelihoods_df\n",
    "# directional_likelihoods_df\n",
    "\n",
    "# 2023-12-15 - Newest method:\n",
    "# laps_combined_epoch_stats_df = rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "# ripple_combined_epoch_stats_df: pd.DataFrame  = rank_order_results.ripple_combined_epoch_stats_df\n",
    "# ripple_combined_epoch_stats_df\n",
    "\n",
    "\n",
    "# # Concatenate the three DataFrames along the columns axis:\n",
    "# # Assert that all DataFrames have the same number of rows:\n",
    "# assert len(active_replay_epochs_df) == len(directional_likelihoods_df) == len(ripple_combined_epoch_stats_df), \"DataFrames have different numbers of rows.\"\n",
    "# # Assert that all DataFrames have at least one row:\n",
    "# assert len(active_replay_epochs_df) > 0, \"active_replay_epochs_df is empty.\"\n",
    "# assert len(directional_likelihoods_df) > 0, \"directional_likelihoods_df is empty.\"\n",
    "# assert len(ripple_combined_epoch_stats_df) > 0, \"ripple_combined_epoch_stats_df is empty.\"\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = pd.concat([active_replay_epochs_df.reset_index(drop=True, inplace=False), directional_likelihoods_df.reset_index(drop=True, inplace=False), ripple_combined_epoch_stats_df.reset_index(drop=True, inplace=False)], axis=1)\n",
    "# merged_complete_epoch_stats_df = merged_complete_epoch_stats_df.set_index(active_replay_epochs_df.index, inplace=False)\n",
    "\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "# merged_complete_epoch_stats_df.to_csv('output/2023-12-21_merged_complete_epoch_stats_df.csv')\n",
    "# merged_complete_epoch_stats_df\n",
    "\n",
    "laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08508b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_replay_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ripple_is_most_likely_direction_LR_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "753ca336",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(active_epochs_df): 611\n",
      "min_num_unique_aclu_inclusions: 15\n",
      "len(active_epochs_df): 345\n",
      "df_column_names: [['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch'], ['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch'], ['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch'], ['start', 'stop', 'label', 'duration', 'score', 'velocity', 'intercept', 'speed', 'wcorr', 'P_decoder', 'pearsonr', 'travel', 'coverage', 'jump', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness', 'longest_sequence_length', 'longest_sequence_length_ratio', 'direction_change_bin_ratio', 'congruent_dir_bins_ratio', 'total_congruent_direction_change', 'total_variation', 'integral_second_derivative', 'stddev_of_diff', 'is_user_annotated_epoch', 'is_valid_epoch']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>n_unique_aclus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.187177</td>\n",
       "      <td>40.488202</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301025</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.011916</td>\n",
       "      <td>41.359114</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347197</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.588736</td>\n",
       "      <td>44.829950</td>\n",
       "      <td>4</td>\n",
       "      <td>0.241213</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.994329</td>\n",
       "      <td>46.249336</td>\n",
       "      <td>7</td>\n",
       "      <td>0.255007</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.517979</td>\n",
       "      <td>57.987841</td>\n",
       "      <td>12</td>\n",
       "      <td>0.469862</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61.397145</td>\n",
       "      <td>61.662074</td>\n",
       "      <td>14</td>\n",
       "      <td>0.264929</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2033.583486</td>\n",
       "      <td>2033.962356</td>\n",
       "      <td>628</td>\n",
       "      <td>0.378870</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2034.655153</td>\n",
       "      <td>2034.765468</td>\n",
       "      <td>629</td>\n",
       "      <td>0.110315</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2043.892191</td>\n",
       "      <td>2044.079430</td>\n",
       "      <td>630</td>\n",
       "      <td>0.187238</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2045.095432</td>\n",
       "      <td>2045.393262</td>\n",
       "      <td>631</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>2051.142292</td>\n",
       "      <td>2051.269043</td>\n",
       "      <td>632</td>\n",
       "      <td>0.126751</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>2053.276532</td>\n",
       "      <td>2053.445093</td>\n",
       "      <td>633</td>\n",
       "      <td>0.168561</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           start         stop  label  duration  n_unique_aclus\n",
       "0      40.187177    40.488202      0  0.301025              19\n",
       "1      41.011916    41.359114      1  0.347197              27\n",
       "2      44.588736    44.829950      4  0.241213              24\n",
       "3      45.994329    46.249336      7  0.255007              24\n",
       "4      57.517979    57.987841     12  0.469862              19\n",
       "5      61.397145    61.662074     14  0.264929              25\n",
       "..           ...          ...    ...       ...             ...\n",
       "339  2033.583486  2033.962356    628  0.378870              17\n",
       "340  2034.655153  2034.765468    629  0.110315              17\n",
       "341  2043.892191  2044.079430    630  0.187238              15\n",
       "342  2045.095432  2045.393262    631  0.297830              27\n",
       "343  2051.142292  2051.269043    632  0.126751              20\n",
       "344  2053.276532  2053.445093    633  0.168561              20\n",
       "\n",
       "[345 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import HeuristicReplayScoring\n",
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_filter_replay_epochs\n",
    "\n",
    "filtered_epochs_df, filtered_decoder_filter_epochs_decoder_result_dict, filtered_ripple_all_epoch_bins_marginals_df = _perform_filter_replay_epochs(curr_active_pipeline, global_epoch_name, track_templates, decoder_ripple_filter_epochs_decoder_result_dict, ripple_all_epoch_bins_marginals_df, ripple_decoding_time_bin_size=ripple_decoding_time_bin_size,\n",
    "                                                                                                                            should_only_include_user_selected_epochs=False)\n",
    "filtered_epochs_df\n",
    "# filtered_ripple_all_epoch_bins_marginals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6e077",
   "metadata": {},
   "source": [
    "### 2024-02-29 - 4pm - Filter the events for those meeting wcorr criteria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wcorr_threshold: float = 0.33\n",
    "min_wcorr_diff_threshold: float = 0.2\n",
    "\n",
    "is_included_large_wcorr_diff = np.any((filtered_ripple_all_epoch_bins_marginals_df[['wcorr_abs_diff']].abs() > min_wcorr_diff_threshold), axis=1)\n",
    "# is_included_large_wcorr_diff\n",
    "is_included_high_wcorr = np.any((filtered_ripple_all_epoch_bins_marginals_df[['long_best_wcorr', 'short_best_wcorr']].abs() > min_wcorr_threshold), axis=1)\n",
    "\n",
    "df = filtered_ripple_all_epoch_bins_marginals_df[is_included_large_wcorr_diff]\n",
    "# df = filtered_ripple_all_epoch_bins_marginals_df[is_included_high_wcorr]\n",
    "df\n",
    "\n",
    "# delta_aligned_start_t\n",
    "\n",
    "significant_epochs_start_ts = np.squeeze(df['ripple_start_t'].to_numpy()) ## for filtering\n",
    "\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(significant_epochs_start_ts) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict\n",
    "filtered_epochs_df = filtered_epochs_df.epochs.matching_epoch_times_slice(significant_epochs_start_ts)\n",
    "# filtered_ripple_all_epoch_bins_marginals_df = filtered_ripple_all_epoch_bins_marginals_df.epochs.matching_epoch_times_slice(significant_epochs_start_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15332e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "included_qclu_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(filtered_ripple_all_epoch_bins_marginals_df, t_split=split_time_t, active_context=active_context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripple_merged_complete_epoch_stats_df\n",
    "laps_merged_complete_epoch_stats_df\n",
    "['long_best_direction_indices', 'short_best_direction_indices', 'combined_best_direction_indicies', 'long_relative_direction_likelihoods', 'short_relative_direction_likelihoods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time series of Long-likely events\n",
    "# type(long_RL_results) # DynamicParameters\n",
    "long_LR_pf1D_Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_directional_decoder_dict_value)\n",
    "list(all_directional_decoder_dict_value.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df\n",
    "laps_most_likely_direction_from_decoder\n",
    "long_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdabd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ripple_result_tuple) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.DirectionalRankOrderResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "\n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots \n",
    "\n",
    "# @register_type_display(DirectionalRankOrderResult)\n",
    "def plot_histograms(self: DirectionalRankOrderResult, **kwargs) -> \"MatplotlibRenderPlots\":\n",
    "\t\"\"\" \n",
    "\tnum='RipplesRankOrderZscore'\n",
    "\t\"\"\"\n",
    "\tprint(f'.plot_histograms(..., kwargs: {kwargs})')\n",
    "\tfig = plt.figure(layout=\"constrained\", **kwargs)\n",
    "\tax_dict = fig.subplot_mosaic(\n",
    "\t\t[\n",
    "\t\t\t[\"long_short_best_z_score_diff\", \"long_short_best_z_score_diff\"],\n",
    "\t\t\t[\"long_best_z_scores\", \"short_best_z_scores\"],\n",
    "\t\t],\n",
    "\t)\n",
    "\tplots = (pd.DataFrame({'long_best_z_scores': self.long_best_dir_z_score_values}).hist(ax=ax_dict['long_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'short_best_z_scores': self.short_best_dir_z_score_values}).hist(ax=ax_dict['short_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'long_short_best_z_score_diff': self.long_short_best_dir_z_score_diff_values}).hist(ax=ax_dict['long_short_best_z_score_diff'], bins=21, alpha=0.8),\n",
    "\t)\n",
    "\treturn MatplotlibRenderPlots(name='plot_histogram_figure', figures=[fig], axes=ax_dict)\n",
    "\n",
    "\n",
    "register_type_display(plot_histograms, DirectionalRankOrderResult)\n",
    "## Call the newly added `plot_histograms` function on the `ripple_result_tuple` object which is of type `DirectionalRankOrderResult`:\n",
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 CSVs \n",
    "print(f'\\t try saving to CSV...')\n",
    "merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "merged_complete_epoch_stats_df\n",
    "merged_complete_ripple_epoch_stats_df_output_path = curr_active_pipeline.get_output_path().joinpath(f'{DAY_DATE_TO_USE}_merged_complete_epoch_stats_df.csv').resolve()\n",
    "merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
    "print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732743e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import reorder_columns\n",
    "\n",
    "dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4))\n",
    "reorder_columns(merged_complete_epoch_stats_df, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43327521",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_not(np.isnan(rank_order_results.ripple_combined_epoch_stats_df.index).any())\n",
    "# ripple_combined_epoch_stats_df.label.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.label).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.index).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60749347",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "print(f'\\tdone. building global result.')\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_output_tuple (output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles) = ripple_new_output_tuple\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc018065",
   "metadata": {},
   "source": [
    "# Call perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6522e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from attrs import make_class\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "return_full_decoding_results: bool = True\n",
    "# desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.5, num=10)\n",
    "desired_shared_decoding_time_bin_sizes = np.linspace(start=0.005, stop=0.03, num=10)\n",
    "save_hdf: bool = True\n",
    "\n",
    "# SimpleBatchComputationDummy = make_class('SimpleBatchComputationDummy', attrs=['BATCH_DATE_TO_USE', 'collected_outputs_path'])\n",
    "# a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path)\n",
    "\n",
    "_across_session_results_extended_dict = {}\n",
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(a_dummy, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, save_hdf=save_hdf, return_full_decoding_results=return_full_decoding_results,\n",
    "                                                desired_shared_decoding_time_bin_sizes=desired_shared_decoding_time_bin_sizes,\n",
    "                                                )\n",
    "if return_full_decoding_results:\n",
    "    # with `return_full_decoding_results == True`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_full_directional_merged_decoders_result, output_directional_decoders_epochs_decode_results_dict = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    # validate the result:\n",
    "    {k:v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size for k,v in output_full_directional_merged_decoders_result.items()}\n",
    "    assert np.all([np.isclose(dict(k)['desired_shared_decoding_time_bin_size'], v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size) for k,v in output_full_directional_merged_decoders_result.items()]), f\"the desired time_bin_size in the parameters should match the one used that will appear in the decoded result\"\n",
    "\n",
    "else:\n",
    "    # with `return_full_decoding_results == False`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    output_full_directional_merged_decoders_result = None\n",
    "\n",
    "\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v: DecoderDecodedEpochsResult = list(output_directional_decoders_epochs_decode_results_dict.values())[0]\n",
    "type(v)\n",
    "\n",
    "v.add_all_extra_epoch_columns(curr_active_pipeline=curr_active_pipeline, track_templates=track_templates)\n",
    "_out = v.export_csvs(parent_output_path=collected_outputs_path, active_context=curr_active_pipeline.get_session_context(), session_name=curr_active_pipeline.session_name, curr_session_t_delta=t_delta)\n",
    "\n",
    "# assert self.collected_outputs_path.exists()\n",
    "# curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "# CURR_BATCH_OUTPUT_PREFIX: str = f\"{self.BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "# print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "# from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# batch_extended_computations(curr_active_pipeline, include_includelist=['merged_directional_placefields'], include_global_functions=True, fail_on_exception=True, force_recompute=False)\n",
    "# directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "# active_context = curr_active_pipeline.get_session_context()\n",
    "# _out = directional_merged_decoders_result.compute_and_export_marginals_df_csvs(parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
    "# print(f'successfully exported marginals_df_csvs to {self.collected_outputs_path}!')\n",
    "# (laps_marginals_df, laps_out_path), (ripple_marginals_df, ripple_out_path) = _out\n",
    "(laps_marginals_df, laps_out_path, laps_time_bin_marginals_df, laps_time_bin_marginals_out_path), (ripple_marginals_df, ripple_out_path, ripple_time_bin_marginals_df, ripple_time_bin_marginals_out_path) = _out\n",
    "print(f'\\tlaps_out_path: {laps_out_path}\\n\\tripple_out_path: {ripple_out_path}\\n\\tdone.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.ripple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take extra computations from `_decode_and_evaluate_epochs_using_directional_decoders` and integrate into the multi-time-bin results from `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_all_df_score_metrics\n",
    "\n",
    "should_skip_radon_transform = True\n",
    "## Recompute the epoch scores/metrics such as radon transform and wcorr:\n",
    "\n",
    "a_sweep_tuple, a_pseudo_2D_result = list(output_full_directional_merged_decoders_result.items())[0]\n",
    "a_decoder_laps_filter_epochs_decoder_result_dict = deepcopy(a_pseudo_2D_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "a_decoder_ripple_filter_epochs_decoder_result_dict = deepcopy(a_pseudo_2D_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "\n",
    "(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict), merged_df_outputs_tuple, raw_dict_outputs_tuple = _compute_all_df_score_metrics(directional_merged_decoders_result, track_templates,\n",
    "                                                                                                                                                                                    decoder_laps_filter_epochs_decoder_result_dict=a_decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict=a_decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                    spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df),\n",
    "                                                                                                                                                                                    should_skip_radon_transform=should_skip_radon_transform)\n",
    "laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df, laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df = merged_df_outputs_tuple\n",
    "decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict, decoder_ripple_radon_transform_extras_dict, decoder_laps_weighted_corr_df_dict, decoder_ripple_weighted_corr_df_dict = raw_dict_outputs_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_sweep_tuple, a_pseudo_2D_result in output_full_directional_merged_decoders_result.items():\n",
    "    a_pseudo_2D_result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfda868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `_perform_compute_custom_epoch_decoding`\n",
    "\n",
    "a_sweep_tuple\n",
    "# a_pseudo_2D_result.all_directional_laps_filter_epochs_decoder_result\n",
    "# a_pseudo_2D_result\n",
    "# a_pseudo_2D_result.short_directional_decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_keys_if_possible('several_time_bin_sizes_laps_df', several_time_bin_sizes_laps_df)\n",
    "print_keys_if_possible('output_full_directional_merged_decoders_result', output_full_directional_merged_decoders_result, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a71abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file_pat\n",
    "collected_outputs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_laps_decoding_accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# from neuropy.utils.matplotlib_helpers import pho_jointplot\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import pho_jointplot, plot_histograms\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# def pho_jointplot(*args, **kwargs):\n",
    "# \t\"\"\" wraps sns.jointplot to allow adding titles/axis labels/etc.\"\"\"\n",
    "# \ttitle = kwargs.pop('title', None)\n",
    "# \t_out = sns.jointplot(*args, **kwargs)\n",
    "# \tif title is not None:\n",
    "# \t\tplt.suptitle(title)\n",
    "# \treturn _out\n",
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_histograms\n",
    "\n",
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "plot_histograms('Ripples', 'One Session', several_time_bin_sizes_time_bin_ripple_df, \"several\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "several_time_bin_sizes_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(\n",
    "#     several_time_bin_sizes_laps_df, x=\"P_Long\", col=\"species\", row=\"time_bin_size\",\n",
    "#     binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    "# )\n",
    "\n",
    "sns.displot(\n",
    "    several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', row=\"time_bin_size\",\n",
    "    binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be351c18",
   "metadata": {},
   "source": [
    "# 2024-01-31 - Reinvestigation regarding remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d7495",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d9b54",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## From Jonathan Long/Short Peaks\n",
    "\n",
    "adds `active_peak_prominence_2d_results` to existing `neuron_replay_stats_df` from `jonathan_firing_rate_analysis_result`, adding the `['long_pf2D_peak_x', 'long_pf2D_peak_y'] + ['short_pf2D_peak_x', 'short_pf2D_peak_y']` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3641aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df: pd.DataFrame = deepcopy(jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "neuron_replay_stats_df, all_modified_columns = jonathan_firing_rate_analysis_result.add_peak_promenance_pf_peaks(curr_active_pipeline=curr_active_pipeline, track_templates=track_templates)\n",
    "neuron_replay_stats_df, all_modified_columns = jonathan_firing_rate_analysis_result.add_directional_pf_maximum_peaks(track_templates=track_templates)\n",
    "both_included_neuron_stats_df = deepcopy(neuron_replay_stats_df[neuron_replay_stats_df['LS_pf_peak_x_diff'].notnull()]).drop(columns=['track_membership', 'neuron_type'])\n",
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jonathan_firing_rate_analysis_result) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.JonathanFiringRateAnalysisResult\n",
    "\n",
    "rdf_df: pd.DataFrame = deepcopy(jonathan_firing_rate_analysis_result.rdf.rdf)\n",
    "rdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to JSON\n",
    "output_path = Path(f'output/{get_now_day_str()}_rdf_df.json').resolve()\n",
    "rdf_df.to_json(output_path, orient='records', lines=True) ## This actually looks pretty good!\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a677cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to JSON\n",
    "output_path = Path(f'output/{get_now_day_str()}_neuron_replay_stats_df.json').resolve()\n",
    "neuron_replay_stats_df.to_json(output_path, orient='records', lines=True) ## This actually looks pretty good!\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b794a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_columns = ['start', 'end']\n",
    "invalid_columns = ['active_aclus', 'is_neuron_active', 'firing_rates']\n",
    "invalid_df_subset = rdf_df[join_columns + invalid_columns]\n",
    "invalid_df_subset\n",
    "\n",
    "# Reload DataFrame from JSON\n",
    "df_read: pd.DataFrame = pd.read_json(output_path, orient='records', lines=True)\n",
    "df_read\n",
    "\n",
    "# rdf_df.convert_dtypes().dtypes\n",
    "# rdf_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_long_pf].to_numpy()\n",
    "short_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_short_pf].to_numpy()\n",
    "\n",
    "long_pf_aclus, short_pf_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c34fd9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# 2024-04-09 - Maximum peaks only for each template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import NumpyHelpers\n",
    "from neuropy.utils.indexing_helpers import intersection_of_arrays, union_of_arrays\n",
    "from neuropy.utils.indexing_helpers import unwrap_single_item\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing import NewType\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "DecoderName = NewType('DecoderName', str)\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _get_directional_pf_peaks_dfs\n",
    "\n",
    "# (LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = _get_directional_pf_peaks_dfs(track_templates, drop_aclu_if_missing_long_or_short=False)\n",
    "\n",
    "(LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=False)\n",
    "\n",
    "\n",
    "AnyDir_decoder_aclu_MAX_peak_maps_df\n",
    "# LR_only_decoder_aclu_MAX_peak_maps_df\n",
    "# RL_only_decoder_aclu_MAX_peak_maps_df\n",
    "\n",
    "long_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['long_LR'].to_numpy()\n",
    "short_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['short_LR'].to_numpy()\n",
    "peak_x_diff = LR_only_decoder_aclu_MAX_peak_maps_df['peak_diff'].to_numpy()\n",
    "# decoder_aclu_peak_maps_dict\n",
    "\n",
    "## OUTPUTS: AnyDir_decoder_aclu_MAX_peak_maps_df,\n",
    "## OUTPUTS: LR_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n",
    "## OUTPUTS: RL_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n",
    "\n",
    "AnyDir_decoder_aclu_MAX_peak_maps_df\n",
    "LR_only_decoder_aclu_MAX_peak_maps_df\n",
    "RL_only_decoder_aclu_MAX_peak_maps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed05490",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_filtered_flat_peaks_df: pd.DataFrame = deepcopy(AnyDir_decoder_aclu_MAX_peak_maps_df).reset_index(drop=False, names=['aclu'])\n",
    "a_filtered_flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_peak_prominence_2d_results.filtered_flat_peaks_df\n",
    "\n",
    "binned_peak_columns = ['peak_center_binned_x', 'peak_center_binned_y']\n",
    "continuous_peak_columns = ['peak_center_x', 'peak_center_y']\n",
    "\n",
    "['peak_prominence', 'peak_relative_height', 'slice_level_multiplier']\n",
    "\n",
    "['neuron_id', 'neuron_peak_firing_rate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fe1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_track_remapping_diagram(track_templates, grid_bin_bounds, active_context=None, perform_write_to_file_callback=None, defer_render: bool=False, **kwargs):    \n",
    "    \"\"\" Based off of `plot_bidirectional_track_remapping_diagram`\n",
    "    Usage:\n",
    "    \n",
    "        from pyphoplacecellanalysis.Pho2D.track_shape_drawing import plot_bidirectional_track_remapping_diagram\n",
    "\n",
    "        collector = plot_combined_track_remapping_diagram(track_templates, grid_bin_bounds=long_pf2D.config.grid_bin_bounds, active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='plot_bidirectional_track_remapping_diagram'))\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "    from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "    from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "    from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    from neuropy.utils.matplotlib_helpers import build_or_reuse_figure, perform_update_title_subtitle\n",
    "    from pyphoplacecellanalysis.Pho2D.track_shape_drawing import _plot_track_remapping_diagram\n",
    "\n",
    "    use_separate_plot_for_each_direction: bool = True\n",
    "\n",
    "    if active_context is not None:\n",
    "            display_context = active_context.adding_context('display_fn', display_fn_name='bidir_track_remap')\n",
    "        \n",
    "    with mpl.rc_context({'figure.figsize': (10, 4), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "        # Create a FigureCollector instance\n",
    "        with FigureCollector(name='plot_bidirectional_track_remapping_diagram', base_context=display_context) as collector:\n",
    "\n",
    "            ## Define common operations to do after making the figure:\n",
    "            def setup_common_after_creation(a_collector, fig, axes, sub_context, title=f'<size:22>Track <weight:bold>Remapping</></>'):\n",
    "                \"\"\" Captures:\n",
    "\n",
    "                t_split\n",
    "                \"\"\"\n",
    "                a_collector.contexts.append(sub_context)\n",
    "                \n",
    "                # `flexitext` version:\n",
    "                text_formatter = FormattedFigureText()\n",
    "                # ax.set_title('')\n",
    "                fig.suptitle('')\n",
    "                text_formatter.setup_margins(fig)\n",
    "                title_text_obj = flexitext(text_formatter.left_margin, text_formatter.top_margin,\n",
    "                                        title,\n",
    "                                        va=\"bottom\", xycoords=\"figure fraction\")\n",
    "                footer_text_obj = flexitext((text_formatter.left_margin * 0.1), (text_formatter.bottom_margin * 0.25),\n",
    "                                            text_formatter._build_footer_string(active_context=sub_context),\n",
    "                                            va=\"top\", xycoords=\"figure fraction\")\n",
    "            \n",
    "                if ((perform_write_to_file_callback is not None) and (sub_context is not None)):\n",
    "                    perform_write_to_file_callback(sub_context, fig)\n",
    "\n",
    "\n",
    "            # BEGIN FUNCTION BODY\n",
    "            drop_aclu_if_missing_long_or_short=True\n",
    "            # LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df = _get_directional_pf_peaks_dfs(track_templates, drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)\n",
    "            # drop_aclu_if_missing_long_or_short =False\n",
    "            (LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)\n",
    "\n",
    "            ## Make a single figure for both LR/RL remapping cells:\n",
    "            # kwargs = dict(draw_point_aclu_labels=True, enable_interactivity=True)\n",
    "            kwargs = dict(draw_point_aclu_labels=True, enable_interactivity=False)\n",
    "\n",
    "            if use_separate_plot_for_each_direction:\n",
    "                fig, axs = collector.subplots(nrows=2, ncols=1, sharex=True, sharey=True, num='Track Remapping', figsize=kwargs.pop('figsize', (10, 4)), dpi=kwargs.pop('dpi', None), constrained_layout=True, clear=True)\n",
    "                assert len(axs) == 2, f\"{len(axs)}\"\n",
    "                ax_dict = {'ax_LR': axs[0], 'ax_RL': axs[1]}\n",
    "\n",
    "                fig, ax_LR, _outputs_tuple_LR = _plot_track_remapping_diagram(LR_only_decoder_aclu_MAX_peak_maps_df, grid_bin_bounds=grid_bin_bounds, long_column_name='long_LR', short_column_name='short_LR', ax=ax_dict['ax_LR'], defer_render=defer_render, **kwargs)\n",
    "                perform_update_title_subtitle(fig=fig, ax=ax_LR, title_string=None, subtitle_string=f\"LR Track Remapping - {len(LR_only_decoder_aclu_MAX_peak_maps_df)} aclus\")\n",
    "                fig, ax_RL, _outputs_tuple_RL = _plot_track_remapping_diagram(RL_only_decoder_aclu_MAX_peak_maps_df, grid_bin_bounds=grid_bin_bounds, long_column_name='long_RL', short_column_name='short_RL', ax=ax_dict['ax_RL'], defer_render=defer_render, **kwargs)\n",
    "                perform_update_title_subtitle(fig=fig, ax=ax_RL, title_string=None, subtitle_string=f\"RL Track Remapping - {len(RL_only_decoder_aclu_MAX_peak_maps_df)} aclus\")\n",
    "\n",
    "                setup_common_after_creation(collector, fig=fig, axes=[ax_LR, ax_RL], sub_context=display_context.adding_context('subplot', subplot_name='Track Remapping'))\n",
    "            else:\n",
    "                fig, axs = collector.subplots(nrows=1, ncols=1, sharex=True, sharey=True, num='Track Remapping', figsize=kwargs.pop('figsize', (10, 4)), dpi=kwargs.pop('dpi', None), constrained_layout=True, clear=True)\n",
    "                # assert len(axs) == 1, f\"{len(axs)}\"\n",
    "                ax = axs\n",
    "\n",
    "                fig, ax, _outputs_tuple = _plot_track_remapping_diagram(AnyDir_decoder_aclu_MAX_peak_maps_df, grid_bin_bounds=grid_bin_bounds, long_column_name='long_LR', short_column_name='short_LR', ax=ax, defer_render=defer_render, **kwargs)\n",
    "                perform_update_title_subtitle(fig=fig, ax=ax, title_string=None, subtitle_string=f\"LR+RL Track Remapping - {len(LR_only_decoder_aclu_MAX_peak_maps_df)} aclus\")\n",
    "\n",
    "                setup_common_after_creation(collector, fig=fig, axes=[ax, ], sub_context=display_context.adding_context('subplot', subplot_name='Track Remapping'))\n",
    "\n",
    "\n",
    "    return collector\n",
    "\n",
    "\n",
    "collector = plot_combined_track_remapping_diagram(track_templates, grid_bin_bounds=long_pf2D.config.grid_bin_bounds, active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='plot_bidirectional_track_remapping_diagram'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b90dc",
   "metadata": {},
   "source": [
    "## 2024-02-08 - Filter to find only the clear remap examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphocorehelpers.indexing_helpers import dict_to_full_array\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0cf56",
   "metadata": {},
   "source": [
    "### Get num peaks exclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: `directional_active_lap_pf_results_dicts`, not sure why\n",
    "\n",
    "neuron_ids_dict = {k:v.neuron_ids for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "neuron_ids_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42a8d",
   "metadata": {},
   "source": [
    "### Get stability for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ec2d7",
   "metadata": {},
   "source": [
    "#### 2024-02-08 - 3pm - new stability dataframe to look at stability of each cell across decoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57869e27",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## INPUTS: directional_active_lap_pf_results_dicts\n",
    "\n",
    "# for k,v in directional_active_lap_pf_results_dicts.items():\n",
    "# stability_dict = {k:v.aclu_to_stability_score_dict for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict = {k:dict_to_full_array(v.aclu_to_stability_score_dict, full_indicies=any_decoder_neuron_IDs, fill_value=0.0) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "\n",
    "\n",
    "# list(stability_dict.values())\n",
    "\n",
    "stability_dict = {k:list(v.aclu_to_stability_score_dict.values()) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "## all the same size hopefully!\n",
    "# [len(v) for v in list(stability_dict.values())]\n",
    "\n",
    "stability_df: pd.DataFrame = pd.DataFrame({'aclu': any_decoder_neuron_IDs, **stability_dict})\n",
    "# stability_df.rename(dict(zip([], [])))\n",
    "stability_df\n",
    "\n",
    "## OUTPUTS: stability_df, stability_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef13541",
   "metadata": {},
   "source": [
    "# 2024-02-02 - napari_plot_directional_trial_by_trial_activity_viz Trial-by-trial Correlation Matrix C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4df5b",
   "metadata": {},
   "source": [
    "### 🎨 Show Trial-by-trial Correlation Matrix C in `napari`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72df59",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import napari\n",
    "from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_plot_directional_trial_by_trial_activity_viz, napari_trial_by_trial_activity_viz, build_aclu_label, napari_export_image_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magicgui.widgets import ComboBox, Container\n",
    "\n",
    "# annotating a paramater as `napari.Viewer` will automatically provide\n",
    "# the viewer that the function is embedded in, when the function is added to\n",
    "# the viewer with add_function_widget.\n",
    "def my_function(viewer: napari.Viewer):\n",
    "    print(viewer, f\"with {len(viewer.layers)} layers\")\n",
    "\n",
    "# Add our magic function to napari\n",
    "directional_viewer.window.add_function_widget(my_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Directional\n",
    "directional_viewer, directional_image_layer_dict, custom_direction_split_layers_dict = napari_plot_directional_trial_by_trial_activity_viz(directional_active_lap_pf_results_dicts, include_trial_by_trial_correlation_matrix=True)\n",
    "a_result = list(directional_active_lap_pf_results_dicts.values())[0]\n",
    "a_matrix_IDX_to_aclu_map = {v:k for k, v in a_result.aclu_to_matrix_IDX_map.items()}\n",
    "on_update_slider, points_layer = build_aclu_label(directional_viewer, a_matrix_IDX_to_aclu_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d79d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: directional_trial_by_trial_activity_result\n",
    "for a_decoder_name, a_result in directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts.items():\n",
    "    # a_result.z_scored_tuning_map_matrix\n",
    "\n",
    "    if a_decoder_name == 'maze_any':\n",
    "        # smoothed_spikes_maps_matrix\n",
    "        curr_cum_z_scored_tuning_map_matrix = np.cumsum(a_result.z_scored_tuning_map_matrix, axis=0) \n",
    "        # curr_cum_z_scored_tuning_map_matrix = curr_cum_z_scored_tuning_map_matrix / np.nansum(curr_cum_z_scored_tuning_map_matrix, axis=2, keepdims=True) # sum across all position bins for each neuron\n",
    "        \n",
    "        ## Global:\n",
    "        # viewer, image_layer_dict = napari_trial_by_trial_activity_viz(a_result.z_scored_tuning_map_matrix, a_result.C_trial_by_trial_correlation_matrix, title=f'Trial-by-trial Correlation Matrix C - Decoder {a_decoder_name}', axis_labels=('aclu', 'lap', 'xbin')) # GLOBAL\n",
    "        \n",
    "        # viewer, image_layer_dict = napari_trial_by_trial_activity_viz(curr_cum_z_scored_tuning_map_matrix, a_result.C_trial_by_trial_correlation_matrix, title=f'Trial-by-trial Cumulative Decoder {a_decoder_name}', axis_labels=('aclu', 'lap', 'xbin')) # GLOBAL\n",
    "        a_viewer, image_layer_dict = napari_trial_by_trial_activity_viz(a_result.z_scored_tuning_map_matrix, a_result.C_trial_by_trial_correlation_matrix, curr_cum_z_scored_tuning_map_matrix, title=f'Trial-by-trial Cumulative Decoder {a_decoder_name}', axis_labels=('aclu', 'lap', 'xbin')) # GLOBAL\n",
    "        a_matrix_IDX_to_aclu_map = {v:k for k, v in a_result.aclu_to_matrix_IDX_map.items()}\n",
    "\n",
    "        on_update_slider, points_layer = build_aclu_label(a_viewer, a_matrix_IDX_to_aclu_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4ec86",
   "metadata": {},
   "source": [
    "# 2023-09-07 - Track Graphics Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b53254",
   "metadata": {},
   "source": [
    "## 🟢🖼️🎨 2024-02-16 - NOW - Working Track Remapping Diagram Figure!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import plot_bidirectional_track_remapping_diagram, _plot_track_remapping_diagram\n",
    "\n",
    "collector = plot_bidirectional_track_remapping_diagram(track_templates, grid_bin_bounds=long_pf2D.config.grid_bin_bounds, active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='plot_bidirectional_track_remapping_diagram'),\n",
    "                                                        enable_adjust_overlapping_text=False, draw_point_aclu_labels=False, enable_interactivity=False, is_dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "\n",
    "curr_active_pipeline.display('_display_directional_track_remapping_diagram', save_figure=True, is_dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4701a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_aclu_if_missing_long_or_short = True\n",
    "# LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df = _get_directional_pf_peaks_dfs(track_templates, drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)\n",
    "# drop_aclu_if_missing_long_or_short =False\n",
    "(LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=drop_aclu_if_missing_long_or_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c35494",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnyDir_decoder_aclu_MAX_peak_maps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfe113",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "_by_ANY = AnyDir_decoder_aclu_MAX_peak_maps_df.sort_values(by=['long_LR', 'long_RL'], inplace=False)\n",
    "long_peak_sorted_unit_colors_ndarray_map = dict(zip(_by_ANY.index.to_numpy(), list(_unit_colors_ndarray_map.values())))\n",
    "long_peak_sorted_unit_colors_ndarray_map\n",
    "\n",
    "# LR_only_decoder_aclu_MAX_peak_maps_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5286baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnyDir_decoder_aclu_MAX_peak_maps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_helper_neuron_id_to_sort_IDX_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7183e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_peak_sorted_unit_colors_ndarray_map_LR = dict(zip(sorted_neuron_IDs_lists[0], list(_unit_colors_ndarray_map.values())))\n",
    "long_peak_sorted_unit_colors_ndarray_map_RL = dict(zip(sorted_neuron_IDs_lists[1], list(_unit_colors_ndarray_map.values())))\n",
    "long_peak_sorted_unit_colors_ndarray_map_LR\n",
    "long_peak_sorted_unit_colors_ndarray_map_RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colormap = mcolors.ListedColormap(['white'])\n",
    "normalize = mcolors.Normalize(vmin=active_aclus.min(), vmax=active_aclus.max())\n",
    "scalar_map = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "\n",
    "# Create a constant colormap with only white color\n",
    "\n",
    "color = scalar_map.to_rgba(active_aclus)\n",
    "\n",
    "color = [_unit_colors_ndarray_map[an_aclu] for an_aclu in active_aclus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS:\n",
    "neuron_replay_stats_df\n",
    "\n",
    "_active_LR_aclus = np.array(list(_output_by_aclu_dict_LR.keys()))\n",
    "_active_LR_aclus\n",
    "\n",
    "is_active_LR_aclus = np.isin(neuron_replay_stats_df.aclu, _active_LR_aclus)\n",
    "_temp_neuron_replay_stats_df = neuron_replay_stats_df[is_active_LR_aclus]\n",
    "\n",
    "is_active_LR_long_peak_either_cap_dict = _temp_neuron_replay_stats_df['is_long_peak_either_cap'].to_dict()\n",
    "is_active_LR_long_peak_either_cap_dict\n",
    "\n",
    "\n",
    "# either_cap_aclu = {k:v for k,v in is_active_LR_long_peak_either_cap_dict.items() if (v is True)}\n",
    "\n",
    "active_LR_either_cap_aclus = np.array([k for k,v in is_active_LR_long_peak_either_cap_dict.items() if (v is True)])\n",
    "active_LR_either_cap_aclus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dac9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Selected ACLUS manually:\n",
    "\n",
    "## `FakePickEvent` is used to highlight specified aclus by emulating a selection event.\n",
    "#  matplotlib.backend_bases.PickEvent\n",
    "import attrs\n",
    "FakePickEvent = attrs.make_class(\"FakePickEvent\", {k:field() for k in (\"ind\", )})\n",
    "\n",
    "included_aclus = [45, 24, 17, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: included_aclus, LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df, _outputs_tuple_LR, _outputs_tuple_RL\n",
    "included_aclus = active_LR_either_cap_aclus\n",
    "# LR:\n",
    "LR_included_indicies = np.where(np.isin(LR_only_decoder_aclu_MAX_peak_maps_df.index, included_aclus))[0] # LR_included_indicies # [ 6,  9, 22, 36]\n",
    "LR_fake_event: FakePickEvent = FakePickEvent(ind=np.array(LR_included_indicies))\n",
    "_output_dict_LR, _output_by_aclu_dict_LR = _outputs_tuple_LR\n",
    "scatter_select_function_LR = _output_dict_LR['scatter_select_function']\n",
    "scatter_select_function_LR(LR_fake_event)\n",
    "\n",
    "## RL:\n",
    "RL_included_indicies = np.where(np.isin(RL_only_decoder_aclu_MAX_peak_maps_df.index, included_aclus))[0]\n",
    "RL_fake_event: FakePickEvent = FakePickEvent(ind=np.array(RL_included_indicies))\n",
    "_output_dict_RL, _output_by_aclu_dict_RL = _outputs_tuple_RL\n",
    "scatter_select_function_RL = _output_dict_RL['scatter_select_function']\n",
    "scatter_select_function_RL(RL_fake_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.sess.preprocessing_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {},
   "source": [
    "# 🎨 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPlacefieldsPlotter import TimeSynchronizedPlacefieldsPlotter\n",
    "\n",
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968df7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.gui.Qt.tree_helpers import find_tree_item_by_text\n",
    "from pyphoplacecellanalysis.GUI.Qt.MainApplicationWindows.LauncherWidget.LauncherWidget import LauncherWidget\n",
    "\n",
    "widget = LauncherWidget()\n",
    "treeWidget = widget.mainTreeWidget # QTreeWidget\n",
    "widget.build_for_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650eea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.any_decoder_neuron_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_replay_epochs.to_file('new_replays.csv')\n",
    "new_replay_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.minimum_inclusion_fr_Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.neuron_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a672c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg # Used to get the app for TopLevelWindowHelper.top_level_windows\n",
    "## For searching with `TopLevelWindowHelper.all_widgets(...)`:\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike3DRaster import Spike3DRaster\n",
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n",
    "\n",
    "found_spike_raster_windows = TopLevelWindowHelper.all_widgets(pg.mkQApp(), searchType=Spike3DRasterWindowWidget)\n",
    "\n",
    "if len(found_spike_raster_windows) < 1:\n",
    "\t# no existing spike_raster_windows. Make a new one\n",
    "\tprint(f'no existing SpikeRasterWindow. Creating a new one.')\n",
    "\t# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "\tactive_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "else:\n",
    "\tprint(f'found {len(found_spike_raster_windows)} existing Spike3DRasterWindowWidget windows using TopLevelWindowHelper.all_widgets(...). Will use the most recent.')\n",
    "\t# assert len(found_spike_raster_windows) == 1, f\"found {len(found_spike_raster_windows)} Spike3DRasterWindowWidget windows using TopLevelWindowHelper.all_widgets(...) but require exactly one.\"\n",
    "\t# Get the most recent existing one and reuse that:\n",
    "\tspike_raster_window = found_spike_raster_windows[0]\n",
    "\n",
    "\n",
    "# Extras:\n",
    "active_2d_plot = spike_raster_window.spike_raster_plt_2d # <pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster.Spike2DRaster at 0x196c7244280>\n",
    "active_3d_plot = spike_raster_window.spike_raster_plt_3d # <pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster.Spike2DRaster at 0x196c7244280>\n",
    "main_graphics_layout_widget = active_2d_plot.ui.main_graphics_layout_widget # GraphicsLayoutWidget\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "background_static_scroll_plot_widget = active_2d_plot.plots.background_static_scroll_window_plot # PlotItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b245cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import General2DRenderTimeEpochs, inline_mkColor\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.EpochRenderingMixin import EpochRenderingMixin, RenderedEpochsItemsContainer\n",
    "from pyphoplacecellanalysis.General.Model.Datasources.IntervalDatasource import IntervalsDatasource\n",
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n",
    "from neuropy.core.epoch import ensure_dataframe\n",
    "\n",
    "## Use the three dataframes as separate Epoch series:\n",
    "train_test_split_laps_dfs_dict = {\n",
    "    'ReplayNew': ensure_dataframe(new_replay_epochs),\n",
    "    # 'LapsTrain': train_df,\n",
    "    # 'LapsTest': test_df,\n",
    "}\n",
    "\n",
    "train_test_split_laps_epochs_formatting_dict = {\n",
    "    'ReplayNew':dict(y_location=-10.0, height=7.5, pen_color=inline_mkColor('white', 0.8), brush_color=inline_mkColor('white', 0.5)),\n",
    "    # 'LapsTrain':dict(y_location=-2.0, height=1.5, pen_color=inline_mkColor('purple', 0.8), brush_color=inline_mkColor('purple', 0.5)),\n",
    "    # 'LapsTest':dict(y_location=-12.0, height=1.5, pen_color=inline_mkColor('green', 0.8), brush_color=inline_mkColor('green', 0.5)),\n",
    "}\n",
    "\n",
    "required_vertical_offsets, required_interval_heights = EpochRenderingMixin.build_stacked_epoch_layout([1.0], epoch_render_stack_height=40.0, interval_stack_location='below') # ratio of heights to each interval\n",
    "stacked_epoch_layout_dict = {interval_key:dict(y_location=y_location, height=height) for interval_key, y_location, height in zip(list(train_test_split_laps_epochs_formatting_dict.keys()), required_vertical_offsets, required_interval_heights)} # Build a stacked_epoch_layout_dict to update the display\n",
    "# stacked_epoch_layout_dict # {'LapsAll': {'y_location': -3.6363636363636367, 'height': 3.6363636363636367}, 'LapsTrain': {'y_location': -21.818181818181817, 'height': 18.18181818181818}, 'LapsTest': {'y_location': -40.0, 'height': 18.18181818181818}}\n",
    "\n",
    "# replaces 'y_location', 'position' for each dict:\n",
    "train_test_split_laps_epochs_formatting_dict = {k:(v|stacked_epoch_layout_dict[k]) for k, v in train_test_split_laps_epochs_formatting_dict.items()}\n",
    "train_test_split_laps_epochs_formatting_dict\n",
    "\n",
    "# OUTPUTS: train_test_split_laps_dfs_dict, train_test_split_laps_epochs_formatting_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: train_test_split_laps_dfs_dict\n",
    "train_test_split_laps_dfs_dict = {k:TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(df=v, required_columns_synonym_dict=IntervalsDatasource._time_column_name_synonyms) for k, v in train_test_split_laps_dfs_dict.items()}\n",
    "\n",
    "## Build interval datasources for them:\n",
    "train_test_split_laps_dfs_datasources_dict = {k:General2DRenderTimeEpochs.build_render_time_epochs_datasource(v) for k, v in train_test_split_laps_dfs_dict.items()}\n",
    "## INPUTS: active_2d_plot, train_test_split_laps_epochs_formatting_dict, train_test_split_laps_dfs_datasources_dict\n",
    "assert len(train_test_split_laps_epochs_formatting_dict) == len(train_test_split_laps_dfs_datasources_dict)\n",
    "for k, an_interval_ds in train_test_split_laps_dfs_datasources_dict.items():\n",
    "    an_interval_ds.update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, **(train_test_split_laps_epochs_formatting_dict[k] | kwargs)))\n",
    "\n",
    "## Full output: train_test_split_laps_dfs_datasources_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually add the epochs:\n",
    "for k, an_interval_ds in train_test_split_laps_dfs_datasources_dict.items():\n",
    "    active_2d_plot.add_rendered_intervals(an_interval_ds, name=f'{k}', debug_print=False) # adds the interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ecbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_display_items = widget.get_display_function_items()\n",
    "_display_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fcn_name = '_display_batch_pho_jonathan_replay_firing_rate_comparison'\n",
    "a_fn_handle = widget._perform_get_display_function_code(a_fcn_name=a_fcn_name)\n",
    "assert a_fn_handle is not None\n",
    "# args = []\n",
    "# kwargs = {}\n",
    "a_disp_fn_item = widget.get_display_function_item(a_fn_name=a_fcn_name)\n",
    "assert a_disp_fn_item is not None, f\"a_disp_fn_item is None! for a_fn_name='{a_fcn_name}'\"\n",
    "\n",
    "a_disp_fn_item.is_global\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display(display_function=a_fcn_name, active_session_configuration_context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d735d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import BatchPhoJonathanFiguresHelper\n",
    "\n",
    "active_out_figures_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, n_max_page_rows=10, disable_top_row=True, write_png=False, write_vector_format=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out.figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_display_spike_rasters_pyqtplot_3D_with_2D_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(_display_items.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29925698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import fig_remapping_cells\n",
    "\n",
    "collector: FigureCollector = fig_remapping_cells(curr_active_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not isinstance(curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis, JonathanFiringRateAnalysisResult):\n",
    "    jonathan_firing_rate_analysis_result = JonathanFiringRateAnalysisResult(**curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis.to_dict())\n",
    "else:\n",
    "    jonathan_firing_rate_analysis_result = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "\n",
    "neuron_replay_stats_df = jonathan_firing_rate_analysis_result.neuron_replay_stats_df.copy()\n",
    "neuron_replay_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sorted_neuron_stats_df = neuron_replay_stats_df.sort_values(by=sortby, ascending=[True, True, True], inplace=False).copy() # also did test_df = neuron_replay_stats_df.sort_values(by=['long_pf_peak_x'], inplace=False, ascending=True).copy()\n",
    "_sorted_neuron_stats_df = _sorted_neuron_stats_df[np.isin(_sorted_neuron_stats_df.index, curr_any_context_neurons)] # clip to only those neurons included in `curr_any_context_neurons`\n",
    "_sorted_aclus = _sorted_neuron_stats_df.index.to_numpy()\n",
    "_sorted_neuron_IDXs = _sorted_neuron_stats_df.neuron_IDX.to_numpy()\n",
    "if debug_print:\n",
    "    print(f'_sorted_aclus: {_sorted_aclus}')\n",
    "    print(f'_sorted_neuron_IDXs: {_sorted_neuron_IDXs}')\n",
    "\n",
    "## Use this sort for the 'curr_any_context_neurons' sort order:\n",
    "new_all_aclus_sort_indicies, desired_sort_arr = find_desired_sort_indicies(curr_any_context_neurons, _sorted_aclus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2fa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _directional_laps_overview = curr_active_pipeline.plot._display_directional_laps_overview(curr_active_pipeline.computation_results, a)\n",
    "# _directional_laps_overview = curr_active_pipeline.display('_display_directional_laps_overview')\n",
    "# _directional_laps_overview = curr_active_pipeline.display('_display_grid_bin_bounds_validation')\n",
    "_directional_laps_overview = curr_active_pipeline.display('_display_long_short_pf1D_comparison')\n",
    "\n",
    "_directional_laps_overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9992e42",
   "metadata": {},
   "source": [
    "### 🟢🔝🖼️🎨 2024-06-06 - Works to render the contour curve at a fixed promenence (the shape of the placefield's cap/crest) for each placefield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.peak_prominences import render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter\n",
    "\n",
    "display_output = {}\n",
    "active_config_name = long_LR_name\n",
    "print(f'active_config_name: {active_config_name}')\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "pActiveTuningCurvesPlotter = None\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "active_config_modifiying_kwargs = {\n",
    "    'plotting_config': {'should_use_linear_track_geometry': True, \n",
    "                        't_start': t_start, 't_delta': t_delta, 't_end': t_end,\n",
    "                        }\n",
    "}\n",
    "display_output = display_output | curr_active_pipeline.display('_display_3d_interactive_tuning_curves_plotter', active_config_name, extant_plotter=display_output.get('pActiveTuningCurvesPlotter', None),\n",
    "                                                panel_controls_mode='Qt', should_nan_non_visited_elements=False, zScalingFactor=2000.0, active_config_modifiying_kwargs=active_config_modifiying_kwargs,\n",
    "                                                params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                            ) # Works now!\n",
    "ipcDataExplorer = display_output['ipcDataExplorer']\n",
    "display_output['pActiveTuningCurvesPlotter'] = display_output.pop('plotter') # rename the key from the generic \"plotter\" to \"pActiveSpikesBehaviorPlotter\" to avoid collisions with others\n",
    "pActiveTuningCurvesPlotter = display_output['pActiveTuningCurvesPlotter']\n",
    "root_dockAreaWindow, placefieldControlsContainerWidget, pf_widgets = display_output['pane'] # for Qt mode\n",
    "\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter(ipcDataExplorer, active_peak_prominence_2d_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecde7d",
   "metadata": {},
   "source": [
    "### 2024-06-06 - Works to disable/hide all elements except the contour curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ff05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_placefield_surfaces_are_hidden: bool = True\n",
    "all_placefield_points_are_hidden: bool = True\n",
    "\n",
    "disabled_peak_subactors_names_list = ['boxes', 'text', 'peak_points']\n",
    "# disabled_peak_subactors_names_list = ['text', 'peak_points']\n",
    "for active_neuron_id, a_plot_dict in ipcDataExplorer.plots['tuningCurvePlotActors'].items():\n",
    "    if a_plot_dict is not None:\n",
    "        # a_plot_dict.peaks\n",
    "        print(f'active_neuron_id: {active_neuron_id}, a_plot_dict.keys(): {list(a_plot_dict.keys())}')\n",
    "        # ['main', 'points', 'peaks']\n",
    "        if a_plot_dict.main is not None:\n",
    "            if all_placefield_surfaces_are_hidden:\n",
    "                a_plot_dict.main.SetVisibility(False)\n",
    "                # pass\n",
    "            \n",
    "        if a_plot_dict.points is not None:\n",
    "            if all_placefield_points_are_hidden:\n",
    "                a_plot_dict.points.SetVisibility(False)\n",
    "                # pass\n",
    "\n",
    "        if a_plot_dict.peaks is not None:\n",
    "            print(f'active_neuron_id: {active_neuron_id}, a_plot_dict.peaks: {list(a_plot_dict.peaks.keys())}')\n",
    "            for a_subactor_name in disabled_peak_subactors_names_list:\n",
    "                a_subactor = a_plot_dict.peaks.get(a_subactor_name, None)\n",
    "                if a_subactor is not None:\n",
    "                    a_subactor.SetVisibility(False)\n",
    "            # if all_placefield_surfaces_are_hidden:\n",
    "            #     a_plot_dict.main.SetVisibility(False) # Change the visibility to match the current tuning_curve_visibility_state\n",
    "\n",
    "# Once done, render\n",
    "ipcDataExplorer.p.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c88dd",
   "metadata": {},
   "source": [
    "### 2024-06-05 - Offset the long and short track to match the `_plot_track_remapping_diagram` 2D remapping figure\n",
    "\n",
    "[/c:/Users/pho/repos/Spike3DWorkEnv/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Pho2D/track_shape_drawing.py:1236](vscode://file/c:/Users/pho/repos/Spike3DWorkEnv/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Pho2D/track_shape_drawing.py:1236)\n",
    "```python\n",
    "# From `Pho2D.track_shape_drawing.a_dir_decoder_aclu_MAX_peak_maps_df`\n",
    "_plot_track_remapping_diagram\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391be83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "track_half_offset: float = 25.0\n",
    "\n",
    "# Long:\n",
    "actor = ipcDataExplorer.long_maze_bg\n",
    "# Get the current position\n",
    "current_position = actor.GetPosition()\n",
    "# Translate by 5.0 units in the y-direction\n",
    "# new_position = (current_position[0], current_position[1] + 5.0, current_position[2])\n",
    "new_position = (current_position[0], track_half_offset, current_position[2])\n",
    "# Set the new position\n",
    "actor.SetPosition(new_position)\n",
    "\n",
    "## Short\n",
    "actor = ipcDataExplorer.short_maze_bg\n",
    "# Get the current position\n",
    "current_position = actor.GetPosition()\n",
    "# Translate by 5.0 units in the y-direction\n",
    "# new_position = (current_position[0], current_position[1] + 5.0, current_position[2])\n",
    "new_position = (current_position[0], -track_half_offset, current_position[2])\n",
    "# Set the new position\n",
    "actor.SetPosition(new_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_two_step_decoder_prediction_error_2D', 'maze_any') # 'maze_any'\n",
    "\n",
    "update_fn = _out_graphics_dict.plot_data['draw_update_fn']\n",
    "num_frames = _out_graphics_dict.plot_data['num_frames']\n",
    "\n",
    "print(f'num_frames: {num_frames}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b374e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "all_save_paths = {}\n",
    "\n",
    "ani = animation.FuncAnimation(_out_graphics_dict.figures[0], update_fn, frames=num_frames, blit=False, repeat=False, interval=20, save_count=50)\n",
    "\n",
    "# ani.to_html5_video()\n",
    "\n",
    "# # To save the animation using Pillow as a gif\n",
    "# _temp_gif_save_path = Path('scatter.gif').resolve()\n",
    "# writer = animation.PillowWriter(fps=15, metadata=dict(artist='Pho Hale'), bitrate=1800)\n",
    "# ani.save(_temp_gif_save_path, writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f69ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.show()\n",
    "\n",
    "# # Save the animation to a BytesIO buffer\n",
    "# buf = io.BytesIO()\n",
    "# ani.save(buf, codec='gif', writer='imagemagick', fps=10)\n",
    "# buf.seek(0)\n",
    "\n",
    "# # Display the GIF\n",
    "# display(Image(data=buf.getvalue(), format='gif'))\n",
    "# Display the GIF\n",
    "# assert _temp_gif_save_path.exists()\n",
    "# Image(_temp_gif_save_path)\n",
    "\n",
    "\n",
    "# for i in np.arange(num_frames):\n",
    "#     update_fn(i) ## Adjust the slider, using its callbacks as well to update the displayed epoch.\n",
    "    \n",
    "#     # _out_rank_order_event_raster_debugger.on_update_epoch_IDX(an_epoch_idx=i)\n",
    "#     active_epoch_label = self.active_epoch_label\n",
    "\n",
    "#     save_paths = []\n",
    "\n",
    "#     for a_decoder, a_plot in self.root_plots_dict.items():\n",
    "#         curr_filename_prefix = f'Epoch{active_epoch_label}_{a_decoder}'\n",
    "#         # a_plot.setYRange(-0.5, float(self.max_n_neurons))\n",
    "#         out_path = export_path.joinpath(f'{curr_filename_prefix}_plot.png').resolve()\n",
    "#         export_pyqtgraph_plot(a_plot, savepath=out_path, background=pg.mkColor(0, 0, 0, 0))\n",
    "#         save_paths.append(out_path)\n",
    "\n",
    "#     all_save_paths[active_epoch_label] = save_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77194b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_long_short_laps', '_display_long_short_pf1D_comparison', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b267e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_two_step_decoder_prediction_error_2D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adcd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import Image, display\n",
    "import io\n",
    "from pyphocorehelpers.plotting.media_output_helpers import fig_to_clipboard\n",
    "\n",
    "\n",
    "# Generate the frames for the animation\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "line, = ax.plot(x, np.sin(x))\n",
    "\n",
    "def update(frame):\n",
    "    line.set_ydata(np.sin(x + frame / 10.0))\n",
    "    return line,\n",
    "\n",
    "frames = len(x) - 1\n",
    "ani = animation.FuncAnimation(fig, update, frames=frames, blit=True, repeat=True, interval=50)\n",
    "\n",
    "# To save the animation using Pillow as a gif\n",
    "_temp_gif_save_path = Path('scatter.gif').resolve()\n",
    "writer = animation.PillowWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "ani.save(_temp_gif_save_path, writer=writer)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # Save the animation to a BytesIO buffer\n",
    "# buf = io.BytesIO()\n",
    "# ani.save(buf, codec='gif', writer='imagemagick', fps=10)\n",
    "# buf.seek(0)\n",
    "\n",
    "# # Display the GIF\n",
    "# display(Image(data=buf.getvalue(), format='gif'))\n",
    "# Display the GIF\n",
    "assert _temp_gif_save_path.exists()\n",
    "Image(_temp_gif_save_path)\n",
    "\n",
    "\n",
    "# fig_to_clipboard(fig, format='gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7be129",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "graphics_output_dict = curr_active_pipeline.display('_display_long_short_laps')\n",
    "graphics_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedac3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs, plot_data = graphics_output_dict['fig'], graphics_output_dict['axs'], graphics_output_dict['plot_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_display_grid_bin_bounds_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68008d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_long_short_laps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685113bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "# active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_spike_rasters_pyqtplot_2D', 'maze_any') # 'maze_any'\n",
    "assert isinstance(_out_graphics_dict, dict)\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = _out_graphics_dict['spike_raster_plt_2d'], _out_graphics_dict['spike_raster_plt_3d'], _out_graphics_dict['spike_raster_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_renderables_menu = active_2d_plot.ui.menus.custom_context_menus.add_renderables[0].programmatic_actions_dict\n",
    "menu_commands = ['AddTimeIntervals.PBEs', 'AddTimeIntervals.Ripples', 'AddTimeIntervals.Replays', 'AddTimeIntervals.Laps', 'AddTimeIntervals.SessionEpochs']\n",
    "for a_command in menu_commands:\n",
    "    add_renderables_menu[a_command].trigger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5650ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(add_renderables_menu.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('add_renderables_menu', add_renderables_menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d_interactive_tuning_curves_plotter\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "active_config_modifiying_kwargs = {\n",
    "    'plotting_config': {'should_use_linear_track_geometry': True, \n",
    "                        't_start': t_start, 't_delta': t_delta, 't_end': t_end,\n",
    "                        }\n",
    "}\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_3d_interactive_tuning_curves_plotter', active_session_configuration_context=global_epoch_context,\n",
    "                                            active_config_modifiying_kwargs=active_config_modifiying_kwargs,\n",
    "                                            params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                           )\n",
    "ipcDataExplorer = _out_graphics_dict['ipcDataExplorer'] # InteractivePlaceCellTuningCurvesDataExplorer \n",
    "p = _out_graphics_dict['plotter']\n",
    "pane = _out_graphics_dict['pane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=global_epoch_context) # , computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.025}]\n",
    "ipspikesDataExplorer = _out['ipspikesDataExplorer']\n",
    "p = _out['plotter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplapsDataExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "\n",
    "an_image_file_path = Path('an_image.png').resolve()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_image_plotter', active_session_configuration_context=global_epoch_context, image_file=an_image_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ce93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_config in curr_active_pipeline.active_configs.items():\n",
    "    print(f'a_config.plotting_config.should_use_linear_track_geometry: {a_config.plotting_config.should_use_linear_track_geometry}')\n",
    "    a_config.plotting_config.should_use_linear_track_geometry = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d560b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "\n",
    "_out = TemplateDebugger.init_templates_debugger(track_templates) # , included_any_context_neuron_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1323cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "\n",
    "\n",
    "_out = batch_perform_all_plots(curr_active_pipeline=curr_active_pipeline, enable_neptune=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 2D matrix\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import pv\n",
    "\n",
    "matrix = np.random.rand(10, 10)\n",
    "\n",
    "# Coordinates\n",
    "x, y = np.meshgrid(np.arange(matrix.shape[1]), np.arange(matrix.shape[0]))\n",
    "z = matrix.flatten()\n",
    "\n",
    "# Colors based on recency of updates (for example purposes, random values)\n",
    "colors = np.random.rand(matrix.size)\n",
    "\n",
    "# Create the plotter\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "# Add points (dots)\n",
    "points = np.column_stack((x.flatten(), y.flatten(), z))\n",
    "point_cloud = pv.PolyData(points)\n",
    "point_cloud['colors'] = colors\n",
    "plotter.add_mesh(point_cloud, render_points_as_spheres=True, point_size=10, scalars='colors', cmap='viridis')\n",
    "\n",
    "# Add stems\n",
    "for i in range(len(z)):\n",
    "    line = pv.Line([x.flatten()[i], y.flatten()[i], 0], [x.flatten()[i], y.flatten()[i], z[i]])\n",
    "    plotter.add_mesh(line, color='black')\n",
    "\n",
    "# Show plot\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23611eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot.display_function_items\n",
    "\n",
    "# '_display_directional_template_debugger'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "directional_laps_overview = curr_active_pipeline.display(display_function='_display_directional_laps_overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ee6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pic_placefields = curr_active_pipeline.display('_display_1d_placefields', long_LR_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pic_placefields_short_LR = curr_active_pipeline.display('_display_1d_placefields', short_LR_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c334080",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eafe14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f93040",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_immediate_dependencies(remaining_comp_specifiers_dict, provided_global_keys):\n",
    "    dependent_validators = {}\n",
    "    for a_name, a_validator in remaining_comp_specifiers_dict.items():\n",
    "        # set(provided_global_keys)\n",
    "        # set(a_validator.results_specification.requires_global_keys)\n",
    "        if a_validator.is_dependency_in_required_global_keys(provided_global_keys):\n",
    "            dependent_validators[a_name] = a_validator\n",
    "        # (provided_global_keys == (a_validator.results_specification.requires_global_keys or []))\n",
    "\n",
    "    for a_name, a_found_validator in dependent_validators.items():\n",
    "        new_provided_global_keys = a_found_validator.results_specification.provides_global_keys\n",
    "        provided_global_keys.extend(new_provided_global_keys)\n",
    "        remaining_comp_specifiers_dict.pop(a_name) # remove\n",
    "\n",
    "    remaining_comp_specifiers_dict = {k:v for k,v in remaining_comp_specifiers_dict.items() if k not in dependent_validators}\n",
    "    # dependent_validators\n",
    "    # remaining_comp_specifiers_dict\n",
    "    print(f'len(remaining_comp_specifiers_dict): {len(remaining_comp_specifiers_dict)}, dependent_validators: {dependent_validators}')\n",
    "    return remaining_comp_specifiers_dict, dependent_validators, provided_global_keys\n",
    "\n",
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd15f5",
   "metadata": {},
   "source": [
    "# 🎨 2024-04-23 - 3D Posterior Plot\n",
    "<!-- t_delta -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7ac95",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=global_epoch_context,\n",
    "                                    params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                    )\n",
    "iplapsDataExplorer: InteractiveCustomDataExplorer = _out['iplapsDataExplorer']\n",
    "pActiveInteractiveLapsPlotter = _out['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "pActiveInteractiveLapsPlotter[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "pActiveInteractiveLapsPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplapsDataExplorer.active_config.plotting_config.subplots_shape # '1|5'\n",
    "iplapsDataExplorer.active_config.plotting_config.plotter_type # 'BackgroundPlotter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots_shape_str: str = '1|5'\n",
    "subplots_shape_arr_strs = subplots_shape_str.split('|')\n",
    "\n",
    "subplots_shape = [int(k) for k in subplots_shape_arr_strs]\n",
    "subplots_shape\n",
    "\n",
    "total_n_plots: int = np.prod(subplots_shape)\n",
    "if total_n_plots > 1:\n",
    "    iplapsDataExplorer.active_config.plotting_config.plotter_type = 'BackgroundPlotter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iplapsDataExplorer.p.\n",
    "\n",
    "p = iplapsDataExplorer.p[0,0]\n",
    "p\n",
    "# p = self.p[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_global = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=global_epoch_context) # , config_override_kwargs={'plotting_config': {'should_use_linear_track_geometry': True}}\n",
    "# ipspikesDataExplorer = _out_global['ipspikesDataExplorer']\n",
    "# p = _out_global['plotter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: active_config\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "active_config_modifiying_kwargs = {\n",
    "    'plotting_config': {'should_use_linear_track_geometry': True, \n",
    "                        't_start': t_start, 't_delta': t_delta, 't_end': t_end,\n",
    "                        }\n",
    "}\n",
    "_out_global = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=global_epoch_context,\n",
    "                                            active_config_modifiying_kwargs=active_config_modifiying_kwargs,\n",
    "                                            params_kwargs=dict(enable_historical_spikes=False, enable_recent_spikes=False, should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                           )\n",
    "ipspikesDataExplorer = _out_global['ipspikesDataExplorer']\n",
    "p = _out_global['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k, v in active_config_modifiying_kwargs.items():\n",
    "    curr_subdict = active_config.get(k, {})\n",
    "    for sub_k, sub_v in v.items():\n",
    "        try:\n",
    "            curr_subdict[sub_k] = sub_v # apply the update\n",
    "        except TypeError as err:\n",
    "            # TypeError: 'PlottingConfig' object does not support item assignment\n",
    "            setattr(curr_subdict, sub_k, sub_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27140db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_config.plotting_config.should_use_linear_track_geometry\n",
    "active_config.plotting_config.t_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.time_animations import TrackConfigurationTimeAnimationRoutine\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "custom_track_animatior: TrackConfigurationTimeAnimationRoutine = TrackConfigurationTimeAnimationRoutine(t_start=t_start, t_delta=t_delta, t_end=t_end, \n",
    "        long_maze_bg=ipspikesDataExplorer.plots['long_maze_bg'], short_maze_bg=ipspikesDataExplorer.plots['short_maze_bg'],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveSliderWrapper import InteractiveSliderWrapper\n",
    "\n",
    "# interactive_plotter = ipspikesDataExplorer.ui.interactive_plotter # PhoInteractivePlotter\n",
    "\n",
    "active_timestamp_slider_wrapper: InteractiveSliderWrapper = ipspikesDataExplorer.ui.interactive_plotter.interface_properties.active_timestamp_slider_wrapper # InteractiveSliderWrapper \n",
    "active_timestamp_slider_wrapper.curr_value # 17659.517659\n",
    "active_timestamp_slider_wrapper.curr_index # 17659\n",
    "\n",
    "\n",
    "curr_i: int = int(active_timestamp_slider_wrapper.curr_index)\n",
    "active_window_sample_indicies = np.squeeze(ipspikesDataExplorer.params.pre_computed_window_sample_indicies[curr_i,:]) # Get the current precomputed indicies for this curr_i\n",
    "\n",
    "## Spike Plotting:\n",
    "# Get the times that fall within the current plot window:\n",
    "curr_time_fixedSegments = ipspikesDataExplorer.t[active_window_sample_indicies] # New Way\n",
    "t_start = curr_time_fixedSegments[0]\n",
    "t_stop = curr_time_fixedSegments[-1]\n",
    "\n",
    "# \n",
    "t_start, t_stop\n",
    "# custom_track_animatior.on_update_current_window(t_start=t_start, t_stop=t_stop)\n",
    "# curr_index\n",
    "active_timestamp_slider_wrapper.slider_obj.SetEnabled(False) # hide the typical timestamp slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_one_step_decoder = deepcopy(global_results.pf2D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _update_nearest_decoded_most_likely_position_callback, _conn = add_nearest_decoded_position_indicator_circle(self, active_one_step_decoder, _debug_print = False)\n",
    "\n",
    "_update_nearest_decoded_most_likely_position_callback, _conn = ipspikesDataExplorer.add_nearest_decoded_position_indicator_circle(active_one_step_decoder=active_one_step_decoder, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter\n",
    "\n",
    "## plots a decoder posterior viewer with two sliders: one for epoch_idx and another for epoch_time_bin_idx within that epoch\n",
    "active_one_step_decoder = deepcopy(global_results.pf2D_Decoder) # just used for position binning info\n",
    "# a_result: DecodedFilterEpochsResult = deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'])\n",
    "a_result: DecodedFilterEpochsResult = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'])\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = ipspikesDataExplorer.add_decoded_posterior_bars(a_result=a_result,\n",
    "                                                                                                                         xbin=active_one_step_decoder.xbin, xbin_centers=active_one_step_decoder.xbin_centers, ybin=active_one_step_decoder.ybin, ybin_centers=active_one_step_decoder.ybin_centers,\n",
    "                                                                                                                         enable_plot_all_time_bins_in_epoch_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer.params.curr_view_window_length_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79651414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer.clear_all_added_decoded_posterior_plots()\n",
    "ipspikesDataExplorer.p.clear_slider_widgets() # does not actually clear the added sliders\n",
    "ipspikesDataExplorer.on_slider_update_mesh(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ipspikesDataExplorer.params.curr_view_window_length_samples # 299\n",
    "ipspikesDataExplorer.params.curr_view_window_length_samples = 60.0 * 5.0 * ipspikesDataExplorer.active_session.position.sampling_rate # 5 minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipspikesDataExplorer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_interactions.widgets import RangeSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipspikesDataExplorer.add_grid_bin_bounds_box(\n",
    "ipspikesDataExplorer.on_slider_update_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59bafd",
   "metadata": {},
   "source": [
    "# 🖼️🎨 2024-02-28 - WE gotta see the replays on the 3D track. Or the 2D track.\n",
    "2024-04-28 - This is working in both 3D and 2D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ad549",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: directional_laps_results, global_replays, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "# global_pf1D\n",
    "# long_replays\n",
    "# direction_max_indices = ripple_all_epoch_bins_marginals_df[['P_Long', 'P_Short']].values.argmax(axis=1)\n",
    "# track_identity_max_indices = ripple_all_epoch_bins_marginals_df[['P_Long', 'P_Short']].values.argmax(axis=1)\n",
    "\n",
    "## How do I get the replays?\n",
    "# long_replay_df: pd.DataFrame = long_replays.to_dataframe() ## These work.\n",
    "# global_replay_df: pd.DataFrame = global_replays.to_dataframe() ## These work.\n",
    "# global_replay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1D version:\n",
    "## INPUTS: directional_laps_results, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "xbin = deepcopy(directional_laps_results.get_decoders()[0].xbin)\n",
    "xbin_centers = deepcopy(directional_laps_results.get_decoders()[0].xbin_centers)\n",
    "ybin_centers = None\n",
    "ybin = None\n",
    "\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_laps_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "## 1D:\n",
    "a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "\n",
    "## OUTPUTS: a_decoded_filter_epochs_decoder_result_dict, xbin_centers, ybin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cdc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2D version:\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_lap_and_ripple_epochs_decoding_for_decoder\n",
    "\n",
    "## INPUTS: long_results, short_results\n",
    "# long_one_step_decoder_2D\n",
    "\n",
    "long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "one_step_decoder_dict_2D: Dict[str, BayesianPlacemapPositionDecoder] = dict(zip(('long', 'short'), (long_one_step_decoder_2D, short_one_step_decoder_2D)))\n",
    "long_pf2D = long_results.pf2D\n",
    "# short_pf2D = short_results.pf2D\n",
    "\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "xbin_centers = deepcopy(long_pf2D.xbin_centers)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "ybin_centers = deepcopy(long_pf2D.ybin_centers)\n",
    "\n",
    "## OUTPUTS: one_step_decoder_dict_2D, xbin_centers, ybin_centers\n",
    "\n",
    "## INPUTS: one_step_decoder_dict_2D\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "pos_bin_size: Tuple[float, float] = list(one_step_decoder_dict_2D.values())[0].pos_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "\n",
    "## Decode epochs for the two decoders ('long', 'short'):\n",
    "LS_decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {}\n",
    "LS_decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {}\n",
    "\n",
    "for a_name, a_decoder in one_step_decoder_dict_2D.items():\n",
    "    LS_decoder_laps_filter_epochs_decoder_result_dict[a_name], LS_decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _compute_lap_and_ripple_epochs_decoding_for_decoder(a_decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "\n",
    "# LS_decoder_ripple_filter_epochs_decoder_result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2D:\n",
    "# Choose the ripple epochs to plot:\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(LS_decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long'] # 2D\n",
    "# Choose the laps epochs to plot:\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(LS_decoder_laps_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "\n",
    "# a_result: DecodedFilterEpochsResult = LS_decoder_laps_filter_epochs_decoder_result_dict['long'] # 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "\n",
    "## INPUTS: a_result: DecodedFilterEpochsResult, an_epoch_idx: int = 18\n",
    "# e.g. `a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR']`\n",
    "\n",
    "# a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "\n",
    "## Convert to plottable posteriors\n",
    "# an_epoch_idx: int = 0\n",
    "\n",
    "# valid_aclus = deepcopy(decoder_aclu_peak_location_df_merged.aclu.unique())\n",
    "num_filter_epochs: int = a_result.num_filter_epochs\n",
    "a_decoded_traj_plotter = DecodedTrajectoryMatplotlibPlotter(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers)\n",
    "fig, axs, laps_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(global_session, curr_num_subplots=8, active_page_index=0, plot_actual_lap_lines=False, use_theoretical_tracks_instead=True)\n",
    "\n",
    "integer_slider = a_decoded_traj_plotter.plot_epoch_with_slider_widget(an_epoch_idx=6)\n",
    "integer_slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(laps_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps[0].remove()\n",
    "\n",
    "# an_ax.remove(heatmaps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_ax = axs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plotActors, data_dict = plot_3d_stem_points(pCustom, active_epoch_placefields2D.ratemap.xbin, active_epoch_placefields2D.ratemap.ybin, active_epoch_placefields2D.ratemap.occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4627224",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_plot(value=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cfc42",
   "metadata": {},
   "source": [
    "## add to 3D plotter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12058a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter\n",
    "from pyphoplacecellanalysis.Pho3D.PyVista.graphs import plot_3d_stem_points, plot_3d_binned_bars\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=global_epoch_context,\n",
    "                                    params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                    )\n",
    "iplapsDataExplorer: InteractiveCustomDataExplorer = _out['iplapsDataExplorer']\n",
    "pActiveInteractiveLapsPlotter = _out['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b011ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: a_result, xbin_centers, ybin_centers, iplapsDataExplorer\n",
    "# a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = DecodedTrajectoryPyVistaPlotter(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, p=iplapsDataExplorer.p)\n",
    "# a_decoded_trajectory_pyvista_plotter.build_ui()\n",
    "# a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=True)\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=False, active_plot_fn=plot_3d_stem_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=False, active_plot_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplapsDataExplorer.clear_all_added_decoded_posterior_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968821ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_plot_fn = a_decoded_trajectory_pyvista_plotter.data_dict['plot_3d_binned_bars[55.63197815967686]']['update_plot_fn']\n",
    "# update_plot_fn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_posterior_p_x_given_n, n_epoch_timebins = a_decoded_trajectory_pyvista_plotter._perform_get_curr_posterior(a_result=a_result, an_epoch_idx=a_decoded_trajectory_pyvista_plotter.curr_epoch_idx, time_bin_index=np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins))\n",
    "# np.shape(a_posterior_p_x_given_n)\n",
    "\n",
    "\n",
    "a_posterior_p_x_given_n, n_epoch_timebins = a_decoded_trajectory_pyvista_plotter.get_curr_posterior(an_epoch_idx=a_decoded_trajectory_pyvista_plotter.curr_epoch_idx, time_bin_index=np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins))\n",
    "np.shape(a_posterior_p_x_given_n)\n",
    "\n",
    "n_epoch_timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = a_decoded_trajectory_pyvista_plotter.plotActors['plot_3d_binned_bars[49.11980797704307]']\n",
    "# v['main'].remove()\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter.p.remove_actor(v['main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.graphs import clear_3d_binned_bars_plots\n",
    "\n",
    "clear_3d_binned_bars_plots(p=a_decoded_trajectory_pyvista_plotter.p, plotActors=a_decoded_trajectory_pyvista_plotter.plotActors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.plotActors_CenterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.perform_update_plot_epoch_time_bin_range(value=None) # select all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.perform_clear_existing_decoded_trajectory_plots()\n",
    "iplapsDataExplorer.p.update()\n",
    "iplapsDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin_index = np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins)\n",
    "type(time_bin_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.slider_epoch.RemoveAllObservers()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch.Off()\n",
    "# a_decoded_trajectory_pyvista_plotter.slider_epoch.FastDelete()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch = None\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.RemoveAllObservers()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.Off()\n",
    "# a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.FastDelete()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin = None\n",
    "iplapsDataExplorer.p.clear_slider_widgets()\n",
    "iplapsDataExplorer.p.update()\n",
    "iplapsDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66de9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecoderRenderingPyVistaMixin\n",
    "\n",
    "(plotActors, data_dict), (plotActors_CenterLabels, data_dict_CenterLabels) = DecoderRenderingPyVistaMixin.perform_plot_posterior_bars(iplapsDataExplorer.p, xbin=xbin, ybin=ybin, xbin_centers=xbin_centers, ybin_centers=ybin_centers,\n",
    "                                               posterior_p_x_given_n=a_posterior_p_x_given_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d46f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.20720657697753883 * 24.130508176591324"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293adac0",
   "metadata": {},
   "source": [
    "# 🖼️🎨 Rasters Debugger (via `RankOrderRastersDebugger`)\n",
    "<!-- ![image.png|350](attachment:image.png) -->\n",
    "![image.png](attachment:image.png){ width=300; max-width: 300px; }\n",
    "<!-- <img src=\"path_to_your_image.png\" style=\"max-width: 300px;\" /> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff576652",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps) # .trimmed_to_non_overlapping()\n",
    "global_laps_epochs_df = global_laps.to_dataframe()\n",
    "\n",
    "RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "_out_laps_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, global_laps_epochs_df, track_templates, rank_order_results, RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_laps_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a586d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "# global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)\n",
    "# global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps) # .trimmed_to_non_overlapping()\n",
    "# global_laps_epochs_df = global_laps.to_dataframe()\n",
    "global_ripple_epochs_df = global_replays.to_dataframe()\n",
    "\n",
    "RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "_out_ripple_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, global_ripple_epochs_df, track_templates, rank_order_results, RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_ripple_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict = None\n",
    "# rank_order_results\n",
    "# used_rank_order_results = deepcopy(rank_order_results)\n",
    "used_rank_order_results = None\n",
    "_out_ripple_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, deepcopy(filtered_ripple_simple_pf_pearson_merged_df),\n",
    "                                                                                                   track_templates, used_rank_order_results,\n",
    "                                                                                                    RL_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict, LR_active_epoch_selected_spikes_fragile_linear_neuron_IDX_dict)\n",
    "_out_ripple_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_spikes_df = deepcopy(curr_active_pipeline.computation_results[global_epoch_name]['computed_data'].pf1D.spikes_df)\n",
    "_out_ripple_rasters: RankOrderRastersDebugger = RankOrderRastersDebugger.init_rank_order_debugger(global_spikes_df, deepcopy(filtered_ripple_simple_pf_pearson_merged_df),\n",
    "                                                                                                   track_templates, None,\n",
    "                                                                                                    None, None,\n",
    "                                                                                                    dock_add_locations = dict(zip(('long_LR', 'long_RL', 'short_LR', 'short_RL'), (['right'], ['right'], ['right'], ['right']))),\n",
    "                                                                                                    )\n",
    "_out_ripple_rasters.set_top_info_bar_visibility(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_ripple_rasters.set_top_info_bar_visibility(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700afa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide top info bar:\n",
    "LongShortColumnsInfo_dock_layout, LongShortColumnsInfo_dock_Dock = _out_ripple_rasters.plots.dock_widgets['LongShortColumnsInfo_dock']\n",
    "# LongShortColumnsInfo_dock_layout.hide() # No use\n",
    "# _out_ripple_rasters.ui.long_short_info_layout.hide() # No use\n",
    "LongShortColumnsInfo_dock_Dock.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95532207",
   "metadata": {},
   "outputs": [],
   "source": [
    "LongShortColumnsInfo_dock_Dock.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_IDX = _out_ripple_rasters.find_nearest_time_index(193.65)\n",
    "# if found_IDX is not None:\n",
    "#     print(f'found_IDX: {found_IDX}')\n",
    "#     _out_ripple_rasters.programmatically_update_epoch_IDX(found_IDX)\n",
    "\n",
    "\n",
    "_out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(193.65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965556b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_ripple_rasters.on_update_epoch_IDX(45)\n",
    "# on_update_epoch_IDX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_a_ScrollBarWithSpinBox = _out_ripple_rasters.ui.ctrls_widget # ScrollBarWithSpinBox \n",
    "_a_ScrollBarWithSpinBox.setValue(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_directional_template_debugger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_template_debugger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_track_template_pf1Ds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_two_step_decoder_prediction_error_2D', global_epoch_context, variable_name='p_x_given_n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_plot_most_likely_position_comparisons', global_epoch_context) # , variable_name='p_x_given_n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddebd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d805c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_display_directional_merged_pfs'\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pfs', plot_all_directions=False, plot_long_directional=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47076a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_1d_placefield_occupancy'\n",
    "'_display_placemaps_pyqtplot_2D'\n",
    " '_display_2d_placefield_occupancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_2d_placefield_occupancy', global_any_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_grid_bin_bounds_validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58951b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import add_rectangular_selector, add_range_selector\n",
    "\n",
    "\n",
    "# epoch_name = global_any_name\n",
    "epoch_name = short_epoch_name\n",
    "computation_result = curr_active_pipeline.computation_results[epoch_name]\n",
    "grid_bin_bounds = computation_result.computation_config['pf_params'].grid_bin_bounds\n",
    "epoch_context = curr_active_pipeline.filtered_contexts[epoch_name]\n",
    "            \n",
    "fig, ax = computation_result.computed_data.pf2D.plot_occupancy(identifier_details_list=[epoch_name], active_context=epoch_context) \n",
    "\n",
    "# rect_selector, set_extents, reset_extents = add_rectangular_selector(fig, ax, initial_selection=grid_bin_bounds) # (24.82, 257.88), (125.52, 149.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_vertical_track_bounds_lines\n",
    "\n",
    "grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "long_track_line_collection, short_track_line_collection = add_vertical_track_bounds_lines(grid_bin_bounds=grid_bin_bounds, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b862a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import compute_placefield_center_of_mass_positions\n",
    "\n",
    "\n",
    "epoch_name = global_any_name\n",
    "computation_result = curr_active_pipeline.computation_results[epoch_name]\n",
    "grid_bin_bounds = deepcopy(computation_result.computation_config['pf_params'].grid_bin_bounds)\n",
    "epoch_context = curr_active_pipeline.filtered_contexts[epoch_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "long_pf2D.xbin\n",
    "long_pf2D.ybin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy = deepcopy(long_pf2D.occupancy) # occupancy.shape # (60, 15)\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0416d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage # used for `compute_placefield_center_of_masses`\n",
    "from neuropy.utils.mixins.peak_location_representing import compute_occupancy_center_of_mass_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6352663",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_x_center_dict = {k:compute_occupancy_center_of_mass_positions(v.pf.occupancy, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n",
    "occupancy_x_center_dict # {'long_LR': 162.99271603199625, 'long_RL': 112.79866056603696, 'short_LR': 138.45611791646, 'short_RL': 130.78889937230684}\n",
    "\n",
    "occupancy_mask_x_center_dict = {k:compute_occupancy_center_of_mass_positions(v.pf.visited_occupancy_mask, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n",
    "occupancy_mask_x_center_dict # {'long_LR': 135.66781520875904, 'long_RL': 130.0042755113645, 'short_LR': 133.77996864296085, 'short_RL': 143.21920147195175}\n",
    "\n",
    "\n",
    "# {k:compute_occupancy_center_of_mass_positions(v.pf.occupancy, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb029d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy = deepcopy(long_pf2D.occupancy) # occupancy.shape # (60, 15)\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "\n",
    "# masked_nonzero_occupancy = deepcopy(long_pf2D.nan_never_visited_occupancy)\n",
    "\n",
    "masked_nonzero_occupancy = deepcopy(long_pf2D.visited_occupancy_mask)\n",
    "\n",
    "# occupancy_CoM_positions = compute_occupancy_center_of_mass_positions(occupancy, xbin=long_pf2D.xbin, ybin=long_pf2D.ybin)\n",
    "occupancy_CoM_positions = compute_occupancy_center_of_mass_positions(masked_nonzero_occupancy, xbin=long_pf2D.xbin, ybin=long_pf2D.ybin) # array([127.704, 145.63])\n",
    "occupancy_CoM_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf2D.nan_never_visited_occupancy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4caa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict\n",
    "\n",
    "\n",
    "'_display_grid_bin_bounds_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f963a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting on 2024-02-06 to display the LR/RL directions instead of the All/Long/Short pfs:\n",
    "def _display_directional_merged_pfs(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist=None, save_figure=True, included_any_context_neuron_ids=None,\n",
    "\t\t\t\t\t\t\t\t\tplot_all_directions=True, plot_long_directional=False, plot_short_directional=False, **kwargs):\n",
    "\t\"\"\" Plots the merged pseduo-2D pfs/ratemaps. Plots: All-Directions, Long-Directional, Short-Directional in seperate windows. \n",
    "\t\n",
    "\tHistory: this is the Post 2022-10-22 display_all_pf_2D_pyqtgraph_binned_image_rendering-based method:\n",
    "\t\"\"\"\n",
    "\tfrom pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "\tfrom pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow \n",
    "\tfrom pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import LayoutScrollability\n",
    "\n",
    "\tdefer_render = kwargs.pop('defer_render', False)\n",
    "\tdirectional_merged_decoders_result: DirectionalPseudo2DDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\tactive_merged_pf_plots_data_dict = {} #empty dict\n",
    "\t\n",
    "\tif plot_all_directions:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='All-Directions', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.all_directional_pf1D_Decoder.pf # all-directions\n",
    "\tif plot_long_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Long-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.long_directional_pf1D_Decoder.pf # Long-only\n",
    "\tif plot_short_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Short-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.short_directional_pf1D_Decoder.pf # Short-only\n",
    "\n",
    "\tout_plots_dict = {}\n",
    "\t\n",
    "\tfor active_context, active_pf_2D in active_merged_pf_plots_data_dict.items():\n",
    "\t\t# figure_format_config = {} # empty dict for config\n",
    "\t\tfigure_format_config = {'scrollability_mode': LayoutScrollability.NON_SCROLLABLE} # kwargs # kwargs as default figure_format_config\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig: BasicBinnedImageRenderingWindow  = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow\n",
    "\t\n",
    "\t\t# Set the window title from the context\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.setWindowTitle(f'{active_context.get_description()}')\n",
    "\t\tout_plots_dict[active_context] = out_all_pf_2D_pyqtgraph_binned_image_fig\n",
    "\n",
    "\t\t# Tries to update the display of the item:\n",
    "\t\tnames_list = [v for v in list(out_all_pf_2D_pyqtgraph_binned_image_fig.plots.keys()) if v not in ('name', 'context')]\n",
    "\t\tfor a_name in names_list:\n",
    "\t\t\t# Adjust the size of the text for the item by passing formatted text\n",
    "\t\t\ta_plot: pg.PlotItem = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[a_name].mainPlotItem # PlotItem \n",
    "\t\t\t# no clue why 2 is a good value for this...\n",
    "\t\t\ta_plot.titleLabel.setMaximumHeight(2)\n",
    "\t\t\ta_plot.layout.setRowFixedHeight(0, 2)\n",
    "\t\t\t\n",
    "\n",
    "\t\tif not defer_render:\n",
    "\t\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.show()\n",
    "\n",
    "\treturn out_plots_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e434945",
   "metadata": {},
   "source": [
    "# 2023-12-18 - Simpily detect bimodal cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import ContinuousPeakLocationRepresentingMixin\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from scipy.signal import find_peaks\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', sortby=None)\n",
    "\n",
    "# active_ratemap = deepcopy(long_pf1D.ratemap)\n",
    "active_ratemap: Ratemap = deepcopy(long_LR_pf1D.ratemap)\n",
    "peaks_dict, aclu_n_peaks_dict, peaks_results_df = active_ratemap.compute_tuning_curve_modes(height=0.2, width=None)\n",
    "\n",
    "\n",
    "## INPUTS: track_templates\n",
    "included_columns = ['pos', 'peak_heights'] # the columns of interest that you want in the final dataframe.\n",
    "included_columns_renamed = dict(zip(included_columns, ['peak', 'peak_height']))\n",
    "decoder_peaks_results_dfs = [a_decoder.pf.ratemap.get_tuning_curve_peak_df(height=0.2, width=None) for a_decoder in (track_templates.long_LR_decoder, track_templates.long_RL_decoder, track_templates.short_LR_decoder, track_templates.short_RL_decoder)]\n",
    "prefix_names = [f'{a_decoder_name}_' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "all_included_columns = ['aclu', 'series_idx', 'subpeak_idx'] + included_columns # Used to filter out the unwanted columns from the output\n",
    "\n",
    "# [['aclu', 'series_idx', 'subpeak_idx', 'pos']]\n",
    "\n",
    "# rename_list_fn = lambda a_prefix: {'pos': f\"{a_prefix}pos\"}\n",
    "rename_list_fn = lambda a_prefix: {a_col_name:f\"{a_prefix}{included_columns_renamed[a_col_name]}\" for a_col_name in included_columns}\n",
    "\n",
    "# column_names = [f'{a_decoder_name}_peak' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "\n",
    "# dataFrames = decoder_peaks_results_dfs\n",
    "# names = self.get_decoder_names()\n",
    "\n",
    "# rename 'pos' column in each dataframe and then reduce to perform cumulative outer merge\n",
    "result_df = decoder_peaks_results_dfs[0][all_included_columns].rename(columns=rename_list_fn(prefix_names[0]))\n",
    "for df, a_prefix in zip(decoder_peaks_results_dfs[1:], prefix_names[1:]):\n",
    "    result_df = pd.merge(result_df, df[all_included_columns].rename(columns=rename_list_fn(a_prefix)), on=['aclu', 'series_idx', 'subpeak_idx'], how='outer')\n",
    "\n",
    "# result = reorder_columns(result, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "# list(filter(lambda column: column.endswith('_peak_heights'), result.columns))\n",
    "# result_df = reorder_columns(result_df, column_name_desired_index_dict=dict(zip(list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), np.arange(len(result_df.columns)-4, len(result_df.columns)))))\n",
    "# result_df\n",
    "\n",
    "# print(list(result.columns))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "result_df: pd.DataFrame = reorder_columns_relative(result_df, column_names=list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), relative_mode='end').sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually Excluded endcap aclus:\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43')\n",
    "excluded_endcap_aclus: NDArray = np.array(list(set([40, 60, 85, 102, 52, 6] + [83, 60, 52, 102, 40] + [59, 67, 95, 28, 101] + [14, 15, 87, 71] + [43, 84, 87, 19, 33, 51, 53])))\n",
    "excluded_endcap_aclus\n",
    "\n",
    "\n",
    "np.array([  6,  14,  15,  19,  28,  33,  40,  43,  51,  52,  53,  59,  60,  67,  71,  83,  84,  85,  87,  95, 101, 102])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fdee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_peaks_dict_dict, decoder_aclu_n_peaks_dict_dict, decoder_peaks_results_df_dict = track_templates.get_decoders_tuning_curve_modes()\n",
    "decoder_aclu_n_peaks_dict_dict\n",
    "# decoder_peaks_results_df_dict\n",
    "# decoder_peaks_dict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f49ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aclu = 51\n",
    "\n",
    "{k:v[test_aclu] for k, v in decoder_aclu_n_peaks_dict_dict.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_results_df = track_templates.get_decoders_aclu_peak_location_df().sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True) ## Does not seem to merge entries as I would expect via intution. It keeps LR/RL peaks distinct and leaves pd.NA values for the entries.\n",
    "peaks_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c29838",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_n_peaks_dict: Dict = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index().set_index('aclu').to_dict()['subpeak_idx_count'] # number of peaks (\"models\" for each aclu)\n",
    "aclu_n_peaks_dict\n",
    "\n",
    "# peaks_results_df = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index()\n",
    "\n",
    "# peaks_results_df[peaks_results_df.aclu == 5]\n",
    "# peaks_results_df.aclu.value_counts()\n",
    "\n",
    "aclu_n_peaks_dict[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ratemap.n_neurons\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=active_ratemap.neuron_ids, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aclu_n_peaks_dict\n",
    "unimodal_only_aclus = np.array(list(unimodal_peaks_dict.keys()))\n",
    "unimodal_only_aclus\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=unimodal_only_aclus, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c9e39",
   "metadata": {},
   "source": [
    "# 🖼️🎨 2024-02-08 - `PhoPaginatedMultiDecoderDecodedEpochsWindow` - Plot Ripple Metrics like Radon Transforms, WCorr, Simple Pearson, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf989bf6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import RadonTransformPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "## INPUTS: directional_decoders_epochs_decode_result, filtered_epochs_df\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }')\n",
    "\n",
    "# 0.025\n",
    "\n",
    "## OUTPUTS: filtered_decoder_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_epochs = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2162a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_epochs['duration_num_decoding_bins'] = filter_epochs['duration']/ripple_decoding_time_bin_size\n",
    "filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c01240",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [v.edge_info.step for v in filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].time_bin_containers]\n",
    "\n",
    "\n",
    "[np.max(np.diff(v)) for v in filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].time_bin_edges]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b7dab",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "\n",
    "## filter the epochs by something and only show those:\n",
    "# INPUTS: filtered_epochs_df\n",
    "# filtered_ripple_simple_pf_pearson_merged_df = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(active_epochs_df[['start', 'stop']].to_numpy())\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Update the `decoder_ripple_filter_epochs_decoder_result_dict` with the included epochs:\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# print(f\"any_good_selected_epoch_times.shape: {any_good_selected_epoch_times.shape}\") # (142, 2)\n",
    "\n",
    "pre_cols = {a_name:set(a_result.filter_epochs.columns) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "\n",
    "# 🟪 2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "filtered_decoder_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "## 2024-03-08 - Also constrain the user-selected ones (just to try it):\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "# ## Constrain again now by the user selections\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(any_good_selected_epoch_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Instead, add in the 'is_user_annotated_epoch' column instead of filtering\n",
    "## INPUTS: any_good_selected_epoch_times\n",
    "num_user_selected_times: int = len(any_good_selected_epoch_times)\n",
    "print(f'num_user_selected_times: {num_user_selected_times}')\n",
    "any_good_selected_epoch_indicies = None\n",
    "print(f'adding user annotation column!')\n",
    "\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "\n",
    "## OUT: filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "# ## specifically long_LR\n",
    "# filter_epochs: pd.DataFrame = deepcopy(ensure_dataframe(filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs))\n",
    "\n",
    "\n",
    "## OUTPUTS: filtered_epochs_df\n",
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd98310",
   "metadata": {},
   "source": [
    "### 2024-05-09 - get the most-likely decoder for each epoch using the sequenceless probabilities and used this to selected the appopriate column for each of the heuristic measures.\n",
    "Modifies `extracted_merged_scores_df`, adding \"*_BEST\" columns for each specified heuristic score column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_merged_scores_df: pd.DataFrame =  directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df()\n",
    "extracted_merged_scores_df\n",
    "\n",
    "ripple_weighted_corr_merged_df = deepcopy(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df)\n",
    "\n",
    "## Need 'best_decoder_index':... actually 'most_likely_decoder_index'\n",
    "\n",
    "# best_decoder_index = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs['best_decoder_index']) # hope this is correct and not just like the best wcorr or something\n",
    "best_decoder_index = deepcopy(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df['most_likely_decoder_index'])\n",
    "\n",
    "new_heuristic_checking_columns = ['total_variation', 'integral_second_derivative', 'stddev_of_diff', 'score'] # , 'integral_second_derivative', 'stddev_of_diff', 'score'\n",
    "# best_decoder_names = [['long_LR', 'long_RL', 'short_LR', 'short_RL'][an_idx] for an_idx in best_decoder_index]\n",
    "## Example: extracted_merged_scores_df[['total_variation_long_LR', 'total_variation_long_RL', 'total_variation_short_LR', 'total_variation_short_RL']]\n",
    "\n",
    "for a_score_col in new_heuristic_checking_columns:\n",
    "    curr_score_col_decoder_col_names = [f\"{a_score_col}_{a_decoder_name}\" for a_decoder_name in ['long_LR', 'long_RL', 'short_LR', 'short_RL']]\n",
    "    print(f'curr_score_col_decoder_col_names: {curr_score_col_decoder_col_names}')\n",
    "    # extracted_merged_scores_df\n",
    "    _final_out = [extracted_merged_scores_df[curr_score_col_decoder_col_names].to_numpy()[epoch_idx, a_decoder_idx] for epoch_idx, a_decoder_idx in zip(np.arange(np.shape(extracted_merged_scores_df)[0]), best_decoder_index.to_numpy())]\n",
    "    extracted_merged_scores_df[f\"{a_score_col}_BEST\"] = _final_out # extracted_merged_scores_df[curr_score_col_decoder_col_names].to_numpy()[best_decoder_index]\n",
    "\n",
    "extracted_merged_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_merged_scores_df.groupby('is_user_annotated_epoch').agg(['mean', 'min', 'max', 'std']) ## successfully got the most-likely decoder for each epoch using the sequenceless probabilities and used this to selected the appopriate column for each of the heuristic measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34a23a",
   "metadata": {},
   "source": [
    "### Continue something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_epochs_df_dict = {k:deepcopy(v.filter_epochs) for k,v in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "# filter_epochs_df_dict\n",
    "\n",
    "high_wcorr_filter_epochs_dict = {k:np.where((v['wcorr'].abs() >= 0.9))[0] for k,v in filter_epochs_df_dict.items()}\n",
    "# high_wcorr_filter_epochs_dict\n",
    "\n",
    "high_wcorr_any_epochs = union_of_arrays(*[v for k,v in high_wcorr_filter_epochs_dict.items()]) # get unique indicies\n",
    "# high_wcorr_any_epochs\n",
    "\n",
    "# high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "high_wcorr_included_epoch_times = {k:v.iloc[high_wcorr_any_epochs][['start', 'stop']].to_numpy() for k,v in filter_epochs_df_dict.items()}\n",
    "# high_wcorr_included_epoch_times\n",
    "\n",
    "high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(high_wcorr_included_epoch_times[a_name]) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find high wcorr values:\n",
    "filter_epochs = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "\n",
    "# np.sum((filter_epochs['wcorr'].abs() > 0.9))\n",
    "\n",
    "np.where((filter_epochs['wcorr'].abs() > 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs.directionality_ratio.unique()\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs.sweep_score.unique()\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs.laplacian_smoothness.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a merged (single df) frame\n",
    "decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb235522",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_epochs_df\n",
    "filtered_epochs_df[filtered_epochs_df['start'] >= t_delta]\n",
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "## INPUTS filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                decoder_decoded_epochs_result_dict=filtered_decoder_filter_epochs_decoder_result_dict,\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict,\n",
    "                                                                                                epochs_name='ripple',\n",
    "                                                                                                included_epoch_indicies=None, debug_print=False,\n",
    "                                                                                                params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, \n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 10,\n",
    "                                                                                                    'scrollable_figure': False,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': False,\n",
    "                                                                                                })\n",
    "\n",
    "\n",
    "# paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5928125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_out_selections = paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_out_selections = paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(source='diba_evt_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.setWindowTitle('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.perform_update_titles_from_context(page_idx=page_idx, included_page_data_indicies=included_page_data_indicies)\n",
    "update_titles(self, window_title: str, suptitle: str = None)\n",
    "\n",
    "\n",
    "def update_titles(self, window_title: str, suptitle: str = None):\n",
    "    \"\"\" sets the suptitle and window title for the figure \"\"\"\n",
    "    if suptitle is None:\n",
    "        suptitle = window_title # same as window title\n",
    "    # Set the window title:\n",
    "    self.ui.mw.setWindowTitle(window_title)\n",
    "    self.ui.mw.fig.suptitle(suptitle, wrap=True) # set the plot suptitle\n",
    "    self.ui.mw.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import ClickActionCallbacks\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.params.on_middle_click_item_callbacks['copy_axis_image_to_clipboard_callback'] = ClickActionCallbacks.copy_axis_image_to_clipboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.enable_middle_click_selected_epoch_times_to_clipboard()\n",
    "\n",
    "# clicked_epoch = np.array([132.51138943410479, 132.79100273095537])\n",
    "\n",
    "# clicked_epoch = np.array([149.95935746072792, 150.25439218967222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654374c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81780963",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.show_message(\"test message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca820df",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289385ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6097ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get radon transform data:\n",
    "a_pagination_controller = pagination_controller_dict['long_LR']\n",
    "radon_transform_data = a_pagination_controller.plots_data['radon_transform_data']\n",
    "radon_transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30830fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120293e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_selections_dict = paginated_multi_decoder_decoded_epochs_window.save_selections()\n",
    "# paginated_multi_decoder_decoded_epochs_window.ui.print = print\n",
    "_annotations = paginated_multi_decoder_decoded_epochs_window.print_user_annotations()\n",
    "_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d82308",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagination_controller_dict['long_LR'].params.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eeaad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpldatacursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f785638",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3435812",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.params.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd64912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show crosshair at cursor position\n",
    "plt.connect('motion_notify_event', lambda event: plt.gcf().gca().format_coord(event.xdata, event.ydata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c382b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_keys_if_possible('paginated_multi_decoder_decoded_epochs_window', paginated_multi_decoder_decoded_epochs_window.ui, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a14372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.gui.Qt.widgets.toast_notification_widget import ToastWidget, ToastShowingWidgetMixin\n",
    "# paginated_multi_decoder_decoded_epochs_window.ui._contents.windows\n",
    "\n",
    "for a_name, a_window in paginated_multi_decoder_decoded_epochs_window.ui._contents.windows.items():\n",
    "    message = 'This is a toast message!'\n",
    "    a_window.toast.show_message(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_epoch = np.array([1316.0564141790383, 1316.2703788694926])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c265cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import build_attached_raster_viewer_widget\n",
    "\n",
    "_out_ripple_rasters, update_attached_raster_viewer_epoch_callback = build_attached_raster_viewer_widget(paginated_multi_decoder_decoded_epochs_window=paginated_multi_decoder_decoded_epochs_window, track_templates=track_templates, active_spikes_df=active_spikes_df, filtered_ripple_simple_pf_pearson_merged_df=filtered_ripple_simple_pf_pearson_merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625daf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = _out_ripple_rasters.ui.root_dockAreaWindow\n",
    "win.setWindowTitle(f'Debug Directional Template Rasters <Controlled by DecodedEpochSlices window>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae668b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_ripple_rasters.setWindowTitle(f'Debug Directional Template Rasters <Controlled by DecodedEpochSlices window>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_epoch_start_stop_time = [488.296 488.484]\n",
    "start_t = 488.29642327222973\n",
    "found_IDX = 24\n",
    "\n",
    "# ripple_idx=80, ripple_start_t=488.29642327222973\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_attributes(short_name=None, tags=['callback'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-29 17:16', related_items=[])\n",
    "def an_alt_clicked_epoch_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user middle-clicks an epoch \n",
    "    \n",
    "    captures: _out_ripple_rasters\n",
    "    \"\"\"\n",
    "    print(f'an_alt_clicked_epoch_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            print(f'start_t: {start_t}')\n",
    "            _out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1998f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable programmatically updating the rasters viewer to the clicked epoch index when middle clicking on a posterior.\n",
    "@function_attributes(short_name=None, tags=['callback'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-29 17:16', related_items=[])\n",
    "def an_alt_clicked_epoch_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user middle-clicks an epoch \n",
    "    \n",
    "    captures: _out_ripple_rasters\n",
    "    \"\"\"\n",
    "    print(f'an_alt_clicked_epoch_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            print(f'start_t: {start_t}')\n",
    "            _out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(start_t)\n",
    "\n",
    "\n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # a_pagination_controller.params.debug_print = True\n",
    "    if not a_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "        a_pagination_controller.params['on_middle_click_item_callbacks'] = {}    \n",
    "    a_pagination_controller.params.on_middle_click_item_callbacks['an_alt_clicked_epoch_callback'] = an_alt_clicked_epoch_callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to set identical low and high xlims makes transformation singular; automatically expanding. Is this what is causing the white posteriors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR'].params.posterior_heatmap_imshow_kwargs = dict(vmin=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23369f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# paginated_multi_decoder_decoded_epochs_window.update_params(posterior_heatmap_imshow_kwargs = dict(vmin=0.0))\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.update_params(enable_per_epoch_action_buttons = True)\n",
    "paginated_multi_decoder_decoded_epochs_window.refresh_current_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.get_children_props('params')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('plots')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('plots.fig')\n",
    "paginated_multi_decoder_decoded_epochs_window.get_children_props('plots.fig')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('params.posterior_heatmap_imshow_kwargs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ca528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window# AttributeError: 'PhoPaginatedMultiDecoderDecodedEpochsWindow' object has no attribute 'params'\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR'].params.should_suppress_callback_exceptions = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a19394",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.jump_to_page(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea69a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f136d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2150f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # v.params.enable_radon_transform_info = False\n",
    "    # v.params.enable_weighted_correlation_info = False\n",
    "    v._subfn_clear_selectability_rects()\n",
    "    \n",
    "# paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_ctrlr in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    a_ctrlr.perform_update_selections(defer_render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009775d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[785.7379401021171, 785.9232737672282]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[427.4610240198672, 427.55720829055645]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[833.3391086903866, 833.4508065531263]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[491.7975491596153, 492.17844624456484], [940.0164351915009, 940.2191870877286]]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [array([785.738, 785.923])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [array([427.461, 427.557])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [array([833.339, 833.451])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [array([491.798, 492.178]), array([940.016, 940.219])]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[785.7379401021171, 785.9232737672282]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[427.4610240198672, 427.55720829055645]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[833.3391086903866, 833.4508065531263]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[491.7975491596153, 492.17844624456484], [940.0164351915009, 940.2191870877286]]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[208.356, 208.523], [693.842, 693.975], [954.574, 954.679]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[224.037, 224.312]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[145.776, 146.022], [198.220, 198.582], [220.041, 220.259], [511.570, 511.874], [865.238, 865.373]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[191.817, 192.100], [323.147, 323.297]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-paginated_multi_decoder_decoded_epochs_window_page.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "    paginated_multi_decoder_decoded_epochs_window.jump_to_page(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f513296",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.jump_to_page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98478063",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.get_decoder_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec6078",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # v.params.enable_radon_transform_info = False\n",
    "    # v.params.enable_weighted_correlation_info = False\n",
    "    v.params.enable_radon_transform_info = True\n",
    "    v.params.enable_weighted_correlation_info = True\n",
    "    v.params.debug_enabled = True\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    print(f'decoder[{k}]:')\n",
    "    v.params.name\n",
    "    # v.params.on_render_page_callbacks\n",
    "    # v.params.enable_radon_transform_info\n",
    "    len(v.plots_data.radon_transform_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ff3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b447b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.refresh_current_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sub_subfn_wrapped_in_brackets(s: str, bracket_strings = (\"[\", \"]\")) -> str:\n",
    "        return bracket_strings[0] + s + bracket_strings[1]\n",
    "    \n",
    "def _sub_subfn_format_nested_list(arr, precision:int=3, num_sep=\", \", array_sep=', ') -> str:\n",
    "    \"\"\"\n",
    "    Converts a nested list of floats into a single string,\n",
    "    with each float formatted to the specified precision.\n",
    "    \n",
    "    arr = np.array([[491.798, 492.178], [940.016, 940.219]])\n",
    "    _sub_subfn_format_nested_list(arr)\n",
    "\n",
    "    >> '[[491.798, 492.178], [940.016, 940.219]]'\n",
    "\n",
    "    arr = np.array([[785.738, 785.923]])\n",
    "    _sub_subfn_format_nested_list(arr)\n",
    "    >> '[[785.738, 785.923]]'\n",
    "    \"\"\"\n",
    "    return _sub_subfn_wrapped_in_brackets(array_sep.join([_sub_subfn_wrapped_in_brackets(num_sep.join([f\"{num:.{precision}f}\" for num in row])) for row in arr]))\n",
    "    \n",
    "# arr = np.array([[491.798, 492.178], [940.016, 940.219]])\n",
    "arr = np.array([[785.738, 785.923]])\n",
    "_sub_subfn_format_nested_list(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0ab5d",
   "metadata": {},
   "source": [
    "### 2024-02-29 3pm - Get the active user-annotated epoch times from the `paginated_multi_decoder_decoded_epochs_window` and use these to filter `filtered_ripple_simple_pf_pearson_merged_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c982e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inputs: paginated_multi_decoder_decoded_epochs_window, filtered_ripple_simple_pf_pearson_merged_df\n",
    "any_good_selected_epoch_times = deepcopy(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times)\n",
    "any_good_selected_epoch_indicies = deepcopy(paginated_multi_decoder_decoded_epochs_window.find_data_indicies_from_epoch_times(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1f903",
   "metadata": {},
   "source": [
    "## 🔶 2024-03-01 - Get the active user-annotated epoch times from the `UserAnnotationsManager` and use these to filter `filtered_ripple_simple_pf_pearson_merged_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc751d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.misc import numpyify_array\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.epoch import EpochsAccessor\n",
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "## Get from UserAnnotations directly instead of the intermediate viewer\n",
    "\n",
    "## # inputs: any_good_selected_epoch_times, any_good_selected_epoch_times, any_good_selected_epoch_indicies \n",
    "\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies\n",
    "# Add user-selection columns to df\n",
    "a_df = deepcopy(filtered_ripple_simple_pf_pearson_merged_df)\n",
    "# a_df = deepcopy(ripple_weighted_corr_merged_df)\n",
    "a_df['is_user_annotated_epoch'] = False\n",
    "# any_good_selected_epoch_indicies = a_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, np.squeeze(any_good_selected_epoch_times[:,0]), t_column_names=['ripple_start_t',])\n",
    "# any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, any_good_selected_epoch_times, t_column_names=['ripple_start_t',])\n",
    "any_good_selected_epoch_indicies\n",
    "# a_df['is_user_annotated_epoch'] = np.isin(a_df.index.to_numpy(), any_good_selected_epoch_indicies)\n",
    "a_df['is_user_annotated_epoch'].loc[any_good_selected_epoch_indicies] = True # Here's another .iloc issue! Changing to .loc\n",
    "a_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DecoderDecodedEpochsResult.filter_epochs_dfs_by_annotation_times(curr_active_pipeline, any_good_selected_epoch_times, ripple_decoding_time_bin_size, filtered_ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dee7b3",
   "metadata": {},
   "source": [
    "### 2024-02-29 - 4pm - Filter the events for those meeting wcorr criteria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wcorr_threshold: float = 0.33\n",
    "min_wcorr_diff_threshold: float = 0.2\n",
    "\n",
    "is_included_large_wcorr_diff = np.any((df[['wcorr_abs_diff']].abs() > min_wcorr_diff_threshold), axis=1)\n",
    "is_included_high_wcorr = np.any((df[['long_best_wcorr', 'short_best_wcorr']].abs() > min_wcorr_threshold), axis=1)\n",
    "\n",
    "df = df[is_included_high_wcorr]\n",
    "df\n",
    "\n",
    "# delta_aligned_start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifts the absolute times to delta-relative values, as would be needed to draw on a 'delta_aligned_start_t' axis:\n",
    "delta_relative_t_start, delta_relative_t_delta, delta_relative_t_end = np.array([earliest_delta_aligned_t_start, t_delta, latest_delta_aligned_t_end]) - t_delta\n",
    "delta_relative_t_start, delta_relative_t_delta, delta_relative_t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_wcorr_y_col'] = df['long_best_wcorr'].abs()\n",
    "df['_wcorr_y_col_y_diff_col'] = df['long_best_wcorr'].abs() - df['short_best_wcorr'].abs()\n",
    "# df.plot.scatter(x='ripple_start_t', y='wcorr_y_col')\n",
    "df.plot.scatter(x='delta_aligned_start_t', y='_wcorr_y_col_y_diff_col')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5438dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pearsonr_long_abs'] = df['long_best_pf_peak_x_pearsonr'].abs()\n",
    "# df['pearsonr_short_abs'] = df['short_best_pf_peak_x_pearsonr'].abs()\n",
    "# df['pearsonr_diff'] = df['long_best_pf_peak_x_pearsonr'].abs() - df['short_best_pf_peak_x_pearsonr'].abs()\n",
    "\n",
    "# df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_long_abs')\n",
    "# df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_short_abs')\n",
    "df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_abs_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f951c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02556ea",
   "metadata": {},
   "source": [
    "### Add utility footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3888f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig, get_utility_dock_colors\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ThinButtonBar.ThinButtonBarWidget import ThinButtonBarWidget\n",
    "\n",
    "\n",
    "def _add_utility_footer(paginated_multi_decoder_decoded_epochs_window):\n",
    "    ui = paginated_multi_decoder_decoded_epochs_window.ui._contents\n",
    "    # ui.dock_widgets\n",
    "    # ui.dock_configs\n",
    "\n",
    "\n",
    "    ## Build the utility controls at the bottom:\n",
    "    ctrls_dock_config = CustomDockDisplayConfig(custom_get_colors_callback_fn=get_utility_dock_colors, showCloseButton=True, orientation='horizontal')\n",
    "\n",
    "    button_bar_height = 21\n",
    "    ctrls_button_bar_widget = ThinButtonBarWidget()\n",
    "    ctrls_button_bar_widget.setObjectName(\"ctrls_button_bar\")\n",
    "    # Set the background color to blue with 40% opacity (RGBA)\n",
    "    ctrls_button_bar_widget.setStyleSheet(\"background-color: rgba(0, 0, 255, 102);\")\n",
    "\n",
    "    ctrl_layout = pg.LayoutWidget()\n",
    "    ctrl_layout.addWidget(ctrls_button_bar_widget, row=1, rowspan=1, col=1, colspan=2)\n",
    "    ctrl_widgets_dict = dict(ctrls_widget=ctrls_button_bar_widget)\n",
    "    # Set the background color to green with 40% opacity (RGBA)\n",
    "    ctrl_layout.setStyleSheet(\"background-color: rgba(0, 255, 10, 102);\")\n",
    "\n",
    "    # ctrl_layout.setSizePolicy(\n",
    "\n",
    "    def onCopySelectionsClicked():\n",
    "        print(f'onCopySelectionsClicked()')\n",
    "        saved_selections_contexts_dict = paginated_multi_decoder_decoded_epochs_window.print_user_annotations()\n",
    "\n",
    "    ctrl_widgets_dict['copy_selection_connection'] = ctrls_button_bar_widget.sigCopySelections.connect(onCopySelectionsClicked)\n",
    "\n",
    "    ui.dock_widgets['bottom_controls'] = paginated_multi_decoder_decoded_epochs_window.add_display_dock(identifier='bottom_controls', widget=ctrl_layout, dockSize=(600, button_bar_height), dockAddLocationOpts=['bottom'], display_config=ctrls_dock_config, autoOrientation=False)\n",
    "    # ui.dock_widgets['bottom_controls'][1].hideTitleBar()\n",
    "    ui.dock_widgets['bottom_controls']\n",
    "\n",
    "    button_bar_height = 21\n",
    "\n",
    "    a_layout = ui.dock_widgets['bottom_controls'][0]\n",
    "    a_layout.size()\n",
    "    a_layout.setContentsMargins(0,0,0,0)\n",
    "    a_layout.setFixedHeight(21)\n",
    "    ui.dock_widgets['bottom_controls'][1].size()\n",
    "    ui.dock_widgets['bottom_controls'][1].setContentsMargins(0,0,0,0)\n",
    "    ui.dock_widgets['bottom_controls'][1].setStyleSheet(\"background-color: rgba(255, 10, 10, 102);\") # RED\n",
    "\n",
    "    # ui.dock_widgets['bottom_controls'][1].hideTitleBar()\n",
    "    # ui.dock_widgets['bottom_controls'][1].size\n",
    "\n",
    "    return ctrl_layout, ctrls_dock_config, ui\n",
    "\n",
    "\n",
    "ctrl_layout, ctrls_dock_config, ui = _add_utility_footer(paginated_multi_decoder_decoded_epochs_window=new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window=new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window\n",
    "ui = paginated_multi_decoder_decoded_epochs_window.ui._contents\n",
    "\n",
    "layout_widget, dock_item = ui.dock_widgets['bottom_controls']\n",
    "layout_widget.size()\n",
    "# Set the background color to light grey\n",
    "layout_widget.setStyleSheet(\"background-color: red;\")\n",
    "\n",
    "# layout_widget.setBackgroundColor('black')\n",
    "layout_widget.setAutoFillBackground(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3add584",
   "metadata": {},
   "outputs": [],
   "source": [
    " ui.dock_widgets['bottom_controls'][1].size()\n",
    " ui.dock_widgets['bottom_controls'][1].setFixedHeight(21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b47e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.dock_widgets['bottom_controls'][1].children()\n",
    "# [<pyphoplacecellanalysis.External.pyqtgraph.dockarea.DockDrop.DropAreaOverlay object at 0x00000175C7D24820>,\n",
    "#  <PyQt5.QtWidgets.QGridLayout object at 0x00000175C7D248B0>,\n",
    "#  <pyphoplacecellanalysis.External.pyqtgraph.dockarea.Dock.DockLabel object at 0x00000175C7D24E50>,\n",
    "#  <PyQt5.QtWidgets.QWidget object at 0x00000175C7D245E0>,\n",
    "#  <pyphoplacecellanalysis.External.pyqtgraph.dockarea.DockDrop.DropAreaOverlay object at 0x00000175C7D24B80>]\n",
    "\n",
    "ui.dock_widgets['bottom_controls'][1].layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dock_item.showTitleBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dock_item.setOrientation('horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17726a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dock_item.setContentsMargins(0,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_widget.setContentsMargins(0,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "setMargin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.dock_widgets['bottom_controls'][0].resize(600, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39be52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.find_display_dock('bottom_controls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_display_dock('bottom_controls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "## Set epoch annotations from selections epochs \n",
    "annotations_man = UserAnnotationsManager()\n",
    "user_annotations = annotations_man.get_user_annotations()\n",
    "new_selections_dict = paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(user_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf471332",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_selections_objs_dict = {a_name:EpochSelectionsObject(epoch_times=a_selections_values) for a_name, a_selections_values in loaded_selections_dict.items()}\n",
    "loaded_selections_objs_dict\n",
    "\n",
    "## Select just the selected epoch times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_selections_context_dict = {a_name:v.figure_ctx.adding_context_if_missing(user_annotation='selections') for a_name, v in saved_selections_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d312d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.print_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ee6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the excessively long plot titles?\n",
    "# root_dockAreaWindow.update\n",
    "pagination_controller_dict = paginated_multi_decoder_decoded_epochs_window.pagination_controllers\n",
    "all_widgets = {a_decoder_name:a_pagination_controller.ui.mw for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_windows = {a_decoder_name:a_pagination_controller.ui.mw.window() for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_plots = {a_decoder_name:a_pagination_controller.plots for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_plots_data = {a_decoder_name:a_pagination_controller.plots_data for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_params = {a_decoder_name:a_pagination_controller.params for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_current_page_idx = {a_decoder_name:a_pagination_controller.current_page_idx for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_current_page_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_separate_plots\n",
    "\n",
    "all_separate_weighted_corr_plots = {a_decoder_name:a_pagination_controller.plots.get('weighted_corr', {}) for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_weighted_corr_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ui.print = self.private_print # builtins.print # the print function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69313c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import EpochsAccessor\n",
    "\n",
    "# MLM\n",
    "# {a_name:a_ctrlr.params.is_selected for a_name, a_ctrlr in root_dockAreaWindow.pagination_controllers.items()}\n",
    "# {a_name:a_ctrlr.selected_epoch_times for a_name, a_ctrlr in root_dockAreaWindow.pagination_controllers.items()}\n",
    "\n",
    "any_good_selected_epoch_times: NDArray = paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times # drops duplicate rows (present in multiple decoders), and sorts them ascending\n",
    "# any_good_selected_epoch_times\n",
    "# Only at the decoder-level\n",
    "any_good_epoch_idxs_list = [a_ctrlr.find_data_indicies_from_epoch_times(any_good_selected_epoch_times) for a_name, a_ctrlr in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items()]\n",
    "any_good_epoch_idxs: NDArray = any_good_epoch_idxs_list[0]\n",
    "any_good_epoch_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29282203",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531db971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "\n",
    "found_data_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(epoch_times=any_good_selected_epoch_times)\n",
    "df = filtered_ripple_simple_pf_pearson_merged_df.epochs._obj.iloc[found_data_indicies].copy().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1677479",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_selected_ripple_simple_pf_pearson_merged_df = filtered_ripple_simple_pf_pearson_merged_df.iloc[any_good_epoch_idxs, :].reset_index(drop=True)\n",
    "hand_selected_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d400bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand_selected_ripple_simple_pf_pearson_merged_df['best_decoder_index']\n",
    "\n",
    "is_most_likely_long = (hand_selected_ripple_simple_pf_pearson_merged_df['P_Long'] >= 0.5)\n",
    "# is_most_likely_long\n",
    "\n",
    "long_likely_hand_selected_ripple_simple_pf_pearson_merged_df = hand_selected_ripple_simple_pf_pearson_merged_df[is_most_likely_long]\n",
    "long_likely_hand_selected_ripple_simple_pf_pearson_merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d33190",
   "metadata": {},
   "source": [
    "## 🖼️🎨 Plot laps to compare between decoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4873ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import add_laps_groundtruth_information_to_dataframe\n",
    "\n",
    "# decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs # looks like 'lap_dir' column is wrong\n",
    "updated_laps_dfs_dict = {}\n",
    "\n",
    "## Update the .filter_epochs:\n",
    "for k, v in decoder_laps_filter_epochs_decoder_result_dict.items():\n",
    "    updated_laps_dfs_dict[k] = Epoch(add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=ensure_dataframe(v.filter_epochs)))\n",
    "    decoder_laps_filter_epochs_decoder_result_dict[k].filter_epochs =  updated_laps_dfs_dict[k]\n",
    "\n",
    "# updated_laps_dfs_dict['long_LR']\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_app, laps_paginated_multi_decoder_decoded_epochs_window, laps_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                            decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', included_epoch_indicies=None, \n",
    "    params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': False, \n",
    "    # 'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': False,\n",
    "    'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "    # 'disable_y_label': True,\n",
    "    # 'isPaginatorControlWidgetBackedMode': True,\n",
    "    # 'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "    # 'debug_print': True,\n",
    "    'max_subplots_per_page': 10,\n",
    "    'scrollable_figure': True,\n",
    "    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "    'use_AnchoredCustomText': False,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec05d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "\n",
    "## INPUTS: decoder_laps_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Highlight the correct ones:\n",
    "# {k:Epoch(add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=ensure_dataframe(v.filter_epochs))) for k, v in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "\n",
    "## Select the true laps by emulating user_annotations:\n",
    "filter_epochs = ensure_dataframe(deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)) \n",
    "# filter_epochs\n",
    "\n",
    "decoder_name_idx_map = {'long_LR': 0, 'long_RL': 1, 'short_LR': 2, 'short_RL': 3} \n",
    "selections_dict = {}\n",
    "figure_ctx_dict = laps_paginated_multi_decoder_decoded_epochs_window.figure_ctx_dict\n",
    "loaded_selections_context_dict = {a_name:a_figure_ctx.adding_context_if_missing(user_annotation='selections') for a_name, a_figure_ctx in figure_ctx_dict.items()}\n",
    "\n",
    "for a_name, an_idx in decoder_name_idx_map.items():\n",
    "    a_selections_context = loaded_selections_context_dict[a_name]\n",
    "    selections_dict[a_selections_context] = filter_epochs[filter_epochs['true_decoder_index'] == an_idx][['start', 'stop']].to_numpy()\n",
    "\n",
    "\n",
    "## Clearing the existing selection rects and them having them rebuilt when the selection is updated fixes them being shifted.\n",
    "for k, v in laps_pagination_controller_dict.items():\n",
    "    v._subfn_clear_selectability_rects()\n",
    "\n",
    "# _tmp_out_selections = laps_paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(user_annotations=selections_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6607cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e840a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f558fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89caf8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.remove_data_overlays(defer_refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.remov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clearing the existing selection rects and them having them rebuilt when the selection is updated fixes them being shifted.\n",
    "for k, v in laps_pagination_controller_dict.items():\n",
    "    v._subfn_clear_selectability_rects()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f847f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1556870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(decoder_laps_filter_epochs_decoder_result_dict.keys())\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the figure from the axes:\n",
    "a_fig = ax.get_figure()\n",
    "a_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller = laps_paginated_multi_decoder_decoded_epochs_window.contents.pagination_controllers['long_LR'] # DecodedEpochSlicesPaginatedFigureController\n",
    "a_pagination_controller_figure_widget = paginator_controller_widget = a_controlling_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "paginator_controller_widget = a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget # PaginationControlWidget\n",
    "# paginator_controller_widget\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92048bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = a_controlling_pagination_controller.plots.axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_figure().canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47481538",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_rectangles_dict = a_controlling_pagination_controller.plots.get('selection_rectangles_dict', None)\n",
    "selection_rectangles_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa452e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_controlling_pagination_controller.plots.fig.canvas.draw_idle()\n",
    "# a_controlling_pagination_controller.plots.fig.canvas.draw()\n",
    "# paginator_controller_widget.update()\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator_controller_widget.go_to_page(3)\n",
    "# paginator_controller_widget.jump_to_page(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget.jump_to_page\n",
    "\n",
    "new_obj.plots_data.paginator\n",
    "new_obj.params.active_identifying_figure_ctx\n",
    "new_obj.on_paginator_control_widget_jump_to_page(page_idx=0)\n",
    "new_obj.ui.connections['paginator_controller_widget_jump_to_page']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c54dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, extant_plots in a_plots['weighted_corr'].items():\n",
    "    extant_wcorr_text = extant_plots.get('wcorr_text', None)\n",
    "    # extant_wcorr_text = extant_plots.pop('wcorr_text', None)\n",
    "    print(f'extant_wcorr_text: {extant_wcorr_text}')\n",
    "    # plot the radon transform line on the epoch:\n",
    "    if (extant_wcorr_text is not None):\n",
    "        # already exists, clear the existing ones. \n",
    "        # Let's assume we want to remove the 'Quadratic' line (line2)\n",
    "        print(f'removing extant text object at index: {i}.')\n",
    "        # extant_wcorr_text.remove()\n",
    "        extant_wcorr_text.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    display_context = a_pagination_controller.params.get('active_identifying_figure_ctx', IdentifyingContext())\n",
    "\n",
    "    # Get context for current page of items:\n",
    "    current_page_idx: int = int(a_pagination_controller.current_page_idx)\n",
    "    a_paginator = a_pagination_controller.paginator\n",
    "    total_num_pages = int(a_paginator.num_pages)\n",
    "    page_context = display_context.overwriting_context(page=current_page_idx, num_pages=total_num_pages)\n",
    "    print(page_context)\n",
    "\n",
    "    ## Get the figure/axes:\n",
    "    a_plots = a_pagination_controller.plots # RenderPlots\n",
    "    a_plot_data = a_pagination_controller.plots_data\n",
    "\n",
    "    a_params = a_pagination_controller.params\n",
    "    a_params.skip_plotting_measured_positions\n",
    "\n",
    "    figs = a_plots.fig\n",
    "    axs = a_plots.axs\n",
    "\n",
    "    # # with mpl.rc_context({'figure.figsize': (8.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    # with mpl.rc_context({'figure.figsize': (16.8, 4.8), 'figure.dpi': '420', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    #     curr_active_pipeline.output_figure(final_context=page_context, fig=figs, write_vector_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d32342",
   "metadata": {},
   "source": [
    "## 💾 Export Paginated Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laps_paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline)\n",
    "paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.export_decoder_pagination_controller_figure_page(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd2637",
   "metadata": {},
   "source": [
    "## 🔷🎨 Single Decoder Version (`DecodedEpochSlicesPaginatedFigureController`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d880d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import _subfn_update_decoded_epoch_slices\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import DecodedEpochSlicesPaginatedFigureController # `plot_decoded_epoch_slices_paginated`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import WeightedCorrelationPaginatedPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import DecodedPositionsPlotDataProvider, DecodedAndActualPositionsPlotData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import perform_plot_1D_single_most_likely_position_curve\n",
    "\n",
    "# Inputs: epochs_name, decoder_ripple_filter_epochs_decoder_result_dict, curr_active_pipeline\n",
    "epochs_name = 'ripple'\n",
    "\n",
    "(a_name, a_decoder) = tuple(track_templates.get_decoders_dict().items())[0]\n",
    "\n",
    "# a_decoder_decoded_epochs_result = decoder_ripple_filter_epochs_decoder_result_dict[a_name]\n",
    "\n",
    "# a_decoder_decoded_epochs_result = decoder_ripple_filter_epochs_decoder_result_dict[a_name]\n",
    "a_decoder_decoded_epochs_result = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict[a_name]) ## FILTERED\n",
    "\n",
    "_out_pagination_controller = DecodedEpochSlicesPaginatedFigureController.init_from_decoder_data(active_filter_epochs=a_decoder_decoded_epochs_result.filter_epochs,\n",
    "                                                                                    filter_epochs_decoder_result= a_decoder_decoded_epochs_result,\n",
    "                                                                                    xbin=a_decoder.xbin, global_pos_df=curr_active_pipeline.sess.position.df,\n",
    "                                                                                    a_name=f'DecodedEpochSlices[{a_name}]', active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs=epochs_name, decoder=a_name),\n",
    "                                                                                    max_subplots_per_page=32,\n",
    "                                                                                    params_kwargs={'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, 'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': True, #'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 32,\n",
    "                                                                                                    'scrollable_figure': True,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': True,\n",
    "                                                                                                    'disable_toolbar': False,\n",
    "                                                                                    }, \n",
    "                                                                                    # disable_toolbar=False\n",
    "                                                                                    )\n",
    "\n",
    "_out_pagination_controller.params.should_suppress_callback_exceptions = False\n",
    "_out_pagination_controller.add_data_overlays(a_decoder_decoded_epochs_result)\n",
    "_tmp_out_selections = _out_pagination_controller.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c780e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = _out_pagination_controller.plots.fig\n",
    "# fig.toolbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(_out_pagination_controller)\n",
    "\n",
    "_out_pagination_controller.plot_widget._buildUI_setup_statusbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff74414",
   "metadata": {},
   "source": [
    "single_epoch_field_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e429a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_selected_epochs_changed\n",
    "\n",
    "active_captured_single_epoch_result: SingleEpochDecodedResult = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=3)\n",
    "\n",
    "def get_selected_posterior_on_secondary_clicked_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user alt-clicks an epoch \n",
    "    \n",
    "    captures: active_captured_single_epoch_result\n",
    "    \"\"\"\n",
    "    global active_captured_single_epoch_result\n",
    "    if self.params.debug_print:\n",
    "        print(f'get_selected_posterior_on_secondary_clicked_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            # print(f'start_t: {start_t}')\n",
    "            clicked_data_index: int = _out_pagination_controller.find_data_indicies_from_epoch_times(epoch_times=np.array([start_t, end_t]))[0]\n",
    "            if self.params.debug_print:\n",
    "                print(f'\\tclicked_data_index: {clicked_data_index}')            \n",
    "            active_captured_single_epoch_result = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=clicked_data_index)\n",
    "            if self.params.debug_print:\n",
    "                print(f'\\tactive_captured_single_epoch_result.epoch_info_tuple: {active_captured_single_epoch_result.epoch_info_tuple}')\n",
    "                print(f'\\tdone.')\n",
    "\n",
    "\n",
    "# BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "if not _out_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "    _out_pagination_controller.params['on_middle_click_item_callbacks'] = {}\n",
    "\n",
    "_out_pagination_controller.params.on_middle_click_item_callbacks['get_selected_posterior_on_secondary_clicked_callback'] = get_selected_posterior_on_secondary_clicked_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder_decoded_epochs_result.active_filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a619872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.media_output_helpers import get_array_as_image\n",
    "\n",
    "posterior_image = active_captured_single_epoch_result.get_posterior_as_image(desired_width=2048)\n",
    "posterior_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Define 8x8 blur filter kernel\n",
    "blur_kernel = np.ones((8, 8)) / 64\n",
    "\n",
    "# Apply blur to a 2D matrix\n",
    "blurred_matrix = convolve2d(active_captured_single_epoch_result.p_x_given_n, blur_kernel, mode='same', boundary='wrap')\n",
    "\n",
    "get_array_as_image(blurred_matrix, desired_height=400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "{i:col for i, col in enumerate(a_decoder_decoded_epochs_result.active_filter_epochs.columns)}\n",
    "\n",
    "column_indicies = np.arange(12, 19)\n",
    "column_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_pagination_controller.params.debug_print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc81f8",
   "metadata": {},
   "source": [
    "## 2024-04-30 Heuristic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21abb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *position_relative\": mapped between the ends of the track, 0.0 to 1.0\n",
    "most_likely_position_relative = (np.squeeze(active_captured_single_epoch_result.most_likely_position_indicies) / float(active_captured_single_epoch_result.n_xbins-1))\n",
    "most_likely_position_relative\n",
    "\n",
    "\n",
    "plt.hlines([0], colors='k', xmin=active_captured_single_epoch_result.time_bin_edges[0], xmax=active_captured_single_epoch_result.time_bin_edges[-1])\n",
    "plt.step(active_captured_single_epoch_result.time_bin_container.centers[1:], np.diff(most_likely_position_relative))\n",
    "plt.scatter(active_captured_single_epoch_result.time_bin_container.centers, most_likely_position_relative, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c52d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.Qt import QtGui, QtCore, QtWidgets\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph.parametertree.parameterTypes.file import popupFilePicker\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.widgets.FileDialog import FileDialog\n",
    "\n",
    "from silx.gui import qt\n",
    "from silx.gui.dialog.ImageFileDialog import ImageFileDialog\n",
    "from silx.gui.dialog.DataFileDialog import DataFileDialog\n",
    "import silx.io\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import saveFile\n",
    "\n",
    "app = pg.mkQApp('silx_testing')\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc644214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from silx.gui.plot import Plot2D\n",
    "\n",
    "matrix = np.random.rand(10, 10)  # Example 2D matrix\n",
    "plot = Plot2D()\n",
    "plot.addImage(matrix, colormap=\"viridis\", vmin=0, vmax=1)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "HeuristicReplayScoring.bin_wise_track_coverage_score_fn(a_result=a_decoder_decoded_epochs_result, an_epoch_idx=active_captured_single_epoch_result.epoch_data_index, a_decoder_track_length=170.0)\n",
    "\n",
    "# np.diff(active_captured_single_epoch_result.most_likely_position_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5407a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaeb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CodeConversion.convert_dictionary_to_class_defn(\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = _out_pagination_controller.plots.axs[0]\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d52d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.format_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f28b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ascending sequences of most-likely positions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_coord(x, y):\n",
    "    col = round(x)\n",
    "    row = round(y)\n",
    "    nrows, ncols = X.shape\n",
    "    if 0 <= col < ncols and 0 <= row < nrows:\n",
    "        z = X[row, col]\n",
    "        return f'x={x:1.4f}, y={y:1.4f}, z={z:1.4f}'\n",
    "    else:\n",
    "        return f'x={x:1.4f}, y={y:1.4f}'\n",
    "\n",
    "\n",
    "ax.format_coord = format_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de603c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_pagination_controller.plot_widget.setStatusTip('LONG STATUS TIP TEST')\n",
    "\n",
    "_out_pagination_controller.plot_widget.update_status('LONG STATUS TIP TEST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b187c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_pagination_controller.plots.radon_transform\n",
    "fig = _out_pagination_controller.plots.fig\n",
    "\n",
    "# plt.subplots_adjust(left=0.15, right=0.85, top=0.9, bottom=0.1)\n",
    "# Adjust the margins using subplots_adjust\n",
    "fig.subplots_adjust(left=0.15, right=0.85, bottom=0.15, top=0.85)\n",
    "\n",
    "# Adjust the margins using the Figure object\n",
    "# fig.set_tight_layout(dict(rect=[0.1, 0.2, 0.8, 0.8]))\n",
    "# fig.tight_layout(dict(rect=[0.1, 0.2, 0.8, 0.8]))\n",
    "# fig.tight_layout(pad=1.0, rect=[0.1, 0.1, 0.8, 0.8])\n",
    "_out_pagination_controller.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9643ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a_name, a_decoder) = tuple(track_templates.get_decoders_dict().items())[0]\n",
    "a_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d1a25",
   "metadata": {},
   "source": [
    "## 🔷🎨 2024-03-06 - Uni Page Scrollable Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_decoded_epochs_result_dict: generic\n",
    "single_page_app, single_page_paginated_multi_decoder_decoded_epochs_window, single_page_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                included_epoch_indicies=None, debug_print=False,\n",
    "                                                                                                params_kwargs={'skip_plotting_most_likely_positions': False, 'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                               'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                                # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                                # 'disable_y_label': True,\n",
    "                                                                                                                'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                                'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                                # 'debug_print': True,\n",
    "                                                                                                                'max_subplots_per_page': 64,\n",
    "                                                                                                                'scrollable_figure': True,\n",
    "                                                                                                                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2565e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_page_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "_tmp_out_selections = single_page_paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for curr_results_obj: LeaveOneOutDecodingAnalysisResult object\n",
    "num_filter_epochs:int = curr_results_obj.active_filter_epochs.n_epochs\n",
    "\n",
    "# `active_filter_epochs_df` native columns approach\n",
    "active_filter_epochs_df = curr_results_obj.active_filter_epochs.to_dataframe().copy()\n",
    "assert np.isin(['score', 'velocity', 'intercept', 'speed'], active_filter_epochs_df.columns).all()\n",
    "epochs_linear_fit_df = active_filter_epochs_df[['score', 'velocity', 'intercept', 'speed']].copy() # get the `epochs_linear_fit_df` as a subset of the filter epochs df\n",
    "# epochs_linear_fit_df approach\n",
    "assert curr_results_obj.all_included_filter_epochs_decoder_result.num_filter_epochs == np.shape(epochs_linear_fit_df)[0]\n",
    "\n",
    "num_filter_epochs:int = curr_results_obj.all_included_filter_epochs_decoder_result.num_filter_epochs # curr_results_obj.num_filter_epochs\n",
    "try:\n",
    "    time_bin_containers: List[BinningContainer] = deepcopy(curr_results_obj.time_bin_containers)\n",
    "except AttributeError as e:\n",
    "    # AttributeError: 'LeaveOneOutDecodingAnalysisResult' object has no attribute 'time_bin_containers' is expected when `curr_results_obj: LeaveOneOutDecodingAnalysisResult - for Long/Short plotting`\n",
    "    time_bin_containers: List[BinningContainer] = deepcopy(curr_results_obj.all_included_filter_epochs_decoder_result.time_bin_containers) # for curr_results_obj: LeaveOneOutDecodingAnalysisResult - for Long/Short plotting\n",
    "\n",
    "radon_transform_data = RadonTransformPlotDataProvider._subfn_build_radon_transform_plotting_data(active_filter_epochs_df=active_filter_epochs_df,\n",
    "            num_filter_epochs = num_filter_epochs, time_bin_containers = time_bin_containers, radon_transform_column_names=['score', 'velocity', 'intercept', 'speed'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fa458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _display_long_and_short_stacked_epoch_slices\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out_dict = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f3190",
   "metadata": {},
   "source": [
    "## Other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = _out_pagination_controller.plots['radon_transform'][7]\n",
    "extant_line = _out['line'] # matplotlib.lines.Line2D\n",
    "extant_line.linestyle = 'none'\n",
    "# extant_line.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e00165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(curr_active_pipeline.filtered_contexts.keys())) # ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Converting between decoder names and filtered epoch names:\n",
    "# {'long':'maze1', 'short':'maze2'}\n",
    "# {'LR':'odd', 'RL':'even'}\n",
    "long_LR_name, short_LR_name, long_RL_name, short_RL_name = ['maze1_odd', 'maze2_odd', 'maze1_even', 'maze2_even']\n",
    "decoder_name_to_session_context_name: Dict[str,str] = dict(zip(track_templates.get_decoder_names(), (long_LR_name, long_RL_name, short_LR_name, short_RL_name))) # {'long_LR': 'maze1_odd', 'long_RL': 'maze1_even', 'short_LR': 'maze2_odd', 'short_RL': 'maze2_even'}\n",
    "session_context_to_decoder_name: Dict[str,str] = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), track_templates.get_decoder_names())) # {'maze1_odd': 'long_LR', 'maze1_even': 'long_RL', 'maze2_odd': 'short_LR', 'maze2_even': 'short_RL'}\n",
    "\n",
    "decoder_name_to_session_context_name\n",
    "session_context_to_decoder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_num_slices: int = _out_pagination_controller.params.active_num_slices\n",
    "single_plot_fixed_height: float = _out_pagination_controller.params.single_plot_fixed_height\n",
    "all_plots_height: float = _out_pagination_controller.params.all_plots_height\n",
    "print(f'all_plots_height: {all_plots_height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fed6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import _add_maze_id_to_epochs\n",
    "\n",
    "\n",
    "## Add new weighted correlation results as new columns in existing filter_epochs df:\n",
    "active_filter_epochs = long_results_obj.active_filter_epochs\n",
    "# Add the maze_id to the active_filter_epochs so we can see how properties change as a function of which track the replay event occured on:\n",
    "active_filter_epochs = _add_maze_id_to_epochs(active_filter_epochs, short_session.t_start)\n",
    "active_filter_epochs._df['weighted_corr_LONG'] = epoch_long_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_SHORT'] = epoch_short_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_spearman_LONG'] = epoch_long_weighted_corr_results[:,1]\n",
    "active_filter_epochs._df['weighted_corr_spearman_SHORT'] = epoch_short_weighted_corr_results[:,1]\n",
    "\n",
    "\n",
    "active_filter_epochs\n",
    "active_filter_epochs.to_dataframe()\n",
    "## plot the `weighted_corr_LONG` over time\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=1, nrows=active_num_rows, sharex=True, sharey=sharey, figsize=figsize)\n",
    "\n",
    "## Weighted Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_LONG', title='weighted_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "## Weighted Spearman Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_LONG', title='weighted_spearman_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Spearman Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='score_LONG', title='Radon Transform Score during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='score_SHORT', xlabel='Replay Epoch Time', ylabel='Replay Radon Transform Score', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "example_stacked_epoch_graphics = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', defer_render=False, save_figure=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4f9a0",
   "metadata": {},
   "source": [
    "# 2024-02-15 - Do simple spike-t vs. template pf peak correlation like Kamran suggested this morning\n",
    "\n",
    "Replays can be of trajectories on either the current track configuration or on a temporally distant one (such as a trajectory on the long track after the track has been shortened). \n",
    "The goal of the decoder scoring methods are to evaluate how likely each decoder was. This means for each Epoch we obtain a score for all four decoders: Long_LR, Long_RL, Short_LR, Short_RL\n",
    "\n",
    "#### `posterior decoder likelihoods` - This scoring method produces a probability that the\n",
    "\n",
    "#### Radon Transform - TODO\n",
    "\n",
    "#### `compute_simple_spike_time_v_pf_peak_x_by_epoch` - This epoch scoring metric plots the placefield peak x position against the time in seconds of each spike relative to the start of the epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1e967",
   "metadata": {},
   "source": [
    "## TODO 2024-02-15 8pm - Add in to previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd539a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "\n",
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "laps_simple_pf_pearson_merged_df\n",
    "# laps_radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "decoder_aclu_peak_location_df_merged = deepcopy(track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=False))\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "decoder_aclu_peak_location_df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74381521",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result: TrialByTrialActivity = directional_active_lap_pf_results_dicts['long_LR']\n",
    "# a_result.sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421fd1a",
   "metadata": {},
   "source": [
    "# 💾 2024-03-04 - Export `DecoderDecodedEpochsResult` CSVs with user annotations for epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0ae73",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "_, _, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_valid_epoch_times = filtered_epochs_df[['start', 'stop']].to_numpy()\n",
    "\n",
    "## 2024-03-08 - Also constrain the user-selected ones (just to try it):\n",
    "decoder_user_selected_epoch_times_dict, any_user_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "\n",
    "a_result_dict = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# {a_name:ensure_dataframe(a_result.filter_epochs) for a_name, a_result in a_result_dict.items()}\n",
    "\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=True)\n",
    "\n",
    "# 🟪 2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "## Merge the heuristic columns into the wcorr df columns for exports\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "\n",
    "# {a_name:DecoderDecodedEpochsResult.try_add_is_user_annotated_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=filtered_valid_epoch_times) for a_name, a_result in a_result_dict.items()}\n",
    "\n",
    "for a_name, a_result in a_result_dict.items():\n",
    "    # a_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=True)\n",
    "\n",
    "    ## Merge the heuristic columns into the wcorr df columns for exports\n",
    "    # directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "    a_wcorr_result = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict[a_name]\n",
    "    \n",
    "    # did_update_user_annotation_col = DecoderDecodedEpochsResult.try_add_is_user_annotated_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=any_user_selected_epoch_times, t_column_names=None)\n",
    "    # print(f'did_update_user_annotation_col: {did_update_user_annotation_col}')\n",
    "    # did_update_is_valid = DecoderDecodedEpochsResult.try_add_is_valid_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=filtered_valid_epoch_times, t_column_names=None)\n",
    "    # print(f'did_update_is_valid: {did_update_is_valid}')\n",
    "\n",
    "# ['start',]\n",
    "\n",
    "a_result_dict = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "# {a_name:ensure_dataframe(a_result.filter_epochs) for a_name, a_result in a_result_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04fb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_new_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9f746",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pathlib import Path\n",
    "\n",
    "# 💾 export_csvs\n",
    "\n",
    "BATCH_DATE_TO_USE: str = f'{get_now_day_str()}_APOGEE' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "\n",
    "known_collected_outputs_paths = [Path(v).resolve() for v in ('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs', '/Users/pho/Dropbox (University of Michigan)/MED-DibaLabDropbox/Data/Pho/Outputs/output/collected_outputs', '/home/halechr/cloud/turbo/Data/Output/collected_outputs', '/home/halechr/FastData/gen_scripts/', '/home/halechr/FastData/collected_outputs/', 'output/gen_scripts/')]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_outputs_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: '{collected_outputs_path}' does not exist! Is the right computer's config commented out above?\"\n",
    "print(f'collected_outputs_path: \"{collected_outputs_path}\"')\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "print(f'\\tComputation complete. Exporting .CSVs...')\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "_, _, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_valid_epoch_times = filtered_epochs_df[['start', 'stop']].to_numpy()\n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_output_csv_paths = directional_decoders_epochs_decode_result.export_csvs(parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                              user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                              valid_epochs_selections={'ripple': filtered_valid_epoch_times})\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported directional_decoders_epochs_decode_result to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _output_csv_paths.items()])\n",
    "# print(f'\\t\\t\\tCSV Paths: {_output_csv_paths}\\n')\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')\n",
    "\n",
    "# {'laps_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'laps_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_simple_pf_pearson_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_simple_pf_pearson_merged_df)_tbin-0.025.csv')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b77273",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0661fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88335249",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_good_selected_epoch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7420982",
   "metadata": {},
   "source": [
    "# 2024-03-04 - Filter out the epochs based on the criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.mixins.time_slicing import add_epochs_id_identity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, required_min_percentage_of_active_cells=0.333333, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68233f",
   "metadata": {},
   "source": [
    "# 2024-03-27 - Look at active set cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.HDF5_representable import HDFConvertableEnum\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import TruncationCheckingResults\n",
    "\n",
    "\n",
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_aclus_dict, jonathan_firing_rate_analysis_result.neuron_replay_stats_df = truncation_checking_result.build_truncation_checking_aclus_dict(neuron_replay_stats_df=jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "\n",
    "frs_index_inclusion_magnitude:float = 0.5\n",
    "\n",
    "jonathan_firing_rate_analysis_result = JonathanFiringRateAnalysisResult(**curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis.to_dict())\n",
    "\n",
    "## Unrefined:\n",
    "# neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "\n",
    "## Refine the LxC/SxC designators using the firing rate index metric:\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "jonathan_firing_rate_analysis_result.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "\n",
    "neuron_replay_stats_df, *exclusivity_tuple = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "# short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = exclusivity_tuple\n",
    "exclusivity_aclus_tuple = [v.track_exclusive_aclus for v in exclusivity_tuple]\n",
    "exclusivity_aclus_dict = dict(zip(['short_exclusive', 'long_exclusive', 'BOTH', 'EITHER', 'XOR', 'NEITHER'], exclusivity_aclus_tuple))\n",
    "any_aclus = union_of_arrays(*exclusivity_aclus_tuple)\n",
    "exclusivity_aclus_dict['any'] = any_aclus\n",
    "refined_exclusivity_aclus_tuple = [v.get_refined_track_exclusive_aclus() for v in exclusivity_tuple]\n",
    "neuron_replay_stats_df: pd.DataFrame = HDFConvertableEnum.convert_dataframe_columns_for_hdf(neuron_replay_stats_df)\n",
    "\n",
    "# These keys exhaustively span all aclus:\n",
    "exhaustive_key_names = ['short_exclusive', 'long_exclusive', 'BOTH', 'NEITHER']\n",
    "assert np.all(any_aclus == union_of_arrays(*[exclusivity_aclus_dict[k] for k in exhaustive_key_names]))\n",
    "exhaustive_key_dict = {k:v for k, v in exclusivity_aclus_dict.items() if k in exhaustive_key_names}\n",
    "\n",
    "\n",
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_any_aclus = np.array([  3,   4,   5,   7,  10,  11,  13,  14,  15,  17,  23,  24,  25,  26,  31,  32,  33,  34,  45,  49,  50,  51,  52,  54,  55,  58,  61,  64,  68,  69,  70,  71,  73,  74,  75,  76,  78,  81,  82,  83,  84,  85,  87,  90,  92,  93,  96,  97, 102, 109])\n",
    "old_appearing_aclus = np.array([ 4, 11, 13, 23, 52, 58, 87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af361ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_aclus = union_of_arrays(*[v for v in truncation_checking_aclus_dict.values() if len(v) > 0])\n",
    "any_aclus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520650ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import perform_sweep_lap_groud_truth_performance_testing, _perform_variable_time_bin_lap_groud_truth_performance_testing\n",
    "\n",
    "desired_laps_decoding_time_bin_size: float = 0.75\n",
    "\n",
    "## INPUTS: exclusivity_aclus_tuple, desired_laps_decoding_time_bin_size: float\n",
    "# short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = exclusivity_aclus_tuple\n",
    "# included_neuron_ids_list = [short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset]\n",
    "\n",
    "# included_neuron_ids_list = [*exclusivity_aclus_tuple]\n",
    "\n",
    "## INPUTS: truncation_checking_aclus_dict\n",
    "included_neuron_ids_list = list(truncation_checking_aclus_dict.values())\n",
    "row_names = list(truncation_checking_aclus_dict.keys())\n",
    "\n",
    "_output_tuples_list = perform_sweep_lap_groud_truth_performance_testing(curr_active_pipeline, \n",
    "                                                                        included_neuron_ids_list=included_neuron_ids_list,\n",
    "                                                                        desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_size)\n",
    "\n",
    "percent_laps_correctness_df: pd.DataFrame = pd.DataFrame.from_records([complete_decoded_context_correctness_tuple.percent_correct_tuple for (a_directional_merged_decoders_result, result_laps_epochs_df, complete_decoded_context_correctness_tuple) in _output_tuples_list],\n",
    "                          columns=(\"track_ID_correct\", \"dir_correct\", \"complete_correct\"), index=row_names)\n",
    "percent_laps_correctness_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb00384",
   "metadata": {},
   "source": [
    "# 2024-03-29 - Rigorous Decoder Performance assessment\n",
    "Quantify cell contributions to decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9715a6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Inputs: all_directional_pf1D_Decoder, alt_directional_merged_decoders_result\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestLapsSplitting, CustomDecodeEpochsResult, decoder_name, epoch_split_key, get_proper_global_spikes_df\n",
    "\n",
    "## INPUTS: directional_laps_results, track_templates, directional_laps_results\n",
    "\n",
    "## Split the lap epochs into training and test periods.\n",
    "##### Ideally we could test the lap decoding error by sampling randomly from the time bins and omitting 1/6 of time bins from the placefield building (effectively the training data). These missing bins will be used as the \"test data\" and the decoding error will be computed by decoding them and subtracting the actual measured position during these bins.\n",
    "\n",
    "# ### Get the laps to train on\n",
    "# training_data_portion: float = 9.0/10.0\n",
    "# test_data_portion: float = 1.0 - training_data_portion # test data portion is 1/6 of the total duration\n",
    "# print(f'training_data_portion: {training_data_portion}, test_data_portion: {test_data_portion}')\n",
    "\n",
    "# decoders_dict = deepcopy(track_templates.get_decoders_dict())\n",
    "\n",
    "# # debug_output_hdf5_file_path = Path('output', 'laps_train_test_split.h5').resolve()\n",
    "# debug_output_hdf5_file_path = None\n",
    "\n",
    "# # (train_epochs_dict, test_epochs_dict), train_lap_specific_pf1D_Decoder_dict, split_train_test_lap_specific_configs = TrainTestLapsSplitting.compute_train_test_split_laps_decoders(directional_laps_results, track_templates, training_data_portion=training_data_portion,\n",
    "# #                                                                                                                                                              debug_output_hdf5_file_path=debug_output_hdf5_file_path, debug_plot=False, debug_print=True)  # type: Tuple[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, BasePositionDecoder], Any]\n",
    "\n",
    "# train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = train_lap_specific_pf1D_Decoder_dict\n",
    "\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestSplitResult\n",
    "\n",
    "directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data.get('TrainTestSplit', None)\n",
    "\n",
    "training_data_portion: float = directional_train_test_split_result.training_data_portion\n",
    "test_data_portion: float = directional_train_test_split_result.test_data_portion\n",
    "print(f'training_data_portion: {training_data_portion}, test_data_portion: {test_data_portion}')\n",
    "\n",
    "test_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.test_epochs_dict\n",
    "train_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.train_epochs_dict\n",
    "train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_train_test_split_result.train_lap_specific_pf1D_Decoder_dict\n",
    "\n",
    "\n",
    "# Tuple[Tuple[Dict, Dict], Dict[str, BasePositionDecoder], Dict]\n",
    "\n",
    "# OUTPUTS: train_test_split_laps_df_dict\n",
    "\n",
    "# ## Get test epochs:\n",
    "# train_epoch_names: List[str] = [k for k in train_test_split_laps_df_dict.keys() if k.endswith('_train')]\n",
    "# test_epoch_names: List[str] = [k for k in train_test_split_laps_df_dict.keys() if k.endswith('_test')]\n",
    "# train_lap_specific_pf1D_Decoder_dict: Dict[str,BasePositionDecoder] = {k.split('_train', maxsplit=1)[0]:split_train_test_lap_specific_pf1D_Decoder_dict[k] for k in train_epoch_names} # the `k.split('_train', maxsplit=1)[0]` part just gets the original key like 'long_LR'\n",
    "# test_epochs_dict: Dict[str,Epoch] = {k.split('_test', maxsplit=1)[0]:v for k,v in train_test_split_laps_epoch_obj_dict.items() if k.endswith('_test')} # the `k.split('_test', maxsplit=1)[0]` part just gets the original key like 'long_LR'\n",
    "\n",
    "# a_training_test_split_laps_epoch_obj_dict[a_training_test_names[0]].to_hdf('output/laps_train_test_split.h5', f'{a_train_epoch_name}/laps_training_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _do_custom_decode_epochs_dict\n",
    "\n",
    "active_laps_decoding_time_bin_size: float = 0.75\n",
    "\n",
    "global_spikes_df: pd.DataFrame = get_proper_global_spikes_df(curr_active_pipeline)\n",
    "global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "\n",
    "# Dict[epoch_split_key, Dict[decoder_name, CustomDecodeEpochsResult]]\n",
    "\n",
    "## INPUTS: flat_epochs_to_decode_dict, active_laps_decoding_time_bin_size\n",
    "## Decoding of the test epochs (what matters):\n",
    "test_decoder_results_dict: Dict[decoder_name, CustomDecodeEpochsResult] = _do_custom_decode_epochs_dict(global_spikes_df=global_spikes_df, global_measured_position_df=global_measured_position_df,\n",
    "                                                                                                                                 pf1D_Decoder_dict=train_lap_specific_pf1D_Decoder_dict,\n",
    "                                                                                                                                 epochs_to_decode_dict=test_epochs_dict, \n",
    "                                                                                                                                 decoding_time_bin_size=active_laps_decoding_time_bin_size,\n",
    "                                                                                                                                 decoder_and_epoch_keys_independent=False)\n",
    "\n",
    "\n",
    "# flat_epochs_to_decode_dict = {f'{k}_train':v for k,v in train_epochs_dict.items()} | {f'{k}_test':v for k,v in test_epochs_dict.items()} # (train_epochs_dict, test_epochs_dict)\n",
    "# final_decoder_results_dict: Dict[epoch_split_key, Dict[decoder_name, CustomDecodeEpochsResult]] = _do_custom_decode_epochs_dict(curr_active_pipeline,\n",
    "#                                                                                                                                  pf1D_Decoder_dict=train_lap_specific_pf1D_Decoder_dict,\n",
    "#                                                                                                                                  epochs_to_decode_dict=flat_epochs_to_decode_dict,\n",
    "#                                                                                                                                  decoding_time_bin_size=active_laps_decoding_time_bin_size,\n",
    "#                                                                                                                                  decoder_and_epoch_keys_independent=True) # epochs_to_decode_dict.keys(): ['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train', 'long_LR_test', 'long_RL_test', 'short_LR_test', 'short_RL_test']\n",
    "# matched_decoder_epochs_final_decoder_results_dict: Dict[decoder_name, CustomDecodeEpochsResult] = {k:v[k.replace('_train', '').replace('_test', '')] for k, v in final_decoder_results_dict.items()} # flatten down to only the matching decoder\n",
    "# matched_decoder_epochs_final_decoder_results_dict\n",
    "# print(list(matched_decoder_epochs_final_decoder_results_dict.keys())) # ['long_LR_train', 'long_RL_train', 'short_LR_train', 'short_RL_train', 'long_LR_test', 'long_RL_test', 'short_LR_test', 'short_RL_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import compute_weighted_correlations\n",
    "\n",
    "train_decoded_results_dict: Dict[str, DecodedFilterEpochsResult] = {k:v.decoder_result for k, v in test_decoder_results_dict.items()}\n",
    "\n",
    "weighted_corr_data_dict = compute_weighted_correlations(train_decoded_results_dict, debug_print=True)\n",
    "weighted_corr_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_decoded_wcorr_df = pd.concat(weighted_corr_data_dict)\n",
    "train_decoded_wcorr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result.p_x_given_n_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_decoded_measured_diff_df: pd.DataFrame = test_decoder_results_dict['long_LR'].measured_decoded_position_comparion.decoded_measured_diff_df\n",
    "\n",
    "\n",
    "train_decoded_measured_diff_df_dict: Dict[str, pd.DataFrame] = {k:v.measured_decoded_position_comparion.decoded_measured_diff_df for k, v in test_decoder_results_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce176e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import pho_jointplot\n",
    "import seaborn as sns\n",
    "\n",
    "plot_key: str = 'err_cm'\n",
    "\n",
    "# Plot each list as a separate time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, value in train_decoded_measured_diff_df_dict.items():\n",
    "    # sns.lineplot(x=range(len(value)), y=value, label=key)\n",
    "    _out_line = sns.lineplot(data=value, x='t', y=plot_key, label=key)\n",
    "    _out_scatter = sns.scatterplot(data=value, x='t', y=plot_key) # no `, label=key` because we only want one entry in the legend\n",
    "\n",
    "plt.xlabel('lap_center_t (sec)')\n",
    "plt.ylabel('mean_error [cm]')\n",
    "plt.title('LAp Decoding Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10117f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epochs_dict = {k:Epoch(ensure_dataframe(v.measured_decoded_position_comparion.decoded_measured_diff_df)) for k, v in test_decoder_results_dict.items()}\n",
    "active_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_epochs_dict = {k:Epoch(ensure_dataframe(v)) for k, v in train_decoded_measured_diff_df_dict.items()}\n",
    "active_epochs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8f168",
   "metadata": {},
   "source": [
    "# 2024-04-03 - Time-bin effect on lap decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attrs import make_class\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "\n",
    "return_full_decoding_results: bool = True\n",
    "desired_laps_decoding_time_bin_size = np.linspace(start=0.030, stop=1.0, num=4)\n",
    "\n",
    "\n",
    "SimpleBatchComputationDummy = make_class('SimpleBatchComputationDummy', attrs=['BATCH_DATE_TO_USE', 'collected_outputs_path'])\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path)\n",
    "\n",
    "custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_size,\n",
    "                                                                        use_single_time_bin_per_epoch=[False],\n",
    "                                                                        minimum_event_duration=[desired_laps_decoding_time_bin_size[-1]])\n",
    "\n",
    "\n",
    "_across_session_results_extended_dict = {}\n",
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(a_dummy, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, return_full_decoding_results=return_full_decoding_results,\n",
    "                                                save_hdf=True, save_csvs=True,\n",
    "                                                # desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.5, num=4),\n",
    "                                                custom_all_param_sweep_options=custom_all_param_sweep_options, # directly provide the parameter sweeps\n",
    "                                                )\n",
    "if return_full_decoding_results:\n",
    "    # with `return_full_decoding_results == True`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_full_directional_merged_decoders_result = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    # validate the result:\n",
    "    # {k:v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size for k,v in output_full_directional_merged_decoders_result.items()}\n",
    "    # assert np.all([np.isclose(dict(k)['desired_shared_decoding_time_bin_size'], v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size) for k,v in output_full_directional_merged_decoders_result.items()]), f\"the desired time_bin_size in the parameters should match the one used that will appear in the decoded result\"\n",
    "\n",
    "\n",
    "else:\n",
    "    # with `return_full_decoding_results == False`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    output_full_directional_merged_decoders_result = None\n",
    "\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d97fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _show_sweep_result\n",
    "\n",
    "## INPUTS: output_full_directional_merged_decoders_result\n",
    "\n",
    "\n",
    "## RUN\n",
    "global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "# sweep_key_name: str=\"desired_shared_decoding_time_bin_size\"\n",
    "sweep_key_name: str=\"desired_laps_decoding_time_bin_size\"\n",
    "_out_pagination_controller, (all_swept_measured_positions_dfs_dict, all_swept_decoded_positions_df_dict, all_swept_decoded_measured_diff_df_dict) = _show_sweep_result(output_full_directional_merged_decoders_result, global_measured_position_df=global_measured_position_df,\n",
    "                                                                                                                                                        xbin=long_results_obj.original_1D_decoder.xbin,\n",
    "                                                                                                                                                        active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='laps', decoder='all_dir'),\n",
    "                                                                                                                                                        sweep_params_idx=2, sweep_key_name=sweep_key_name, max_subplots_per_page=4)\n",
    "# _out_pagination_controller\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed630cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_laps_decoding_time_bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context Mask - provides additional information about an Identifying context, like whether a certain component of it should print:\n",
    "# has tags like 'print_debug', 'print_session', 'print_across_sessions'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import CustomDecodeEpochsResult\n",
    "\n",
    "\n",
    "## INPUTS: output_full_directional_merged_decoders_result\n",
    "\n",
    "# Interpolated measured position DataFrame - looks good\n",
    "global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "all_swept_measured_positions_dfs_dict, all_swept_decoded_positions_df_dict, all_swept_decoded_measured_diff_df_dict = CustomDecodeEpochsResult.build_measured_decoded_position_comparison({k:deepcopy(v.all_directional_laps_filter_epochs_decoder_result) for k, v in output_full_directional_merged_decoders_result.items()}, global_measured_position_df=global_measured_position_df)\n",
    "# all_swept_decoded_measured_diff_df_dict = {k:pd.DataFrame(v, columns=['t', 'err']) for k, v in all_swept_decoded_measured_diff_df_dict.items()}\n",
    "\n",
    "# global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "# test_measured_positions_dfs_dict, test_decoded_positions_df_dict, test_decoded_measured_diff_df_dict = CustomDecodeEpochsResult.build_measured_decoded_position_comparison(test_laps_decoder_results_dict, global_measured_position_df=global_measured_position_df)\n",
    "# train_measured_positions_dfs_dict, train_decoded_positions_df_dict, train_decoded_measured_diff_df_dict = CustomDecodeEpochsResult.build_measured_decoded_position_comparison(train_laps_decoder_results_dict, global_measured_position_df=global_measured_position_df)\n",
    "\n",
    "\n",
    "## OUTPUTS: all_swept_measured_positions_dfs_dict, all_swept_decoded_positions_df_dict, all_swept_decoded_measured_diff_df_dict\n",
    "all_swept_decoded_measured_diff_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# # Plot the time series using Seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.lineplot(data=df_melted, x=df_melted.index, y='value', hue='type')\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Time Series Plot')\n",
    "# plt.show()\n",
    "\n",
    "sweep_key_name: str=\"desired_laps_decoding_time_bin_size\"\n",
    "\n",
    "## INPUTS: all_swept_decoded_measured_diff_df_dict, sweep_key_name,\n",
    "\n",
    "\n",
    "# Plot each list as a separate time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, error_df in all_swept_decoded_measured_diff_df_dict.items():\n",
    "    # convert frozenset back to dict\n",
    "    a_sweep_params_dict = {s[0]:s[1] for i, s in enumerate(key)}\n",
    "    # error_df['err_cm'] = np.sqrt(error_df['err'])\n",
    "\n",
    "    # plot_key: str = 'err'\n",
    "    plot_key: str = 'err_cm'\n",
    "    \n",
    "    key_label = f'{round(a_sweep_params_dict[sweep_key_name], ndigits=3)}s'\n",
    "    # sns.lineplot(x=range(len(value)), y=value, label=key)\n",
    "    _out_line = sns.lineplot(data=error_df, x='t', y=plot_key, label=key_label)\n",
    "    _out_scatter = sns.scatterplot(data=error_df, x='t', y=plot_key) #  label=key_label, legend=None\n",
    "\n",
    "\n",
    "plt.xlabel('lap_center_t (sec)')\n",
    "plt.ylabel('mean_squared_error')\n",
    "plt.title('All Swept Time Bin Sizes Lap Decoding Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2024-04-03 - Interactively show the lap decoding performance for a single time bin size:\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import DecodedEpochSlicesPaginatedFigureController\n",
    "\n",
    "\n",
    "_out_pagination_controller = DecodedEpochSlicesPaginatedFigureController.init_from_decoder_data(an_all_directional_laps_filter_epochs_decoder_result.active_filter_epochs,\n",
    "                                                                                            an_all_directional_laps_filter_epochs_decoder_result,\n",
    "                                                                                            xbin=long_results_obj.original_1D_decoder.xbin, global_pos_df=global_session.position.df,\n",
    "                                                                                            active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices',\n",
    "                                                                                                                                                                #   t_bin=f'{an_all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size}s',\n",
    "                                                                                                                                                                  t_bin=f\"{a_sweep_params_dict['desired_shared_decoding_time_bin_size']}s\",\n",
    "                                                                                                                                                                   epochs='laps', decoder='all_dir'),\n",
    "                                                                                            a_name='an_all_directional_laps_filter_epochs_decoder_result', max_subplots_per_page=20)\n",
    "_out_pagination_controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import compute_weighted_correlations\n",
    "\n",
    "\n",
    "# out_wcorr_df_dict = compute_weighted_correlations({k:[a_p_x_given_n for a_p_x_given_n in deepcopy(v.all_directional_laps_filter_epochs_decoder_result.p_x_given_n_list)] for k, v in output_full_directional_merged_decoders_result.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f59e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result: DirectionalPseudo2DDecodersResult = list(output_full_directional_merged_decoders_result.values())[-2]\n",
    "all_directional_laps_filter_epochs_decoder_result: DecodedFilterEpochsResult = a_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\n",
    "# out_wcorr_df_dict = compute_weighted_correlations({k:deepcopy(v.all_directional_laps_filter_epochs_decoder_result) for k, v in output_full_directional_merged_decoders_result.items()})\n",
    "# out_wcorr_df_dict = compute_weighted_correlations({'out': all_directional_laps_filter_epochs_decoder_result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997b1e4",
   "metadata": {},
   "source": [
    "# Completely Different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import add_laps_groundtruth_information_to_dataframe\n",
    "\n",
    "laps_weighted_corr_merged_df = add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=laps_weighted_corr_merged_df)\n",
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5436c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df[laps_weighted_corr_merged_df['best_decoder_index'] != laps_weighted_corr_merged_df['true_decoder_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df[laps_weighted_corr_merged_df['best_decoder_index'] == laps_weighted_corr_merged_df['true_decoder_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e57782",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325be3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult], decoder_laps_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict,\n",
    "# laps_weighted_corr_merged_df: pd.DataFrame, decoder_laps_weighted_corr_df_dict: Dict[str, pd.DataFrame],\n",
    "# laps_simple_pf_pearson_merged_df: pd.DataFrame\n",
    "# laps_simple_pf_pearson_merged_df\n",
    "laps_weighted_corr_merged_df\n",
    "\n",
    "# ['best_decoder_index'] # gives the index of the decoder with the best value of wcorr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the ground truth for the decoded laps:\n",
    "groudtruth_laps_epochs_df: pd.DataFrame = directional_merged_decoders_result.add_groundtruth_information(curr_active_pipeline=curr_active_pipeline)\n",
    "groudtruth_laps_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_column_names = ['maze_id', 'is_LR_dir', 'is_most_likely_track_identity_Long', 'is_most_likely_direction_LR']\n",
    "groudtruth_laps_epochs_df[groundtruth_column_names]\n",
    "\n",
    "lap_idxs = groudtruth_laps_epochs_df['lap_id'] - 1\n",
    "lap_idxs\n",
    "## add the truth columns to `laps_weighted_corr_merged_df`:\n",
    "laps_weighted_corr_merged_df[groundtruth_column_names] = groudtruth_laps_epochs_df[groundtruth_column_names]\n",
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6402fe8",
   "metadata": {},
   "source": [
    "## 📈 2024-03-07 - measured v. best-decoded Position + Derivatives Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import _compute_pos_derivs\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring, HeuristicScoresTuple\n",
    "\n",
    "\n",
    "measured_position_df = deepcopy(curr_active_pipeline.sess.position.to_dataframe())\n",
    "# # lap positions only:\n",
    "# measured_position_df = measured_position_df[~measured_position_df['lap'].isnull()] # only get the positions during the laps\n",
    "# measured_position_df['lap'] = measured_position_df['lap'].astype('int64')\n",
    "measured_position_df\n",
    "\n",
    "new_measured_pos_df: pd.DataFrame = _compute_pos_derivs(measured_position_df['t'].to_numpy(), position = deepcopy(measured_position_df['lin_pos'].to_numpy()), decoding_time_bin_size=laps_decoding_time_bin_size) \n",
    "# new_measured_pos_df\n",
    "\n",
    "extra_column_names = ['lap', 'lap_dir'] # 'y', \n",
    "assert np.shape(new_measured_pos_df)[0] == np.shape(measured_position_df)[0]\n",
    "new_measured_pos_df[extra_column_names] = measured_position_df[extra_column_names].copy()\n",
    "\n",
    "# lap positions only:\n",
    "new_measured_pos_df = new_measured_pos_df[~new_measured_pos_df['lap'].isnull()] # only get the positions during the laps\n",
    "new_measured_pos_df['lap'] = new_measured_pos_df['lap'].astype('int64')\n",
    "new_measured_pos_df\n",
    "\n",
    "\n",
    "# new_measured_pos_df['lap_dir'] = new_measured_pos_df['lap_dir'].astype('int64')\n",
    "\n",
    "# new_measured_pos_df\n",
    "\n",
    "# new_measured_pos_df.describe()\n",
    "\n",
    "\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_laps_filter_epochs_decoder_result_dict)\n",
    "\n",
    "# all_epochs_position_derivatives_df_dict_dict = {}\n",
    "all_epochs_position_derivatives_df_dict: Dict[str, pd.DataFrame] = {}\n",
    "_out_new_scores = {}\n",
    "\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "for an_epoch_type_name, a_decoded_filter_epochs_decoder_result_dict in zip(('lap', 'ripple'), (deepcopy(decoder_laps_filter_epochs_decoder_result_dict), deepcopy(decoder_ripple_filter_epochs_decoder_result_dict))):\n",
    "    # all_epochs_position_derivatives_df_dict_dict[an_epoch_type_name] =\n",
    "    # any_good_selected_epoch_times = deepcopy(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times)\n",
    "    # any_good_selected_epoch_indicies = deepcopy(paginated_multi_decoder_decoded_epochs_window.find_data_indicies_from_epoch_times(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times))\n",
    "    # any_good_selected_epoch_indicies\n",
    "\n",
    "    # with suppress_print_context():\n",
    "    # with disable_function_context(builtins, \"print\"):\n",
    "    # _out_new_scores = {}\n",
    "    # an_epoch_idx: int = 0 # 7\n",
    "    axs = None\n",
    "    # all_epochs_position_derivatives_df_dict: Dict[str, pd.DataFrame] = {}\n",
    "    for a_decoder_name, a_result in a_decoded_filter_epochs_decoder_result_dict.items():\n",
    "        # print(f'\\na_name: {a_name}')\n",
    "        combined_key: str = f\"_\".join([an_epoch_type_name, a_decoder_name])\n",
    "                                      \n",
    "        # 🟪 2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "        # directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "        _out_new_scores[combined_key] = [HeuristicReplayScoring.compute_pho_heuristic_replay_scores(a_result=a_result, an_epoch_idx=an_epoch_idx, enable_debug_plot=False, debug_plot_axs=axs, debug_plot_name=f\"{combined_key}\") for an_epoch_idx in np.arange(a_result.num_filter_epochs)]\n",
    "        all_epochs_position_derivatives_df_dict[combined_key] = pd.concat([a_scores.position_derivatives_df for a_scores in _out_new_scores[combined_key]], ignore_index=True)\n",
    "\n",
    "_out_new_scores\n",
    "\n",
    "## Outputs: all_epochs_position_derivatives_df_dict, _out_new_scores, (measured_position_df, new_measured_pos_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41abbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import debug_plot_helper_add_position_and_derivatives\n",
    "\n",
    "\n",
    "fig, debug_plot_axs = debug_plot_helper_add_position_and_derivatives(new_measured_pos_df['t'].to_numpy(), new_measured_pos_df['x'].to_numpy(), new_measured_pos_df['vel_x'].to_numpy(), new_measured_pos_df['accel_x'].to_numpy(),\n",
    "                                                                        debug_plot_axs=None, debug_plot_name='measured', common_plot_kwargs=dict(color='k', markersize='5', marker='.', linestyle='None', alpha=0.35))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import debug_plot_position_and_derivatives_figure\n",
    "\n",
    "## INPUTS: new_measured_pos_df, all_epochs_position_derivatives_df_dict\n",
    "fig, debug_plot_axs = debug_plot_position_and_derivatives_figure(new_measured_pos_df, all_epochs_position_derivatives_df_dict, debug_plot_axs=None, debug_figure_title=None, enable_debug_plot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.position_derivatives import debug_plot_position_derivatives_stack\n",
    "\n",
    "# fig = debug_plot_position_derivatives_stack(new_measured_pos_df, all_epochs_position_derivatives_df_dict)\n",
    "fig = debug_plot_position_derivatives_stack(new_measured_pos_df, all_epochs_position_derivatives_df_dict, show_scatter=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e791125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import sanitize_filename_for_Windows\n",
    "\n",
    "def save_plotly(a_fig, a_fig_context):\n",
    "    \"\"\" \n",
    "    captures: TODAY_DAY_DATE\n",
    "    \"\"\"\n",
    "    fig_save_path: Path = figures_folder.joinpath('_'.join([get_now_day_str(), sanitize_filename_for_Windows(a_fig_context.get_description())])).resolve()\n",
    "    figure_out_paths = {'.html': fig_save_path.with_suffix('.html'), '.png': fig_save_path.with_suffix('.png')}\n",
    "    a_fig.write_html(figure_out_paths['.html'])\n",
    "    display(fullwidth_path_widget(figure_out_paths['.html'], file_name_label='.html'))\n",
    "    # print(file_uri_from_path(figure_out_paths['.html']))\n",
    "    a_fig.write_image(figure_out_paths['.png'])\n",
    "    # print(file_uri_from_path(figure_out_paths['.png']))\n",
    "    display(fullwidth_path_widget(figure_out_paths['.png'], file_name_label='.png'))\n",
    "    return figure_out_paths\n",
    "\n",
    "\n",
    "figures_folder = Path('output').resolve()\n",
    "a_fig_context = curr_active_pipeline.build_display_context_for_session(display_fn_name='debug_plot_position_derivatives_stack')\n",
    "save_plotly(fig, a_fig_context)\n",
    "\n",
    "# fig_save_path: Path = figures_folder.joinpath('_'.join([get_now_day_str(), sanitize_filename_for_Windows(a_fig_context.get_description())])).resolve()\n",
    "# figure_out_paths = {'.html': fig_save_path.with_suffix('.html'), '.png': fig_save_path.with_suffix('.png')}\n",
    "# a_fig.write_html(figure_out_paths['.html'])\n",
    "# display(fullwidth_path_widget(figure_out_paths['.html'], file_name_label='.html'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7fbba6",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: df1, df2\n",
    "position_deriv_column_names1 = pos_deriv_column_names\n",
    "df1 = measured_position_df[position_deriv_column_names1]\n",
    "\n",
    "position_deriv_column_names2 = ['x', 'vel_x', 'accel_x']\n",
    "df2 = deepcopy(all_epochs_position_derivatives_df[position_deriv_column_names2])\n",
    "\n",
    "# Set up the figure and axes.\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 6))\n",
    "\n",
    "# List of columns to compare\n",
    "columns_to_compare = ['col1', 'col2', 'col3']\n",
    "\n",
    "\n",
    "# Loop through the list of columns and create a histogram for each.\n",
    "for i, (col1, col2) in enumerate(zip(position_deriv_column_names1, position_deriv_column_names2)):\n",
    "# for i, col in enumerate(columns_to_compare):\n",
    "    # Use the same bin edges for both histograms by computing them from the combined range of both DataFrames\n",
    "    combined_range = pd.concat([df1[col1], df2[col2]])\n",
    "    bins = np.histogram_bin_edges(combined_range, bins='auto')\n",
    "\n",
    "    # Plot the first DataFrame histogram\n",
    "    df1[col1].hist(bins=bins, ax=axes[i], alpha=0.5, label='Decoded')\n",
    "\n",
    "    # Plot the second DataFrame histogram\n",
    "    df2[col2].hist(bins=bins, ax=axes[i], alpha=0.5, label='Measured')\n",
    "\n",
    "    # Set the title and labels\n",
    "    axes[i].set_title(f'Histogram of {col1}')\n",
    "    axes[i].set_xlabel(col1)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    # Add a legend\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust layout for readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5670d2",
   "metadata": {},
   "source": [
    "# 💾🖼️ 2024-04-27 - Save Posteriors as Yellow-Blue plots to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import save_posterior\n",
    "\n",
    "# directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "# laps_marginals_df\n",
    "\n",
    "# ## Get the result after computation:\n",
    "# directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "# all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "# all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# # long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# # long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# # short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# # short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "# all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "# all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "# laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "# laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "# ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "# ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "# ripple_decoding_time_bin_size: float = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.decoding_time_bin_size\n",
    "# ripple_decoding_time_bin_size\n",
    "# laps_decoding_time_bin_size: float = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size\n",
    "# laps_decoding_time_bin_size\n",
    "\n",
    "# laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "# ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n",
    "\n",
    "# ripple_all_epoch_bins_marginals_df\n",
    "# ripple_directional_marginals\n",
    "\n",
    "directional_merged_decoders_result.perform_compute_marginals()\n",
    "directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n",
    "\n",
    "parent_array_as_image_output_folder = Path(r'E:\\Dropbox (Personal)\\Active\\Kamran Diba Lab\\Presentations\\2024-05-30 - Pho iNAV Poster\\Exports\\array_as_image').resolve()\n",
    "assert parent_array_as_image_output_folder.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicked_epoch = np.array([169.95631618227344, 170.15983607806265])\n",
    "clicked_epoch = np.array([132.51138943410479, 132.79100273095537])\n",
    "clicked_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.p_x_given_n_list[1])\n",
    "         \n",
    "# directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.marginal_x_list\n",
    "active_marginals_df: pd.DataFrame = deepcopy(directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df)\n",
    "# active_marginals_df.ripple_idx\n",
    "# directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.marginal_x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b44569",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_filter_epochs_decoder_result: DecodedFilterEpochsResult = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "active_filter_epochs_decoder_result.filter_epochs.epochs.find_data_indicies_from_epoch_times(np.atleast_1d(clicked_epoch[0]))\n",
    "\n",
    "# active_filter_epochs_decoder_result.all_directional_ripple_filter_epochs_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2daeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import save_posterior\n",
    "from pyphocorehelpers.plotting.media_output_helpers import get_array_as_image_stack, save_array_as_image_stack\n",
    "\n",
    "def save_marginals_arrays_as_image(directional_merged_decoders_result: DirectionalPseudo2DDecodersResult, parent_array_as_image_output_folder: Path, epoch_id_identifier_str: str = 'ripple', epoch_ids=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert epoch_id_identifier_str in ['ripple', 'lap']\n",
    "    # active_marginals_df: pd.DataFrame = deepcopy(ripple_all_epoch_bins_marginals_df)\n",
    "    active_marginals_df: pd.DataFrame = deepcopy(directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df)\n",
    "    # ripple_filter_epochs_decoder_result = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "    # ripple_filter_epochs_decoder_result = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "    active_filter_epochs_decoder_result: DecodedFilterEpochsResult = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "\n",
    "    raw_posterior_active_marginals = deepcopy(active_filter_epochs_decoder_result.p_x_given_n_list)\n",
    "\n",
    "    collapsed_per_lap_epoch_marginal_track_identity_point = active_marginals_df[['P_Long', 'P_Short']].to_numpy().astype(float)\n",
    "    collapsed_per_lap_epoch_marginal_dir_point = active_marginals_df[['P_LR', 'P_RL']].to_numpy().astype(float)\n",
    "\n",
    "    ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "    # directional_merged_decoders_result.ripple_track_identity_marginals_tuple = DirectionalPseudo2DDecodersResult.determine_long_short_likelihoods(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "    ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "    # raw_posterior_laps_marginals\n",
    "    # raw_posterior_active_marginals = directional_merged_decoders_result.build_non_marginalized_raw_posteriors(active_filter_epochs_decoder_result)\n",
    "    # raw_posterior_active_marginals\n",
    "\n",
    "    # INPUTS:\n",
    "    # raw_posterior_laps_marginals = deepcopy(raw_posterior_laps_marginals)\n",
    "    # active_directional_marginals = deepcopy(laps_directional_marginals)\n",
    "    # active_track_identity_marginals = deepcopy(laps_track_identity_marginals)\n",
    "    # raw_posterior_active_marginals = deepcopy(raw_posterior_laps_marginals)\n",
    "    active_directional_marginals = deepcopy(ripple_directional_marginals)\n",
    "    active_track_identity_marginals = deepcopy(ripple_track_identity_marginals)\n",
    "\n",
    "    assert parent_array_as_image_output_folder.exists()\n",
    "\n",
    "    if epoch_ids is None:\n",
    "        epoch_ids = np.arange(active_filter_epochs_decoder_result.num_filter_epochs)\n",
    "\n",
    "    for epoch_id in epoch_ids:\n",
    "        raw_tuple, marginal_dir_tuple, marginal_track_identity_tuple, marginal_dir_point_tuple, marginal_track_identity_point_tuple = save_posterior(raw_posterior_active_marginals, active_directional_marginals,\n",
    "                                                                            active_track_identity_marginals, collapsed_per_lap_epoch_marginal_dir_point, collapsed_per_lap_epoch_marginal_track_identity_point,\n",
    "                                                                                    parent_array_as_image_output_folder=parent_array_as_image_output_folder, epoch_id_identifier_str=epoch_id_identifier_str, epoch_id=epoch_id,\n",
    "                                                                                    debug_print=True)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_marginals_arrays_as_image(directional_merged_decoders_result=directional_merged_decoders_result, parent_array_as_image_output_folder=parent_array_as_image_output_folder, epoch_id_identifier_str='ripple', epoch_ids=[17])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f77b16",
   "metadata": {},
   "source": [
    "# 🎯🟢 2024-05-24 07:24 - Shuffed WCorr Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wcorr is computed on each decoded posterior: `curr_results_obj.p_x_given_n_list`\n",
    "\n",
    "## Actually need to shuffle the unit idenities and recompute the posteriors\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "from typing import NewType\n",
    "\n",
    "import neuropy.utils.type_aliases as types\n",
    "from neuropy.utils.misc import build_shuffled_ids, shuffle_ids # used in _SHELL_analyze_leave_one_out_decoding_results\n",
    "from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "\n",
    "from neuropy.utils.mixins.indexing_helpers import get_dict_subset\n",
    "\n",
    "DecodedEpochsResultsDict = NewType('DecodedEpochsResultsDict', Dict[types.DecoderName, DecodedFilterEpochsResult]) # A Dict containing the decoded filter epochs result for each of the four 1D decoder names\n",
    "ShuffleIdx = NewType('ShuffleIdx', int)\n",
    "\n",
    "wcorr_shuffle_results: SequenceBasedComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('SequenceBased', None)\n",
    "if wcorr_shuffle_results is not None:    \n",
    "    wcorr_ripple_shuffle: WCorrShuffle = wcorr_shuffle_results.wcorr_ripple_shuffle\n",
    "    wcorr_ripple_shuffle: WCorrShuffle = WCorrShuffle(**get_dict_subset(wcorr_ripple_shuffle.to_dict(), subset_excludelist=['_VersionedResultMixin_version']))\n",
    "    curr_active_pipeline.global_computation_results.computed_data.SequenceBased.wcorr_ripple_shuffle = wcorr_ripple_shuffle\n",
    "    filtered_epochs_df: pd.DataFrame = deepcopy(wcorr_ripple_shuffle.filtered_epochs_df)\n",
    "    print(f'wcorr_ripple_shuffle.n_completed_shuffles: {wcorr_ripple_shuffle.n_completed_shuffles}')\n",
    "else:\n",
    "    print(f'SequenceBased is not computed.')\n",
    "\n",
    "# wcorr_ripple_shuffle: WCorrShuffle = WCorrShuffle.init_from_templates(curr_active_pipeline=curr_active_pipeline, enable_saving_entire_decoded_shuffle_result=True)\n",
    "\n",
    "n_epochs: int = wcorr_ripple_shuffle.n_epochs\n",
    "print(f'n_epochs: {n_epochs}')\n",
    "n_completed_shuffles: int = wcorr_ripple_shuffle.n_completed_shuffles\n",
    "print(f'n_completed_shuffles: {n_completed_shuffles}')\n",
    "wcorr_ripple_shuffle.compute_shuffles(num_shuffles=2, curr_active_pipeline=curr_active_pipeline)\n",
    "n_completed_shuffles: int = wcorr_ripple_shuffle.n_completed_shuffles\n",
    "print(f'n_completed_shuffles: {n_completed_shuffles}')\n",
    "desired_ripple_decoding_time_bin_size: float = wcorr_shuffle_results.wcorr_ripple_shuffle.all_templates_decode_kwargs['desired_ripple_decoding_time_bin_size']\n",
    "print(f'{desired_ripple_decoding_time_bin_size = }')\n",
    "# filtered_epochs_df\n",
    "\n",
    "# 7m - 200 shuffles\n",
    "(_out_p, _out_p_dict), (_out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT), (total_n_shuffles_more_extreme_than_real_df, total_n_shuffles_more_extreme_than_real_dict), _out_shuffle_wcorr_arr = wcorr_ripple_shuffle.post_compute(decoder_names=deepcopy(track_templates.get_decoder_names()))\n",
    "wcorr_ripple_shuffle_all_df, all_shuffles_wcorr_df = wcorr_ripple_shuffle.build_all_shuffles_dataframes(decoder_names=deepcopy(track_templates.get_decoder_names()))\n",
    "## Prepare for plotting in histogram:\n",
    "wcorr_ripple_shuffle_all_df = wcorr_ripple_shuffle_all_df.dropna(subset=['start', 'stop'], how='any', inplace=False)\n",
    "wcorr_ripple_shuffle_all_df = wcorr_ripple_shuffle_all_df.dropna(subset=['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL'], how='all', inplace=False)\n",
    "wcorr_ripple_shuffle_all_df = wcorr_ripple_shuffle_all_df.convert_dtypes()\n",
    "# {'long_best_dir_decoder_IDX': int, 'short_best_dir_decoder_IDX': int}\n",
    "# wcorr_ripple_shuffle_all_df\n",
    "## Gets the absolutely most extreme value from any of the four decoders and uses that\n",
    "best_wcorr_max_indices = np.abs(wcorr_ripple_shuffle_all_df[['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL']].values).argmax(axis=1)\n",
    "wcorr_ripple_shuffle_all_df[f'abs_best_wcorr'] = [wcorr_ripple_shuffle_all_df[['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL']].values[i, best_idx] for i, best_idx in enumerate(best_wcorr_max_indices)] #  np.where(direction_max_indices, wcorr_ripple_shuffle_all_df['long_LR'].filter_epochs[a_column_name].to_numpy(), wcorr_ripple_shuffle_all_df['long_RL'].filter_epochs[a_column_name].to_numpy())\n",
    "# wcorr_ripple_shuffle_all_df\n",
    "# wcorr_ripple_shuffle_all_df[['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL']].max(axis=1, skipna=True)\n",
    "\n",
    "## OUTPUTS: wcorr_ripple_shuffle_all_df\n",
    "\n",
    "all_shuffles_only_best_decoder_wcorr_df = pd.concat([all_shuffles_wcorr_df[np.logical_and((all_shuffles_wcorr_df['epoch_idx'] == epoch_idx), (all_shuffles_wcorr_df['decoder_idx'] == best_idx))] for epoch_idx, best_idx in enumerate(best_wcorr_max_indices)])\n",
    "all_shuffles_only_best_decoder_wcorr_df\n",
    "\n",
    "wcorr_ripple_shuffle_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d073d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wcorr_ripple_shuffle_all_df.to_clipboard()\n",
    "# wcorr_ripple_shuffle.plot_histogram_figure(a_decoder_idx=0, selected_epoch_index=np.arange(278))\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample data creation\n",
    "# plot_var_name: str = 'wcorr_long_LR'\n",
    "plot_var_name: str = 'abs_best_wcorr'\n",
    "df = deepcopy(wcorr_ripple_shuffle_all_df) # pd.DataFrame(data)\n",
    "df = df.dropna(subset=[plot_var_name], how='any', inplace=False)\n",
    "# df\n",
    "# np.sum(df[plot_var_name].isna())\n",
    "\n",
    "\n",
    "# Create horizontal histogram\n",
    "# fig = px.histogram(df, x=plot_var_name) # , orientation='h'\n",
    "\n",
    "fig = go.Figure()\n",
    "wcorr_ripple_hist_trace = fig.add_trace(go.Histogram(x=df[plot_var_name]))\n",
    "shuffle_trace = fig.add_trace(go.Histogram(x=all_shuffles_only_best_decoder_wcorr_df['shuffle_wcorr']))\n",
    "# fig.add_trace(go.Histogram(x=all_shuffles_wcorr_df['shuffle_wcorr']))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig = fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig = fig.update_traces(opacity=0.75)\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig = fig.update_layout(\n",
    "    title='Horizontal Histogram of wcorr_long_LR',\n",
    "    xaxis_title='Count',\n",
    "    yaxis_title=plot_var_name\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d707712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import plot_histogram_for_z_scores\n",
    "\n",
    "# selected_epoch_index = None\n",
    "# a_decoder_idx = None\n",
    "\n",
    "selected_epoch_index = 0\n",
    "a_decoder_idx = 0\n",
    "\n",
    "if (selected_epoch_index is None) and (a_decoder_idx is None):\n",
    "    _single_epoch_all_shuffles_wcorr_arr = deepcopy(wcorr_ripple_shuffle.all_shuffles_wcorr_array)\n",
    "    _single_epoch_real_wcorr: float = wcorr_ripple_shuffle.real_decoder_ripple_weighted_corr_arr \n",
    "    a_single_decoder_epoch_z_scored_values: NDArray = wcorr_ripple_shuffle.compute_z_transformed_scores(_single_epoch_all_shuffles_wcorr_arr)\n",
    "    a_single_decoder_epoch_z_score: float = wcorr_ripple_shuffle.compute_z_score(_single_epoch_all_shuffles_wcorr_arr, _single_epoch_real_wcorr)\n",
    "    title = f\"histogram_for_z_scores - all decoders, all epochs\"\n",
    "    title_suffix=f': all decoders, all epochs'\n",
    "\n",
    "else: \n",
    "    _single_epoch_all_shuffles_wcorr_arr = wcorr_ripple_shuffle.all_shuffles_wcorr_array[:, selected_epoch_index, a_decoder_idx]\n",
    "    _single_epoch_real_wcorr: float = wcorr_ripple_shuffle.real_decoder_ripple_weighted_corr_arr[selected_epoch_index, a_decoder_idx]\n",
    "\n",
    "    a_single_decoder_epoch_z_scored_values: NDArray = wcorr_ripple_shuffle.compute_z_transformed_scores(_single_epoch_all_shuffles_wcorr_arr)\n",
    "    a_single_decoder_epoch_z_score: float = wcorr_ripple_shuffle.compute_z_score(_single_epoch_all_shuffles_wcorr_arr, _single_epoch_real_wcorr)\n",
    "    title = f\"histogram_for_z_scores - decoder[{a_decoder_idx}], epoch[{selected_epoch_index}]\"\n",
    "    title_suffix=f': decoder[{a_decoder_idx}], epoch[{selected_epoch_index}]'\n",
    "\n",
    "print(f'np.shape(_single_epoch_all_shuffles_wcorr_arr): {np.shape(_single_epoch_all_shuffles_wcorr_arr)}') # (n_shuffles, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b75810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'a_single_decoder_epoch_z_score: {a_single_decoder_epoch_z_score}')\n",
    "fig = plt.figure(num=title, clear=True)\n",
    "# List of z-scored values\n",
    "z_scores = a_single_decoder_epoch_z_scored_values\n",
    "plot_histogram_for_z_scores(z_scores, title_suffix=title_suffix)\n",
    "plt.axvline(a_single_decoder_epoch_z_score, color='red', linestyle='--', linewidth=2, label='Actual Value')\n",
    "\n",
    "# plot_histogram_for_z_scores(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df_dict['ripple_WCorrShuffle_df']['export_date'] = get_now_rounded_time_str()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorr_ripple_shuffle.plot_histogram_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorr_ripple_shuffle.plot_histogram_figure(a_decoder_idx=2, selected_epoch_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3961cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of z-scored values\n",
    "all_shuffles_wcorr_array = deepcopy(wcorr_ripple_shuffle.all_shuffles_wcorr_array)\n",
    "(_out_p, _out_p_dict), (_out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT), (total_n_shuffles_more_extreme_than_real_df, total_n_shuffles_more_extreme_than_real_dict), all_shuffles_wcorr_array = wcorr_ripple_shuffle.post_compute()\n",
    "## OUTPUTS: all_shuffles_wcorr_array, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_shuffle_wcorr_ZScore_LONG\n",
    "all_shuffles_wcorr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standalone save\n",
    "standalone_pkl_filename: str = f'{get_now_rounded_time_str()}_standalone_wcorr_ripple_shuffle_data_only_{wcorr_ripple_shuffle.n_completed_shuffles}.pkl' \n",
    "standalone_pkl_filepath = curr_active_pipeline.get_output_path().joinpath(standalone_pkl_filename).resolve() # Path(\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\output\\2024-05-30_0925AM_standalone_wcorr_ripple_shuffle_data_only_1100.pkl\")\n",
    "print(f'saving to \"{standalone_pkl_filepath}\"...')\n",
    "wcorr_ripple_shuffle.save_data(standalone_pkl_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: wcorr_ripple_shuffle\n",
    "standalone_mat_filename: str = f'{get_now_rounded_time_str()}_standalone_all_shuffles_wcorr_array.mat' \n",
    "standalone_mat_filepath = curr_active_pipeline.get_output_path().joinpath(standalone_mat_filename).resolve() # r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-06-03_0400PM_standalone_all_shuffles_wcorr_array.mat\"\n",
    "wcorr_ripple_shuffle.save_data_mat(filepath=standalone_mat_filepath, **{'session': curr_active_pipeline.get_session_context().to_dict()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorr_ripple_shuffle.discover_load_and_append_shuffle_data_from_directory(save_directory=curr_active_pipeline.get_output_path().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94572934",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_context = curr_active_pipeline.get_session_context()\n",
    "session_ctxt_key:str = active_context.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "session_name: str = curr_active_pipeline.session_name\n",
    "export_files_dict = wcorr_ripple_shuffle.export_csvs(parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=session_name, curr_active_pipeline=curr_active_pipeline)\n",
    "export_files_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966cf23",
   "metadata": {},
   "source": [
    "### Find clicked index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e823ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "\n",
    "# clicked_epoch = np.array([[149.95935746072792, 150.25439218967222]]).T\n",
    "# clicked_epoch = np.atleast_2d(np.array([637.7847819341114, 638.1821449307026])).T\n",
    "# clicked_epoch = np.atleast_2d(np.array([105.400, 638.1821449307026])).T\n",
    "clicked_epoch = np.atleast_2d(np.array([105.40014315512963, 105.56255971186329])).T\n",
    "# clicked_epoch\n",
    "clicked_epoch_start_times = clicked_epoch[0, :]\n",
    "# clicked_epoch_start_times\n",
    "# clicked_epoch_start_times = np.array([637.7847819341114,])\n",
    "selected_epoch_indicies = find_data_indicies_from_epoch_times(wcorr_ripple_shuffle.filtered_epochs_df, epoch_times=np.squeeze(clicked_epoch_start_times), t_column_names=['start',], atol=0.001, not_found_action='skip_index', debug_print=False)\n",
    "# selected_epoch_indicies\n",
    "\n",
    "assert len(selected_epoch_indicies) > 0\n",
    "\n",
    "selected_epoch_index: int = selected_epoch_indicies[0]\n",
    "print(f'selected_epoch_index: {selected_epoch_index}')\n",
    "print(f'{clicked_epoch_start_times = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97546452",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_decoder_ripple_wcorr_df.iloc[selected_epoch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_decoder_ripple_wcorr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8acdd",
   "metadata": {},
   "source": [
    "# 2024-06-03 - Plot decoded wcorr shuffle z-values using `PhoPaginatedMultiDecoderDecodedEpochsWindow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.decoders import wcorr\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# REAL wcorr for other\n",
    "# ==================================================================================================================== #\n",
    "active_spikes_df = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "alt_directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = deepcopy(directional_merged_decoders_result)\n",
    "\n",
    "## Filter the epochs by minimum duration:\n",
    "replay_epochs_df: pd.DataFrame = deepcopy(filtered_epochs_df)\n",
    "desired_ripple_decoding_time_bin_size: float = alt_directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "minimum_event_duration: float = 2.0 * float(alt_directional_merged_decoders_result.ripple_decoding_time_bin_size)\n",
    "print(f'{desired_ripple_decoding_time_bin_size = }, {minimum_event_duration = }')\n",
    "\n",
    "all_templates_decode_kwargs = dict(desired_ripple_decoding_time_bin_size=desired_ripple_decoding_time_bin_size,\n",
    "                    override_replay_epochs_df=replay_epochs_df, ## Use the filtered epochs\n",
    "                    use_single_time_bin_per_epoch=False, minimum_event_duration=minimum_event_duration)\n",
    "\n",
    "(real_directional_merged_decoders_result, real_decoder_ripple_filter_epochs_decoder_result_dict), real_decoder_ripple_weighted_corr_arr = WCorrShuffle.build_real_result(track_templates=track_templates, directional_merged_decoders_result=alt_directional_merged_decoders_result, active_spikes_df=active_spikes_df, all_templates_decode_kwargs=all_templates_decode_kwargs)\n",
    "real_decoder_ripple_filter_epochs_decoder_result_dict : Dict[str, DecodedFilterEpochsResult] = deepcopy(real_decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "real_decoder_ripple_filter_epochs_decoder_result_dict\n",
    "# real_decoder_ripple_weighted_corr_arr\n",
    "\n",
    "# real_directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = deepcopy(directional_merged_decoders_result)\n",
    "# # real_output_alt_directional_merged_decoders_result, (real_decoder_laps_filter_epochs_decoder_result_dict, real_decoder_ripple_filter_epochs_decoder_result_dict) = _try_all_templates_decode(spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df), a_directional_merged_decoders_result=real_directional_merged_decoders_result, shuffled_decoders_dict=real_directional_merged_decoders_result.all_directional_decoder_dict, **a_sweep_dict)\n",
    "# real_output_alt_directional_merged_decoders_result, (real_decoder_laps_filter_epochs_decoder_result_dict, real_decoder_ripple_filter_epochs_decoder_result_dict) = cls._try_all_templates_decode(spikes_df=active_spikes_df, a_directional_merged_decoders_result=real_directional_merged_decoders_result, shuffled_decoders_dict=track_templates.get_decoders_dict(), \n",
    "#                                                                                                                                                                                             skip_merged_decoding=True, **all_templates_decode_kwargs)\n",
    "# real_decoder_ripple_weighted_corr_df_dict = compute_weighted_correlations(decoder_decoded_epochs_result_dict=deepcopy(real_decoder_ripple_filter_epochs_decoder_result_dict))\n",
    "# real_decoder_ripple_weighted_corr_dict = {k:v['wcorr'].to_numpy() for k, v in real_decoder_ripple_weighted_corr_df_dict.items()}\n",
    "# real_decoder_ripple_weighted_corr_df = pd.DataFrame(real_decoder_ripple_weighted_corr_dict) ## (n_epochs, 4)\n",
    "# real_decoder_ripple_weighted_corr_arr = real_decoder_ripple_weighted_corr_df.to_numpy()\n",
    "# print(f'real_decoder_ripple_weighted_corr_arr: {np.shape(real_decoder_ripple_weighted_corr_arr)}')\n",
    "# real_directional_merged_decoders_result\n",
    "\n",
    "# real_output_alt_directional_merged_decoders_result, (real_decoder_laps_filter_epochs_decoder_result_dict, real_decoder_ripple_filter_epochs_decoder_result_dict) = WCorrShuffle._try_all_templates_decode(spikes_df=active_spikes_df, a_directional_merged_decoders_result=real_directional_merged_decoders_result, shuffled_decoders_dict=track_templates.get_decoders_dict(), \n",
    "#                                                                                                                                                                                             skip_merged_decoding=True, **all_templates_decode_kwargs)\n",
    "# real_decoder_ripple_filter_epochs_decoder_result_dict : Dict[str, DecodedFilterEpochsResult] = deepcopy(real_decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# real_decoder_ripple_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS filtered_decoder_filter_epochs_decoder_result_dict\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "new_wcorr_shuffle_app, new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window, new_wcorr_shuffle_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "    # decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "    decoder_decoded_epochs_result_dict=real_decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "    # decoder_decoded_epochs_result_dict=high_wcorr_only_filtered_decoder_filter_epochs_decoder_result_dict,\n",
    "    epochs_name='ripple',\n",
    "    included_epoch_indicies=None, debug_print=False,\n",
    "    params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "        'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, \n",
    "        'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True,\n",
    "        # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "        # 'disable_y_label': True,\n",
    "        'isPaginatorControlWidgetBackedMode': True,\n",
    "        'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "        # 'debug_print': True,\n",
    "        'max_subplots_per_page': 10,\n",
    "        'scrollable_figure': False,\n",
    "        # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "        'use_AnchoredCustomText': False,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(None, real_decoder_ripple_filter_epochs_decoder_result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4453253",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304948c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "_tmp_out_selections = new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(source='diba_evt_file') # : # gets the annotations for the kdiba-evt file exported ripples, consistent with his 2009 paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window.setWindowTitle(f'PhoPaginatedMultiDecoderDecodedEpochsWindow: NEW Decoded Epoch Slices - real_decoder_ripple_filter_epochs_decoder_result_dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fa60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuropy\n",
    "\n",
    "all_directional_decoder_dict: Dict[str, neuropy.analyses.placefields.PfND] = alt_directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_decoder_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60921549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('alt_directional_merged_decoders_result.all_directional_decoder_dict', alt_directional_merged_decoders_result.all_directional_decoder_dict, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc12642",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_container = alt_directional_merged_decoders_result.all_directional_decoder_dict\n",
    "num_elements: int = len(a_container)\n",
    "\n",
    "\n",
    "if isinstance(a_container, dict):\n",
    "    for i in np.arange(num_elements):\n",
    "        for k, v in a_container.items()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ce09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _perform_compute_custom_epoch_decoding\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_all_df_score_metrics\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "pos_bin_size: float = track_templates.get_decoders()[0].pos_bin_size\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict = _perform_compute_custom_epoch_decoding(curr_active_pipeline, directional_merged_decoders_result, track_templates) # Dict[str, Optional[DecodedFilterEpochsResult]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1562be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fefaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From `General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions._decode_and_evaluate_epochs_using_directional_decoders`\n",
    "## Recompute the epoch scores/metrics such as radon transform and wcorr:\n",
    "(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict), merged_df_outputs_tuple, raw_dict_outputs_tuple = _compute_all_df_score_metrics(directional_merged_decoders_result, track_templates,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdecoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tspikes_df=deepcopy(curr_active_pipeline.sess.spikes_df),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshould_skip_radon_transform=True)\n",
    "laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df, laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df = merged_df_outputs_tuple\n",
    "decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict, decoder_ripple_radon_transform_extras_dict, decoder_laps_weighted_corr_df_dict, decoder_ripple_weighted_corr_df_dict = raw_dict_outputs_tuple\n",
    "## INPUT: ripple_weighted_corr_merged_df, wcorr_ripple_shuffle\n",
    "# Synchronize `ripple_weighted_corr_merged_df` to match `wcorr_ripple_shuffle.filtered_epochs_df` to compare wcorr values:\n",
    "filtered_epoch_start_times = deepcopy(wcorr_ripple_shuffle.filtered_epochs_df)['start'].to_numpy() # get the included times from `wcorr_ripple_shuffle`\n",
    "## Apply the filter to `ripple_weighted_corr_merged_df`\n",
    "filtered_epoch_indicies = find_data_indicies_from_epoch_times(ripple_weighted_corr_merged_df, epoch_times=np.squeeze(filtered_epoch_start_times), t_column_names=['ripple_start_t',], atol=0.001, not_found_action='skip_index', debug_print=False)\n",
    "# filtered_epoch_indicies\n",
    "assert len(filtered_epoch_indicies) > 0\n",
    "# filtered_epoch_indicies\n",
    "# ripple_weighted_corr_merged_df.epochs.matching_epoch_times_slice(epoch_times=np.squeeze(filtered_epoch_start_times), t_column_names=['ripple_start_t',])\n",
    "filtered_ripple_weighted_corr_merged_df: pd.DataFrame = ripple_weighted_corr_merged_df.loc[filtered_epoch_indicies].copy().reset_index(drop=True)\n",
    "filtered_ripple_weighted_corr_merged_df\n",
    "\n",
    "## OUTPUT: filtered_ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfe145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare real_decoder_ripple_wcorr_df and filtered_ripple_weighted_corr_merged_df\n",
    "\n",
    "## INPUT: filtered_ripple_weighted_corr_merged_df, real_decoder_ripple_wcorr_df\n",
    "assert len(real_decoder_ripple_wcorr_df) == len(filtered_ripple_weighted_corr_merged_df), f\"len(real_decoder_ripple_wcorr_df): {len(real_decoder_ripple_wcorr_df)} != len(filtered_ripple_weighted_corr_merged_df): {len(filtered_ripple_weighted_corr_merged_df)}\"\n",
    "assert np.all(np.isclose(real_decoder_ripple_wcorr_df['start_t'].to_numpy(), filtered_ripple_weighted_corr_merged_df['ripple_start_t'].to_numpy())), f\"all epoch start times must be the same!\"\n",
    "real_decoder_ripple_wcorr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4a926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.enable_middle_click_selected_epoch_times_to_clipboard()\n",
    "\n",
    "# clicked_epoch = np.array([132.51138943410479, 132.79100273095537])\n",
    "\n",
    "# clicked_epoch = np.array([149.95935746072792, 150.25439218967222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77930766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_epoch_index: int = filtered_epoch_indicies[0]\n",
    "print(f'selected_epoch_index: {selected_epoch_index}')\n",
    "print(f'{clicked_epoch_start_times = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f869b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_wcorr_df: pd.DataFrame = pd.DataFrame({a_name:v['wcorr'].to_numpy() for a_name, v in decoder_ripple_weighted_corr_df_dict.items()})\n",
    "# original_wcorr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epoch_indicies = find_data_indicies_from_epoch_times(wcorr_ripple_shuffle.filtered_epochs_df, epoch_times=np.squeeze(clicked_epoch_start_times), t_column_names=['start',], atol=0.001, not_found_action='skip_index', debug_print=False)\n",
    "# selected_epoch_indicies\n",
    "\n",
    "assert len(selected_epoch_indicies) > 0\n",
    "\n",
    "selected_epoch_index: int = selected_epoch_indicies[0]\n",
    "print(f'selected_epoch_index: {selected_epoch_index}')\n",
    "print(f'{clicked_epoch_start_times = }')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(merged_df_outputs_tuple)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97648899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8357ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_decoder_ripple_wcorr_df[np.isclose(real_decoder_ripple_wcorr_df['start_t'], 637.7847819341114)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting complete results:\n",
    "a_shuffle_outcome: Dict[types.DecoderName, DecodedFilterEpochsResult] = output_all_shuffles_decoded_results_list[-1]\n",
    "a_shuffle_wcorrs = output_extracted_result_wcorrs_list[-1]\n",
    "\n",
    "## start with one decoder:\n",
    "a_decoder_name: types.DecoderName = 'long_LR'\n",
    "a_decoder_idx: int = 0\n",
    "a_shuffle_decoder_result: DecodedFilterEpochsResult = a_shuffle_outcome[a_decoder_name]\n",
    "# a_shuffle_decoder_result.filter_epochs\n",
    "\n",
    "a_shuffle_decoder_wcorr_result: NDArray = np.squeeze(a_shuffle_wcorrs[a_decoder_name].to_numpy()) # (n_epochs, )\n",
    "\n",
    "# a_shufle_wcorrs\n",
    "a_shuffle_decoder_wcorr_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4846e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: a_decoder_name\n",
    "_out_shuffle_wcorr_arr = np.stack([np.squeeze(v[a_decoder_name].to_numpy()) for v in output_extracted_result_wcorrs_list]) # .shape ## (n_shuffles, n_epochs) \n",
    "print(f'_out_shuffle_wcorr_arr.shape: {np.shape(_out_shuffle_wcorr_arr)}') # _out_shuffle_wcorr_arr.shape: (n_shuffles, n_epochs)\n",
    "n_shuffles: int = np.shape(_out_shuffle_wcorr_arr)[0]\n",
    "print(f'n_shuffles: {n_shuffles}')\n",
    "n_epochs: int = np.shape(_out_shuffle_wcorr_arr)[1]\n",
    "print(f'n_epochs: {n_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: `wcorr_ripple_shuffle`\n",
    "wcorr_ripple_shuffle.n_completed_shuffles\n",
    "(_out_p, _out_p_dict), (_out_shuffle_wcorr_ZScore_LONG, _out_shuffle_wcorr_ZScore_SHORT), (total_n_shuffles_more_extreme_than_real_df, total_n_shuffles_more_extreme_than_real_dict), all_shuffles_wcorr_array = wcorr_ripple_shuffle.post_compute(curr_active_pipeline=curr_active_pipeline)\n",
    "## OUTPUTS: all_shuffles_wcorr_array, \n",
    "all_shuffles_wcorr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c91a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all shuffles for a single epoch:\n",
    "## INPUTS: all_shuffles_wcorr_array, a_decoder_idx, selected_epoch_index\n",
    "\n",
    "## start with one decoder:\n",
    "a_decoder_name: types.DecoderName = 'long_LR'\n",
    "a_decoder_idx: int = 0\n",
    "selected_epoch_index: int = 0\n",
    "\n",
    "# _single_epoch_all_shuffles_wcorr_arr = _out_shuffle_wcorr_arr[:, selected_epoch_index]\n",
    "_single_epoch_all_shuffles_wcorr_arr = wcorr_ripple_shuffle.all_shuffles_wcorr_array[:, selected_epoch_index, a_decoder_idx]\n",
    "print(f'np.shape(_single_epoch_all_shuffles_wcorr_arr): {np.shape(_single_epoch_all_shuffles_wcorr_arr)}') # (n_shuffles, )\n",
    "_single_epoch_all_shuffles_wcorr_arr\n",
    "\n",
    "_single_epoch_real_wcorr: float = wcorr_ripple_shuffle.real_decoder_ripple_weighted_corr_arr[selected_epoch_index, a_decoder_idx]\n",
    "_single_epoch_real_wcorr # -0.35003949543741564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2030bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_single_decoder_epoch_z_scored_values: NDArray = wcorr_ripple_shuffle.compute_z_transformed_scores(_single_epoch_all_shuffles_wcorr_arr)\n",
    "a_single_decoder_epoch_z_scored_values\n",
    "a_single_decoder_epoch_z_score: float = wcorr_ripple_shuffle.compute_z_score(_single_epoch_all_shuffles_wcorr_arr, _single_epoch_real_wcorr)\n",
    "print(f'a_single_decoder_epoch_z_score: {a_single_decoder_epoch_z_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba84417",
   "metadata": {},
   "source": [
    "# :🟢 Build Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import pho_jointplot, plot_histograms\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "import plotly.io as pio\n",
    "import plotly.subplots as sp\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from pyphoplacecellanalysis.Pho2D.plotly.plotly_templates import PlotlyHelpers\n",
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import plot_histograms_across_sessions, plot_stacked_histograms\n",
    "from pyphoplacecellanalysis.Pho2D.plotly.Extensions.plotly_helpers import plotly_helper_save_figures, _helper_build_figure, plotly_pre_post_delta_scatter, plot_across_sessions_scatter_results\n",
    "\n",
    "# Plotly Imports:\n",
    "fig_size_kwargs = {'width': 1650, 'height': 480}\n",
    "is_dark_mode, template = PlotlyHelpers.get_plotly_template(is_dark_mode=False)\n",
    "pio.templates.default = template\n",
    "\n",
    "df: pd.DataFrame = deepcopy(wcorr_ripple_shuffle_all_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new scatter plot:\n",
    "# px_scatter_kwargs = {'x': 'epoch_idx', 'y': 'shuffle_wcorr', 'color':\"decoder_idx\", 'title': f\"'wcorr scatter results'\"} # , 'color': 'time_bin_size', 'range_y': [-1.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size', 'is_user_annotated_epoch':'user_sel'}\n",
    "# df = all_shuffles_wcorr_df\n",
    "\n",
    "## INPUTS: wcorr_ZScore_real_LR_df\n",
    "# px_scatter_kwargs = {'x': 'start_t', 'y': 'long', 'title': f\"'wcorr scatter results LONG'\"} # , 'color': 'time_bin_size', 'range_y': [-1.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size', 'is_user_annotated_epoch':'user_sel'}\n",
    "# df = wcorr_ZScore_real_LR_df\n",
    "\n",
    "## INPUTS: real_decoder_ripple_wcorr_df\n",
    "px_scatter_kwargs = {'x': 'start', 'y': 'wcorr_z_long', 'title': f\"'real_decoder_ripple_wcorr_df long_RL'\"} # , 'color': 'time_bin_size', 'range_y': [-1.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size', 'is_user_annotated_epoch':'user_sel'}\n",
    "# df = real_decoder_ripple_wcorr_df\n",
    "\n",
    "\n",
    "# px_scatter_kwargs.pop('color')\n",
    "out_scatter_fig = px.scatter(df, **px_scatter_kwargs)\n",
    "out_scatter_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da466ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_bins = 25\n",
    "concatenated_ripple_df = deepcopy(all_shuffles_wcorr_df)\n",
    "# variable_name = 'P_Long'\n",
    "variable_name = 'P_Short' # Shows expected effect - short-only replay prior to delta and then split replays post-delta\n",
    "# variable_name = 'long_best_pf_peak_x_pearsonr'\n",
    "# variable_name = 'long_best_jump'\n",
    "# variable_name = 'wcorr_abs_diff'\n",
    "# variable_name = 'pearsonr_abs_diff'\n",
    "# variable_name = 'direction_change_bin_ratio_diff'\n",
    "# variable_name = 'longest_sequence_length_ratio_diff'\n",
    "# variable_name = 'long_best_longest_sequence_length_ratio'\n",
    "# variable_name = 'long_best_congruent_dir_bins_ratio'\n",
    "# variable_name = 'congruent_dir_bins_ratio_diff'\n",
    "# variable_name = 'total_congruent_direction_change_diff'\n",
    "# variable_name = 'long_best_congruent_dir_bins_ratio'\n",
    "# variable_name = 'long_best_direction_change_bin_ratio'\n",
    "# variable_name = 'long_best_congruent_dir_bins_ratio'\n",
    "# 'color':'is_user_annotated_epoch'\n",
    "# 'color': 'is_user_annotated_epoch', \n",
    "\n",
    "px_scatter_kwargs = {'x': 'delta_aligned_start_t', 'y': variable_name, 'color':\"is_user_annotated_epoch\", 'title': f\"'{variable_name}'\"} # , 'color': 'time_bin_size', 'range_y': [-1.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size', 'is_user_annotated_epoch':'user_sel'}\n",
    "\n",
    "px_scatter_kwargs.pop('color')\n",
    "\n",
    "# hist_kwargs = dict(color=\"time_bin_size\")\n",
    "hist_kwargs = dict(color=\"is_user_annotated_epoch\") # , histnorm='probability density'\n",
    "hist_kwargs.pop('color')\n",
    "new_fig_ripples, new_fig_ripples_context = plotly_pre_post_delta_scatter(data_results_df=concatenated_ripple_df, out_scatter_fig=None, histogram_bins=histogram_bins,\n",
    "                        px_scatter_kwargs=px_scatter_kwargs, histogram_variable_name=variable_name, hist_kwargs=hist_kwargs, forced_range_y=None,\n",
    "                        time_delta_tuple=(earliest_delta_aligned_t_start, 0.0, latest_delta_aligned_t_end), legend_title_text='Is User Selected', is_dark_mode=is_dark_mode)\n",
    "new_fig_ripples = new_fig_ripples.update_layout(fig_size_kwargs)\n",
    "\n",
    "_extras_output_dict = {}\n",
    "_extras_output_dict[\"y_mid_line\"] = new_fig_ripples.add_hline(y=0.5, line=dict(color=\"rgba(0.8,0.8,0.8,.75)\", width=2), row='all', col='all')\n",
    "\n",
    "# # Update layout to add a title to the legend\n",
    "# new_fig_ripples.update_layout(\n",
    "#     legend_title_text='Is User Selected'  # Add a title to the legend\n",
    "# )\n",
    "\n",
    "# fig_to_clipboard(new_fig_ripples, **fig_size_kwargs)\n",
    "figure_out_paths = save_plotly(a_fig=new_fig_ripples, a_fig_context=figure_ripples_context)\n",
    "new_fig_ripples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# mpg = sns.load_dataset(\"mpg\")\n",
    "\n",
    "## INPUT: pd.DataFrame\n",
    "sns.catplot(\n",
    "    data=all_shuffles_wcorr_df, x=\"epoch_idx\", y=\"shuffle_wcorr\", hue=\"decoder_idx\",\n",
    "    native_scale=True, zorder=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89aac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pd.DataFrame(_out_shuffle_wcorr_Zscore_val, columns=['LongLR', 'LongRL', 'ShortLR', 'ShortRL'])\n",
    "\n",
    "# _out_wcorr_ZScore_LR_dict = dict(zip(['LongLR', 'LongRL', 'ShortLR', 'ShortRL'], [v for v in _out_shuffle_wcorr_Zscore_val.T]))\n",
    "# _out_wcorr_ZScore_LR_dict\n",
    "\n",
    "figure_identifier: str = f\"wcorr (best dir selected for each event) fig\"\n",
    "fig = plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "# for time_bin_size in time_bin_sizes:\n",
    "#     df_tbs = pre_delta_df[pre_delta_df['time_bin_size']==time_bin_size]\n",
    "#     df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "\n",
    "n_shuffles: int = wcorr_ripple_shuffle.n_completed_shuffles\n",
    "print(f'n_shuffles')\n",
    "\n",
    "_out_dfs = []\n",
    "# for i, (name, v) in enumerate(total_n_shuffles_more_extreme_than_real_dict.items()):\n",
    "# for i, (name, v) in enumerate(_out_p_dict.items()):\n",
    "# for i, (name, v) in enumerate(_out_shuffle_wcorr_arr_ZScores_LR_dict.items()):\n",
    "for i, (name, v) in enumerate(_out_wcorr_ZScore_LR_dict.items()):\n",
    "\n",
    "    # if i == 0:\n",
    "    assert np.shape(epoch_start_t) == np.shape(_out_shuffle_wcorr_ZScore_LONG)\n",
    "    assert np.shape(epoch_start_t) == np.shape(_out_shuffle_wcorr_ZScore_SHORT)\n",
    "    # curr_is_valid_epoch_shuffle_indicies = np.any(valid_shuffle_indicies[:,:,i]) # (n_shuffles, n_epochs)\n",
    "    # curr_t = np.squeeze(epoch_start_t[curr_is_valid_epoch_shuffle_indicies])\n",
    "    curr_t = np.squeeze(epoch_start_t)\n",
    "    print(f'np.shape(curr_t): {np.shape(curr_t)}')\n",
    "    print(f'np.shape(v): {np.shape(v)}')\n",
    "    curr_shuffle_wcorr_arr = wcorr_ripple_shuffle.all_shuffles_wcorr_array[:,:,i] # (n_shuffles, n_epochs)\n",
    "\n",
    "    # curr_shuffle_wcorr_df[name] = v\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([curr_shuffle_wcorr_arr, np.repeat(i, n_shuffles)])) # , columns=[]\n",
    "\n",
    "    n_shuffles, n_epochs = np.shape(curr_shuffle_wcorr_arr)\n",
    "    print(f\"{n_shuffles = }, {n_epochs = }\")\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(curr_shuffle_wcorr_arr) # , columns=[]\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([np.atleast_2d(np.repeat(i, n_shuffles)).T, curr_shuffle_wcorr_arr]), columns=(['epoch_idx'] + [f'{int(i)}' for i in np.arange(n_epochs)]))\n",
    "    # curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([np.atleast_2d(np.repeat(i, n_shuffles)).T, np.atleast_2d(np.arange(n_shuffles)).T, curr_shuffle_wcorr_arr]), columns=(['epoch_idx', 'shuffle_idx'] + [f'{int(i)}' for i in np.arange(n_epochs)]))\n",
    "\n",
    "    curr_shuffle_wcorr_df = pd.DataFrame(np.hstack([np.atleast_2d(np.repeat(i, n_shuffles)).T, np.atleast_2d(np.arange(n_shuffles)).T]), columns=(['epoch_idx', 'shuffle_idx']))\n",
    "\n",
    "    epoch_id_sequence = np.array([f'{int(i)}' for i in np.arange(n_epochs)])\n",
    "\n",
    "    curr_shuffle_wcorr_df['epoch_idx'] = np.tile(epoch_id_sequence, n_shuffles)\n",
    "    \n",
    "    # curr_shuffle_wcorr_arr\n",
    "    # \n",
    "    curr_shuffle_wcorr_df['epoch_idx'] = curr_shuffle_wcorr_df['epoch_idx'].astype(int)\n",
    "    curr_shuffle_wcorr_df['shuffle_idx'] = curr_shuffle_wcorr_df['shuffle_idx'].astype(int)\n",
    "    curr_shuffle_wcorr_df['track_id'] = name\n",
    "    # curr_shuffle_wcorr_df\n",
    "\n",
    "    _out_dfs.append(curr_shuffle_wcorr_df)\n",
    "    \n",
    "    # _flat_v = np.nanmean(v, axis=0)\n",
    "    # _flat_v = np.abs(v)\n",
    "    # _flat_v = v\n",
    "    # _temp_df = pd.DataFrame({'epoch_start_t': curr_t, 'p': v})\n",
    "    # _temp_df = pd.DataFrame({'epoch_start_t': curr_t, 'flat_Z_corr': _flat_v})\n",
    "    # plt.scatter(np.arange(np.shape(v)[0]), v, label=name)\n",
    "    # plt.scatter(epoch_start_t, _flat_v, label=name)\n",
    "\n",
    "    # _temp_df.hist(column='flat_Z_corr', alpha=0.5, label=str(name)) \n",
    "\n",
    "    # plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "\n",
    "\n",
    "all_shuffles_wcorr_df: pd.DataFrame = pd.concat(_out_dfs)\n",
    "all_shuffles_wcorr_df\n",
    "\n",
    "\n",
    "all_shuffles_wcorr_df['epoch_idx'].unique()\n",
    "# plt.title(f'{figure_identifier}')\n",
    "# plt.xlabel('time')\n",
    "# plt.ylabel('Z-scored Wcorr')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c401f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would I represent the results of 1000 shuffles for each epoch. (n_shuffles, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d395c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_shuffle_wcorr_df.co\n",
    "\n",
    "curr_shuffle_wcorr_df.columns[-1] = 'epoch_idx'\n",
    "# curr_shuffle_wcorr_df.set_axis(labels=['n_shuffles', 'n_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_shuffle_wcorr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38ce8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aed53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# \"Melt\" the dataset to \"long-form\" or \"tidy\" representation\n",
    "iris = iris.melt(id_vars=\"species\", var_name=\"measurement\")\n",
    "\n",
    "# Initialize the figure\n",
    "f, ax = plt.subplots()\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(\n",
    "    data=iris, x=\"value\", y=\"measurement\", hue=\"species\",\n",
    "    dodge=True, alpha=.25, zorder=1, legend=False,\n",
    ")\n",
    "\n",
    "# Show the conditional means, aligning each pointplot in the\n",
    "# center of the strips by adjusting the width allotted to each\n",
    "# category (.8 by default) by the number of hue levels\n",
    "sns.pointplot(\n",
    "    data=iris, x=\"value\", y=\"measurement\", hue=\"species\",\n",
    "    dodge=.8 - .8 / 3, palette=\"dark\", errorbar=None,\n",
    "    markers=\"d\", markersize=4, linestyle=\"none\",\n",
    ")\n",
    "\n",
    "# Improve the legend\n",
    "sns.move_legend(\n",
    "    ax, loc=\"lower right\", ncol=3, frameon=True, columnspacing=1, handletextpad=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35184d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_shuffle_wcorr_ZScore_LONG\n",
    "_out_shuffle_wcorr_ZScore_SHORT\n",
    "epoch_start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b378dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcorr_ripple_shuffle_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2943319",
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_jointplot(data=wcorr_ripple_shuffle_all_df, x='start', y='wcorr_z_long', kind=\"scatter\", title='Ripple: wcorr long per epoch')\n",
    "pho_jointplot(data=wcorr_ripple_shuffle_all_df, x='start', y='wcorr_z_short', kind=\"scatter\", title='Ripple: wcorr short per epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_shuffle_wcorr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_decoder_ripple_weighted_corr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.logical_not(a_decoder_ripple_weighted_corr_df.notna()))\n",
    "\n",
    "a_shuffle_is_more_extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c84388",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder_ripple_weighted_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49403abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decoder_ripple_weighted_corr_df_dicts = np.hstack([output_extracted_result_tuples[i][-1] for i, v in enumerate(output_extracted_result_tuples)])\n",
    "output_decoder_ripple_weighted_corr_df_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df), (decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict) = _subfn_compute_complete_df_metrics(directional_merged_decoders_result, track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                                            decoder_laps_df_dict=deepcopy(decoder_laps_weighted_corr_df_dict), decoder_ripple_df_dict=deepcopy(decoder_ripple_weighted_corr_df_dict), active_df_columns = ['wcorr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82089d",
   "metadata": {},
   "source": [
    "# 🎯🟢 2024-05-29 - Trial-by-Trial Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "\n",
    "# # spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "# rank_order_results = global_computation_results.computed_data['RankOrder'] # : \"RankOrderComputationsContainer\"\n",
    "# minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "# # included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "# directional_laps_results: DirectionalLapsResult = global_computation_results.computed_data['DirectionalLaps']\n",
    "# track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "# # long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# # Unpack all directional variables:\n",
    "# ## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "# long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# # Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "# long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "\n",
    "## INPUTS: curr_active_pipeline, track_templates, global_epoch_name, (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)\n",
    "any_decoder_neuron_IDs: NDArray = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "# long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "# ## Directional Trial-by-Trial Activity:\n",
    "if 'pf1D_dt' not in curr_active_pipeline.computation_results[global_epoch_name].computed_data:\n",
    "    # if `KeyError: 'pf1D_dt'` recompute\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "active_pf_1D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D_dt'])\n",
    "# active_pf_2D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D_dt'])\n",
    "\n",
    "active_pf_dt: PfND_TimeDependent = active_pf_1D_dt\n",
    "# Limit only to the placefield aclus:\n",
    "active_pf_dt = active_pf_dt.get_by_id(ids=any_decoder_neuron_IDs)\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_2D_dt) # 2D\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "## OUTPUTS: directional_active_lap_pf_results_dicts\n",
    "a_trial_by_trial_result: TrialByTrialActivityResult = TrialByTrialActivityResult(any_decoder_neuron_IDs=any_decoder_neuron_IDs,\n",
    "                                                                                active_pf_dt=active_pf_dt,\n",
    "                                                                                directional_lap_epochs_dict=directional_lap_epochs_dict,\n",
    "                                                                                directional_active_lap_pf_results_dicts=directional_active_lap_pf_results_dicts,\n",
    "                                                                                is_global=True)  # type: Tuple[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, BasePositionDecoder], Any]\n",
    "\n",
    "a_trial_by_trial_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71633da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_trial_by_trial_result.directional_active_lap_pf_results_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62264383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_trial_by_trial_result.active_pf_dt.plot_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "aTbyT:TrialByTrialActivity = a_trial_by_trial_result.directional_active_lap_pf_results_dicts['long_LR']\n",
    "aTbyT.C_trial_by_trial_correlation_matrix.shape # (40, 21, 21)\n",
    "aTbyT.z_scored_tuning_map_matrix.shape # (21, 40, 57) (n_epochs, n_neurons, n_pos_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14366a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aTbyT.neuron_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_xbins = len(active_pf_dt.xbin_centers)\n",
    "n_xbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aTbyT.z_scored_tuning_map_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b29c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_trial_by_trial_result.active_pf_dt.historical_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308530d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pf2D_dt.historical_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "saveData('test_a_trial_by_trial_result_data.pkl', a_trial_by_trial_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_a_trial_by_trial_result = loadData('test_a_trial_by_trial_result_data.pkl')\n",
    "loaded_a_trial_by_trial_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e425b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b1025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa979505",
   "metadata": {},
   "source": [
    "# 2024-06-25 - Advanced Time-dependent decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56621c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directional Versions: 'long_LR':\n",
    "from neuropy.core.epoch import subdivide_epochs, ensure_dataframe\n",
    "\n",
    "## INPUTS: long_LR_epochs_obj, long_LR_results\n",
    "\n",
    "a_pf1D_dt: PfND_TimeDependent = deepcopy(long_LR_results.pf1D_dt)\n",
    "a_pf2D_dt: PfND_TimeDependent = deepcopy(long_LR_results.pf2D_dt)\n",
    "\n",
    "# Example usage\n",
    "df: pd.DataFrame = ensure_dataframe(deepcopy(long_LR_epochs_obj)) \n",
    "df['epoch_type'] = 'lap'\n",
    "df['interval_type_id'] = 666\n",
    "\n",
    "subdivide_bin_size = 0.200  # Specify the size of each sub-epoch in seconds\n",
    "subdivided_df: pd.DataFrame = subdivide_epochs(df, subdivide_bin_size)\n",
    "# print(subdivided_df)\n",
    "\n",
    "## Evolve the ratemaps:\n",
    "_a_pf1D_dt_snapshots = a_pf1D_dt.batch_snapshotting(subdivided_df, reset_at_start=True)\n",
    "_a_pf2D_dt_snapshots = a_pf2D_dt.batch_snapshotting(subdivided_df, reset_at_start=True)\n",
    "# a_pf2D_dt.plot_ratemaps_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf75670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "\n",
    "active_pf_2D = deepcopy(a_pf2D_dt)\n",
    "figure_format_config = {}\n",
    "out_all_pf_2D_pyqtgraph_binned_image_fig = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_all_pf_2D_pyqtgraph_binned_image_fig.params.scrollability_mode.is_scrollable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name in out_all_pf_2D_pyqtgraph_binned_image_fig.plot_names:\n",
    "    local_plots_data = out_all_pf_2D_pyqtgraph_binned_image_fig.plots_data[name]\n",
    "    local_plots = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[name]\n",
    "    newPlotItem = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[name].mainPlotItem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_all_pf_2D_pyqtgraph_binned_image_fig.toolBarArea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523071",
   "metadata": {},
   "source": [
    "# 2024-05-30 - Continuous decoded posterior output videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "spikes_df = directional_decoders_decode_result.spikes_df\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = directional_decoders_decode_result.most_recent_continuously_decoded_dict\n",
    "pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('pseudo2D', None)\n",
    "pseudo2D_decoder_continuously_decoded_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('long_LR', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.media_output_helpers import save_array_as_video\n",
    "\n",
    "\n",
    "def save_posterior_to_video(a_decoder_continuously_decoded_result: DecodedFilterEpochsResult, result_name: str='a_decoder_continuously_decoded_result'):\n",
    "    a_p_x_given_n = deepcopy(a_decoder_continuously_decoded_result.p_x_given_n_list[0]) # (57, 4, 83755) (n_x_bins, n_decoders, n_time_bins)\n",
    "    if np.ndim(a_p_x_given_n) > 2:\n",
    "        n_x_bins, n_decoders, n_time_bins = np.shape(a_p_x_given_n)\n",
    "        transpose_axes_tuple = (2, 1, 0,)\n",
    "    else:\n",
    "        assert np.ndim(a_p_x_given_n) == 2, f\"np.ndim(a_p_x_given_n): {np.ndim(a_p_x_given_n)}\"\n",
    "        n_x_bins, n_time_bins = np.shape(a_p_x_given_n)\n",
    "        a_p_x_given_n = a_p_x_given_n[:, np.newaxis, :]\n",
    "        assert np.ndim(a_p_x_given_n) == 3, f\"np.ndim(a_p_x_given_n): {np.ndim(a_p_x_given_n)}\"\n",
    "        # transpose_axes_tuple = (1, 0,)\n",
    "        transpose_axes_tuple = (2, 1, 0,)\n",
    "    \n",
    "        a_p_x_given_n = np.tile(a_p_x_given_n, (1, 8, 1,))\n",
    "        # display(a_p_x_given_n)\n",
    "    # time_window_centers = deepcopy(a_decoder_continuously_decoded_result.time_window_centers[0])\n",
    "\n",
    "    ## get tiny portion just to test\n",
    "    # a_p_x_given_n = a_p_x_given_n[:, :, :2000]\n",
    "    # a_p_x_given_n\n",
    "\n",
    "    # a_p_x_given_n = np.reshape(a_p_x_given_n, (n_time_bins, n_decoders, n_x_bins))\n",
    "    a_p_x_given_n = np.transpose(a_p_x_given_n, transpose_axes_tuple)\n",
    "    # display(a_p_x_given_n)\n",
    "\n",
    "    decoding_realtime_FPS: float = 1.0 / float(a_decoder_continuously_decoded_result.decoding_time_bin_size)\n",
    "    print(f'decoding_realtime_FPS: {decoding_realtime_FPS}')\n",
    "    ## save video\n",
    "    video_out_path = save_array_as_video(array=a_p_x_given_n, video_filename=f'output/videos/{result_name}.avi', isColor=False, fps=decoding_realtime_FPS)\n",
    "    print(f'video_out_path: {video_out_path}')\n",
    "    # reveal_in_system_file_manager(video_out_path)\n",
    "    return video_out_path\n",
    "\n",
    "\n",
    "a_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('long_LR', None)\n",
    "save_posterior_to_video(a_decoder_continuously_decoded_result=a_decoder_continuously_decoded_result, result_name='continuous_long_LR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_posterior_to_video(a_decoder_continuously_decoded_result=pseudo2D_decoder_continuously_decoded_result, result_name='continuous_pseudo2D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f828c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: global_results, global_epoch_name\n",
    "\n",
    "# Get the decoders from the computation result:\n",
    "active_one_step_decoder = global_results['pf2D_Decoder']\n",
    "active_two_step_decoder = global_results.get('pf2D_TwoStepDecoder', None)\n",
    "if active_two_step_decoder is None:\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['position_decoding_two_step'], computation_kwargs_list=[{}], enabled_filter_names=[global_epoch_name, global_LR_name, global_RL_name], fail_on_exception=True, debug_print=False)\n",
    "    active_two_step_decoder = global_results.get('pf2D_TwoStepDecoder', None)\n",
    "    assert active_two_step_decoder is not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pyphocorehelpers.plotting.media_output_helpers import get_array_as_image\n",
    "from pyphocorehelpers.plotting.media_output_helpers import save_array_as_video\n",
    "\n",
    "def colormap_and_save_as_video(array, video_filename='output.avi', fps=30.0, colormap=cv2.COLORMAP_VIRIDIS):\n",
    "    # array = ((array - array.min()) / (array.max() - array.min()) * 255).astype(np.uint8)\n",
    "    # color_array = cv2.applyColorMap(array, colormap)\n",
    "    return save_array_as_video(array, video_filename=video_filename, fps=fps, isColor=True, colormap=colormap)\n",
    "\n",
    "# image = get_array_as_image(img_data, desired_height=100, desired_width=None, skip_img_normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_input_posterior = deepcopy(active_two_step_decoder.p_x_given_n_and_x_prev)\n",
    "result_name: str = f'two_step_maze_all'\n",
    "\n",
    "# an_input_posterior = deepcopy(active_one_step_decoder.p_x_given_n)\n",
    "# result_name: str = f'one_step_2D_maze_all'\n",
    "\n",
    "\n",
    "n_x_bins, n_y_bins, n_time_bins = np.shape(an_input_posterior)\n",
    "transpose_axes_tuple = (2, 1, 0,)\n",
    "an_input_posterior = np.transpose(an_input_posterior, transpose_axes_tuple)\n",
    "decoding_realtime_FPS: float = 1.0 / float(active_one_step_decoder.time_bin_size)\n",
    "print(f'decoding_realtime_FPS: {decoding_realtime_FPS}')\n",
    "## save video\n",
    "video_out_path = save_array_as_video(array=an_input_posterior, video_filename=f'output/videos/{result_name}.avi', isColor=True, fps=decoding_realtime_FPS, colormap=cv2.COLORMAP_VIRIDIS)\n",
    "# video_out_path = colormap_and_save_as_video(array=an_input_posterior, video_filename=f'output/videos/{result_name}.avi', fps=decoding_realtime_FPS)\n",
    "\n",
    "print(f'video_out_path: {video_out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['position_decoding_two_step'], computation_kwargs_list=[{}], enabled_filter_names=[global_epoch_name, global_LR_name, global_RL_name], fail_on_exception=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96151def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_binned_position_df: pd.DataFrame = global_results.get('extended_stats', {}).get('time_binned_position_df', None)\n",
    "time_binned_position_df\n",
    "# active_measured_positions = computation_result.sess.position.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c51a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cde2025",
   "metadata": {},
   "source": [
    "# 2024-06-07 - PhoDiba2023Paper figure generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import main_complete_figure_generations\n",
    "\n",
    "main_complete_figure_generations(curr_active_pipeline, save_figure=True, save_figures_only=True, enable_default_neptune_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c41f7",
   "metadata": {},
   "source": [
    "# 2024-06-10 - Across Sessions Bar Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae95fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "## long_short_post_decoding:\n",
    "inst_spike_rate_groups_result: InstantaneousSpikeRateGroupsComputation = curr_active_pipeline.global_computation_results.computed_data.long_short_inst_spike_rate_groups\n",
    "inst_spike_rate_groups_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import InstantaneousSpikeRateGroupsComputation\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_session_instantaneous_spike_rates_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "instantaneous_time_bin_size_seconds: float = 0.0002 # 10ms\n",
    "save_pickle = False\n",
    "save_hdf = True\n",
    "save_across_session_hdf = True\n",
    "\n",
    "_across_session_results_extended_dict = {}\n",
    "## Combine the output of `compute_and_export_session_instantaneous_spike_rates_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_session_instantaneous_spike_rates_completion_function(a_dummy, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, instantaneous_time_bin_size_seconds=instantaneous_time_bin_size_seconds,\n",
    "                                                save_hdf=save_hdf, save_pickle=save_pickle, save_across_session_hdf=save_across_session_hdf, #return_full_decoding_results=return_full_decoding_results, desired_shared_decoding_time_bin_sizes=desired_shared_decoding_time_bin_sizes,\n",
    "                                                )\n",
    "\n",
    "# '_perform_long_short_instantaneous_spike_rate_groups_analysis'\n",
    "# global_computation_results = curr_active_pipeline.global_computation_results\n",
    "# global_computation_results.get('computation_config', {})\n",
    "\n",
    "\n",
    "# instantaneous_time_bin_size_seconds: float = global_computation_results.computation_config.instantaneous_time_bin_size_seconds # 0.01 # 10ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict\n",
    "\n",
    "\n",
    "## Specify the output file:\n",
    "common_file_path = Path('output/active_across_session_scatter_plot_results.h5')\n",
    "print(f'common_file_path: {common_file_path}')\n",
    "InstantaneousFiringRatesDataframeAccessor.add_results_to_inst_fr_results_table(curr_active_pipeline, common_file_path, file_mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible(curr_key='pipeline', curr_value=_out, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a979a",
   "metadata": {},
   "source": [
    "# New Replay Events from Diba-2009-style quiescent period detection followed by active period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0cf6c79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neurons: 32, min_num_active_neurons: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quiescent_start</th>\n",
       "      <th>quiescent_end</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>unique_neurons</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.124170</td>\n",
       "      <td>10.190894</td>\n",
       "      <td>10.190894</td>\n",
       "      <td>10.490894</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.451823</td>\n",
       "      <td>12.543953</td>\n",
       "      <td>12.543953</td>\n",
       "      <td>12.698228</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.704861</td>\n",
       "      <td>13.770448</td>\n",
       "      <td>13.770448</td>\n",
       "      <td>13.984566</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.214118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.292012</td>\n",
       "      <td>14.376062</td>\n",
       "      <td>14.376062</td>\n",
       "      <td>14.644524</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.268462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.791838</td>\n",
       "      <td>31.241763</td>\n",
       "      <td>31.241763</td>\n",
       "      <td>31.361817</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.120054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34.889577</td>\n",
       "      <td>34.971937</td>\n",
       "      <td>34.971937</td>\n",
       "      <td>35.271937</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>2079.040093</td>\n",
       "      <td>2079.112223</td>\n",
       "      <td>2079.112223</td>\n",
       "      <td>2079.403541</td>\n",
       "      <td>7</td>\n",
       "      <td>647</td>\n",
       "      <td>0.291318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>2083.184956</td>\n",
       "      <td>2083.338187</td>\n",
       "      <td>2083.338187</td>\n",
       "      <td>2083.638187</td>\n",
       "      <td>9</td>\n",
       "      <td>648</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>2085.506004</td>\n",
       "      <td>2085.831390</td>\n",
       "      <td>2085.831390</td>\n",
       "      <td>2086.131390</td>\n",
       "      <td>9</td>\n",
       "      <td>649</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>2086.558624</td>\n",
       "      <td>2086.652689</td>\n",
       "      <td>2086.652689</td>\n",
       "      <td>2086.927234</td>\n",
       "      <td>7</td>\n",
       "      <td>650</td>\n",
       "      <td>0.274544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>2089.343391</td>\n",
       "      <td>2089.419177</td>\n",
       "      <td>2089.419177</td>\n",
       "      <td>2089.557540</td>\n",
       "      <td>9</td>\n",
       "      <td>651</td>\n",
       "      <td>0.138363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>2089.557540</td>\n",
       "      <td>2089.649424</td>\n",
       "      <td>2089.649424</td>\n",
       "      <td>2089.879854</td>\n",
       "      <td>8</td>\n",
       "      <td>652</td>\n",
       "      <td>0.230431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     quiescent_start  quiescent_end        start         stop  unique_neurons label  duration\n",
       "0          10.124170      10.190894    10.190894    10.490894               7     0  0.300000\n",
       "1          12.451823      12.543953    12.543953    12.698228               8     1  0.154276\n",
       "2          13.704861      13.770448    13.770448    13.984566               7     2  0.214118\n",
       "3          14.292012      14.376062    14.376062    14.644524               7     3  0.268462\n",
       "4          30.791838      31.241763    31.241763    31.361817               7     4  0.120054\n",
       "5          34.889577      34.971937    34.971937    35.271937              10     5  0.300000\n",
       "..               ...            ...          ...          ...             ...   ...       ...\n",
       "647      2079.040093    2079.112223  2079.112223  2079.403541               7   647  0.291318\n",
       "648      2083.184956    2083.338187  2083.338187  2083.638187               9   648  0.300000\n",
       "649      2085.506004    2085.831390  2085.831390  2086.131390               9   649  0.300000\n",
       "650      2086.558624    2086.652689  2086.652689  2086.927234               7   650  0.274544\n",
       "651      2089.343391    2089.419177  2089.419177  2089.557540               9   651  0.138363\n",
       "652      2089.557540    2089.649424  2089.649424  2089.879854               8   652  0.230431\n",
       "\n",
       "[653 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neuropy.core.epoch import Epoch\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import compute_diba_quiescent_style_replay_events\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import get_proper_global_spikes_df\n",
    "\n",
    "spikes_df = get_proper_global_spikes_df(curr_active_pipeline)\n",
    "(qclu_included_aclus, active_track_templates, active_spikes_df, quiescent_periods), (new_replay_epochs_df, new_replay_epochs) = compute_diba_quiescent_style_replay_events(curr_active_pipeline=curr_active_pipeline, directional_laps_results=directional_laps_results, included_qclu_values=[1,2], minimum_inclusion_fr_Hz=5.0, spikes_df=spikes_df)\n",
    "new_replay_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# evt_filepath = Path('/Users/pho/Downloads/2006-6-07_16-40-19.bst.evt').resolve()\n",
    "evt_filepath = Path('/Users/pho/Downloads/2006-6-08_14-26-15.bst.evt').resolve()\n",
    "\n",
    "evt_epochs: Epoch = Epoch.from_neuroscope(in_filepath=evt_filepath)\n",
    "evt_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_output_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "792efd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved out newly computed epochs to \"/media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/2006-6-08_14-26-15.PHONEW.evt\".\n"
     ]
    }
   ],
   "source": [
    "## Save computed epochs out to a neuroscope .evt file:\n",
    "filename = f\"{curr_active_pipeline.session_name}\"\n",
    "filepath = curr_active_pipeline.get_output_path().joinpath(filename).resolve()\n",
    "filepath.exists()\n",
    "new_replay_epochs.filename = filepath\n",
    "filepath = new_replay_epochs.to_neuroscope(ext='PHONEW')\n",
    "filepath.exists()\n",
    "print(F'saved out newly computed epochs to \"{filepath}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41ebc579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs_source: compute_diba_quiescent_style_replay_events\n",
      "did_change: True\n",
      "perform_drop_computed_result(computed_data_keys_to_drop: ['SequenceBased'], config_names_includelist: None)\n",
      "\t Dropped global_computation_results.computed_data['SequenceBased'].\n",
      "for global computations: Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], ...)...\n",
      "\trun_specific_computations_single_context(including only 3 out of 16 registered computation functions): active_computation_functions: [<function DirectionalPlacefieldGlobalComputationFunctions._build_merged_directional_placefields at 0x7f903cb0ea60>, <function DirectionalPlacefieldGlobalComputationFunctions._decode_and_evaluate_epochs_using_directional_decoders at 0x7f903cb0eb80>, <function DirectionalPlacefieldGlobalComputationFunctions._decoded_epochs_heuristic_scoring at 0x7f903cb0ec10>]...\n",
      "Performing _execute_computation_functions(...) with 3 registered_computation_functions...\n",
      "Executing [0/3]: <function DirectionalPlacefieldGlobalComputationFunctions._build_merged_directional_placefields at 0x7f8e76d738b0>\n",
      "skipping lap recomputation because laps_decoding_time_bin_size == None\n",
      "\t done.\n",
      "Executing [1/3]: <function DirectionalPlacefieldGlobalComputationFunctions._decode_and_evaluate_epochs_using_directional_decoders at 0x7f8e76d73670>\n",
      "laps_decoding_time_bin_size: 0.25, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.793023081021702\n",
      "laps_decoding_time_bin_size: 0.25, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.793023081021702\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "neighbours will be calculated from margin and pos_bin_size. n_neighbours: 1 = int(margin: 4.0 / pos_bin_size: 3.793023081021702)\n",
      "WARNING: n_jobs > 1 (n_jobs: 6) but _allow_parallel_run_general == False, so parallel computation will not be performed.\n",
      "Performance: Radon Transform:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 6/80\n",
      "\tagreeing_rows_ratio: 0.075\n",
      "\tPerformance: Ripple: Radon Transform:\n",
      "agreeing_rows_count/num_total_epochs: 45/653\n",
      "\tagreeing_rows_ratio: 0.06891271056661562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 67/80\n",
      "\tagreeing_rows_ratio: 0.8375\n",
      "Performance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 238/653\n",
      "\tagreeing_rows_ratio: 0.36447166921898927\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 68/80\n",
      "\tagreeing_rows_ratio: 0.85\n",
      "Performance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 240/653\n",
      "\tagreeing_rows_ratio: 0.3675344563552833\n",
      "\t done.\n",
      "Executing [2/3]: <function DirectionalPlacefieldGlobalComputationFunctions._decoded_epochs_heuristic_scoring at 0x7f8e76d73a60>\n",
      "\t done.\n",
      "\t all computations complete! (Computed 3 with no errors!.\n",
      "for global computations: Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['wcorr_shuffle_analysis'], ...)...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function SequenceBasedComputationsGlobalComputationFunctions.perform_wcorr_shuffle_analysis at 0x7f8fe82e1160>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function SequenceBasedComputationsGlobalComputationFunctions.perform_wcorr_shuffle_analysis at 0x7f8e76863c10>\n",
      "WARN: perform_wcorr_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_wcorr_shuffle_analysis(..., num_shuffles=10)\n",
      "len(active_epochs_df): 653\n",
      "min_num_unique_aclu_inclusions: 15\n",
      "len(active_epochs_df): 308\n",
      "len(active_epochs_df): 653\n",
      "min_num_unique_aclu_inclusions: 15\n",
      "len(active_epochs_df): 308\n",
      "real_decoder_ripple_weighted_corr_arr: (306, 4)\n",
      "n_prev_completed_shuffles: 0.\n",
      "needed num_shuffles: 10.\n",
      "need desired_new_num_shuffles: 10 more shuffles.\n",
      "a_shuffle_IDXs: [64  4 10 37 20 58 16 26 51 38 31 52 40 21 15 44 28 19  0  3 70 62 27 39 76 49 73 14 35 25 54 61 43 72 23 65 67 47  8 59 69 63  6 68 36  2 41 53  1 79 48 75 66 74 17 50 60 11  7 42 33 29 57 46 30 32  9 18 13 45 55 78 24 56  5 12 22 71 77 34], a_shuffle_aclus: [ 86   7  14  54  27  78  23  35  69  55  48  70  57  28  21  61  37  26   2   5  93  84  36  56 105  67  98  19  52  34  73  83  60  97  32  87  89  64  11  81  92  85   9  90  53   4  58  71   3 109  66 102  88 100  24  68  82  15  10  59  50  41  76  63  45  49  13  25  17  62  74 108  33  75   8  16  31  96 107  51]\n",
      "a_shuffle_IDXs: [36 71 51 58 66 60  9 78 64 49 69  2 55 62  5 14 76 18 77  4 63 48 28 21 47 20 38 10 44 79 67 52 27 39 65 33 26 17 68 72 15 22 23 19 54 32 30 61 59 42 70 29 74  7 11 75 43 41 46 56 12 25 57 34 40 16 53 73  3 37  1 50 13  8 35 31 45 24  0  6], a_shuffle_aclus: [ 53  96  69  78  88  82  13 108  86  67  92   4  74  84   8  19 105  25 107   7  85  66  37  28  64  27  55  14  61 109  89  70  36  56  87  50  35  24  90  97  21  31  32  26  73  49  45  83  81  59  93  41 100  10  15 102  60  58  63  75  16  34  76  51  57  23  71  98   5  54   3  68  17  11  52  48  62  33   2   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 62 12 61 66 24 46 39 27 21 14 22 57  4 72 36 74 41 18  8 16 64 31 37  6 71 10 45 38 54 68 42 55 69 26 58 35 73 17 79  2 30 56  3 33 60 67 20 13 53 28 49 51 59  0  5  7  9 15 75 43 11 63 25 50 19 70 65 32 44 40 23 76 48 29 78 34 52  1 77], a_shuffle_aclus: [ 64  84  16  83  88  33  63  56  36  28  19  31  76   7  97  53 100  58  25  11  23  86  48  54   9  96  14  62  55  73  90  59  74  92  35  78  52  98  24 109   4  45  75   5  50  82  89  27  17  71  37  67  69  81   2   8  10  13  21 102  60  15  85  34  68  26  93  87  49  61  57  32 105  66  41 108  51  70   3 107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 14 52 42 44 71 34 13 77 66 60 63 72 27 33 36 51  7 55 38 54 15  5 35 45 53 22  2 11 18 59 67 57  4 49 25 62 65  3 47 69  9  0 31 19 48 70 58 10 30 75  1 37 21 76 32 46 12 29  8 73 56 78 17 39 79 26  6 24 41 20 40 43 28 23 68 74 64 16 61], a_shuffle_aclus: [ 68  19  70  59  61  96  51  17 107  88  82  85  97  36  50  53  69  10  74  55  73  21   8  52  62  71  31   4  15  25  81  89  76   7  67  34  84  87   5  64  92  13   2  48  26  66  93  78  14  45 102   3  54  28 105  49  63  16  41  11  98  75 108  24  56 109  35   9  33  58  27  57  60  37  32  90 100  86  23  83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 79 34 64 44 71 41 60 65 22 23 66 69 58  2 17 45 21 62 43 25  0 47 74 50 11  5 36 55 49 37 29 24 42 70  3 48 53 57 30 73 19 15 68  9  4 76 52 13  1  6 31 75 32 46 20 33 59 38 40 18  7 78 51 77 14 16 35 39 63 28 61 72 26 10  8 56 67 12 54], a_shuffle_aclus: [ 36 109  51  86  61  96  58  82  87  31  32  88  92  78   4  24  62  28  84  60  34   2  64 100  68  15   8  53  74  67  54  41  33  59  93   5  66  71  76  45  98  26  21  90  13   7 105  70  17   3   9  48 102  49  63  27  50  81  55  57  25  10 108  69 107  19  23  52  56  85  37  83  97  35  14  11  75  89  16  73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75 31 24 47 15 19 41 18 25 28 71 10 37 68 61 17 46 55 72 56 63 49 12 52 42 51 50 43 74  4 38  0 11 64 45 29 13  1 35 53 48 59 62 40 67 77  7 76 32  8  3 21 79 26 65 70 78 16 44 36 66  9 54 60 39 14 30  5  2  6 57 73 69 27 58 20 34 23 22 33], a_shuffle_aclus: [102  48  33  64  21  26  58  25  34  37  96  14  54  90  83  24  63  74  97  75  85  67  16  70  59  69  68  60 100   7  55   2  15  86  62  41  17   3  52  71  66  81  84  57  89 107  10 105  49  11   5  28 109  35  87  93 108  23  61  53  88  13  73  82  56  19  45   8   4   9  76  98  92  36  78  27  51  32  31  50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [71 60 65 18 20 64 29 25 61 53 32 27 24 59  3 63 33 42 13 41 55 72 49 52 69 15 38 46 74 48 79  2 10 28 67 16 45 58 11 30 50 47 76 22  7 66  5 62 31  1 70 43  8 26 40 57 54 75 34 39  4 51 19 36 35 44 17 56  0 77 12 23  9 68 78 73 21  6 14 37], a_shuffle_aclus: [ 96  82  87  25  27  86  41  34  83  71  49  36  33  81   5  85  50  59  17  58  74  97  67  70  92  21  55  63 100  66 109   4  14  37  89  23  62  78  15  45  68  64 105  31  10  88   8  84  48   3  93  60  11  35  57  76  73 102  51  56   7  69  26  53  52  61  24  75   2 107  16  32  13  90 108  98  28   9  19  54]\n",
      "a_shuffle_IDXs: [15 14 70 44  3 75 60 62 49  0 28 22 38 40 25 77 10 24 11 48  4 31  8 61 43 27 20 26 30  7 29 67 34 51 23 78 56 73 59 64 68 79 57 46 69 54  2 47 76 36 32 55 63 12 74 39 17 58 42 53 45 52 37  1 33 71  5 65 66  6  9 21 41 35 19 50 13 72 16 18], a_shuffle_aclus: [ 21  19  93  61   5 102  82  84  67   2  37  31  55  57  34 107  14  33  15  66   7  48  11  83  60  36  27  35  45  10  41  89  51  69  32 108  75  98  81  86  90 109  76  63  92  73   4  64 105  53  49  74  85  16 100  56  24  78  59  71  62  70  54   3  50  96   8  87  88   9  13  28  58  52  26  68  17  97  23  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75 22 67  2  0 66  6 37 54  1 42 39 51 24 60 58 57 77 40 55 69 70 12 20 56 62 64 18 30 19 78 31 45 52 10 73 34 48 21  7 14 47 72 27 61 59 25 33 26 65 76 41  5 11  9 68 23 16 63 49  4 36 32 17 13 44 79 53 71  8 46 74 35 50 28 29 38  3 15 43], a_shuffle_aclus: [102  31  89   4   2  88   9  54  73   3  59  56  69  33  82  78  76 107  57  74  92  93  16  27  75  84  86  25  45  26 108  48  62  70  14  98  51  66  28  10  19  64  97  36  83  81  34  50  35  87 105  58   8  15  13  90  32  23  85  67   7  53  49  24  17  61 109  71  96  11  63 100  52  68  37  41  55   5  21  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [68 62 34 79 54 55 47 30 69 20  3 35 13 57 41 23 17 12 45 73 25 14 33 78 21 60 36 49 10 48 61  1 59 72 52 71 22 40  5 18 46 58  7  0 39 76 28 65 19 50 51 63  8 64 44 15  4 67 66  2 16 11 29 38 77 37 31 32 74 26 75 53 56 27 42 24  6  9 43 70], a_shuffle_aclus: [ 90  84  51 109  73  74  64  45  92  27   5  52  17  76  58  32  24  16  62  98  34  19  50 108  28  82  53  67  14  66  83   3  81  97  70  96  31  57   8  25  63  78  10   2  56 105  37  87  26  68  69  85  11  86  61  21   7  89  88   4  23  15  41  55 107  54  48  49 100  35 102  71  75  36  59  33   9  13  60  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "\t all computations complete! (Computed 1 with no errors!.\n",
      "custom_save_filenames: {'pipeline_pkl': 'loadedSessPickle_withNewComputedReplays.pkl', 'global_computation_pkl': 'global_computation_results_withNewComputedReplays.pkl', 'pipeline_h5': 'pipeline_withNewComputedReplays.h5'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:save_pipeline(): Attempting to save pipeline to disk...\n",
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\tfinalized_loaded_sess_pickle_path: /media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle_withNewComputedReplays.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalized_loaded_sess_pickle_path: /media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle_withNewComputedReplays.pkl\n",
      "Saving (file mode 'w+b') session pickle file results :... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-06-26_19-06-39.RDLU0039.kdiba.gor01.one.2006-6-08_14-26-15:\t save complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved session pickle file\n",
      "global_computation_results_pickle_path: /media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results_withNewComputedReplays.pkl\n",
      "Saving (file mode 'w+b') session pickle file results :... saved session pickle file\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import overwrite_replay_epochs_and_recompute\n",
    "\n",
    "did_change, custom_save_filenames, custom_save_filepaths = overwrite_replay_epochs_and_recompute(curr_active_pipeline=curr_active_pipeline, new_replay_epochs=new_replay_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c08e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.sess.replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0138099",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:a_filtered_session.basepath for k, a_filtered_session in curr_active_pipeline.filtered_sessions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de178044",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = curr_active_pipeline.save_split_global_computation_results(debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef16e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['long_short_decoding_analyses',\n",
    "    # 'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    # 'long_short_post_decoding',\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    ], enabled_filter_names=None, fail_on_exception=True, debug_print=False) # , computation_kwargs_list=[{'should_skip_radon_transform': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1bbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_merged_computation_function_validators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8464376",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['jonathan_firing_rate_analysis'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['long_short_post_decoding'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecaa604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['rank_order_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 100, 'skip_laps': True}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['extended_stats'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['trial_by_trial_metrics'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_evaluate_epochs'], computation_kwargs_list=[{'should_skip_radon_transform': False}], enabled_filter_names=None, fail_on_exception=True, debug_print=True)\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_epoch_heuristic_scoring'], enabled_filter_names=None, fail_on_exception=True, debug_print=False) # OK FOR PICKLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f766805",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_data_keys = list(curr_active_pipeline.global_computation_results.computed_data.keys())\n",
    "print(f\"computed_data_keys: {computed_data_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79ce8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for global computations: Performing run_specific_computations_single_context(..., computation_functions_name_includelist=['perform_wcorr_shuffle_analysis'], ...)...\n",
      "\trun_specific_computations_single_context(including only 1 out of 16 registered computation functions): active_computation_functions: [<function SequenceBasedComputationsGlobalComputationFunctions.perform_wcorr_shuffle_analysis at 0x7f8fe82e1160>]...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "Executing [0/1]: <function SequenceBasedComputationsGlobalComputationFunctions.perform_wcorr_shuffle_analysis at 0x7f8e4431cf70>\n",
      "WARN: perform_wcorr_shuffle_analysis(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "perform_wcorr_shuffle_analysis(..., num_shuffles=600)\n",
      "n_prev_completed_shuffles: 10.\n",
      "needed num_shuffles: 600.\n",
      "need desired_new_num_shuffles: 590 more shuffles.\n",
      "a_shuffle_IDXs: [ 6 40 69 73 65 68 55 18 56  4 35 39  1 46 10 13 58 21 20 44 15 30 75 74 66 28 24 57 70 23 26 47 14 71 25 33 45 16 34 12 27 59 38 79 17 41 77 78 32 49 67 29 36 22 37  3 43 11 19  9 76 51 53 31  0 52  8 54  5  2 72 48  7 62 61 63 64 42 50 60], a_shuffle_aclus: [  9  57  92  98  87  90  74  25  75   7  52  56   3  63  14  17  78  28  27  61  21  45 102 100  88  37  33  76  93  32  35  64  19  96  34  50  62  23  51  16  36  81  55 109  24  58 107 108  49  67  89  41  53  31  54   5  60  15  26  13 105  69  71  48   2  70  11  73   8   4  97  66  10  84  83  85  86  59  68  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 58 39 64 23 30  4 73 53 33  5 61  9 65  8 18 20 27 29 38 13 25 68 12 37 78 45 48 74  7 35 62 46 26 16 76 43 63 19 44 41  0 60 51 31 40 72 34 50 70 32 28  1  3 79 36 42 54 77 49 24 10 71 56 15 17 14 69 11 22 66  6 75 55 47 59 52  2 21 67], a_shuffle_aclus: [ 76  78  56  86  32  45   7  98  71  50   8  83  13  87  11  25  27  36  41  55  17  34  90  16  54 108  62  66 100  10  52  84  63  35  23 105  60  85  26  61  58   2  82  69  48  57  97  51  68  93  49  37   3   5 109  53  59  73 107  67  33  14  96  75  21  24  19  92  15  31  88   9 102  74  64  81  70   4  28  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4 47 29 10 44 20 25 41 32 70 64 65 59 30 67  0  8 46 63 43 60 69  6 72 24 66 12 40  5 23 35 49 53 18 33 74 79 15 36 75 50 71 58 42  7 73 22  3 78 16 45 56  2  9 34 21 55 28 13 48 14  1 27 62 26 38 76 11 57 19 77 17 54 51 61 37 31 68 52 39], a_shuffle_aclus: [  7  64  41  14  61  27  34  58  49  93  86  87  81  45  89   2  11  63  85  60  82  92   9  97  33  88  16  57   8  32  52  67  71  25  50 100 109  21  53 102  68  96  78  59  10  98  31   5 108  23  62  75   4  13  51  28  74  37  17  66  19   3  36  84  35  55 105  15  76  26 107  24  73  69  83  54  48  90  70  56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [45 61 32 73 41 13 39 38 53 62 12 57 15 72 54 23 66 33 27 55 40 42  3 28  5  9 43 47  2  8 49 16 11  1 25 79 52 76 65 36 10 18 17 75 78 50 58 26 77  0 44 64  4 67 74 31 69 21 63 22 56 71  6  7 20 34 37 30 19 48 59 35 60 29 70 14 24 51 68 46], a_shuffle_aclus: [ 62  83  49  98  58  17  56  55  71  84  16  76  21  97  73  32  88  50  36  74  57  59   5  37   8  13  60  64   4  11  67  23  15   3  34 109  70 105  87  53  14  25  24 102 108  68  78  35 107   2  61  86   7  89 100  48  92  28  85  31  75  96   9  10  27  51  54  45  26  66  81  52  82  41  93  19  33  69  90  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [59 72 27  3 39 33 56 77 25 70 13 46 29 53 18  0 10 78 35 20 47 23 44 58 57 55 37 31 64 50 63 24 52 66 22 34 36 15 73 32 67  9  1  4 75 54 69 41 21 60 62  7 26 79 45  2 17 68 11 74 30 12  6 14 19 43 38 48 16 76 49 42 65 71 28 40 61  8  5 51], a_shuffle_aclus: [ 81  97  36   5  56  50  75 107  34  93  17  63  41  71  25   2  14 108  52  27  64  32  61  78  76  74  54  48  86  68  85  33  70  88  31  51  53  21  98  49  89  13   3   7 102  73  92  58  28  82  84  10  35 109  62   4  24  90  15 100  45  16   9  19  26  60  55  66  23 105  67  59  87  96  37  57  83  11   8  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [14 11 68 67 71  4 33 13 23 66 38 46 28 52 39 35 25 29  7 54 26 22 56 78 55 77  6 47 43 41 75 15 17  0 18 50 76 27 24 40 63 57 45  1 74 48 34 42 64 12 49 32 72  3 79 62 21 20  2 70 51 36 58 73 60  9 31 10  5 37 44 53  8 69 19 59 61 65 16 30], a_shuffle_aclus: [ 19  15  90  89  96   7  50  17  32  88  55  63  37  70  56  52  34  41  10  73  35  31  75 108  74 107   9  64  60  58 102  21  24   2  25  68 105  36  33  57  85  76  62   3 100  66  51  59  86  16  67  49  97   5 109  84  28  27   4  93  69  53  78  98  82  13  48  14   8  54  61  71  11  92  26  81  83  87  23  45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 18  5 46 55 50 71 22 14 32 15 47 78 34 39 23  0 31 54 65 53 26 60 25  4 51 68 57 69  3 64 16 59 79 58 41 17 72 13 66  8 36  1 70 10 43 67 49 42 74 62 76 52 63  6 19 12 35 61 73 75 27 44 40 24 28 29 45 37 21 33 56 48 77 30 20  2  7 38 11], a_shuffle_aclus: [ 13  25   8  63  74  68  96  31  19  49  21  64 108  51  56  32   2  48  73  87  71  35  82  34   7  69  90  76  92   5  86  23  81 109  78  58  24  97  17  88  11  53   3  93  14  60  89  67  59 100  84 105  70  85   9  26  16  52  83  98 102  36  61  57  33  37  41  62  54  28  50  75  66 107  45  27   4  10  55  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 17 71 51  5 74 60 46 52 63  6  2 25 18 75  0 73 49  8 77 19  3 28 66 72 55 33 14 23 32 45 47 41 27 24 34 29 64 26  7 36 22 69 44 56 16 42 61 11 54 78 53 79 40 39 62 65 12 70 21 13  4 43  9 38 15 30 31 58 67 48 50 20  1 37 10 59 76 35 68], a_shuffle_aclus: [ 76  24  96  69   8 100  82  63  70  85   9   4  34  25 102   2  98  67  11 107  26   5  37  88  97  74  50  19  32  49  62  64  58  36  33  51  41  86  35  10  53  31  92  61  75  23  59  83  15  73 108  71 109  57  56  84  87  16  93  28  17   7  60  13  55  21  45  48  78  89  66  68  27   3  54  14  81 105  52  90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [32 50 79  0 33 65 55 53 38 54 28 73 51 61 36 46 11 14 71 21 56  4 76  6 34 19 25 52 77 70 13 29 39 69  3 12 23 24 37 59 64  7 72  8 60 35 74 16 63  5 47 78 31 43 45 68 15 75 57 42 22 17 44 41 27 18 49  1 30 20 26 67 48 62  9 40 58  2 10 66], a_shuffle_aclus: [ 49  68 109   2  50  87  74  71  55  73  37  98  69  83  53  63  15  19  96  28  75   7 105   9  51  26  34  70 107  93  17  41  56  92   5  16  32  33  54  81  86  10  97  11  82  52 100  23  85   8  64 108  48  60  62  90  21 102  76  59  31  24  61  58  36  25  67   3  45  27  35  89  66  84  13  57  78   4  14  88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 23 70 14 52 76 64 57 29  5 11 69 10 65 74 37 36 32 46 43 48 50 44 35 24 16 38 13 54 39 73  1 66  0  2 61 53 60 33 31 20 21 12 56 79 67 55 47 71 41 42 25 78 17 77  4  9 34 30 40  7 51 68 49  8 27 58 62 72 63 19 18 45 59  6 26 22  3 75 15], a_shuffle_aclus: [ 37  32  93  19  70 105  86  76  41   8  15  92  14  87 100  54  53  49  63  60  66  68  61  52  33  23  55  17  73  56  98   3  88   2   4  83  71  82  50  48  27  28  16  75 109  89  74  64  96  58  59  34 108  24 107   7  13  51  45  57  10  69  90  67  11  36  78  84  97  85  26  25  62  81   9  35  31   5 102  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [68 24 73 27 54  4 69 48 52 14  3 62 38 37 20 10  2 22 35 70 41  7 60 23 72  8 59 18 40 66 21 50 12  1 13 39 61 78 75 76 17 51 36 28 42 29 49 30 71 26 44 53 58 43 16  9  0 46 67 47 74 56 64 25 19  6 33  5 15 63 57 65 45 34 32 11 77 55 79 31], a_shuffle_aclus: [ 90  33  98  36  73   7  92  66  70  19   5  84  55  54  27  14   4  31  52  93  58  10  82  32  97  11  81  25  57  88  28  68  16   3  17  56  83 108 102 105  24  69  53  37  59  41  67  45  96  35  61  71  78  60  23  13   2  63  89  64 100  75  86  34  26   9  50   8  21  85  76  87  62  51  49  15 107  74 109  48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [25 38 20 72  3 10 42 59 58 77  7 57 41 39 60 27 76 55 11 26  6 30 32 78 18 28 51 33 22  9  4 17 56 69 14 21 40 53 13 24 68 71 52 19 62 50 67 46 48 66 37 64 49 35 54 43 63 31 44 70  0  5 36 79 12 16 73  2 45 47  8 15 61 74 29 65 23 75  1 34], a_shuffle_aclus: [ 34  55  27  97   5  14  59  81  78 107  10  76  58  56  82  36 105  74  15  35   9  45  49 108  25  37  69  50  31  13   7  24  75  92  19  28  57  71  17  33  90  96  70  26  84  68  89  63  66  88  54  86  67  52  73  60  85  48  61  93   2   8  53 109  16  23  98   4  62  64  11  21  83 100  41  87  32 102   3  51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 46 14 43 63  3 54  9  4 35  6 44 10  1 32  7 11  8 37 34 74 41 53 26 49 16 19 79 23 18 68 42 25 52 73 29 47 64 58 69 21 67 45 22 17 38 31 12 30  0  5 56 39 62 40 48 61 27 76 72 57 65 50 59 75 71 51 33 77 60 70 36  2 20 15 55 66 78 24 13], a_shuffle_aclus: [ 37  63  19  60  85   5  73  13   7  52   9  61  14   3  49  10  15  11  54  51 100  58  71  35  67  23  26 109  32  25  90  59  34  70  98  41  64  86  78  92  28  89  62  31  24  55  48  16  45   2   8  75  56  84  57  66  83  36 105  97  76  87  68  81 102  96  69  50 107  82  93  53   4  27  21  74  88 108  33  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [78  9 50 43 25 17 39  4 52 62 77 33 22 44 21  5 10 13 66 46 24  1  2 70 49 26 53 37 11 28  3 60 45 47 23 35 12 56 73 38 15 54 34 58 55  8 42 32 65 27 51 19 69 30 59 36 76 16 75 40  6 64 79  7 20 63 61 67 31 41 14 71 29 72 57 68 48 18 74  0], a_shuffle_aclus: [108  13  68  60  34  24  56   7  70  84 107  50  31  61  28   8  14  17  88  63  33   3   4  93  67  35  71  54  15  37   5  82  62  64  32  52  16  75  98  55  21  73  51  78  74  11  59  49  87  36  69  26  92  45  81  53 105  23 102  57   9  86 109  10  27  85  83  89  48  58  19  96  41  97  76  90  66  25 100   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [68 46 54 36 13 76 51 31 43 73 58 47 16 69 38 23 78  3 79 35  5 75 29 55 71 17 10 53 11 41 62 12 49 21  4 67 39 25 70 42 56 22 28 50  6 66 60 14 65 40 19  2  9 20 44 26  1 74 61 48 64 24 45 63 27 18 72 32 37 34  8 30 52  7 57 15  0 77 59 33], a_shuffle_aclus: [ 90  63  73  53  17 105  69  48  60  98  78  64  23  92  55  32 108   5 109  52   8 102  41  74  96  24  14  71  15  58  84  16  67  28   7  89  56  34  93  59  75  31  37  68   9  88  82  19  87  57  26   4  13  27  61  35   3 100  83  66  86  33  62  85  36  25  97  49  54  51  11  45  70  10  76  21   2 107  81  50]\n",
      "a_shuffle_IDXs: [23  8 40 27 73  1 51 20  0 18 63 25 13 42 12 62 57 50 17 14  2 58 47 59  6  3 45 60 55 19 75 29 74 32 53 71  4  5 28 11 67 34  9 68 44 43 78 61 65 56 37 10 33 64 35 26 72 22 66 31 16 69 21 79  7 70 30 36 15 24 52 77 39 49 38 41 76 48 46 54], a_shuffle_aclus: [ 32  11  57  36  98   3  69  27   2  25  85  34  17  59  16  84  76  68  24  19   4  78  64  81   9   5  62  82  74  26 102  41 100  49  71  96   7   8  37  15  89  51  13  90  61  60 108  83  87  75  54  14  50  86  52  35  97  31  88  48  23  92  28 109  10  93  45  53  21  33  70 107  56  67  55  58 105  66  63  73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [69 73 79 57 72 37  7 63 65 20 41 15 18 17 29  6 40 54 23  2 68 60 45 31 21 70 64 76 51 30 58 77 42  3 49  4 52 25 75 66 53 36  9  1 13 22 55 47 43 62  0 32 56 14 24 50 19 16 67 48 44 71 27 35 12 61 34 59 46 26  8 28 33 38 39 78 74 11  5 10], a_shuffle_aclus: [ 92  98 109  76  97  54  10  85  87  27  58  21  25  24  41   9  57  73  32   4  90  82  62  48  28  93  86 105  69  45  78 107  59   5  67   7  70  34 102  88  71  53  13   3  17  31  74  64  60  84   2  49  75  19  33  68  26  23  89  66  61  96  36  52  16  83  51  81  63  35  11  37  50  55  56 108 100  15   8  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51 32 55 25 24 75 38  0 45 59 11 78 10 62 31 13 49 20 53 54 17  6 35 52 40  4 71 65 39 36 12  2 70 19 16 33 74 77 43 28 22 67 79 37 73 26 44  3 58 14 34 23 30  8 29 46  7 64 47 41 56 15 21 72 42 66  1 68 48 57 69  5 60 63 61 50 18 76  9 27], a_shuffle_aclus: [ 69  49  74  34  33 102  55   2  62  81  15 108  14  84  48  17  67  27  71  73  24   9  52  70  57   7  96  87  56  53  16   4  93  26  23  50 100 107  60  37  31  89 109  54  98  35  61   5  78  19  51  32  45  11  41  63  10  86  64  58  75  21  28  97  59  88   3  90  66  76  92   8  82  85  83  68  25 105  13  36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [72 22 69 27 63 46 52  7  2 31 50 40 59 48  0 56  6 23 61 11 60 74 77 18 37 79 75 35 76 58  1 71 51 49 29 21 78 30  4 62 47 25  3 14 28 16 68 33 39 64 38 26 19  9 36 41 32 73 12 42 57 20 66 67 24  5 53 55 10 70 13 15 45 65 43  8 54 17 44 34], a_shuffle_aclus: [ 97  31  92  36  85  63  70  10   4  48  68  57  81  66   2  75   9  32  83  15  82 100 107  25  54 109 102  52 105  78   3  96  69  67  41  28 108  45   7  84  64  34   5  19  37  23  90  50  56  86  55  35  26  13  53  58  49  98  16  59  76  27  88  89  33   8  71  74  14  93  17  21  62  87  60  11  73  24  61  51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 23 29 25 60 65 49  7  0 13 75 20 54 36 45 71 61 67 28 47  5 52 62 57  8 74 64 12 32 69 50 21 78 39 73 30 37  3 79 59  9 66 18 51 72 15 70 19 14 53 58  6 46 34 43 10 17 33 76 55 42  4 68  2 27 26 44 24 38 77 35 56  1 11 40 48 22 63 31 16], a_shuffle_aclus: [ 58  32  41  34  82  87  67  10   2  17 102  27  73  53  62  96  83  89  37  64   8  70  84  76  11 100  86  16  49  92  68  28 108  56  98  45  54   5 109  81  13  88  25  69  97  21  93  26  19  71  78   9  63  51  60  14  24  50 105  74  59   7  90   4  36  35  61  33  55 107  52  75   3  15  57  66  31  85  48  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [53 21 47 61 50 57 65 44 71 20  1 33 60 34 30 23 74 64 51 39  5 78  0 14 54 37 26  7 22 58 42 55 29 12 35 40 62  3  9 28 67 17 25 70 73 49 68 36 72 15 63 32 24 77 59 19 18 31  2 38 66 10 13 56 48 76  4 75 46 45 11 41  6 16 43 79 27 69  8 52], a_shuffle_aclus: [ 71  28  64  83  68  76  87  61  96  27   3  50  82  51  45  32 100  86  69  56   8 108   2  19  73  54  35  10  31  78  59  74  41  16  52  57  84   5  13  37  89  24  34  93  98  67  90  53  97  21  85  49  33 107  81  26  25  48   4  55  88  14  17  75  66 105   7 102  63  62  15  58   9  23  60 109  36  92  11  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51  3 11  9 74 31 36 50 39 52 17 32 69 62 13 40 38 20 18 76 41 72 45 60 79  4 21  0  7 12 55 67  6 27 64 35 70 43 34 71 44 42 37 78  5 61 48 10 66 77 53 15 22  8 23 28  1 54 33 68 65 56 19 14 75 46  2 73 58 49 29 63 25 26 30 16 24 47 59 57], a_shuffle_aclus: [ 69   5  15  13 100  48  53  68  56  70  24  49  92  84  17  57  55  27  25 105  58  97  62  82 109   7  28   2  10  16  74  89   9  36  86  52  93  60  51  96  61  59  54 108   8  83  66  14  88 107  71  21  31  11  32  37   3  73  50  90  87  75  26  19 102  63   4  98  78  67  41  85  34  35  45  23  33  64  81  76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55 69 57 52 63 48 20 19 40 17 29 45  5 76 22 65 16 54 61 71 33  0 35 12 21 42 51  2 37 66 28  3 36 43 30 44 73  9 74  6 11 60 26 23 75  1 38 70 50 78 62 49 68 24  8 25 18 41 79 72  4 14 13 56 67  7 27 39 64 58 15 47 46 34 77 53 32 31 59 10], a_shuffle_aclus: [ 74  92  76  70  85  66  27  26  57  24  41  62   8 105  31  87  23  73  83  96  50   2  52  16  28  59  69   4  54  88  37   5  53  60  45  61  98  13 100   9  15  82  35  32 102   3  55  93  68 108  84  67  90  33  11  34  25  58 109  97   7  19  17  75  89  10  36  56  86  78  21  64  63  51 107  71  49  48  81  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 57 66 50 17 28 47 18  7 59 58 38 43 64 19 62 33 40 31 25 65 26 49 67 23 24 30 15 54 20 79 77 37 76  3 48 60 22 34  1 63 78 27 16 46 68  8 10 42 56 29 71  0 45 75 52 11 72 53 12 32 41  2 69 13 73 55  5 44  6 39 70 36  4 74 14 35 51 61 21], a_shuffle_aclus: [ 13  76  88  68  24  37  64  25  10  81  78  55  60  86  26  84  50  57  48  34  87  35  67  89  32  33  45  21  73  27 109 107  54 105   5  66  82  31  51   3  85 108  36  23  63  90  11  14  59  75  41  96   2  62 102  70  15  97  71  16  49  58   4  92  17  98  74   8  61   9  56  93  53   7 100  19  52  69  83  28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6 15 67 19 55 33 51 32 62  2 26 79  0 76 75 25  5 18 73 39 72 77 40 48  3  9  7 66 23 28 71 45 44 43 14 61 46 65 17 31 68 20 22 35 70 21  8 52 59 60 27 69 58 42 38 11 36  4 24 74 30 57 29 64 16  1 10 63 50 49 78 54 12 34 41 53 47 56 37 13], a_shuffle_aclus: [  9  21  89  26  74  50  69  49  84   4  35 109   2 105 102  34   8  25  98  56  97 107  57  66   5  13  10  88  32  37  96  62  61  60  19  83  63  87  24  48  90  27  31  52  93  28  11  70  81  82  36  92  78  59  55  15  53   7  33 100  45  76  41  86  23   3  14  85  68  67 108  73  16  51  58  71  64  75  54  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [21 24 14 41 72 12 17 59 38 49  2 36  9  0 56 77 32 60  6 37 45 75 39 16 28 43 55  8 26 48 13 61 54  3 58 25 20 29 67  7 47 42 73  5 66 78 64 51 15 68 31 34 74 40 79 18 27 63 62 22 65 46 19  1 23 35 50 53 33 30 70 76 69 71 52 10  4 57 44 11], a_shuffle_aclus: [ 28  33  19  58  97  16  24  81  55  67   4  53  13   2  75 107  49  82   9  54  62 102  56  23  37  60  74  11  35  66  17  83  73   5  78  34  27  41  89  10  64  59  98   8  88 108  86  69  21  90  48  51 100  57 109  25  36  85  84  31  87  63  26   3  32  52  68  71  50  45  93 105  92  96  70  14   7  76  61  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [54  1 12 70 77 22 11 60 28 38 23 47 34 48 33 64 63 20 74 18 43 62 49  4  2 24 61 76 75 72  7 79 71 15  3 57 42 13 50 65 25 39 78  8 53 16 21 73  6 36 67 68 55 14 45 52 30 37 59 58 19 46 51 56 32 66 10 26  5 69  0 17 40 27 41 29 31 35  9 44], a_shuffle_aclus: [ 73   3  16  93 107  31  15  82  37  55  32  64  51  66  50  86  85  27 100  25  60  84  67   7   4  33  83 105 102  97  10 109  96  21   5  76  59  17  68  87  34  56 108  11  71  23  28  98   9  53  89  90  74  19  62  70  45  54  81  78  26  63  69  75  49  88  14  35   8  92   2  24  57  36  58  41  48  52  13  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 13  6 51 61 73 67 56 62 54 45  5 21 55 42 29 11  3 53 58 24 64 59 18  2  1 34 71 27 74 39 19 50 77 46 31 57 48 49 17 75 15 63 44 25 70  9 12 14 10 68 16  4 28 37  0 23 30 43 22 41 38 35 47 78 69 72 40 26 79  7 32 36 65 20 33 66 60 76 52], a_shuffle_aclus: [ 11  17   9  69  83  98  89  75  84  73  62   8  28  74  59  41  15   5  71  78  33  86  81  25   4   3  51  96  36 100  56  26  68 107  63  48  76  66  67  24 102  21  85  61  34  93  13  16  19  14  90  23   7  37  54   2  32  45  60  31  58  55  52  64 108  92  97  57  35 109  10  49  53  87  27  50  88  82 105  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 77 30 44 47 27 16 34 25 42 29 23 31  8  3 15 35 10 21  5 64  1 49 13 79 19 53 71  6 55 72 18 66 70 63 76  4 24 61 60 54 40 39 48 11  2 67 12 74 38  9 58 37 56 73 20 52  0 59 78 69 36 65 75 43 50 32 68 45 57 51 14 62 28 26 41  7 33 22 46], a_shuffle_aclus: [ 24 107  45  61  64  36  23  51  34  59  41  32  48  11   5  21  52  14  28   8  86   3  67  17 109  26  71  96   9  74  97  25  88  93  85 105   7  33  83  82  73  57  56  66  15   4  89  16 100  55  13  78  54  75  98  27  70   2  81 108  92  53  87 102  60  68  49  90  62  76  69  19  84  37  35  58  10  50  31  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:391: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ey = np.nansum(arr * y_mat) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:392: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ex = np.nansum(arr * x_mat) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:393: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_xy = np.nansum(arr * (y_mat - ey) * (x_mat - ex)) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:394: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_yy = np.nansum(arr * (y_mat - ey) ** 2) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:395: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_xx = np.nansum(arr * (x_mat - ex) ** 2) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 42 32 58 24  9 53 71 74 73 44  3 68 47 35 77 57 20 28  8 13 12  5 17 23 51 54 50 65 48 31  0 19 55 21 59 16 18  7  2 41 43 33 37 76 30 15 67 60 34 64  1 22 70 25 79 39 75  6 14 38 78 62 72 26 49 66 40 10 69 36 61 46 29 45  4 27 52 56 63], a_shuffle_aclus: [ 15  59  49  78  33  13  71  96 100  98  61   5  90  64  52 107  76  27  37  11  17  16   8  24  32  69  73  68  87  66  48   2  26  74  28  81  23  25  10   4  58  60  50  54 105  45  21  89  82  51  86   3  31  93  34 109  56 102   9  19  55 108  84  97  35  67  88  57  14  92  53  83  63  41  62   7  36  70  75  85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60 22 70  0 12 62 76  7 13 29 41 64 25 14 21  8 15 18 56  2 43 27 17  9 66 11 44  1  4 52 67 24 50 10 54 32 30 59 71 53 34 78 69 55 58 38 72 33 16 23 74  6 75 42 36 61 63 26 65 28 73  3 31 49 45 79 51 46 39 40 57 68 19 20 47 77 35 37  5 48], a_shuffle_aclus: [ 82  31  93   2  16  84 105  10  17  41  58  86  34  19  28  11  21  25  75   4  60  36  24  13  88  15  61   3   7  70  89  33  68  14  73  49  45  81  96  71  51 108  92  74  78  55  97  50  23  32 100   9 102  59  53  83  85  35  87  37  98   5  48  67  62 109  69  63  56  57  76  90  26  27  64 107  52  54   8  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 12 67 72 27 43 73 32 65 39 35 40  0 57 21 70 62 36 31 75 79 20 34 33 41 51  9 59 26  6 18 14 16 23 19  7 30 74 54  8  2 42 17 58  3 68 48 77 28 37 63 44 46 22 24 52 56 69 15 49  5 64 10 61 11 78 66 29 60  4 25 76 38  1 47 45 71 53 13 55], a_shuffle_aclus: [ 68  16  89  97  36  60  98  49  87  56  52  57   2  76  28  93  84  53  48 102 109  27  51  50  58  69  13  81  35   9  25  19  23  32  26  10  45 100  73  11   4  59  24  78   5  90  66 107  37  54  85  61  63  31  33  70  75  92  21  67   8  86  14  83  15 108  88  41  82   7  34 105  55   3  64  62  96  71  17  74]\n",
      "a_shuffle_IDXs: [21 72 76 28  4 62 59 34 66 29 18 47 53 60 45  1  2 12  5 46 43 75 55 36 30 44 35  8 13 54 33 78 23  3 25 51 19 77 69 24 65 49 68 73 71 39 74 61 26 16 37 41 22 17 48 64  7 58  9 11 10 31 27 14 70 32 40 63 20 56 57 50 38 52 42 15  6  0 67 79], a_shuffle_aclus: [ 28  97 105  37   7  84  81  51  88  41  25  64  71  82  62   3   4  16   8  63  60 102  74  53  45  61  52  11  17  73  50 108  32   5  34  69  26 107  92  33  87  67  90  98  96  56 100  83  35  23  54  58  31  24  66  86  10  78  13  15  14  48  36  19  93  49  57  85  27  75  76  68  55  70  59  21   9   2  89 109]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 30 53 54 15 11 67 63 41 49 66 71 34 10 22 39 13 33 28 27 44 42 35 57 55  1  0 18 29 37 77 40 32 31 51 68 56 16  8 25 65  7 73 60 26 61 58  6 36 45 72 69 21 43 38 19 14 64 23  4 47 78 50 79 24 75 52 20 46  5 62  3 74 76 17 12 70 59 48  2], a_shuffle_aclus: [ 13  45  71  73  21  15  89  85  58  67  88  96  51  14  31  56  17  50  37  36  61  59  52  76  74   3   2  25  41  54 107  57  49  48  69  90  75  23  11  34  87  10  98  82  35  83  78   9  53  62  97  92  28  60  55  26  19  86  32   7  64 108  68 109  33 102  70  27  63   8  84   5 100 105  24  16  93  81  66   4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [31 50 18 33 15 45 23 41 17 39  7 47 66 35 32 53 40 34 46 64 19  8 75 21 73 69  3 10  6 26 24  5 79  2 59 48 12 42 71 61  4 63 68 38 74 70 28 77 51 76 14 25 72 54 43 62 27 67 13 56 58  0 44 37 65 16 30 11 60  9 36  1 29 49 22 52 20 55 78 57], a_shuffle_aclus: [ 48  68  25  50  21  62  32  58  24  56  10  64  88  52  49  71  57  51  63  86  26  11 102  28  98  92   5  14   9  35  33   8 109   4  81  66  16  59  96  83   7  85  90  55 100  93  37 107  69 105  19  34  97  73  60  84  36  89  17  75  78   2  61  54  87  23  45  15  82  13  53   3  41  67  31  70  27  74 108  76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [29 11 62 31 20 57 19 55  2 47 33 46 63 52 79 75 25 10 65 39 76 32 58 61  9 64 72  5 34 36 56 67  6 70 53 13  4 50 26 15 35 41 78  0 66 44 21  1 27 59 40 48 18 30 45 24 43 16 54 49 22 68 42  7 71 73 12 60  8 14 28 69 74 77 37 38 17  3 51 23], a_shuffle_aclus: [ 41  15  84  48  27  76  26  74   4  64  50  63  85  70 109 102  34  14  87  56 105  49  78  83  13  86  97   8  51  53  75  89   9  93  71  17   7  68  35  21  52  58 108   2  88  61  28   3  36  81  57  66  25  45  62  33  60  23  73  67  31  90  59  10  96  98  16  82  11  19  37  92 100 107  54  55  24   5  69  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [13 70 73 66 40 65 16 32 19 36 51 56 39  5  3 37 20  0 43 62 46 77 69 79 53 27 49  2  7 68 30 45  6 35 64 22 60 33 74  9 52 72 17 71 54 48  8 61 58 28 25 10 42 26 50  1 14 38 47 21 29 24 12 67 11 57 76 59  4 55 75 23 41 34 63 18 78 31 15 44], a_shuffle_aclus: [ 17  93  98  88  57  87  23  49  26  53  69  75  56   8   5  54  27   2  60  84  63 107  92 109  71  36  67   4  10  90  45  62   9  52  86  31  82  50 100  13  70  97  24  96  73  66  11  83  78  37  34  14  59  35  68   3  19  55  64  28  41  33  16  89  15  76 105  81   7  74 102  32  58  51  85  25 108  48  21  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [35  6 19 59  5 74 12 56 44 20 32 72 27 17 50  1 38 33 18 10 34  2 79 61 62 11 48 28 54 49 24 63 42 23 43 66 71 15 31 40 14 25 37  3 57 68 39 26 64  0 78 55 22  9 53 13  8 21 76 16 29 60  4 65  7 52 70 41 75 45 36 46 73 77 67 47 30 69 58 51], a_shuffle_aclus: [ 52   9  26  81   8 100  16  75  61  27  49  97  36  24  68   3  55  50  25  14  51   4 109  83  84  15  66  37  73  67  33  85  59  32  60  88  96  21  48  57  19  34  54   5  76  90  56  35  86   2 108  74  31  13  71  17  11  28 105  23  41  82   7  87  10  70  93  58 102  62  53  63  98 107  89  64  45  92  78  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [23 69 64 78  4 30  6 52 55 59 40 76  5 74 18 75 34  3 56 37 72 39 46  7 54 48 20 43 47 38  1 29 61  0  2 28 65 31 45 19 41 27  8 66 62 42 51 25 73 63 35 57 16 15 17 71 13 49  9 53 24 70 68 77 79 21 26 58 11 14 67 12 50 60 33 36 10 44 22 32], a_shuffle_aclus: [ 32  92  86 108   7  45   9  70  74  81  57 105   8 100  25 102  51   5  75  54  97  56  63  10  73  66  27  60  64  55   3  41  83   2   4  37  87  48  62  26  58  36  11  88  84  59  69  34  98  85  52  76  23  21  24  96  17  67  13  71  33  93  90 107 109  28  35  78  15  19  89  16  68  82  50  53  14  61  31  49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 24 20  7 11 49 65 68  6 64 27 29 70 33 23 73 50 55 38 43 61 35 14 19 58 78 46 45 63 28 42 54  4 75 66 41 25 76 52 26 21 40 62 53 15 57 30  5 79 39 36 34 72 60 51 17  9  1  0 32 71 13 67 77  3 74 18 69 31 47 16 12  2 59 37 22 48  8 44 56], a_shuffle_aclus: [ 14  33  27  10  15  67  87  90   9  86  36  41  93  50  32  98  68  74  55  60  83  52  19  26  78 108  63  62  85  37  59  73   7 102  88  58  34 105  70  35  28  57  84  71  21  76  45   8 109  56  53  51  97  82  69  24  13   3   2  49  96  17  89 107   5 100  25  92  48  64  23  16   4  81  54  31  66  11  61  75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [76 50 17 19 74 44 45  9  5 66 32 11 53 71  0 68 70 48 78 12 34 42 41 72 64 43  1  4 55 13 30 15 73 58 52 27 61  3  2 49 26  7 56 28 21 47 40 35 24 20 18 63 37 67 62 23 65 77 16 75 39 22 51 25 36  8 10 14  6 57 79 33 29 60 31 59 38 54 69 46], a_shuffle_aclus: [105  68  24  26 100  61  62  13   8  88  49  15  71  96   2  90  93  66 108  16  51  59  58  97  86  60   3   7  74  17  45  21  98  78  70  36  83   5   4  67  35  10  75  37  28  64  57  52  33  27  25  85  54  89  84  32  87 107  23 102  56  31  69  34  53  11  14  19   9  76 109  50  41  82  48  81  55  73  92  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [20 60 62 42 58 49 37 25 30 63 77 34 50 35 23  5 48 26 64 12 53 27  7 68 54 72  2 28 73 24 74 70 14 31 38 16 36 18 51 33 11 78 71 52 39 66 17  8  4 32 13  3 76  1 57 56 46 75 59  6 10 67 79 21 65 45 40 22  0 44 41 69 15  9 61 55 47 19 43 29], a_shuffle_aclus: [ 27  82  84  59  78  67  54  34  45  85 107  51  68  52  32   8  66  35  86  16  71  36  10  90  73  97   4  37  98  33 100  93  19  48  55  23  53  25  69  50  15 108  96  70  56  88  24  11   7  49  17   5 105   3  76  75  63 102  81   9  14  89 109  28  87  62  57  31   2  61  58  92  21  13  83  74  64  26  60  41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7 56 17 15 68 22 58 25 31 41 67 27 73 79 20 38 59 51 69 37  3 30  2 24 43 18  5 62 23 14  9 28  4 32 12 33 29 75 49 48  8 52 16 72 65 78 70 46  1 19 13 47 54 66 64 60 55 26 42 35 21 39 61 36  0 44 11 34 74 77 45 63 53 40 50  6 71 57 10 76], a_shuffle_aclus: [ 10  75  24  21  90  31  78  34  48  58  89  36  98 109  27  55  81  69  92  54   5  45   4  33  60  25   8  84  32  19  13  37   7  49  16  50  41 102  67  66  11  70  23  97  87 108  93  63   3  26  17  64  73  88  86  82  74  35  59  52  28  56  83  53   2  61  15  51 100 107  62  85  71  57  68   9  96  76  14 105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 59 70  8 49 71  1 16  3 15 19 34 78 56 63 67 52 50 68 38 27 75 30 28 48 74 51 79 26  9 60  2 64  0  4 35 14 57 61 55 25 22  6 45 37 43 36 32 13 10 40 29 62 41 58 77 31 18 66 46 12 65 54 44 33 42  7 53 76 73 24 47 23 69 39 20 72 21  5 11], a_shuffle_aclus: [ 24  81  93  11  67  96   3  23   5  21  26  51 108  75  85  89  70  68  90  55  36 102  45  37  66 100  69 109  35  13  82   4  86   2   7  52  19  76  83  74  34  31   9  62  54  60  53  49  17  14  57  41  84  58  78 107  48  25  88  63  16  87  73  61  50  59  10  71 105  98  33  64  32  92  56  27  97  28   8  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [70 22 53 50 30 63  8 26 20 32 13 48 15 75 64 29 79 78 69 43 31  6 66 24 27  0 51 18 76 72 16 12 67 38 25 23  7 49 71 21 10  9 68 62 47 44 57 41  4 59 39 58 56 45  5 11 17  1 52 54 77 33 35 65 46 42 14 55  3 74 34 61  2 37 60 28 73 36 19 40], a_shuffle_aclus: [ 93  31  71  68  45  85  11  35  27  49  17  66  21 102  86  41 109 108  92  60  48   9  88  33  36   2  69  25 105  97  23  16  89  55  34  32  10  67  96  28  14  13  90  84  64  61  76  58   7  81  56  78  75  62   8  15  24   3  70  73 107  50  52  87  63  59  19  74   5 100  51  83   4  54  82  37  98  53  26  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75 50 19 68 42 44 17  8 72 45 56 30  2 57 12 20 21 55 29 67 40  0 63 27 26  6 58 76 31 16 39  7 33  1 77 79 28 70 35 69 23 78 13 18 32 41 51 22 74 25 46 71 11 64 54 10 61 14 43  9 49 34 73 24 65 53  4 60 15 62 36 38 37  5 47 59 66  3 52 48], a_shuffle_aclus: [102  68  26  90  59  61  24  11  97  62  75  45   4  76  16  27  28  74  41  89  57   2  85  36  35   9  78 105  48  23  56  10  50   3 107 109  37  93  52  92  32 108  17  25  49  58  69  31 100  34  63  96  15  86  73  14  83  19  60  13  67  51  98  33  87  71   7  82  21  84  53  55  54   8  64  81  88   5  70  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [37 18 33  2 32 45  1 64 39 50 17 23 57 16 76 68 66 47 62 71 59 58 36  0 48 26 55 60 13 11 19 51  4  3  7 49  6 78 34 24 41 29 42 65 43 67 69 22  9 74 31 40 20 77 27 44 70 10 79 30 38 54 61 72 35 73 52  8 21 63 25 14 12 28 56 53 75 15  5 46], a_shuffle_aclus: [ 54  25  50   4  49  62   3  86  56  68  24  32  76  23 105  90  88  64  84  96  81  78  53   2  66  35  74  82  17  15  26  69   7   5  10  67   9 108  51  33  58  41  59  87  60  89  92  31  13 100  48  57  27 107  36  61  93  14 109  45  55  73  83  97  52  98  70  11  28  85  34  19  16  37  75  71 102  21   8  63]\n",
      "a_shuffle_IDXs: [51 71  8 43 58 57  9  0 70 65 49 36 15  3 54 17  1 52 14 50 74 40  4  2 27 79 56 25 35 76 77 66 46 73 29 42 47 72 55 24 23 26 63 59 11 13 18 12 21 32 22 37 60  5 67 33 75 41 39 31 68 30  7 45 28 20 48 69 38 10 34 62 19 16 44 61  6 53 78 64], a_shuffle_aclus: [ 69  96  11  60  78  76  13   2  93  87  67  53  21   5  73  24   3  70  19  68 100  57   7   4  36 109  75  34  52 105 107  88  63  98  41  59  64  97  74  33  32  35  85  81  15  17  25  16  28  49  31  54  82   8  89  50 102  58  56  48  90  45  10  62  37  27  66  92  55  14  51  84  26  23  61  83   9  71 108  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 78 15 39 74 61 45 34 38 13 27 47 70 22 36 21  8 76 67 69 37  9 20 17 59 19 72 57  0 10 29 53 28 41 40 25 77 66 65 30 56 62 68 42 75  7 12 79 73 26 16 11 33 31 54 44  3 14  2  5 58 55 48 71 49 35  6 32  1 51 52 60 18 64 43 24 46 23 50  4], a_shuffle_aclus: [ 85 108  21  56 100  83  62  51  55  17  36  64  93  31  53  28  11 105  89  92  54  13  27  24  81  26  97  76   2  14  41  71  37  58  57  34 107  88  87  45  75  84  90  59 102  10  16 109  98  35  23  15  50  48  73  61   5  19   4   8  78  74  66  96  67  52   9  49   3  69  70  82  25  86  60  33  63  32  68   7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [66 69 20 39 47  9  0 38 11 51 78 67 44  2 70 74 42 63  7 21 53 56 52 71 68 50 30 57 23  5 48 13 35  8 16 46 15 32  1 43 18 49 75 79 34 17 31 45 19 12 73 24 62 27 36  3 29 10 61 58 59 60 37 14 40 65 26 77  6 33  4 54 41 55 28 72 64 22 25 76], a_shuffle_aclus: [ 88  92  27  56  64  13   2  55  15  69 108  89  61   4  93 100  59  85  10  28  71  75  70  96  90  68  45  76  32   8  66  17  52  11  23  63  21  49   3  60  25  67 102 109  51  24  48  62  26  16  98  33  84  36  53   5  41  14  83  78  81  82  54  19  57  87  35 107   9  50   7  73  58  74  37  97  86  31  34 105]\n",
      "a_shuffle_IDXs: [10 27  8 74  9  0  6 72 76 68 63 32  2 31 71 66 18 75 73 54  5 53 23 11 44 48 58 24 33 37 42 28 20 41 47 43  3 67 29 78 13 65 34 30 17 50 51 52 69 56 22 70 26 40 62 12 36 21 55  4 15 16 79  1 39 59 45 19 77 46 61 64  7 49 14 38 25 60 57 35], a_shuffle_aclus: [ 14  36  11 100  13   2   9  97 105  90  85  49   4  48  96  88  25 102  98  73   8  71  32  15  61  66  78  33  50  54  59  37  27  58  64  60   5  89  41 108  17  87  51  45  24  68  69  70  92  75  31  93  35  57  84  16  53  28  74   7  21  23 109   3  56  81  62  26 107  63  83  86  10  67  19  55  34  82  76  52]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51 18 63 40 57 55  7 72 45 13 47 15 11 75 17 33 73 74 29 48 77  5 70  0 28 32 23 24 78  4 20 50 67 41 62  9  2 30 31 46 68 21 19 27 36 54 71 16 59  6 53 26 65 25 44 49 35 14 37 76 60 56 64 79 61 42  1 43 52 39 22 66 34 58  3 69 38 12  8 10], a_shuffle_aclus: [ 69  25  85  57  76  74  10  97  62  17  64  21  15 102  24  50  98 100  41  66 107   8  93   2  37  49  32  33 108   7  27  68  89  58  84  13   4  45  48  63  90  28  26  36  53  73  96  23  81   9  71  35  87  34  61  67  52  19  54 105  82  75  86 109  83  59   3  60  70  56  31  88  51  78   5  92  55  16  11  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [13 46 68 20 22 40 27 69 32 55 63 12 79 18 66 54 59 23 25 78 62 31 15 43 74 58 41  9 28 35 24 44  4 60 61 51 37 17 67 39 29 57 36 33 42 38  0  1 76  3  5 70  2 50 26 30  7 64 73  6 77 75 45 65 10 48 56 16 19 34 72 49 21 14 52 47 53 71 11  8], a_shuffle_aclus: [ 17  63  90  27  31  57  36  92  49  74  85  16 109  25  88  73  81  32  34 108  84  48  21  60 100  78  58  13  37  52  33  61   7  82  83  69  54  24  89  56  41  76  53  50  59  55   2   3 105   5   8  93   4  68  35  45  10  86  98   9 107 102  62  87  14  66  75  23  26  51  97  67  28  19  70  64  71  96  15  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [14 10 44 66 74 20  4 21 73 24 38 51 50 32 27 39 67 56 28 77 13 25 65 18  5 75 11  2 17 55 52 40 79 68 29 45  3 59  7  9 19 70 53 37 48 58 41 15 71  0 69 47 46 12 36 78 16 34 60 31 63 64 33 61  1 76 30 54 26 35 22 72 23 57 62 42 49  8  6 43], a_shuffle_aclus: [ 19  14  61  88 100  27   7  28  98  33  55  69  68  49  36  56  89  75  37 107  17  34  87  25   8 102  15   4  24  74  70  57 109  90  41  62   5  81  10  13  26  93  71  54  66  78  58  21  96   2  92  64  63  16  53 108  23  51  82  48  85  86  50  83   3 105  45  73  35  52  31  97  32  76  84  59  67  11   9  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [19 53 27 59 28 65 60 74 56 61 35 50 13 72  7  8 38 51  0  5 62 67 26 79 34 75 68 45 77 39 20 15 29 63 47 36 76 21 71  6 12 18 10 78  9 16 52 57 69 11 73 37 55 31  4 43  3 32 30 14 40 64 49 33 42 23 17 66 58 41 44 48 54  2 70 22  1 24 25 46], a_shuffle_aclus: [ 26  71  36  81  37  87  82 100  75  83  52  68  17  97  10  11  55  69   2   8  84  89  35 109  51 102  90  62 107  56  27  21  41  85  64  53 105  28  96   9  16  25  14 108  13  23  70  76  92  15  98  54  74  48   7  60   5  49  45  19  57  86  67  50  59  32  24  88  78  58  61  66  73   4  93  31   3  33  34  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 72  7 37 12 55 40 78 45 73  1 76 18 33 50 29 67 25  4 42 26 74 70 43  6 61 68 15 14 11 20 71 35 77 79 63 75 13  2 30  5 31 34 49 64 21 24 51 48 54 56 10  8 53 52 44  9 62 69 28 41 47 60 36  0 19 58 46 59 27 32 22  3 23 17 39 65 38 16 66], a_shuffle_aclus: [ 76  97  10  54  16  74  57 108  62  98   3 105  25  50  68  41  89  34   7  59  35 100  93  60   9  83  90  21  19  15  27  96  52 107 109  85 102  17   4  45   8  48  51  67  86  28  33  69  66  73  75  14  11  71  70  61  13  84  92  37  58  64  82  53   2  26  78  63  81  36  49  31   5  32  24  56  87  55  23  88]\n",
      "a_shuffle_IDXs: [51 58 57 63 43  8 44 55 49 68 28 31  2 38 18 60 13 56 46 11  9 53 45 75 71 19 47 37 72 39 62 36 27 65 67 23 21  7 59  0 20 29 52 24 74  6 22 69 42 48 34 25 41 16 61 64 73 26 70 40 14 33 50 35  3 17 10 66 54 76 15 12 30  5 77 78 79  1 32  4], a_shuffle_aclus: [ 69  78  76  85  60  11  61  74  67  90  37  48   4  55  25  82  17  75  63  15  13  71  62 102  96  26  64  54  97  56  84  53  36  87  89  32  28  10  81   2  27  41  70  33 100   9  31  92  59  66  51  34  58  23  83  86  98  35  93  57  19  50  68  52   5  24  14  88  73 105  21  16  45   8 107 108 109   3  49   7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [31 25 49 28 20 23 52 43 24 18 75 63 32 57 51 48 47  5  2 53 72  7 74 39 15 26 56 66 37  0 59 42 35 65 21 69 58 29 10  6 19 46 60 77 78 71 41 12 17  4 50  3 73 68  9 40 64 13  8 30 27 14 22 61 36 55 44 11 45 38 70 76 67 34 54 79 16  1 62 33], a_shuffle_aclus: [ 48  34  67  37  27  32  70  60  33  25 102  85  49  76  69  66  64   8   4  71  97  10 100  56  21  35  75  88  54   2  81  59  52  87  28  92  78  41  14   9  26  63  82 107 108  96  58  16  24   7  68   5  98  90  13  57  86  17  11  45  36  19  31  83  53  74  61  15  62  55  93 105  89  51  73 109  23   3  84  50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [62 17 42 63 57 67 53 66 28 71 25  3 21 56 55 39 36 65 74 10 18 14  7  5 49 43 48 20 23 15 40 68 32 72 50  0 79 75 11  2 24 64 45 60 59 34 26 13 69 70 19  6  9 77 51 30 37 61  1 35  4 58 76 31 73 29 12 46 33 41 44 16 78 22 52 54 38 27 47  8], a_shuffle_aclus: [ 84  24  59  85  76  89  71  88  37  96  34   5  28  75  74  56  53  87 100  14  25  19  10   8  67  60  66  27  32  21  57  90  49  97  68   2 109 102  15   4  33  86  62  82  81  51  35  17  92  93  26   9  13 107  69  45  54  83   3  52   7  78 105  48  98  41  16  63  50  58  61  23 108  31  70  73  55  36  64  11]\n",
      "a_shuffle_IDXs: [60 46 26 53 32 62 44 49 51 78 10 57 55  5 27 50 17 16 11 14 37 61 19  2 30 68 13 56 41 42 67  8 65 34 43 75  9 63 64  7 40 45 74  6  4 79 22 35 66 54 70 31 25 24  3 33 77 47 29 48 18 15 23 38 73 20 21 69  0 58 39 71 59 52 72 12 76 36 28  1], a_shuffle_aclus: [ 82  63  35  71  49  84  61  67  69 108  14  76  74   8  36  68  24  23  15  19  54  83  26   4  45  90  17  75  58  59  89  11  87  51  60 102  13  85  86  10  57  62 100   9   7 109  31  52  88  73  93  48  34  33   5  50 107  64  41  66  25  21  32  55  98  27  28  92   2  78  56  96  81  70  97  16 105  53  37   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [72  5 55 29 56 14 15 12 31 26 41 73 49 20 35 79 37  0 48 60 27 32 66 22 71  9 21 77 30 50 40  3 44 42 39 61 54 57 68 76 45 70 78 65 43 38 59 75 28 69 46 34 24 67 62 64 53 74  8 18 33  1  7 52 23 25 13  4 63  2 47 17 36 58 19 11 16 51  6 10], a_shuffle_aclus: [ 97   8  74  41  75  19  21  16  48  35  58  98  67  27  52 109  54   2  66  82  36  49  88  31  96  13  28 107  45  68  57   5  61  59  56  83  73  76  90 105  62  93 108  87  60  55  81 102  37  92  63  51  33  89  84  86  71 100  11  25  50   3  10  70  32  34  17   7  85   4  64  24  53  78  26  15  23  69   9  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [40 61 11 66 50 16 33 35 57 53 78 60  7 28 49 26 64 46 23  1 72 19 42  0 69 77 54 17 56  9 13  2  8 31 39 20 65 15 51 76 47 34 79 30 24 41 27 25 52 43 45 67 71  6  4 38 22 62 75 59 68 29 48 32  5 74  3 36 58 18 73 21 44 12 55 14 37 63 10 70], a_shuffle_aclus: [ 57  83  15  88  68  23  50  52  76  71 108  82  10  37  67  35  86  63  32   3  97  26  59   2  92 107  73  24  75  13  17   4  11  48  56  27  87  21  69 105  64  51 109  45  33  58  36  34  70  60  62  89  96   9   7  55  31  84 102  81  90  41  66  49   8 100   5  53  78  25  98  28  61  16  74  19  54  85  14  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0 41  2 69 71 45 57  1 27 68  8 29 61 42 37 47 48 25 31 15 35 19 64 14 76 33 54 30 49 11 55  9 20 43  6 16  7 39 59 50 51 70 13 21 66 74 78 32  5 46 63 28 62 65 60 17 40 53 79 75 10 38 12 77 18 34  3 67 23 56 52  4 73 36 58 22 26 72 44 24], a_shuffle_aclus: [  2  58   4  92  96  62  76   3  36  90  11  41  83  59  54  64  66  34  48  21  52  26  86  19 105  50  73  45  67  15  74  13  27  60   9  23  10  56  81  68  69  93  17  28  88 100 108  49   8  63  85  37  84  87  82  24  57  71 109 102  14  55  16 107  25  51   5  89  32  75  70   7  98  53  78  31  35  97  61  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 29 72  5 25 23 51 28 35 37 16 22 17 76 30 36 40 71 43 49 64 77 52 69 60  7 38 53 66 26 63 59 31  2 57 55 14 79 13  0 75 61 67 47 56 21 58 39 70 32  9 20 54 74 34 46 27 33  4 48 12  6 41 73  8 78 45  1 19 50 18 44 10  3 62 68 24 11 42 15], a_shuffle_aclus: [ 87  41  97   8  34  32  69  37  52  54  23  31  24 105  45  53  57  96  60  67  86 107  70  92  82  10  55  71  88  35  85  81  48   4  76  74  19 109  17   2 102  83  89  64  75  28  78  56  93  49  13  27  73 100  51  63  36  50   7  66  16   9  58  98  11 108  62   3  26  68  25  61  14   5  84  90  33  15  59  21]\n",
      "a_shuffle_IDXs: [45  1 37 64 25 65 20 63 28  3 52 70 58 27 51 50 21 43 17  5 22 15 40 26  6 12 69 35 78 57 32 14 33 42  0 61 71 56 34 36 60  2 29  4 49 66 76 67  8 47 79 75 23 55 77 74 73 18 54 59 46 44 53  7 48 10 68  9 16 39 11 31 41 19 13 62 30 38 72 24], a_shuffle_aclus: [ 62   3  54  86  34  87  27  85  37   5  70  93  78  36  69  68  28  60  24   8  31  21  57  35   9  16  92  52 108  76  49  19  50  59   2  83  96  75  51  53  82   4  41   7  67  88 105  89  11  64 109 102  32  74 107 100  98  25  73  81  63  61  71  10  66  14  90  13  23  56  15  48  58  26  17  84  45  55  97  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 43 67 53 60 68 39 25 33 41 14 21  8  0 58 78  3 17 35 44 75  6 47 16 50 30 45 48 61 28 49 11 29 15 42 59 20 65 32 38  7 36 71  2 72 63  4 54  5 26 10 12 24 13 69 18 77 51 55 79  1 27 66 37 62 46 34 22 52 73 76 56  9 19 23 74 64 31 40 70], a_shuffle_aclus: [ 76  60  89  71  82  90  56  34  50  58  19  28  11   2  78 108   5  24  52  61 102   9  64  23  68  45  62  66  83  37  67  15  41  21  59  81  27  87  49  55  10  53  96   4  97  85   7  73   8  35  14  16  33  17  92  25 107  69  74 109   3  36  88  54  84  63  51  31  70  98 105  75  13  26  32 100  86  48  57  93]\n",
      "a_shuffle_IDXs: [46 68 50 40 41 45 17  2 20 10 78 74 64 61  6 25 71 62 43 75  0 47 53 59 54 57 34 11 66 37 29 33 30 36 32 21 72  1 48 28 52  3  7  4 26 18 77 56 24  8 35  9 42 38 13 31 76 79 70 69  5 14 12 23 55 58 73 39 19 49 27 63 65 16 51 60 15 44 22 67], a_shuffle_aclus: [ 63  90  68  57  58  62  24   4  27  14 108 100  86  83   9  34  96  84  60 102   2  64  71  81  73  76  51  15  88  54  41  50  45  53  49  28  97   3  66  37  70   5  10   7  35  25 107  75  33  11  52  13  59  55  17  48 105 109  93  92   8  19  16  32  74  78  98  56  26  67  36  85  87  23  69  82  21  61  31  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [53 13 20  1 74 25 14 34 45 58 76 31 57 66 65 44 21 15  5 79 47  4 40 61  0 37 33 23 27 63  3 64 17 30 10 18 68 16 67 24 32 28 43 55 70 56  7 49 50 11 69 12 46  9 26  6 54 29 36 59  2 51 78 77 52 62 41 73 35 48 39 42 72  8 71 75 60 19 22 38], a_shuffle_aclus: [ 71  17  27   3 100  34  19  51  62  78 105  48  76  88  87  61  28  21   8 109  64   7  57  83   2  54  50  32  36  85   5  86  24  45  14  25  90  23  89  33  49  37  60  74  93  75  10  67  68  15  92  16  63  13  35   9  73  41  53  81   4  69 108 107  70  84  58  98  52  66  56  59  97  11  96 102  82  26  31  55]\n",
      "a_shuffle_IDXs: [76  2 25 71 43 18 69  8 31 75 33 47 30 49 66 17 32 10 35 26  9 52 42 20  1 77 64 79  5 65 63 72 68 29 14 53 39 56 40 11 27 16 37 21  4 60  0 78 15 58 67 54 38  3 22 51 70 48 34 62  7 23 73 28 45 24 46 41 57 74 55 61 50 59 36 12 13 44 19  6], a_shuffle_aclus: [105   4  34  96  60  25  92  11  48 102  50  64  45  67  88  24  49  14  52  35  13  70  59  27   3 107  86 109   8  87  85  97  90  41  19  71  56  75  57  15  36  23  54  28   7  82   2 108  21  78  89  73  55   5  31  69  93  66  51  84  10  32  98  37  62  33  63  58  76 100  74  83  68  81  53  16  17  61  26   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [71 38  6 23 44 10 13 48 49 20 11 18 29 50 45  8  2 76 33  0 68 54 59  4 27 19 40 66 75  5 79 35 57 47 51 16 52  1 74 37 31 14 77 34 26 78 36 58 62  3 60 69 22 24 70  9 15 25 73 67 53 41 65 63 21 12 56 72 17 39 46 42 64 61 43 32 30  7 55 28], a_shuffle_aclus: [ 96  55   9  32  61  14  17  66  67  27  15  25  41  68  62  11   4 105  50   2  90  73  81   7  36  26  57  88 102   8 109  52  76  64  69  23  70   3 100  54  48  19 107  51  35 108  53  78  84   5  82  92  31  33  93  13  21  34  98  89  71  58  87  85  28  16  75  97  24  56  63  59  86  83  60  49  45  10  74  37]\n",
      "a_shuffle_IDXs: [71 46 11  5 50 44 39 51 29 32  8 40 47 37  6 66 64  1 28 59 38 36 53 21 52 20 25  0 31 45 58 56 17 61 30 18  2 23 55 24 72 15 41  9 13 57 35 19 48 70 14 77 79 76 16  7 27 26 67 65 68 69 63 43 34  4 78 42 75 12 74 22 10 73 49 62 54 33  3 60], a_shuffle_aclus: [ 96  63  15   8  68  61  56  69  41  49  11  57  64  54   9  88  86   3  37  81  55  53  71  28  70  27  34   2  48  62  78  75  24  83  45  25   4  32  74  33  97  21  58  13  17  76  52  26  66  93  19 107 109 105  23  10  36  35  89  87  90  92  85  60  51   7 108  59 102  16 100  31  14  98  67  84  73  50   5  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 74 51 25 62 70 50  1 40 22 38 56 58  7 19 42 33 72 57 39 15 59 23 47 29 28 53 71 11 17 68 61 52 34 36  3 76 37 79 63 77 20  0 64 43 45 12 31 49 14 10  5 26  8 24 32 78 75 60 65 30 54  6 44 69 16  2 46 13 41 21  9 35 73 48 18 27 55 66  4], a_shuffle_aclus: [ 89 100  69  34  84  93  68   3  57  31  55  75  78  10  26  59  50  97  76  56  21  81  32  64  41  37  71  96  15  24  90  83  70  51  53   5 105  54 109  85 107  27   2  86  60  62  16  48  67  19  14   8  35  11  33  49 108 102  82  87  45  73   9  61  92  23   4  63  17  58  28  13  52  98  66  25  36  74  88   7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 14 76 21 52 18 51 59 63 12 56 23  6 28 47  9 69 58  7 57 15 43  2 66 11 65 74 49 53 46 24 68 39  4 79 72 22 67 70 16  3 62 44  8 32 61 26 33 13 19 73 29 20 77  1 35 71 54  0 10 75 25 60 50  5 30 40 17 34 48 37 38 45 78 42 64 41 55 36 31], a_shuffle_aclus: [ 36  19 105  28  70  25  69  81  85  16  75  32   9  37  64  13  92  78  10  76  21  60   4  88  15  87 100  67  71  63  33  90  56   7 109  97  31  89  93  23   5  84  61  11  49  83  35  50  17  26  98  41  27 107   3  52  96  73   2  14 102  34  82  68   8  45  57  24  51  66  54  55  62 108  59  86  58  74  53  48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 62 57 58  3 52 19 74  9  0 79 33 51 46 20 23 32 30 10 22 31 65  8 18 38 68 47 50 61 76 12 16 35  1 63 48  7 13 49 25 34 55 21 14 54 24 27 75 70 43 17 78 56 66 26 71 67 29 39 73 11 44  4 37 41 36 64 77  5  6 40 45 15 69 59 60 28 53 72 42], a_shuffle_aclus: [  4  84  76  78   5  70  26 100  13   2 109  50  69  63  27  32  49  45  14  31  48  87  11  25  55  90  64  68  83 105  16  23  52   3  85  66  10  17  67  34  51  74  28  19  73  33  36 102  93  60  24 108  75  88  35  96  89  41  56  98  15  61   7  54  58  53  86 107   8   9  57  62  21  92  81  82  37  71  97  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51 79 23 47 48 17 64  5 45  1 66 59 31 28 70 25  4 77  9  3 13 43 38 78 37 49 42 56 27  0 73 75 74 36 53 11 50 68 24 10 62  6 15 72 16 65 26 46 29 67  2 55 61  8 22 32 44 18 12 63 71  7 21 54 30 35 57 76 40 20 58 34 39 41 19 14 33 69 60 52], a_shuffle_aclus: [ 69 109  32  64  66  24  86   8  62   3  88  81  48  37  93  34   7 107  13   5  17  60  55 108  54  67  59  75  36   2  98 102 100  53  71  15  68  90  33  14  84   9  21  97  23  87  35  63  41  89   4  74  83  11  31  49  61  25  16  85  96  10  28  73  45  52  76 105  57  27  78  51  56  58  26  19  50  92  82  70]\n",
      "a_shuffle_IDXs: [20 74 75 46 32 22 68 44 47 41 17 25  8 69 72 27 19 78  4 54 71 77 66 52 33 28 11 36 73 30 67 56 60 23 35 65 48 42 26 18  9 13 39  7 10 51 37 63 34 49 31 79 15 70  1 53 43 24 50  6 59 45  2 55 58 38  5 64 61 21  0 62 12 76 29 40 14  3 16 57], a_shuffle_aclus: [ 27 100 102  63  49  31  90  61  64  58  24  34  11  92  97  36  26 108   7  73  96 107  88  70  50  37  15  53  98  45  89  75  82  32  52  87  66  59  35  25  13  17  56  10  14  69  54  85  51  67  48 109  21  93   3  71  60  33  68   9  81  62   4  74  78  55   8  86  83  28   2  84  16 105  41  57  19   5  23  76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [52 15 27 79 12 21  1 45  3 16  4 29 55 31 60 66 59 37 10 64 73 35 72 70 14 43 76  6 40 57 39 38 69  9 11 30 54 61 33 65 48 49 42 26 46 20 44 36 24  2 71 28 68 41 23 58  8 13 18 77 62 74 63  5 50 47 78 25 56 53 22  7 32 75 34 19 67 17 51  0], a_shuffle_aclus: [ 70  21  36 109  16  28   3  62   5  23   7  41  74  48  82  88  81  54  14  86  98  52  97  93  19  60 105   9  57  76  56  55  92  13  15  45  73  83  50  87  66  67  59  35  63  27  61  53  33   4  96  37  90  58  32  78  11  17  25 107  84 100  85   8  68  64 108  34  75  71  31  10  49 102  51  26  89  24  69   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [44  2 40 36 59 51 65 67 13 19 42 75 39 25  1 64 18 21  3 54 37 45 26 22 62  5  6 68 52 73  7 14  4 60 55 28 17 49 38 78 34  9 70 56 20 61 24 50 69 63 74 46 48 32 12 72 71 10 16 23 33  8 41 35 77 27 66 11 43 30 31 47 15  0 29 53 57 58 76 79], a_shuffle_aclus: [ 61   4  57  53  81  69  87  89  17  26  59 102  56  34   3  86  25  28   5  73  54  62  35  31  84   8   9  90  70  98  10  19   7  82  74  37  24  67  55 108  51  13  93  75  27  83  33  68  92  85 100  63  66  49  16  97  96  14  23  32  50  11  58  52 107  36  88  15  60  45  48  64  21   2  41  71  76  78 105 109]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [30 22  2 14 19 12 41 17 46  9 24 42  3 56  1 57 49 31 66 65 11  8 52 73 60 38 16 76 55 13  6 26 36 77 61 44  5 18 10 79 40 68 15 50 21  7 67 34 23 32 54 28 35 64 75 51 27 72 78  0 33 47 39 63 37  4 71 69 53 20 25 58 29 74 62 48 45 70 59 43], a_shuffle_aclus: [ 45  31   4  19  26  16  58  24  63  13  33  59   5  75   3  76  67  48  88  87  15  11  70  98  82  55  23 105  74  17   9  35  53 107  83  61   8  25  14 109  57  90  21  68  28  10  89  51  32  49  73  37  52  86 102  69  36  97 108   2  50  64  56  85  54   7  96  92  71  27  34  78  41 100  84  66  62  93  81  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [49 37 54 72 14 67 33 45 27 58 17 24 59 60 65 10 18 51 39  6 63 21  0 66 79 42 34  7  9 13 64 31 61 53 15 52 38 70 74 30 19 44 50 46 16  4 23  8 73 20  5 57 47 43 68 62 35  1 77 28 26 75 12 41 76 22 55  3 48 56 40 36 25 11 32  2 69 29 71 78], a_shuffle_aclus: [ 67  54  73  97  19  89  50  62  36  78  24  33  81  82  87  14  25  69  56   9  85  28   2  88 109  59  51  10  13  17  86  48  83  71  21  70  55  93 100  45  26  61  68  63  23   7  32  11  98  27   8  76  64  60  90  84  52   3 107  37  35 102  16  58 105  31  74   5  66  75  57  53  34  15  49   4  92  41  96 108]\n",
      "a_shuffle_IDXs: [24 26 43 46 72 62 14 41 38 13 30 68 23 70  6  0  8 49  9 16 75 36 27 28 40 51 53 19 25 18 55 22 47 63 35 58 32 42 48 45 34 67 37 61 39 66 78  3 60 79 74  7  4 57 71 73 76 59 65 33 54  2 64 12  1  5 31 17 69 29 15 56 77 21 44 10 11 20 50 52], a_shuffle_aclus: [ 33  35  60  63  97  84  19  58  55  17  45  90  32  93   9   2  11  67  13  23 102  53  36  37  57  69  71  26  34  25  74  31  64  85  52  78  49  59  66  62  51  89  54  83  56  88 108   5  82 109 100  10   7  76  96  98 105  81  87  50  73   4  86  16   3   8  48  24  92  41  21  75 107  28  61  14  15  27  68  70]\n",
      "a_shuffle_IDXs: [61  4 52  6 23 25 16 64 27 42 45 14 28 62 74 78 37  7 60 65 15 70 38 22 47 21  8  9 71 76 67 33 53  5 17  0 11 24 55 59 58 44 29 35 18 20 49 41 26 56 10 12 39 63 66 50 48 36 79  3 54 77 69 13 73 68  2  1 46 34 19 40 72 32 31 30 43 51 75 57], a_shuffle_aclus: [ 83   7  70   9  32  34  23  86  36  59  62  19  37  84 100 108  54  10  82  87  21  93  55  31  64  28  11  13  96 105  89  50  71   8  24   2  15  33  74  81  78  61  41  52  25  27  67  58  35  75  14  16  56  85  88  68  66  53 109   5  73 107  92  17  98  90   4   3  63  51  26  57  97  49  48  45  60  69 102  76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4 58 74 38 39 26 68 63 12 77 27 52 73  5 41  0 29  9 50 66 17 43 75 25 10 18 20 67 54 32 53 79 72 36 65 70 40 76 28  3 60 57 19 78  2 46 47 31 11 13 59 49 14 44 71 30 34 24  6 55  8 37 64  7 21 42  1 56 15 23 45 16 35 62 22 33 61 51 48 69], a_shuffle_aclus: [  7  78 100  55  56  35  90  85  16 107  36  70  98   8  58   2  41  13  68  88  24  60 102  34  14  25  27  89  73  49  71 109  97  53  87  93  57 105  37   5  82  76  26 108   4  63  64  48  15  17  81  67  19  61  96  45  51  33   9  74  11  54  86  10  28  59   3  75  21  32  62  23  52  84  31  50  83  69  66  92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [16 32 54 38 26 18  5 40 45 41 25 23 36 35 46 14 19 48  3 47 30 68 49 34 63 11  2  1 61 78 37 50 13 52 60 10 57 55 70 65 15 43 73 72 56 67 27 44 17 69  9 76 20 39 29  6  8 77 59 33 31 74  7  0 51 71 21 64 79 24 22 62 28 42 12 75 53  4 66 58], a_shuffle_aclus: [ 23  49  73  55  35  25   8  57  62  58  34  32  53  52  63  19  26  66   5  64  45  90  67  51  85  15   4   3  83 108  54  68  17  70  82  14  76  74  93  87  21  60  98  97  75  89  36  61  24  92  13 105  27  56  41   9  11 107  81  50  48 100  10   2  69  96  28  86 109  33  31  84  37  59  16 102  71   7  88  78]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [72 51 46 34 57 67 73 43 42  0 64 37 50 52 19 77 16 12 32 22 79 61 75  3  6 74  7 31 49  1 65 23 20 53 25 41  8 45 35 11 30  9 39 54 13 17 26 38 29 62 36 71 66 48 33  4 60 44 24 78 14 63 10  5 69 68 59 40 58 47 55 15 56 70 18 28 27 76  2 21], a_shuffle_aclus: [ 97  69  63  51  76  89  98  60  59   2  86  54  68  70  26 107  23  16  49  31 109  83 102   5   9 100  10  48  67   3  87  32  27  71  34  58  11  62  52  15  45  13  56  73  17  24  35  55  41  84  53  96  88  66  50   7  82  61  33 108  19  85  14   8  92  90  81  57  78  64  74  21  75  93  25  37  36 105   4  28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 45  6 18 47  1  2  9 65 33 37 71 11 54 22 19 77 24 79 10 28 72 26 58 57 17 32 16  4 55 34 67 78 44 76 25 70 35 62  3  7 52 56 63 29 38 60 42 51  0  8 64 13 36 21 53 31 74 61 15 20 75 23 12 39 40 49 48 69 30 59 27 46 43  5 66 14 68 50 73], a_shuffle_aclus: [ 58  62   9  25  64   3   4  13  87  50  54  96  15  73  31  26 107  33 109  14  37  97  35  78  76  24  49  23   7  74  51  89 108  61 105  34  93  52  84   5  10  70  75  85  41  55  82  59  69   2  11  86  17  53  28  71  48 100  83  21  27 102  32  16  56  57  67  66  92  45  81  36  63  60   8  88  19  90  68  98]\n",
      "a_shuffle_IDXs: [46 58  8 18 76 62 21 10 73 64 19 44 33 35 75 39  7  1 20 26 17 51 37 56 36 23 57 69 14 52 71 70 79 72 13  3 53 63 74 59 38 60 29 11 25 47 45 15 31 32  5 22  4 12 42 49 50 61 34 28 40 67  2 24 77 30  0 54 16 78 68 48 27 65  6 43 41 66 55  9], a_shuffle_aclus: [ 63  78  11  25 105  84  28  14  98  86  26  61  50  52 102  56  10   3  27  35  24  69  54  75  53  32  76  92  19  70  96  93 109  97  17   5  71  85 100  81  55  82  41  15  34  64  62  21  48  49   8  31   7  16  59  67  68  83  51  37  57  89   4  33 107  45   2  73  23 108  90  66  36  87   9  60  58  88  74  13]\n",
      "a_shuffle_IDXs: [17 62 67 28 54 43 18 11 32 30 69 53  2 22 13 42  4 10  7 31 36 68  5 71 37 23 26 50 51 63 47 73 79 29 48 57  8 41 15 46  0 27 38 66  9 44  1 56 14 64  3 49 60 33 70 39 75 55 78 61 72 45 24 77 34 74 12 20 21 58 40 52 65 25 76  6 19 59 16 35], a_shuffle_aclus: [ 24  84  89  37  73  60  25  15  49  45  92  71   4  31  17  59   7  14  10  48  53  90   8  96  54  32  35  68  69  85  64  98 109  41  66  76  11  58  21  63   2  36  55  88  13  61   3  75  19  86   5  67  82  50  93  56 102  74 108  83  97  62  33 107  51 100  16  27  28  78  57  70  87  34 105   9  26  81  23  52]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [74 51 50 12 28 54 24 42 44  1 10  3 19 68 11 78 25 58 71 39 47 62  0 14 45 13 57 53 32 70 75 15  8 40  5 46 37 56 38 18 22 79 26 65 41  9 72 55 69 43 73  4 35  6 36 49 33 64 29  7 23 34 16 59 77 21 27 76 30 60 20 63 66 67 31 52 17  2 61 48], a_shuffle_aclus: [100  69  68  16  37  73  33  59  61   3  14   5  26  90  15 108  34  78  96  56  64  84   2  19  62  17  76  71  49  93 102  21  11  57   8  63  54  75  55  25  31 109  35  87  58  13  97  74  92  60  98   7  52   9  53  67  50  86  41  10  32  51  23  81 107  28  36 105  45  82  27  85  88  89  48  70  24   4  83  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4  1 49 11 33  6 79  2 68 26 70 10  9 44 72 61 65 41 51 52  3 74 57 29 25 69 24 32 14 23 54 67  7 15 37 12 17 28  8 63 40 53 38  5 45 42 22 78 62 71 47 31 64 16 59 34 75 13 19 73 27 39 77  0 20 55 60 76 18 66 36 46 43 58 56 48 21 30 35 50], a_shuffle_aclus: [  7   3  67  15  50   9 109   4  90  35  93  14  13  61  97  83  87  58  69  70   5 100  76  41  34  92  33  49  19  32  73  89  10  21  54  16  24  37  11  85  57  71  55   8  62  59  31 108  84  96  64  48  86  23  81  51 102  17  26  98  36  56 107   2  27  74  82 105  25  88  53  63  60  78  75  66  28  45  52  68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55 24 16 52 25 14 26 64 76 41  9 27 13  2 35 11 73 45 34 32 63 33 58 47  8 71 44  7 50 54 42 18 74 20 49 36 46 79 68 19 66 75 40 78 28 48 30 65 17 22 37 67 29 23 51 31 39 53 72 38 62 10  6 70  5  3 69  4 61 60 56 57 12 21 43  0 59  1 77 15], a_shuffle_aclus: [ 74  33  23  70  34  19  35  86 105  58  13  36  17   4  52  15  98  62  51  49  85  50  78  64  11  96  61  10  68  73  59  25 100  27  67  53  63 109  90  26  88 102  57 108  37  66  45  87  24  31  54  89  41  32  69  48  56  71  97  55  84  14   9  93   8   5  92   7  83  82  75  76  16  28  60   2  81   3 107  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0  9 24 16 58 18 32  3 59 42 60 61 35 66 68 76 37 11 15 44 39 36 29 69 28 71 48 70 53 34 73 33 31  4  5 38 21 56 62 12 57 10 20 26 78  2 25  1 41 22 55 19 74 43 47 72  6 79 63 50 14 51  8 46 23 40 45 49 27  7 30 65 67 17 75 64 13 52 54 77], a_shuffle_aclus: [  2  13  33  23  78  25  49   5  81  59  82  83  52  88  90 105  54  15  21  61  56  53  41  92  37  96  66  93  71  51  98  50  48   7   8  55  28  75  84  16  76  14  27  35 108   4  34   3  58  31  74  26 100  60  64  97   9 109  85  68  19  69  11  63  32  57  62  67  36  10  45  87  89  24 102  86  17  70  73 107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55 43 52  2  8 57 74  5 69 53 48 20 12 38 54 67 28 33 27 10 65 24 42 37 25 46 40  9 68  3 75 77 62 64 13 51 58 34  0 31 56 22 44 21 23 59 71 16 11 45 15 78 61 60 14 29 76 49 41 79 39 66 26 30 73 17  6  7 19 32 70 63 72 18 50 47  4 35 36  1], a_shuffle_aclus: [ 74  60  70   4  11  76 100   8  92  71  66  27  16  55  73  89  37  50  36  14  87  33  59  54  34  63  57  13  90   5 102 107  84  86  17  69  78  51   2  48  75  31  61  28  32  81  96  23  15  62  21 108  83  82  19  41 105  67  58 109  56  88  35  45  98  24   9  10  26  49  93  85  97  25  68  64   7  52  53   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [20 62 36 67 11 21 58 42 49 30 27 71 52  0 75 22 70 24 69 61 16 35 65 17 63 51  9 32 72 64 31 41 57 59 53 40  2  3 77 33 76 19 48 60 37 47  7 78 18 15  6 44 68 74 50 79 45  5 12 29 28 55 13 25 54 14 10 46  4 73 23  8 26 39 38 34 56 66  1 43], a_shuffle_aclus: [ 27  84  53  89  15  28  78  59  67  45  36  96  70   2 102  31  93  33  92  83  23  52  87  24  85  69  13  49  97  86  48  58  76  81  71  57   4   5 107  50 105  26  66  82  54  64  10 108  25  21   9  61  90 100  68 109  62   8  16  41  37  74  17  34  73  19  14  63   7  98  32  11  35  56  55  51  75  88   3  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [52 34 11 56  8 46 50 33 51 40  1  3 63 54 57 21 68 14 45 44 32  0 31 12 22 53  7 58 59 47 79 28 15 69 39 75 20 61 41 17 48 64 72 24 65  5 60 26 23 66 16 67 38 27 30 49 10 78 29 62 18  9 74 76 43 37 73 77 36 25  4 19 70 55 42 35 71 13  2  6], a_shuffle_aclus: [ 70  51  15  75  11  63  68  50  69  57   3   5  85  73  76  28  90  19  62  61  49   2  48  16  31  71  10  78  81  64 109  37  21  92  56 102  27  83  58  24  66  86  97  33  87   8  82  35  32  88  23  89  55  36  45  67  14 108  41  84  25  13 100 105  60  54  98 107  53  34   7  26  93  74  59  52  96  17   4   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [77 47 21 79 14  0 63 33 30 19 60 32  4 56 73 53 45 10 40 37 17 65 38 42 64  2 72 46  1 26 24 52 50  9 61 59  7 48  3 31 62 28 35 66 20 51 12 67 58 44 70 69 34 15 71 55  6 23 76 54 27 13 29 25 68 43 78 11 39 74 18 22 49 75 57 41  5 36  8 16], a_shuffle_aclus: [107  64  28 109  19   2  85  50  45  26  82  49   7  75  98  71  62  14  57  54  24  87  55  59  86   4  97  63   3  35  33  70  68  13  83  81  10  66   5  48  84  37  52  88  27  69  16  89  78  61  93  92  51  21  96  74   9  32 105  73  36  17  41  34  90  60 108  15  56 100  25  31  67 102  76  58   8  53  11  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 70 78 39 16 17 20 72 30  6 75 73 40 19 58 47 37  9 44 41 45 65 34 48 24 74  1 25 62 52 50 43  8 23  4 61 32  3 33  5 69 22 28 13 59 64 71 68 27 11 31 57 46 67 56 12 29 26 49 76 14 51 15 53  0  7 35 18 21 79 54 66 42 60 36 63 10 77 55 38], a_shuffle_aclus: [  4  93 108  56  23  24  27  97  45   9 102  98  57  26  78  64  54  13  61  58  62  87  51  66  33 100   3  34  84  70  68  60  11  32   7  83  49   5  50   8  92  31  37  17  81  86  96  90  36  15  48  76  63  89  75  16  41  35  67 105  19  69  21  71   2  10  52  25  28 109  73  88  59  82  53  85  14 107  74  55]\n",
      "a_shuffle_IDXs: [ 6 55 61 53 56 64 45 20 52 21 68 70 19 47 71 51 44 67 42 37 38 17 12 60 13  9 15 25  3 49 31 23 36  8  4 11 40 32 26 14 72 59 10  0 50 27 62 18 58 76 28 35 34 57 77 30 16 78 29 39 63  7 22 24 41 65  1  2 66 54 33 73 69 43 79 46 74 48  5 75], a_shuffle_aclus: [  9  74  83  71  75  86  62  27  70  28  90  93  26  64  96  69  61  89  59  54  55  24  16  82  17  13  21  34   5  67  48  32  53  11   7  15  57  49  35  19  97  81  14   2  68  36  84  25  78 105  37  52  51  76 107  45  23 108  41  56  85  10  31  33  58  87   3   4  88  73  50  98  92  60 109  63 100  66   8 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [12 78 58 74 51 47  9 56 20 30  6 10 60 19 24 52 33 71 62 75 18 61 22 16 54 27 57  7 25 32 48 72 35  4  1 37 73 43 14 23 17 13 26 31 42 46 50  8 76 59 79 28 41 45 53  2  3 39  5 64 77 69 67 38 65 44 40 63 49 15 29 36 68 21 11 66  0 34 70 55], a_shuffle_aclus: [ 16 108  78 100  69  64  13  75  27  45   9  14  82  26  33  70  50  96  84 102  25  83  31  23  73  36  76  10  34  49  66  97  52   7   3  54  98  60  19  32  24  17  35  48  59  63  68  11 105  81 109  37  58  62  71   4   5  56   8  86 107  92  89  55  87  61  57  85  67  21  41  53  90  28  15  88   2  51  93  74]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73 29 12 46 45 76 68 79 23 75 51 59 35 40 70 50 56  6 19 17  2 34 55 57 61 28 31 78 60 54 67  1 18 66 62 63 25  7 33 10  4 30 48 44 49 77 20 74 32 42 38 22 72 39  3  8 11  5 36 16 47 71 26 52 69 43 37 13  0  9 41 65 15 64 21 14 24 58 27 53], a_shuffle_aclus: [ 98  41  16  63  62 105  90 109  32 102  69  81  52  57  93  68  75   9  26  24   4  51  74  76  83  37  48 108  82  73  89   3  25  88  84  85  34  10  50  14   7  45  66  61  67 107  27 100  49  59  55  31  97  56   5  11  15   8  53  23  64  96  35  70  92  60  54  17   2  13  58  87  21  86  28  19  33  78  36  71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75 53 27 62 77 14 55 38 37 50 25 47  8 60 17 73 23 13 68 21 78 52 76 22 44 66 35 61 39  0 34 19 16 29 26 48 20  3 56 71 36  1 59  4 79 54 30  7 67 70 28 33 74 40 51 41  5  2 69  6 64 12 18 57 43 45 65 72 46 15 63 11 49 32 58 31  9 10 24 42], a_shuffle_aclus: [102  71  36  84 107  19  74  55  54  68  34  64  11  82  24  98  32  17  90  28 108  70 105  31  61  88  52  83  56   2  51  26  23  41  35  66  27   5  75  96  53   3  81   7 109  73  45  10  89  93  37  50 100  57  69  58   8   4  92   9  86  16  25  76  60  62  87  97  63  21  85  15  67  49  78  48  13  14  33  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75  4 40 51  2 56 66 54 73 16 13 67  0 63 57 45 31 47 69 21  6 42 50 33 46 52 28 71 43 77 11 74 36  8 65 12 29 41 61  5 38 55 10 23 22 58 30 18 68  9 59 70 20 14 35 48 19 27 37 17 49 25 62 39 64 60 24 53 76 79  7 26  3 78  1 72 34 15 32 44], a_shuffle_aclus: [102   7  57  69   4  75  88  73  98  23  17  89   2  85  76  62  48  64  92  28   9  59  68  50  63  70  37  96  60 107  15 100  53  11  87  16  41  58  83   8  55  74  14  32  31  78  45  25  90  13  81  93  27  19  52  66  26  36  54  24  67  34  84  56  86  82  33  71 105 109  10  35   5 108   3  97  51  21  49  61]\n",
      "a_shuffle_IDXs: [56 30 29  5 22 31 74 41 43 64 62 55 14 54 42 53 40 72 26 19 57 15 61 13  7 18 37 28 58 23 24 48 67  4 60 71 63 17 50 33  1 32  6  8 70 27 21 51 34 69  3  2 20 68 44 36  0 77 39 45  9 10 47 52 49 78 11 46 59 12 76 75 35 25 65 16 66 79 73 38], a_shuffle_aclus: [ 75  45  41   8  31  48 100  58  60  86  84  74  19  73  59  71  57  97  35  26  76  21  83  17  10  25  54  37  78  32  33  66  89   7  82  96  85  24  68  50   3  49   9  11  93  36  28  69  51  92   5   4  27  90  61  53   2 107  56  62  13  14  64  70  67 108  15  63  81  16 105 102  52  34  87  23  88 109  98  55]\n",
      "a_shuffle_IDXs: [46 19 77 33 26 45 18 54 62 48 50 30 12 75  2 29 51 76 28 78 68 40 13  8 35 42 41 11 53 65 61 14 52  5 32 10 70 79 73 20 74  9 31  0 27 17 71 43 44 57  6  4 25 49 24  1 38 59 60 37 66 21 64 69 34 56 67 55 36 63 72  7 39 15 23 47  3 58 16 22], a_shuffle_aclus: [ 63  26 107  50  35  62  25  73  84  66  68  45  16 102   4  41  69 105  37 108  90  57  17  11  52  59  58  15  71  87  83  19  70   8  49  14  93 109  98  27 100  13  48   2  36  24  96  60  61  76   9   7  34  67  33   3  55  81  82  54  88  28  86  92  51  75  89  74  53  85  97  10  56  21  32  64   5  78  23  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 15  3 79 16  5 35 59 54 46  0 45 44  4 21 47 49 57 10 76 53  1  7 26 74 33 63 60  9  8 40 36 67 24 22 70 23 28 52 31 14 78 39 42 61 75 77 65 29 17 66 69 12 18  6 50 27 30 25 41  2 58 37 71 20 64 34 72 55 32 43 51 68 73 11 38 13 19 62 48], a_shuffle_aclus: [ 75  21   5 109  23   8  52  81  73  63   2  62  61   7  28  64  67  76  14 105  71   3  10  35 100  50  85  82  13  11  57  53  89  33  31  93  32  37  70  48  19 108  56  59  83 102 107  87  41  24  88  92  16  25   9  68  36  45  34  58   4  78  54  96  27  86  51  97  74  49  60  69  90  98  15  55  17  26  84  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [29 60  0 62  8 71 65 72 35 59 37 61  6 54 56 20 15 73 46 63 64 16 25  7 47 11 50 42 53 17 40 74 66 44 67 30 55 33 75  5 52 51 31  4 78  2 70 22  9 43 19 27 18 38 57 36 58 32 76 14 39 49 26 24 13 23  1 12 21 10 79 77 34 28 68  3 41 45 69 48], a_shuffle_aclus: [ 41  82   2  84  11  96  87  97  52  81  54  83   9  73  75  27  21  98  63  85  86  23  34  10  64  15  68  59  71  24  57 100  88  61  89  45  74  50 102   8  70  69  48   7 108   4  93  31  13  60  26  36  25  55  76  53  78  49 105  19  56  67  35  33  17  32   3  16  28  14 109 107  51  37  90   5  58  62  92  66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24 58 13 41  5  3 47 27 20 16 79 12  9 11 53 36 32  2 74 45 59 61 57 37 28 48 26 56  7 64 51 34 78 77 23 55 67 14 30 75 63 52 49 31 10  4 19 68 40  0 46 72 66 73 54 62 76 29  8 22  1  6 38 33 50 17 71 69 18 21 70 25 42 35 44 43 65 60 15 39], a_shuffle_aclus: [ 33  78  17  58   8   5  64  36  27  23 109  16  13  15  71  53  49   4 100  62  81  83  76  54  37  66  35  75  10  86  69  51 108 107  32  74  89  19  45 102  85  70  67  48  14   7  26  90  57   2  63  97  88  98  73  84 105  41  11  31   3   9  55  50  68  24  96  92  25  28  93  34  59  52  61  60  87  82  21  56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [20 27  2 72 40 46 21 43 29 62 51 19 23 12 52 79 74 17 63 42 30  4 28 71 70 37 53  5  8 44 36  0 31 18 58 77 48 65 22 13 32 78 50 68 38 54  9 64 25 56 26 57 16 14 34 60  3 41 61  6 66 73 11 33 49 39 24 55 15 47 59 76 45  1 35  7 67 75 69 10], a_shuffle_aclus: [ 27  36   4  97  57  63  28  60  41  84  69  26  32  16  70 109 100  24  85  59  45   7  37  96  93  54  71   8  11  61  53   2  48  25  78 107  66  87  31  17  49 108  68  90  55  73  13  86  34  75  35  76  23  19  51  82   5  58  83   9  88  98  15  50  67  56  33  74  21  64  81 105  62   3  52  10  89 102  92  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [70 68 53  1 69 59 26 76 37 16 35  4 72 64 46 42 32  2 38 41 52 10 27 50  8 11 48 12 60 14 33 17 45 31 49 79 28 71  7  3 58 66 24 15 22 39 23 61 47 20 73 62 77 44  6 19  0 55  5  9 18 78 51 36 29 21 75 43 13 40 65 34 67 56 63 25 57 74 30 54], a_shuffle_aclus: [ 93  90  71   3  92  81  35 105  54  23  52   7  97  86  63  59  49   4  55  58  70  14  36  68  11  15  66  16  82  19  50  24  62  48  67 109  37  96  10   5  78  88  33  21  31  56  32  83  64  27  98  84 107  61   9  26   2  74   8  13  25 108  69  53  41  28 102  60  17  57  87  51  89  75  85  34  76 100  45  73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7 62 34 51 28 71 20  6 21 10 39 63 76  3 60 41 73 69 50 46 59 70 45 42 25  2 22 54 13  8 15 33 49 66 43 79 75 29 32 17 78 44 26  0 56 27 37 61 35 16 31  9 24  4 23 12 74 18 57 67 48 77 14 68 58 19 40 55 30 53  1 64 52 65 47 38 72 11 36  5], a_shuffle_aclus: [ 10  84  51  69  37  96  27   9  28  14  56  85 105   5  82  58  98  92  68  63  81  93  62  59  34   4  31  73  17  11  21  50  67  88  60 109 102  41  49  24 108  61  35   2  75  36  54  83  52  23  48  13  33   7  32  16 100  25  76  89  66 107  19  90  78  26  57  74  45  71   3  86  70  87  64  55  97  15  53   8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [20 74  8 75 78  5 24 43 47 18 27  1 17  0 30 77 65 53 64 40 22 72 56 38 66 12 69 25 54 34 13 60 19 15 58 61 68 14 55 51 62 11  7 48 52 79 23 63 42 29 59 50 41 49  3 45 73 10 39 33 28 37  9 67 44  2 26 71 35 46  6 21 36 57 31 70 16 76 32  4], a_shuffle_aclus: [ 27 100  11 102 108   8  33  60  64  25  36   3  24   2  45 107  87  71  86  57  31  97  75  55  88  16  92  34  73  51  17  82  26  21  78  83  90  19  74  69  84  15  10  66  70 109  32  85  59  41  81  68  58  67   5  62  98  14  56  50  37  54  13  89  61   4  35  96  52  63   9  28  53  76  48  93  23 105  49   7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [59 77 56 35 33 58 54 68 16 50 73 55 32 23 65  6 78 26 19 49  4 76 22 28 12 10 74 11 64 18 38 13 37 57 70 63 71 66 61  8 69 52  2 44  3  5 40  0 31  9 62 72 46 48 34 79 39 53 43 60 29 75 51  7 42 20 24 25 45 21 30 36 67 17 41 27  1 47 14 15], a_shuffle_aclus: [ 81 107  75  52  50  78  73  90  23  68  98  74  49  32  87   9 108  35  26  67   7 105  31  37  16  14 100  15  86  25  55  17  54  76  93  85  96  88  83  11  92  70   4  61   5   8  57   2  48  13  84  97  63  66  51 109  56  71  60  82  41 102  69  10  59  27  33  34  62  28  45  53  89  24  58  36   3  64  19  21]\n",
      "a_shuffle_IDXs: [ 2 15 29 17 67 20 35 43 33 72 50 34 23 11 47 22 45 41 49 74 18 40 42 31 66 12 58 16 76 64 69 63 77 54 52 70 30 32 61 71 55 24  0  7 68 75  9 51 36 25  3 46 48 37 44 21 28 13 79 14 59  8 62  6  1 39 73  5 19 60 38 53 56 57 78  4 27 65 10 26], a_shuffle_aclus: [  4  21  41  24  89  27  52  60  50  97  68  51  32  15  64  31  62  58  67 100  25  57  59  48  88  16  78  23 105  86  92  85 107  73  70  93  45  49  83  96  74  33   2  10  90 102  13  69  53  34   5  63  66  54  61  28  37  17 109  19  81  11  84   9   3  56  98   8  26  82  55  71  75  76 108   7  36  87  14  35]\n",
      "a_shuffle_IDXs: [78 65  5 58 67 79 17 60  3 66 30 53 59 75 39 71 44 23 46 38 32 25  9 40 31 27 29 63 26 70 42  7 51 37 12 11  1 10 50  8 56 64 41  4 48 15  0 47 19 62 20 34 13 14 35 55 36 74 28 22  2 77 16 61  6 68 76 18 57 24 69 21 45 43 72 49 54 73 52 33], a_shuffle_aclus: [108  87   8  78  89 109  24  82   5  88  45  71  81 102  56  96  61  32  63  55  49  34  13  57  48  36  41  85  35  93  59  10  69  54  16  15   3  14  68  11  75  86  58   7  66  21   2  64  26  84  27  51  17  19  52  74  53 100  37  31   4 107  23  83   9  90 105  25  76  33  92  28  62  60  97  67  73  98  70  50]\n",
      "a_shuffle_IDXs: [36 75 12 32  1 59 50 69  3 44 20 29  8 46 71 79 73 39  0 40 42 54 77 51 25 18 38 68 35 57 13 28 72 19  6 64 70 22  4 27  7 55 33 23 78 17 37 49 74 58 31 63 67 53  5 48 66 60 52 43 34 16 15 21 11 24 76 61 26 10 14 45 30  9 47 56  2 65 62 41], a_shuffle_aclus: [ 53 102  16  49   3  81  68  92   5  61  27  41  11  63  96 109  98  56   2  57  59  73 107  69  34  25  55  90  52  76  17  37  97  26   9  86  93  31   7  36  10  74  50  32 108  24  54  67 100  78  48  85  89  71   8  66  88  82  70  60  51  23  21  28  15  33 105  83  35  14  19  62  45  13  64  75   4  87  84  58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [66 67 30 12 60 34  8 68 44 10 64 52 14 61 37 79 49 33 51 13 54 73 35 71 20 78 62 50 48 46 28 22 70 41 77 69 45 31 38 27 15  4 32 76 53 59  0 36 63 75 65 18 47 23 29 43  2 26 11 16  6 74 24  5 25 40 39  3 21 17  9  1 56 19 72 42  7 57 58 55], a_shuffle_aclus: [ 88  89  45  16  82  51  11  90  61  14  86  70  19  83  54 109  67  50  69  17  73  98  52  96  27 108  84  68  66  63  37  31  93  58 107  92  62  48  55  36  21   7  49 105  71  81   2  53  85 102  87  25  64  32  41  60   4  35  15  23   9 100  33   8  34  57  56   5  28  24  13   3  75  26  97  59  10  76  78  74]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [12 79 76 50 29 30 61  0 16 70 10 52  2  1 53 24 32 35 51 64 59  5 26 54 19 42 77 37  7 71 17 44 27 28 69  4 78 14 31  9 15 34 73 40 60 46 39  8 47 62 38 55 75 58 45 13 25 23 56 67 41 72 65 18  6 22 57  3 74 36 11 63 21 66 33 43 48 20 49 68], a_shuffle_aclus: [ 16 109 105  68  41  45  83   2  23  93  14  70   4   3  71  33  49  52  69  86  81   8  35  73  26  59 107  54  10  96  24  61  36  37  92   7 108  19  48  13  21  51  98  57  82  63  56  11  64  84  55  74 102  78  62  17  34  32  75  89  58  97  87  25   9  31  76   5 100  53  15  85  28  88  50  60  66  27  67  90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 47 62  6  1 45 37 40 39 71 10 23 59 51 14 43 27 13 24 15 53 73 64 25 26  7  3 42 32 21 55 18 49 38 41 72 48  0 36 11  2 54 56 12  9 65 31 58 69 75 66 74 33 50 16 76 52 44 77 70 28 22 79 19 61  4 30 60 29 34 57  8 20 35  5 68 63 78 67 46], a_shuffle_aclus: [ 24  64  84   9   3  62  54  57  56  96  14  32  81  69  19  60  36  17  33  21  71  98  86  34  35  10   5  59  49  28  74  25  67  55  58  97  66   2  53  15   4  73  75  16  13  87  48  78  92 102  88 100  50  68  23 105  70  61 107  93  37  31 109  26  83   7  45  82  41  51  76  11  27  52   8  90  85 108  89  63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [74 16  2 24  6 15 64 68 55  3 29 35 51 77  9  8  4 21 30 13 19 63 42 31 45 65 54 25 50 47 37 79 60 11 73 39 20 33 75 58 72  5 56 71 41 62 27 34 14 40 23 76 10 43 52 53 48 69 22 12 78 44 28 66 38  0 32  7  1 36 61 59 17 49 46 18 70 26 57 67], a_shuffle_aclus: [100  23   4  33   9  21  86  90  74   5  41  52  69 107  13  11   7  28  45  17  26  85  59  48  62  87  73  34  68  64  54 109  82  15  98  56  27  50 102  78  97   8  75  96  58  84  36  51  19  57  32 105  14  60  70  71  66  92  31  16 108  61  37  88  55   2  49  10   3  53  83  81  24  67  63  25  93  35  76  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 28 69 67 16 15 72 19 58 70 51  0 52 13 10 60 63 35 22 18 29 66 68 75 41 42 49 50  9 12 55  2  3 53 31 30 24 64 20 38 73  7  4 47 26  1 11 25 27 17 62 34 74 45 40 71  6 65  8 56 37 57 78 23 79 59 77 48 36  5 61 39 32 43 33 21 14 44 54 76], a_shuffle_aclus: [ 63  37  92  89  23  21  97  26  78  93  69   2  70  17  14  82  85  52  31  25  41  88  90 102  58  59  67  68  13  16  74   4   5  71  48  45  33  86  27  55  98  10   7  64  35   3  15  34  36  24  84  51 100  62  57  96   9  87  11  75  54  76 108  32 109  81 107  66  53   8  83  56  49  60  50  28  19  61  73 105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [72  7 54  6  3 47 27 23 39 16 33 51 12 56 46 78 42 45 41 68 48 64 71  2 43 75 74 55 62 59 69 49 29 38 70 35 34 28 36 19 65 63 21 13 15 60 11  1 14 66 20 79 26 44 58 10  9  0 32 76 52 22 37 73 24 67 61 31 53 25 77 40 17 18 50 30 57  5  4  8], a_shuffle_aclus: [ 97  10  73   9   5  64  36  32  56  23  50  69  16  75  63 108  59  62  58  90  66  86  96   4  60 102 100  74  84  81  92  67  41  55  93  52  51  37  53  26  87  85  28  17  21  82  15   3  19  88  27 109  35  61  78  14  13   2  49 105  70  31  54  98  33  89  83  48  71  34 107  57  24  25  68  45  76   8   7  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [37  7 56 14  3 68 65 34 79 15  0 39 46 74  8 17 67 44 40 64 61 62 41 57 49 27 18 54 11 71 50 19 75  9 24 47 70 51 48 52 29 32 78 73 58 21 31 43 63 55  1 13 60 36 77 33 10 45 76 30  5 20  4 35 66 28 42 69  2 12  6 22 25 26 72 38 23 16 59 53], a_shuffle_aclus: [ 54  10  75  19   5  90  87  51 109  21   2  56  63 100  11  24  89  61  57  86  83  84  58  76  67  36  25  73  15  96  68  26 102  13  33  64  93  69  66  70  41  49 108  98  78  28  48  60  85  74   3  17  82  53 107  50  14  62 105  45   8  27   7  52  88  37  59  92   4  16   9  31  34  35  97  55  32  23  81  71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [19 33 73 23 11 22 55 40 14 25  9 17 65  6 24 50 35 72 53  5 45 51 70 69 43 61  4 60 66 52 31  3 56 16 38 47 42 37 79 28 63  8  2  7 68 57 26 41 46 29 76 39 30 13 67 49  1 48 74 32 27 78 18 62 54 75 71 77 64 10 12  0 15 59 34 36 58 21 44 20], a_shuffle_aclus: [ 26  50  98  32  15  31  74  57  19  34  13  24  87   9  33  68  52  97  71   8  62  69  93  92  60  83   7  82  88  70  48   5  75  23  55  64  59  54 109  37  85  11   4  10  90  76  35  58  63  41 105  56  45  17  89  67   3  66 100  49  36 108  25  84  73 102  96 107  86  14  16   2  21  81  51  53  78  28  61  27]\n",
      "a_shuffle_IDXs: [ 1 37 18 72 79 66 21 32 23  4 78 35 57 34 27 77 13 55 52 62 75 59 15 71 42 68 24 43 63  0  9 17 40 33  2 69 58 26 31 30 61 54 38 45 76 67 64 10 22  8  5 49 70 48 20 11 19 73 39 36 53 44 50  6 16 65 28 25 46 14 60  7 56 12 51 47 29  3 41 74], a_shuffle_aclus: [  3  54  25  97 109  88  28  49  32   7 108  52  76  51  36 107  17  74  70  84 102  81  21  96  59  90  33  60  85   2  13  24  57  50   4  92  78  35  48  45  83  73  55  62 105  89  86  14  31  11   8  67  93  66  27  15  26  98  56  53  71  61  68   9  23  87  37  34  63  19  82  10  75  16  69  64  41   5  58 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24 68 63 30 28 31 47 58 43 15 67 76 25 77 36 45 53 49  7 71 56 14 51 40 74 52 64 50  8 18 32 61 75 62 46 23 60 12 72 37 34 19 10 35 65 55 73  6 26 57 17 69 33  5 54 41 44 79 42 48 29 38 11 59 39  9 27 66  2  1 22  4 70  0 78 16 21  3 20 13], a_shuffle_aclus: [ 33  90  85  45  37  48  64  78  60  21  89 105  34 107  53  62  71  67  10  96  75  19  69  57 100  70  86  68  11  25  49  83 102  84  63  32  82  16  97  54  51  26  14  52  87  74  98   9  35  76  24  92  50   8  73  58  61 109  59  66  41  55  15  81  56  13  36  88   4   3  31   7  93   2 108  23  28   5  27  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [55 71 11 48 63 58 33  8 20 47 53 46 29 12  6 30 69  5 19 68 76 28 77 79  2  0 43 74 14 75 22 15  1 44 36 45 72 21 51 62 39 16 26 17  4  9 31 64 38 42 35 60  3 24 61 56 32 34 78 40 57 49 37 70 23 59 54 18 73 66 25 52 50 41 13  7 65 10 67 27], a_shuffle_aclus: [ 74  96  15  66  85  78  50  11  27  64  71  63  41  16   9  45  92   8  26  90 105  37 107 109   4   2  60 100  19 102  31  21   3  61  53  62  97  28  69  84  56  23  35  24   7  13  48  86  55  59  52  82   5  33  83  75  49  51 108  57  76  67  54  93  32  81  73  25  98  88  34  70  68  58  17  10  87  14  89  36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75 61 34 12 33  3 48  9 79 56 65 47 40 16 57 43  2 71 53 62 38 46 55 45 77 18 23 37  7 51 44 66 58 19 63 24 25 69  4 70 39 64 20 28  6 78 13 32 14 11  1 41  5 50  0 54 73 76 52 35  8 31 29 10 67 15 36 68 27 17 26 59 30 49 22 74 60 21 72 42], a_shuffle_aclus: [102  83  51  16  50   5  66  13 109  75  87  64  57  23  76  60   4  96  71  84  55  63  74  62 107  25  32  54  10  69  61  88  78  26  85  33  34  92   7  93  56  86  27  37   9 108  17  49  19  15   3  58   8  68   2  73  98 105  70  52  11  48  41  14  89  21  53  90  36  24  35  81  45  67  31 100  82  28  97  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:391: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ey = np.nansum(arr * y_mat) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:392: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ex = np.nansum(arr * x_mat) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:393: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_xy = np.nansum(arr * (y_mat - ey) * (x_mat - ex)) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:394: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_yy = np.nansum(arr * (y_mat - ey) ** 2) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:395: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_xx = np.nansum(arr * (x_mat - ex) ** 2) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41  3 26  9  2 29 66 65 42 13 28 64 75 67 59 27 36 54 31 45 49 32 79 18 71 52 24 56 73 37 72 19 78  0 33 17 38 39 15 70 48 53 34 68 62 14  1 11 74 60 40 10 20 63 30 51 12 23 77 76  8  5 43 69  4 22 16  7  6 55 58 50 46 61 47 35 44 25 57 21], a_shuffle_aclus: [ 58   5  35  13   4  41  88  87  59  17  37  86 102  89  81  36  53  73  48  62  67  49 109  25  96  70  33  75  98  54  97  26 108   2  50  24  55  56  21  93  66  71  51  90  84  19   3  15 100  82  57  14  27  85  45  69  16  32 107 105  11   8  60  92   7  31  23  10   9  74  78  68  63  83  64  52  61  34  76  28]\n",
      "a_shuffle_IDXs: [52 75  5 56 68 69 66 31 49 44 11 23 65 18 24 78 79 29 38 62  9 15 37  4 19 22 50 32 36 47  8 30 72 45 60 27 51 59 73 10 46  0 13 35 39  6 53 48 12 26 16 33  7 41 42 17 55 63 43 74 67  1 54 58 14 71 34  2 76 25 57 21 20 70  3 64 40 28 61 77], a_shuffle_aclus: [ 70 102   8  75  90  92  88  48  67  61  15  32  87  25  33 108 109  41  55  84  13  21  54   7  26  31  68  49  53  64  11  45  97  62  82  36  69  81  98  14  63   2  17  52  56   9  71  66  16  35  23  50  10  58  59  24  74  85  60 100  89   3  73  78  19  96  51   4 105  34  76  28  27  93   5  86  57  37  83 107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [31 51 30 61 33 66 54  4 77 10 39 22  3 36 63 75 73 43 53 35 29 12 65  2 38 42 50 56 32 60 48 71 52 28  7 58 78 20 19  5 26 21 13 68 41 67 47 62 25 15 24  8 79 18 70 14 74 69  0 17 44 23 64 37 57 59 55  1 34 45 72 27 16 11 46 76 49  9  6 40], a_shuffle_aclus: [ 48  69  45  83  50  88  73   7 107  14  56  31   5  53  85 102  98  60  71  52  41  16  87   4  55  59  68  75  49  82  66  96  70  37  10  78 108  27  26   8  35  28  17  90  58  89  64  84  34  21  33  11 109  25  93  19 100  92   2  24  61  32  86  54  76  81  74   3  51  62  97  36  23  15  63 105  67  13   9  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [45 17  9 43 30  6 62 53 13 41 20  5 48 64 34 61 72  2  0 18  3 69 11 67 25 51 10 52 16 60 63 29 22 28  4 75 79 76 73 33 12 24 26 56 50  1 49 59 58  8 42 54  7 44 77 35 36 65 32 68 19 15 46 14 27 31 55 66 47 70 37 71 57 74 21 38 40 39 78 23], a_shuffle_aclus: [ 62  24  13  60  45   9  84  71  17  58  27   8  66  86  51  83  97   4   2  25   5  92  15  89  34  69  14  70  23  82  85  41  31  37   7 102 109 105  98  50  16  33  35  75  68   3  67  81  78  11  59  73  10  61 107  52  53  87  49  90  26  21  63  19  36  48  74  88  64  93  54  96  76 100  28  55  57  56 108  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 36 31 14 40 76 63 20 50 25  0  3 19 16 69 43 24 78 65 37 57 41 44  4 29 53 11 54 52 35 58 77 47 26 22 13 39 60 48  6  5 55 45 79 34 18 33 74 61 27 71 10  8 28 49 67 51 70 21  1 23 56 66 12 32 15 62 42  2 59 46 64 72  9 38 68 75 73 30  7], a_shuffle_aclus: [ 24  53  48  19  57 105  85  27  68  34   2   5  26  23  92  60  33 108  87  54  76  58  61   7  41  71  15  73  70  52  78 107  64  35  31  17  56  82  66   9   8  74  62 109  51  25  50 100  83  36  96  14  11  37  67  89  69  93  28   3  32  75  88  16  49  21  84  59   4  81  63  86  97  13  55  90 102  98  45  10]\n",
      "a_shuffle_IDXs: [25  4  7 30 16  1 53  0 39 19 21 28 29 69 67 18 50 59 27 68 34 12  3 47 20 32  9 73 41 72 22 55 17 33 74  8  2 71  5 36 54  6 63 57 10 35 15 77 70 56 51 31 24 43 38 26 44 48 58 76 75 46 65 62 11 61 42 40 49 66 64 14 13 45 23 60 79 78 52 37], a_shuffle_aclus: [ 34   7  10  45  23   3  71   2  56  26  28  37  41  92  89  25  68  81  36  90  51  16   5  64  27  49  13  98  58  97  31  74  24  50 100  11   4  96   8  53  73   9  85  76  14  52  21 107  93  75  69  48  33  60  55  35  61  66  78 105 102  63  87  84  15  83  59  57  67  88  86  19  17  62  32  82 109 108  70  54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 66 70  0 39 77  9 45 36  8 69  6 43  5 42 25 17  3 52 33 41 71 23 47 29 44 74 12 49 28 11 26 58 34 22 56 51 55 79 53 21 65 32 10 46 37 73 15 31 13 63 16  1  4 20 40 30 57 78 50 72 68 35 62  2 64 76 18 59 48  7 38 60 54 75 61 24 14 19 27], a_shuffle_aclus: [ 89  88  93   2  56 107  13  62  53  11  92   9  60   8  59  34  24   5  70  50  58  96  32  64  41  61 100  16  67  37  15  35  78  51  31  75  69  74 109  71  28  87  49  14  63  54  98  21  48  17  85  23   3   7  27  57  45  76 108  68  97  90  52  84   4  86 105  25  81  66  10  55  82  73 102  83  33  19  26  36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [38 67 73 24 33 17  9 31 55 61 13 29  8 32 21  0 68 35 15 63 48 47 44 60 64 28 18 34 43 57 36 12  4  6 39 19 11 56 50 53 46 41  2 27 45 42 26 78  7 14  3 10 70  5 65 37 71 52 58 16 25 20 75 62 23 22  1 74 59 66 79 49 51 30 76 54 69 77 72 40], a_shuffle_aclus: [ 55  89  98  33  50  24  13  48  74  83  17  41  11  49  28   2  90  52  21  85  66  64  61  82  86  37  25  51  60  76  53  16   7   9  56  26  15  75  68  71  63  58   4  36  62  59  35 108  10  19   5  14  93   8  87  54  96  70  78  23  34  27 102  84  32  31   3 100  81  88 109  67  69  45 105  73  92 107  97  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [66 29  6  3 78 31 58  4 22  7 42 56 28 18 41 76 69 64 45 59 60 16 44 39 77 14 13 68 72 24  8  9  0  1 79 43 32 67 36 48 33 73 26 27 61 62 71 47 12 37 38 23 11 51 75 15 40  5 54 25 19 50 70 65  2 74 21 34 46 30 52 17 10 20 63 55 35 53 57 49], a_shuffle_aclus: [ 88  41   9   5 108  48  78   7  31  10  59  75  37  25  58 105  92  86  62  81  82  23  61  56 107  19  17  90  97  33  11  13   2   3 109  60  49  89  53  66  50  98  35  36  83  84  96  64  16  54  55  32  15  69 102  21  57   8  73  34  26  68  93  87   4 100  28  51  63  45  70  24  14  27  85  74  52  71  76  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 73 27 16 10 19 78 38 50 52 51 35 53 77 66 59 67 70 62 29 23 15 45 65 72 32 42 11 33 48 14 57 20 47 21 71 43  7 34 17 18 68 54 37 56 63 36 25 24 26 22 75 28  4 55  6 31  0  2 76 41 30 13 12 44 74 49 46  5 40  1 79 64  9  8 39 58 69 61 60], a_shuffle_aclus: [  5  98  36  23  14  26 108  55  68  70  69  52  71 107  88  81  89  93  84  41  32  21  62  87  97  49  59  15  50  66  19  76  27  64  28  96  60  10  51  24  25  90  73  54  75  85  53  34  33  35  31 102  37   7  74   9  48   2   4 105  58  45  17  16  61 100  67  63   8  57   3 109  86  13  11  56  78  92  83  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24 68 40 35 14 16 71 44 15  3 42 79 63 56 37 77 29 53 17 76 22 32  8 49 13 36 38  0 58 62 70 12 57 26 19 75 47 73 64 54 41  2 66 34  9 51 39 60 20 33 43 69  5 52 30 31 25 27 45 74 28 55 65 23  6 50 11 78  4 59 18 67  7 72 21 10  1 46 48 61], a_shuffle_aclus: [ 33  90  57  52  19  23  96  61  21   5  59 109  85  75  54 107  41  71  24 105  31  49  11  67  17  53  55   2  78  84  93  16  76  35  26 102  64  98  86  73  58   4  88  51  13  69  56  82  27  50  60  92   8  70  45  48  34  36  62 100  37  74  87  32   9  68  15 108   7  81  25  89  10  97  28  14   3  63  66  83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 18 65  6 35 73 75  2 72 25 24 54 69 28 58 38 53 36 33 68  0  7 71 77 74 22 42 31 16 30 47 14 32 39 27 43 13  8  1 12 51 23 40 62 50 48 26 11  4 17 79 15 44 67 76 63 29 20 45 52 60 57 34 19  3 64  5 66 56 70 21 55  9 37 46 61 78 49 41 59], a_shuffle_aclus: [ 14  25  87   9  52  98 102   4  97  34  33  73  92  37  78  55  71  53  50  90   2  10  96 107 100  31  59  48  23  45  64  19  49  56  36  60  17  11   3  16  69  32  57  84  68  66  35  15   7  24 109  21  61  89 105  85  41  27  62  70  82  76  51  26   5  86   8  88  75  93  28  74  13  54  63  83 108  67  58  81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [44 26 70 40 22 27  4 74 54 38 11 66  0 59 16 34  5 46  6  2 72 50 79 14 31 21 29 30 78 47 76 77 75 61 63 28 64 19 32 71 13 73 69 45 36 10 33 15 62 25 51 23 39 17 52 35 42 60 58 65  9 12  8 49 53 20 67  3  7  1 57 48 55 68 56 37 43 24 41 18], a_shuffle_aclus: [ 61  35  93  57  31  36   7 100  73  55  15  88   2  81  23  51   8  63   9   4  97  68 109  19  48  28  41  45 108  64 105 107 102  83  85  37  86  26  49  96  17  98  92  62  53  14  50  21  84  34  69  32  56  24  70  52  59  82  78  87  13  16  11  67  71  27  89   5  10   3  76  66  74  90  75  54  60  33  58  25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 40 37 17 21 34 30 60 49 75 68 13 19 76 20 38 25 10 57 52 58 50  7  1 31 46 78 62 16 65 26 28 47 27 24 77 32 72  5 73  8 23 22 12 55  2 39 41 61 71 29  9 70 79 15 54 14 48 66  0  3  4 11 53 18 74 45 43  6 69 51 67 33 63 42 56 59 44 36 35], a_shuffle_aclus: [ 86  57  54  24  28  51  45  82  67 102  90  17  26 105  27  55  34  14  76  70  78  68  10   3  48  63 108  84  23  87  35  37  64  36  33 107  49  97   8  98  11  32  31  16  74   4  56  58  83  96  41  13  93 109  21  73  19  66  88   2   5   7  15  71  25 100  62  60   9  92  69  89  50  85  59  75  81  61  53  52]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73  2 29 13 49 38 45 31 75 65 21 53  3 37  8 34 17 63 76 68 59 26 18 25 11 70 32 78 19 14 60 79 44 66 27 54 41  9 47  1 69 36 30 39  7 71 67 20 22 24 58 10 64 56 46  6 42 52  4 57 77 16 33 43 50 51 28 55  0 35 62 72 48 74 23 40  5 15 12 61], a_shuffle_aclus: [ 98   4  41  17  67  55  62  48 102  87  28  71   5  54  11  51  24  85 105  90  81  35  25  34  15  93  49 108  26  19  82 109  61  88  36  73  58  13  64   3  92  53  45  56  10  96  89  27  31  33  78  14  86  75  63   9  59  70   7  76 107  23  50  60  68  69  37  74   2  52  84  97  66 100  32  57   8  21  16  83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [20  0 34 46 54 53 74 71 51 23 79 31 73 48 64  3  4  2 38 18 75 36 25  5 40 43 78 28 69 33 11 67  9  6 17 15 63 41 76 61 65 72 55 12 45  1  8 27 52 49 10 42 57 13 47 59 77 29 62 30  7 14 58 21 44 68 56 32 37 66 60 24 70 39 50 16 19 26 35 22], a_shuffle_aclus: [ 27   2  51  63  73  71 100  96  69  32 109  48  98  66  86   5   7   4  55  25 102  53  34   8  57  60 108  37  92  50  15  89  13   9  24  21  85  58 105  83  87  97  74  16  62   3  11  36  70  67  14  59  76  17  64  81 107  41  84  45  10  19  78  28  61  90  75  49  54  88  82  33  93  56  68  23  26  35  52  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 67  2 17 37 10 24 41 46 29 66 38 22 65 73  4 62 58 33 59 78  1 44 13 26 42 32  0 43 18 55 64 68 54 28 52 12 20 47 76 30 53 31 56 39 77 75 36 11 35  3 16 15  9  6 50 23 69 45  7 79 14 34 19 57 74 72 51 49 48 71 61 40  5 21 25 60 27  8 70], a_shuffle_aclus: [ 85  89   4  24  54  14  33  58  63  41  88  55  31  87  98   7  84  78  50  81 108   3  61  17  35  59  49   2  60  25  74  86  90  73  37  70  16  27  64 105  45  71  48  75  56 107 102  53  15  52   5  23  21  13   9  68  32  92  62  10 109  19  51  26  76 100  97  69  67  66  96  83  57   8  28  34  82  36  11  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 66 20 25 14 53  6 77 74 62 56 28  9  3 44 17 50 48 16 64 63 61 10 13  7 36 58 65 37 55 70 33  2 79 22 34  8 32 75 60 23 19 51 76 29 35 52 78 38 24 42 31 71 39  5 67 45 47 21 59 57  0 46 26 11  1 18 43  4 41 72 12 49 54 69 30 68 15 73 40], a_shuffle_aclus: [ 36  88  27  34  19  71   9 107 100  84  75  37  13   5  61  24  68  66  23  86  85  83  14  17  10  53  78  87  54  74  93  50   4 109  31  51  11  49 102  82  32  26  69 105  41  52  70 108  55  33  59  48  96  56   8  89  62  64  28  81  76   2  63  35  15   3  25  60   7  58  97  16  67  73  92  45  90  21  98  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 14 57 28 21 56 50  1  3 61 72  5 79 40 35 51 20 10 47 45 38 39 12 69 41 19 59 65 62 31 11 78 44 73 48  2 46 13 55 32 42 37 33 16  6 27 15 49 29 23 25 52 71 36 74 34 76 64 43 66 58  4 24 77 30 26 17 63 22  7 70  8  0 54 67 75 60 18 68 53], a_shuffle_aclus: [ 13  19  76  37  28  75  68   3   5  83  97   8 109  57  52  69  27  14  64  62  55  56  16  92  58  26  81  87  84  48  15 108  61  98  66   4  63  17  74  49  59  54  50  23   9  36  21  67  41  32  34  70  96  53 100  51 105  86  60  88  78   7  33 107  45  35  24  85  31  10  93  11   2  73  89 102  82  25  90  71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [77 18 25  4  7 19 20  2 22 29 28 65 10 57 23 74 45 11 56 55 64 43 67 73 24 12 50 78  3 75 52 38 79 51 61  6 13 44 69 16 42 34 68  0 59 31 72 14 26 17 39 58 54 71 63 70 30 60 46 40 76  5 33 48 49  1 15 21 66  9 32 35 41 36  8 62 37 27 47 53], a_shuffle_aclus: [107  25  34   7  10  26  27   4  31  41  37  87  14  76  32 100  62  15  75  74  86  60  89  98  33  16  68 108   5 102  70  55 109  69  83   9  17  61  92  23  59  51  90   2  81  48  97  19  35  24  56  78  73  96  85  93  45  82  63  57 105   8  50  66  67   3  21  28  88  13  49  52  58  53  11  84  54  36  64  71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [52  1  9 65 76 40 50 70 69 19 38 21 31 45 23 13 63  8  6 75 53 39  3 54  7 77 27  0  4 35 28 73 62 66 56 15 74 32 67 26 34 11 36 61 49 60 22 55 16 24  2 17 14 44 18 57 41 30 12 25 42  5 64 58 46 33 29 37 72 59 71 79 48 47 20 43 68 78 10 51], a_shuffle_aclus: [ 70   3  13  87 105  57  68  93  92  26  55  28  48  62  32  17  85  11   9 102  71  56   5  73  10 107  36   2   7  52  37  98  84  88  75  21 100  49  89  35  51  15  53  83  67  82  31  74  23  33   4  24  19  61  25  76  58  45  16  34  59   8  86  78  63  50  41  54  97  81  96 109  66  64  27  60  90 108  14  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [69 67 37 77 78 15 55 35 11 59 20 26 63 30 57 44 32 19  6 54 42 40 79 62  4 49 23 12 66  9 56 74 70 29 21 51  1 58  8 22 68 27 53 14 75 61 48 34 43 73 24 36 72 16 64 28 33 39 47 65  3 31 71  2 17 76 60  7 50  5 45 46 10 18 13 41  0 38 25 52], a_shuffle_aclus: [ 92  89  54 107 108  21  74  52  15  81  27  35  85  45  76  61  49  26   9  73  59  57 109  84   7  67  32  16  88  13  75 100  93  41  28  69   3  78  11  31  90  36  71  19 102  83  66  51  60  98  33  53  97  23  86  37  50  56  64  87   5  48  96   4  24 105  82  10  68   8  62  63  14  25  17  58   2  55  34  70]\n",
      "a_shuffle_IDXs: [27 36 55 76 52  9 19  0 13 77 38 62 75 73 31 40 28 69 63 53  8 54 56 46 21 70 15 57 58 12 71 68 33 20 43 67 44  4 16 11 32 60 65 42 72 30 61 50 14 47 25 23 18 17 35 49  7  2 51  1 59 10 78 29 64 74 41  6 24 45 22 79 48 39 66 34 37  3  5 26], a_shuffle_aclus: [ 36  53  74 105  70  13  26   2  17 107  55  84 102  98  48  57  37  92  85  71  11  73  75  63  28  93  21  76  78  16  96  90  50  27  60  89  61   7  23  15  49  82  87  59  97  45  83  68  19  64  34  32  25  24  52  67  10   4  69   3  81  14 108  41  86 100  58   9  33  62  31 109  66  56  88  51  54   5   8  35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 58 55 67 26 43 10 48 52  6  5 60 71 50 21 13 33 74 38 31 59 61 15 19 23 34 27 54 65 64  9 57 56 39 44 77 70  7 41 53 28 72 25 35 30 37  4 12 49 22 42 75 78 62 14 11 73 63 68  8 79 51 40  2 16  0 29 45  3 20 66  1 17 32 18 24 46 76 36 69], a_shuffle_aclus: [ 64  78  74  89  35  60  14  66  70   9   8  82  96  68  28  17  50 100  55  48  81  83  21  26  32  51  36  73  87  86  13  76  75  56  61 107  93  10  58  71  37  97  34  52  45  54   7  16  67  31  59 102 108  84  19  15  98  85  90  11 109  69  57   4  23   2  41  62   5  27  88   3  24  49  25  33  63 105  53  92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 50 21 42 75 76 39 63 28 24 13 52 35 14 67 57 55 32 68 30 17  1  7 61 64 44 77 70  9  4 18 54 20 10 58 27 41 73 38  3 25 19 78 22 47 56 72 16  8 65 66 40 11 31 15 69  2 26 59 53 62 29 36 51 45 43 23 74  6 79 60 34 48 33 49 12  5 37  0 71], a_shuffle_aclus: [ 63  68  28  59 102 105  56  85  37  33  17  70  52  19  89  76  74  49  90  45  24   3  10  83  86  61 107  93  13   7  25  73  27  14  78  36  58  98  55   5  34  26 108  31  64  75  97  23  11  87  88  57  15  48  21  92   4  35  81  71  84  41  53  69  62  60  32 100   9 109  82  51  66  50  67  16   8  54   2  96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [29  9 76 12 38 13 63 23 20 50 64 22 17 11 31 25 19 60  5 61 55 73 70 14 42 34 15 16 53 67 26 45  1 28 32 43 75 40 48 72  3 59  7 24 66 54 37 27 57  4 18 69 46 78  6 71 58 36 51  8 35 74 62 10 52 65 47 56 77 30 41 33  2 44 39  0 68 21 49 79], a_shuffle_aclus: [ 41  13 105  16  55  17  85  32  27  68  86  31  24  15  48  34  26  82   8  83  74  98  93  19  59  51  21  23  71  89  35  62   3  37  49  60 102  57  66  97   5  81  10  33  88  73  54  36  76   7  25  92  63 108   9  96  78  53  69  11  52 100  84  14  70  87  64  75 107  45  58  50   4  61  56   2  90  28  67 109]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [40 15 41 22 17 61  6  1 10  9 23 45 59 75  0 48 73 65 28 24 74 57 16 68 66 52 64 25 67 35 58 37 62 56 34 26 70 31 76 20 43  5  8 18 36 54 12  4 39 49  2 60 78 79 29 46 44 50 69 71 42 51 77 38 33 19 13 14 55 30 72 53 47  7 27 21 11 63  3 32], a_shuffle_aclus: [ 57  21  58  31  24  83   9   3  14  13  32  62  81 102   2  66  98  87  37  33 100  76  23  90  88  70  86  34  89  52  78  54  84  75  51  35  93  48 105  27  60   8  11  25  53  73  16   7  56  67   4  82 108 109  41  63  61  68  92  96  59  69 107  55  50  26  17  19  74  45  97  71  64  10  36  28  15  85   5  49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [42 30 73 37 32  6 74 45  9 52 66 76 61 20 11 41 51 34 19 27 17 54 58 22 59 47 28 26  0 24 15 67 40 35 55 18  3 23 70 72 33  5  8 71 10 13 39 56 14 57 44 36 12 21 16 64 38 25 62 69  1 48 50 29  2 43 75 78  7 77 31 63 68 79 53  4 65 49 46 60], a_shuffle_aclus: [ 59  45  98  54  49   9 100  62  13  70  88 105  83  27  15  58  69  51  26  36  24  73  78  31  81  64  37  35   2  33  21  89  57  52  74  25   5  32  93  97  50   8  11  96  14  17  56  75  19  76  61  53  16  28  23  86  55  34  84  92   3  66  68  41   4  60 102 108  10 107  48  85  90 109  71   7  87  67  63  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [48 62 66 42 18 19 26 61 40 29  9 55 52 64 50  3 27 43 63 53 69 47 41 45 17 71 72 44 23 68 34 16 22 31 32  4 11 24 25 14  8 70 13 73 56 59 65 33  0 30 51 76 75 12  2  5 57 58 35 39  6 54 37 77  1 21  7 79 74 15 38 20 67 49 36 60 10 46 78 28], a_shuffle_aclus: [ 66  84  88  59  25  26  35  83  57  41  13  74  70  86  68   5  36  60  85  71  92  64  58  62  24  96  97  61  32  90  51  23  31  48  49   7  15  33  34  19  11  93  17  98  75  81  87  50   2  45  69 105 102  16   4   8  76  78  52  56   9  73  54 107   3  28  10 109 100  21  55  27  89  67  53  82  14  63 108  37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 25 17 26 32  7 56  5 72 61 39 13 38 42 76 23 70 55 14 43 49 54 62 68 45 20 67 77 57 21 79 75 19 44 73 69 66 18 12 30 33 59 41 40 15 11 29  6 50 22 51 28  2 34 16 36  8 35  0 48 60 74  1 71 24  3 78 52  4  9 27 63 58 53 65 47 37 31 46 10], a_shuffle_aclus: [ 86  34  24  35  49  10  75   8  97  83  56  17  55  59 105  32  93  74  19  60  67  73  84  90  62  27  89 107  76  28 109 102  26  61  98  92  88  25  16  45  50  81  58  57  21  15  41   9  68  31  69  37   4  51  23  53  11  52   2  66  82 100   3  96  33   5 108  70   7  13  36  85  78  71  87  64  54  48  63  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 16 68 30 20 62 41 15  9 45 34 66 74 78 21 40 54 38 37 71  8 25 53 27 59 22 72 69 11 50 47 65 49 33 73 44 36 29 18 42 26  3 23 51 77 14 43 12 55 70 28  2 75 63  1 48 10 35  6 13 79 64 39 24 46  4 31 57 32  7 67 60 19 76  0  5 56 17 52 58], a_shuffle_aclus: [ 83  23  90  45  27  84  58  21  13  62  51  88 100 108  28  57  73  55  54  96  11  34  71  36  81  31  97  92  15  68  64  87  67  50  98  61  53  41  25  59  35   5  32  69 107  19  60  16  74  93  37   4 102  85   3  66  14  52   9  17 109  86  56  33  63   7  48  76  49  10  89  82  26 105   2   8  75  24  70  78]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [13 75 14 18 21 70 44 40  1 27  0 19  8 69 38 24  7 64  5  3 58 22 59 32 50 20 10 49 77 46 66 42 72 25  6 73 39 56 29 15 43 12 48 63 52 47 30 76 11 57 45 78  9 16 28 23 36 34 53 55 17 26 61  4 31 33 41 68 35 54 37 51 67 71 65 74 62 79  2 60], a_shuffle_aclus: [ 17 102  19  25  28  93  61  57   3  36   2  26  11  92  55  33  10  86   8   5  78  31  81  49  68  27  14  67 107  63  88  59  97  34   9  98  56  75  41  21  60  16  66  85  70  64  45 105  15  76  62 108  13  23  37  32  53  51  71  74  24  35  83   7  48  50  58  90  52  73  54  69  89  96  87 100  84 109   4  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [34 55 75 37 77 39 14 28 41 68 76 49 17 59 19  6 32  8 43 40 56 10 60 31  4 50 73  2 63 67 53 47 24 46 52 30 12 27 42  1 36 61 62  3 16 45 54 66 70 20 11 26 29  9 69 25 51 13 79 18  5  7 21 48 38 78 22 64 58 15 57 44 72 35 74 33 23  0 65 71], a_shuffle_aclus: [ 51  74 102  54 107  56  19  37  58  90 105  67  24  81  26   9  49  11  60  57  75  14  82  48   7  68  98   4  85  89  71  64  33  63  70  45  16  36  59   3  53  83  84   5  23  62  73  88  93  27  15  35  41  13  92  34  69  17 109  25   8  10  28  66  55 108  31  86  78  21  76  61  97  52 100  50  32   2  87  96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 44  2 33 11 78 29 65 55 74 60 15 22 17 59 24 26 67  9 10 16 51 57 27 35 61 14 18 64 49 31 70 21 76 71 75 30 23  0 66 37 12 36 72 62 19 48 43 28 41  6 77  4 50  7 47 56 25 46 58 38  5 73 63 69 53 39 52 20 45 32 68 13 42  3  8 34 54 40  1], a_shuffle_aclus: [109  61   4  50  15 108  41  87  74 100  82  21  31  24  81  33  35  89  13  14  23  69  76  36  52  83  19  25  86  67  48  93  28 105  96 102  45  32   2  88  54  16  53  97  84  26  66  60  37  58   9 107   7  68  10  64  75  34  63  78  55   8  98  85  92  71  56  70  27  62  49  90  17  59   5  11  51  73  57   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 26 72 57 38 40 70 33  0 27 34 62 21 73 60 54  1 30 22 18 49 31 13 35 66 19 64 15 46  9 76 12 43 67 42 39 37 65 52 16 74  5 20 24 41 59 51 17 75 28 23  4  6 78 53 55 58 25 56 77 47 45 11 48 36 68 61 50 79  3 63 14 10 69  2 44 71 32  7 29], a_shuffle_aclus: [ 11  35  97  76  55  57  93  50   2  36  51  84  28  98  82  73   3  45  31  25  67  48  17  52  88  26  86  21  63  13 105  16  60  89  59  56  54  87  70  23 100   8  27  33  58  81  69  24 102  37  32   7   9 108  71  74  78  34  75 107  64  62  15  66  53  90  83  68 109   5  85  19  14  92   4  61  96  49  10  41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24 77 44 35 29 52  0  5 46 58 50  7 53 21 13 57 18 49 17 14 22 74 28 45 26 70 41  1 73 10 60 36 55 42 75  9 61 54  2  6 43 66 72 48 47 23 27  8 15 68 76 65 40 38 69 30 20 19 34 12 39 62 64 67 51 56 31  4 63  3 78 11 37 25 32 33 79 71 59 16], a_shuffle_aclus: [ 33 107  61  52  41  70   2   8  63  78  68  10  71  28  17  76  25  67  24  19  31 100  37  62  35  93  58   3  98  14  82  53  74  59 102  13  83  73   4   9  60  88  97  66  64  32  36  11  21  90 105  87  57  55  92  45  27  26  51  16  56  84  86  89  69  75  48   7  85   5 108  15  54  34  49  50 109  96  81  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [37 44 18 23  8 68 70  7 35 41 39 50 25 64 16 59 40 46 53 31 67 60 22 71 63 54 15 56 12 38 20 79  6 19  3  0  9 29 47 11  4 43 78 51 73 58 55 48 49 21 77 62 72 74 32 76 69 65 27 24 42 36 61 17 10 75 52  2 33 66 30  5 45 14 13 26 34 28  1 57], a_shuffle_aclus: [ 54  61  25  32  11  90  93  10  52  58  56  68  34  86  23  81  57  63  71  48  89  82  31  96  85  73  21  75  16  55  27 109   9  26   5   2  13  41  64  15   7  60 108  69  98  78  74  66  67  28 107  84  97 100  49 105  92  87  36  33  59  53  83  24  14 102  70   4  50  88  45   8  62  19  17  35  51  37   3  76]\n",
      "a_shuffle_IDXs: [39 51 18 20 41 59 62 22 46 26 77 32 42 12  1 50 78 57 65 47 60 75 69 72 35 31 23 68 19 54 33 43 28 25 73 24 21 79 17 63 48 58  6  8 56 45  4 70 13  3 44 53  0 76 55 40 61  2 74 67 71 49 14  9 30 66 52 16 36  5 34 38 11 15 10  7 64 29 27 37], a_shuffle_aclus: [ 56  69  25  27  58  81  84  31  63  35 107  49  59  16   3  68 108  76  87  64  82 102  92  97  52  48  32  90  26  73  50  60  37  34  98  33  28 109  24  85  66  78   9  11  75  62   7  93  17   5  61  71   2 105  74  57  83   4 100  89  96  67  19  13  45  88  70  23  53   8  51  55  15  21  14  10  86  41  36  54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 18 40 28 74 38 43 70 52 15 67 76 20 35 63 71 72 69 14 50 44 16 58 26 39 45 60 64 57 79 30 78  4 32  8  0 11 75 49 47  7  5 25 77 61 17  1  9 42 54 68 29 73 31 10 21 12 13 37 65 34 66 46 19  6 23  3  2 48 59 56 36 24 53 55 33 27 22 62 51], a_shuffle_aclus: [ 58  25  57  37 100  55  60  93  70  21  89 105  27  52  85  96  97  92  19  68  61  23  78  35  56  62  82  86  76 109  45 108   7  49  11   2  15 102  67  64  10   8  34 107  83  24   3  13  59  73  90  41  98  48  14  28  16  17  54  87  51  88  63  26   9  32   5   4  66  81  75  53  33  71  74  50  36  31  84  69]\n",
      "a_shuffle_IDXs: [70 67 35 77 33 14 45 23 51 24 46 55  9 58 61 12  0 11 36 32 17 76 49 41 64 50  2 21 31  6 16 57 68 37 19 34 15 38  5 43 69 54  7 44  4 66 27 65 74 59 47 26 62 39 13 20 48 25 10 73  1  8 60 40 79 42 71 28  3 72 18 22 52 56 63 30 78 53 75 29], a_shuffle_aclus: [ 93  89  52 107  50  19  62  32  69  33  63  74  13  78  83  16   2  15  53  49  24 105  67  58  86  68   4  28  48   9  23  76  90  54  26  51  21  55   8  60  92  73  10  61   7  88  36  87 100  81  64  35  84  56  17  27  66  34  14  98   3  11  82  57 109  59  96  37   5  97  25  31  70  75  85  45 108  71 102  41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 5 36 54 73 18  3 69 78  8 76 52 48  6  0 64 40 32 27 37 11 50 43 10 68 23  1 26 46 71 77 65 34 31 15 21 24 59 22 41 79 20 44 72 14 63 70 12 39 19 56 57 42 55  4 35 75 49 45 25 47 28 53 51  9 66 74 38 61 29 13 30 60  2 58  7 16 67 33 17 62], a_shuffle_aclus: [  8  53  73  98  25   5  92 108  11 105  70  66   9   2  86  57  49  36  54  15  68  60  14  90  32   3  35  63  96 107  87  51  48  21  28  33  81  31  58 109  27  61  97  19  85  93  16  56  26  75  76  59  74   7  52 102  67  62  34  64  37  71  69  13  88 100  55  83  41  17  45  82   4  78  10  23  89  50  24  84]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6  4 71 45  1 60 53 37 62 70 78  2 11 79 75  8 14 13 73 25 47  7 33 10 26 29 32 65 31 56 59  9 74 58 54 20 18 22 48 43 28 49 44 15 68 42 21 17 23  5 76 12 57 64 27  3 51 34 61 46 40 52 39 19 35 55 16 66 67 63  0 24 30 77 69 50 36 41 38 72], a_shuffle_aclus: [  9   7  96  62   3  82  71  54  84  93 108   4  15 109 102  11  19  17  98  34  64  10  50  14  35  41  49  87  48  75  81  13 100  78  73  27  25  31  66  60  37  67  61  21  90  59  28  24  32   8 105  16  76  86  36   5  69  51  83  63  57  70  56  26  52  74  23  88  89  85   2  33  45 107  92  68  53  58  55  97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 12 79 46 41 54 20 58 26 65 38 34 57 59 76 48 14 64 23 44 27 24 15 37 73 51 61 67 19 50 47 56 53 16  0 39  4 31 11 62 30 10 72 63  3  6  2 22 25 60 69  1 71 13 74  9 49 42 68 35 32 33 75 55 43 29 78  5 36 70 21 17 45 52 66 18 77 28 40  7], a_shuffle_aclus: [ 11  16 109  63  58  73  27  78  35  87  55  51  76  81 105  66  19  86  32  61  36  33  21  54  98  69  83  89  26  68  64  75  71  23   2  56   7  48  15  84  45  14  97  85   5   9   4  31  34  82  92   3  96  17 100  13  67  59  90  52  49  50 102  74  60  41 108   8  53  93  28  24  62  70  88  25 107  37  57  10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4 41 64 19 49  3 45 56 54 50 48  1 53  9  6 77 35 43 42 61 68 25 23 31 21 76 73 10 57 70 52 16 32 14 12 51 39 74  7 11 63 71  8 36 13 18 37 67 34 55 66 24 59 75  5 46 60 20 15 58 30 72 27 47 26 78 29 79 38 62 65 69 28 40  0  2 44 33 17 22], a_shuffle_aclus: [  7  58  86  26  67   5  62  75  73  68  66   3  71  13   9 107  52  60  59  83  90  34  32  48  28 105  98  14  76  93  70  23  49  19  16  69  56 100  10  15  85  96  11  53  17  25  54  89  51  74  88  33  81 102   8  63  82  27  21  78  45  97  36  64  35 108  41 109  55  84  87  92  37  57   2   4  61  50  24  31]\n",
      "a_shuffle_IDXs: [20 63 44 47  5 33 74 77 17 38 67 79 30 49 75 58 72  1 69 59 73 18 76 29 11 19 60 34 36  6 27 57 66 25 12 23 56 53  7 26 15 41 48 43 31 13 24 61 71  9 46 35  8  3  2 70 16 45 51 55 42 32 52 28 21 10 37 22 68 62 65  0 50 78 54  4 64 40 39 14], a_shuffle_aclus: [ 27  85  61  64   8  50 100 107  24  55  89 109  45  67 102  78  97   3  92  81  98  25 105  41  15  26  82  51  53   9  36  76  88  34  16  32  75  71  10  35  21  58  66  60  48  17  33  83  96  13  63  52  11   5   4  93  23  62  69  74  59  49  70  37  28  14  54  31  90  84  87   2  68 108  73   7  86  57  56  19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 76 17 77 58 54 73 55 49 57 40  1 25 74 36 32 34 68 10 43  3 71 51 27  0 11  2 46 30 63 38 24 26 28 13 41 72 75 37 44 42 53 39 22 69 45 79 61 21  7 60 23 66  4 67 12 20  9 29 35 48 59 64 33 65  6 31 14 62  5 18 52 16  8 47 15 78 70 19 50], a_shuffle_aclus: [ 75 105  24 107  78  73  98  74  67  76  57   3  34 100  53  49  51  90  14  60   5  96  69  36   2  15   4  63  45  85  55  33  35  37  17  58  97 102  54  61  59  71  56  31  92  62 109  83  28  10  82  32  88   7  89  16  27  13  41  52  66  81  86  50  87   9  48  19  84   8  25  70  23  11  64  21 108  93  26  68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 70 72 45 58 24 74 46 33 22 57 38 48 53 12  8  2 19 51 21 29 43 26 35 28 76  7 34 18 41 71 11 79 17 77  1 20  0 44 64 47 73  4 10 40 16 25 42 27 68 39 60 31 67 37 30 55 62 54 61 65 23 66 14 59 50 49  6 15 63 56 52 13 69 78 36  3 75  5 32], a_shuffle_aclus: [ 13  93  97  62  78  33 100  63  50  31  76  55  66  71  16  11   4  26  69  28  41  60  35  52  37 105  10  51  25  58  96  15 109  24 107   3  27   2  61  86  64  98   7  14  57  23  34  59  36  90  56  82  48  89  54  45  74  84  73  83  87  32  88  19  81  68  67   9  21  85  75  70  17  92 108  53   5 102   8  49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [72 13 79 33  5 22 64  6  0 37 75 62 78 11 54 40  2 61 29 59 68 69 70 52  1  9 26 44 39 30 50 76 45 24 77 65 20  3 67 35  8 49 18 31 46 28 14 23 58 38 41 25 21 15 17 53 55 56 42 57 16 71 32 74  7 12 10 47 51 36  4 73 43 19 27 34 48 63 60 66], a_shuffle_aclus: [ 97  17 109  50   8  31  86   9   2  54 102  84 108  15  73  57   4  83  41  81  90  92  93  70   3  13  35  61  56  45  68 105  62  33 107  87  27   5  89  52  11  67  25  48  63  37  19  32  78  55  58  34  28  21  24  71  74  75  59  76  23  96  49 100  10  16  14  64  69  53   7  98  60  26  36  51  66  85  82  88]\n",
      "a_shuffle_IDXs: [26 71 44  9 39 78 10 11 57 37 17 42 45 55 75  0 13 76 77 32 16 43 38 20 25  6 27 64 46 35 14 47 74  3 60 48 68 56 31 22 58 21 52 30  1 51 12 70 59 66 62 50 28 79 65 53 67 41 15  4 54 49 73 72  5 36 61 24 29 69 63 33  7 34 18 23 19  2 40  8], a_shuffle_aclus: [ 35  96  61  13  56 108  14  15  76  54  24  59  62  74 102   2  17 105 107  49  23  60  55  27  34   9  36  86  63  52  19  64 100   5  82  66  90  75  48  31  78  28  70  45   3  69  16  93  81  88  84  68  37 109  87  71  89  58  21   7  73  67  98  97   8  53  83  33  41  92  85  50  10  51  25  32  26   4  57  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [62 50 22 36 59 24 18 52 61 68 21  6 31  3 27 35 10  4 53 72 43 30 37 47 42 78  7  9 69 56 79 67 75 20 60 33 51 28 11 38 34 39 65 19 46 54 25 55 70  2 41 23 17 58 12 32  5 13 64 16 74 48 29  8 73 26 63 77 71  0 45 15 76 14  1 44 40 66 57 49], a_shuffle_aclus: [ 84  68  31  53  81  33  25  70  83  90  28   9  48   5  36  52  14   7  71  97  60  45  54  64  59 108  10  13  92  75 109  89 102  27  82  50  69  37  15  55  51  56  87  26  63  73  34  74  93   4  58  32  24  78  16  49   8  17  86  23 100  66  41  11  98  35  85 107  96   2  62  21 105  19   3  61  57  88  76  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [14  6 67 47 30 59 18 79 16  9 29 69 32 38 77 75 10 73 53 37 25 40 24  5 31 12 26 63 72 22 28 43 65 64 20 41  0 74  1 35 62 55 39 34 61 50 48 52 78 44 60 54 45 23  7 56 70 21  4 76 66  3 15 71 42  2 58 57 33 19  8 11 36 68 27 13 46 49 51 17], a_shuffle_aclus: [ 19   9  89  64  45  81  25 109  23  13  41  92  49  55 107 102  14  98  71  54  34  57  33   8  48  16  35  85  97  31  37  60  87  86  27  58   2 100   3  52  84  74  56  51  83  68  66  70 108  61  82  73  62  32  10  75  93  28   7 105  88   5  21  96  59   4  78  76  50  26  11  15  53  90  36  17  63  67  69  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [68 49 71 52 72 77  1  4 36 70 74 51 53 20  2 40 24 42 65 27 59 22 69 64 41 35  8 62 37 47 54 39 25 55  9 16 28 61 30 18 73 11 75 33 21 48 67 66 15 29 78 34 23 26 31  7  5 43 32  6  3 63 14 56 46 19 10 60 13 57 50 12 44 38 79  0 45 76 58 17], a_shuffle_aclus: [ 90  67  96  70  97 107   3   7  53  93 100  69  71  27   4  57  33  59  87  36  81  31  92  86  58  52  11  84  54  64  73  56  34  74  13  23  37  83  45  25  98  15 102  50  28  66  89  88  21  41 108  51  32  35  48  10   8  60  49   9   5  85  19  75  63  26  14  82  17  76  68  16  61  55 109   2  62 105  78  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [12 60 63 14 40 79 74 78 58 17 38  6 61 77  7 31 68 43 28 76 66 57 32 41 10  9 55 42 25 69 50 48 29 26 36 75 71 20 35 73 56 39 34  8  4 27 67 30 59 45 47  0  1 72  5 11 24 65 51  3  2 62 22 21 49 18 54 13 64 15 70 52 37 33 44 16 19 23 46 53], a_shuffle_aclus: [ 16  82  85  19  57 109 100 108  78  24  55   9  83 107  10  48  90  60  37 105  88  76  49  58  14  13  74  59  34  92  68  66  41  35  53 102  96  27  52  98  75  56  51  11   7  36  89  45  81  62  64   2   3  97   8  15  33  87  69   5   4  84  31  28  67  25  73  17  86  21  93  70  54  50  61  23  26  32  63  71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 47 45 59 42  1 79 36 61 58 16 27  8 64  0 21  7 78 69 44 28 67 60 43 72 11 55 37 26 48 77 38  6 46  5 75 56 52 39 65 33 40 71 25 41 68 76 34  3 50 74 19  9 10 63 14 51 73 23 57 32 31 12 62 13 15 49  2 18 35 20 66  4 30 54 53 70 29 22 24], a_shuffle_aclus: [ 24  64  62  81  59   3 109  53  83  78  23  36  11  86   2  28  10 108  92  61  37  89  82  60  97  15  74  54  35  66 107  55   9  63   8 102  75  70  56  87  50  57  96  34  58  90 105  51   5  68 100  26  13  14  85  19  69  98  32  76  49  48  16  84  17  21  67   4  25  52  27  88   7  45  73  71  93  41  31  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [19 62 38 45 76 46 60 79 12 26 47 21 48 71 50  9 72 34 49 22 54 30 37 15 65 31 40 16 77 11 74 10 57 29  8 59 44 43 70  7  2 69  4 64 32 28 67 24 68 18 73 63  5 23 35 78 39 27 52 36  3  1 66 58 41 42 33 14 20 13 56 53 55 75 61  6  0 17 25 51], a_shuffle_aclus: [ 26  84  55  62 105  63  82 109  16  35  64  28  66  96  68  13  97  51  67  31  73  45  54  21  87  48  57  23 107  15 100  14  76  41  11  81  61  60  93  10   4  92   7  86  49  37  89  33  90  25  98  85   8  32  52 108  56  36  70  53   5   3  88  78  58  59  50  19  27  17  75  71  74 102  83   9   2  24  34  69]\n",
      "a_shuffle_IDXs: [41 73 75 34 22 57 74 28 38 16 79 69 11 12  0 35 62 50 55 40  8 20 49 68  1 33 77 52 23 71 44 60  3  2 15 63  6  5 24 48 65 32 25 10 76 27 29 47 51 59 14 54 56 67 17 78 13 21 19 64 53 39 30  7 72  4 61 37 42 45 66 26  9 58 46 43 31 18 70 36], a_shuffle_aclus: [ 58  98 102  51  31  76 100  37  55  23 109  92  15  16   2  52  84  68  74  57  11  27  67  90   3  50 107  70  32  96  61  82   5   4  21  85   9   8  33  66  87  49  34  14 105  36  41  64  69  81  19  73  75  89  24 108  17  28  26  86  71  56  45  10  97   7  83  54  59  62  88  35  13  78  63  60  48  25  93  53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [68 24 34 32 56 19  9 14 64 46 61 30 74 16 15  1 23 55 38  3 48 59 10 47 36 57  2 58 70 44 21 50 33 25 53 63  6 62 28 60 20 11  0 27 69 37 78 52 45 75 67 40 76  8 49 39 72 65 41  5 13 43 73 17 31 22 18 54 29  4 77 51 42 79  7 35 12 66 71 26], a_shuffle_aclus: [ 90  33  51  49  75  26  13  19  86  63  83  45 100  23  21   3  32  74  55   5  66  81  14  64  53  76   4  78  93  61  28  68  50  34  71  85   9  84  37  82  27  15   2  36  92  54 108  70  62 102  89  57 105  11  67  56  97  87  58   8  17  60  98  24  48  31  25  73  41   7 107  69  59 109  10  52  16  88  96  35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [35 53 43 25  7 49 38 59 57 60 32 21 17 51 47 44 56 11 42 76 69 48  9 54 39 30 62 20 26 63 72 10 45 75 14 79  8 68 15 31  6 78  5 23 46  0 65 24 58 37 61 18 70  2 29  3  4 16 67 66 55 28 71 74 41 52 19 50 40  1 12 36 27 73 77 13 34 33 22 64], a_shuffle_aclus: [ 52  71  60  34  10  67  55  81  76  82  49  28  24  69  64  61  75  15  59 105  92  66  13  73  56  45  84  27  35  85  97  14  62 102  19 109  11  90  21  48   9 108   8  32  63   2  87  33  78  54  83  25  93   4  41   5   7  23  89  88  74  37  96 100  58  70  26  68  57   3  16  53  36  98 107  17  51  50  31  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [18 53 76 35 55  1 29 44  2 74 28  5 32  3  4 38 31 66 36 43 16 64 33 77 71 14 70 12 27 65 58 30 57 42 13 11 20 22 68 17 23 79 34 15  0 41  8  9 60 52 49 59 45 61 48 26 72 67 54 47 50 51 40 39 19 56 69 78  6 62 73 37 63 24 10 46 75 25 21  7], a_shuffle_aclus: [ 25  71 105  52  74   3  41  61   4 100  37   8  49   5   7  55  48  88  53  60  23  86  50 107  96  19  93  16  36  87  78  45  76  59  17  15  27  31  90  24  32 109  51  21   2  58  11  13  82  70  67  81  62  83  66  35  97  89  73  64  68  69  57  56  26  75  92 108   9  84  98  54  85  33  14  63 102  34  28  10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63  4 15 26  5 68 52 44 42 55  8 33 43  1 18 74 14 28 70 38 78 41 60 46 48 72 65 69 30 17 49 66 36  0 40 71 29 64 77 50 79  7 27 31 19 67 13 32 37 12 53 62 76 34 45 73  9  6  3 75 57 25 58 22 24 35 56 10 54 21 61 11  2 51 23 59 16 39 47 20], a_shuffle_aclus: [ 85   7  21  35   8  90  70  61  59  74  11  50  60   3  25 100  19  37  93  55 108  58  82  63  66  97  87  92  45  24  67  88  53   2  57  96  41  86 107  68 109  10  36  48  26  89  17  49  54  16  71  84 105  51  62  98  13   9   5 102  76  34  78  31  33  52  75  14  73  28  83  15   4  69  32  81  23  56  64  27]\n",
      "a_shuffle_IDXs: [77 15 12 78 22 35 45 75  0 47 46 57 23 73 50 34 38 68 33 79 13 69 66 72 10 41 52 30 20 29 24  1 17 14  5 71 54 64 18  2 53 11 67 70 27 36 43 44 37 32 63 51 76 74 65 62 49 39 19 26  7 40 48 55 42 59  8 28 60  9 56  4 61  6 25  3 16 58 31 21], a_shuffle_aclus: [107  21  16 108  31  52  62 102   2  64  63  76  32  98  68  51  55  90  50 109  17  92  88  97  14  58  70  45  27  41  33   3  24  19   8  96  73  86  25   4  71  15  89  93  36  53  60  61  54  49  85  69 105 100  87  84  67  56  26  35  10  57  66  74  59  81  11  37  82  13  75   7  83   9  34   5  23  78  48  28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60  5 75 62 50 48 76 22 29 64 12 17 68  9 58 18 67  0 27 43 59 53 56 70 44 15 57 38 49 25 39 63 16 41 69 10 77 55  8 19  2 14 33 52 78 24 71 26 54 37  6 42 36  7 51 47 20 79 32 28 23 11 65 45 66 21  1 34 74  3 72 30 40 35 61 31 73  4 46 13], a_shuffle_aclus: [ 82   8 102  84  68  66 105  31  41  86  16  24  90  13  78  25  89   2  36  60  81  71  75  93  61  21  76  55  67  34  56  85  23  58  92  14 107  74  11  26   4  19  50  70 108  33  96  35  73  54   9  59  53  10  69  64  27 109  49  37  32  15  87  62  88  28   3  51 100   5  97  45  57  52  83  48  98   7  63  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 12 74 54 45 24 71 39 38 28  9 13 70 62 41 26 46 29 78  4  8  1 51 66  2 21  0 27 22 48 69 55 15  7 10 61 59 17 32 34 25 75 36 73 79 37 40 33 16 64  5 76 35 31 42 52 49 14 18 50 77  3 67 44 19 72  6 60 58 43 68 57 65 53 63 20 23 56 47 30], a_shuffle_aclus: [ 15  16 100  73  62  33  96  56  55  37  13  17  93  84  58  35  63  41 108   7  11   3  69  88   4  28   2  36  31  66  92  74  21  10  14  83  81  24  49  51  34 102  53  98 109  54  57  50  23  86   8 105  52  48  59  70  67  19  25  68 107   5  89  61  26  97   9  82  78  60  90  76  87  71  85  27  32  75  64  45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 33 26 14 19 39 71  8 59 16 66 67 46 28 57  1 73 18  9 72 60 32 79 41  0  3 61 49 64 22 75 29 55  2 37 27 31 68 25 69  7 17 40 65 76 44 62 52 53 21 78 10 24 70 63 47 56 43 11 13 45 42 34  5 30  6 23 20 48 36 54 77  4 15 74 58 12 38 35 51], a_shuffle_aclus: [ 68  50  35  19  26  56  96  11  81  23  88  89  63  37  76   3  98  25  13  97  82  49 109  58   2   5  83  67  86  31 102  41  74   4  54  36  48  90  34  92  10  24  57  87 105  61  84  70  71  28 108  14  33  93  85  64  75  60  15  17  62  59  51   8  45   9  32  27  66  53  73 107   7  21 100  78  16  55  52  69]\n",
      "a_shuffle_IDXs: [41 77 11 27  3 17  2 36 26 62 66  0 44 69 47 29 68 63 45 37 51 24 42 28 25 50 31 59 12 55 72 79 52 20 22 15 43 46 78 10  9 73 70 23 64 35  4 21 33 53 76 14 30 19 49  1 13 54  6 65 18 67 40 48 16 56 60 71 34 38 39 74 58 57 32  5  7  8 75 61], a_shuffle_aclus: [ 58 107  15  36   5  24   4  53  35  84  88   2  61  92  64  41  90  85  62  54  69  33  59  37  34  68  48  81  16  74  97 109  70  27  31  21  60  63 108  14  13  98  93  32  86  52   7  28  50  71 105  19  45  26  67   3  17  73   9  87  25  89  57  66  23  75  82  96  51  55  56 100  78  76  49   8  10  11 102  83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 36 13 30 39 41  8 19 28 79 35 48 40 44  9 34 14 72  4 37 60 59 76 73 56 20 62 75  7 63 54 32 12 22 25  5 69 24 29 61  6  3 18 55 46 53 38 70 31 50 52 33 26 16 65 74  2 67 64 15  0 45 49 23  1 51 42 47 58 43 11 77 10 71 57 21 68 17 78 66], a_shuffle_aclus: [ 36  53  17  45  56  58  11  26  37 109  52  66  57  61  13  51  19  97   7  54  82  81 105  98  75  27  84 102  10  85  73  49  16  31  34   8  92  33  41  83   9   5  25  74  63  71  55  93  48  68  70  50  35  23  87 100   4  89  86  21   2  62  67  32   3  69  59  64  78  60  15 107  14  96  76  28  90  24 108  88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 54 26 12 30  4 58 79 56 28 74 11 41 55  9 15  8 19 16 50 47 61 76 22 75 46 21 39  5 52 10 62 45 38 34 48  6 63 23 66 70 17 20  0 65 31  3 14 29 77 73 51 78 13 32 25 40 67 44 33 18 60  1 42 72 59 37 69 35 64 57 43 36  7 24 49 53 68 71 27], a_shuffle_aclus: [  4  73  35  16  45   7  78 109  75  37 100  15  58  74  13  21  11  26  23  68  64  83 105  31 102  63  28  56   8  70  14  84  62  55  51  66   9  85  32  88  93  24  27   2  87  48   5  19  41 107  98  69 108  17  49  34  57  89  61  50  25  82   3  59  97  81  54  92  52  86  76  60  53  10  33  67  71  90  96  36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 51  6 16 64 79 17 50 63 21 43 46 36 55 19 45 23 76 42 39 35 74 67  7 59 52 56 26  4 34 27 15 38 33 11 30 41 12 18 54 57  1 29  2 48 72 32 14 58 13 53 78 20 60 25 37 65  0 69 31 68 10 62 28 44  5  3 47 77 49 66 22 71 75  9 40 70  8 73 24], a_shuffle_aclus: [ 83  69   9  23  86 109  24  68  85  28  60  63  53  74  26  62  32 105  59  56  52 100  89  10  81  70  75  35   7  51  36  21  55  50  15  45  58  16  25  73  76   3  41   4  66  97  49  19  78  17  71 108  27  82  34  54  87   2  92  48  90  14  84  37  61   8   5  64 107  67  88  31  96 102  13  57  93  11  98  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [43 78 19 72 68 26 28  7 13 32 31 24  1 66 67 56 16 30  6 48 17 76 10  8 14 11 15 23  5 29 45 22 20 42 46 47 38 39 37 50 54 51 63 12 70 61 52 60  9 41 73  2 18 44 64 62  4 35 79 36 59 77 71 21 65 34 25 27 74 58 69  0 53 49 55 75 57  3 33 40], a_shuffle_aclus: [ 60 108  26  97  90  35  37  10  17  49  48  33   3  88  89  75  23  45   9  66  24 105  14  11  19  15  21  32   8  41  62  31  27  59  63  64  55  56  54  68  73  69  85  16  93  83  70  82  13  58  98   4  25  61  86  84   7  52 109  53  81 107  96  28  87  51  34  36 100  78  92   2  71  67  74 102  76   5  50  57]\n",
      "a_shuffle_IDXs: [42 75 14 37 18 68 76 72 70 26 43 40 59 46 67 57  1 39 74 44 23 15 64 55 33 36 17  9  0 58 49 12 61 77 25 79 50 20 34 16 13 24 21 62  4 19 53  5 54 63 41 71 56 28  2 38 10 27 51 78 60 69 11  3 48 29 65  6 73 22 35 47 52  8  7 32 45 31 30 66], a_shuffle_aclus: [ 59 102  19  54  25  90 105  97  93  35  60  57  81  63  89  76   3  56 100  61  32  21  86  74  50  53  24  13   2  78  67  16  83 107  34 109  68  27  51  23  17  33  28  84   7  26  71   8  73  85  58  96  75  37   4  55  14  36  69 108  82  92  15   5  66  41  87   9  98  31  52  64  70  11  10  49  62  48  45  88]\n",
      "a_shuffle_IDXs: [ 4 23 52 57  6  1 20 32 24 36  5 71 78  3 34 31 15 44 45 63 48 72 39 22 13 69 65 29 68 42 11 37  9 56 70 59 18 75 41  8 60 49  2 27 33 55 50 74 17 51 30  7 38 76 26 19 25 61 12 67 40 66 73 58 77 79 62 47 46 21 10 54 28 53 64 14  0 16 35 43], a_shuffle_aclus: [  7  32  70  76   9   3  27  49  33  53   8  96 108   5  51  48  21  61  62  85  66  97  56  31  17  92  87  41  90  59  15  54  13  75  93  81  25 102  58  11  82  67   4  36  50  74  68 100  24  69  45  10  55 105  35  26  34  83  16  89  57  88  98  78 107 109  84  64  63  28  14  73  37  71  86  19   2  23  52  60]\n",
      "a_shuffle_IDXs: [59 73 58 63 39 19 37  8 45 77 29 32 26 49 74 67 72 47  0 70 42 10 18 55  7 30 12 31 41 20 61 38 46 22 62 33  3 43 16 76 65 68 71 17 48 34 50 54 13 78 36 64 75 57 35  9 25  6 53 21  5 56 23 66 27 14 11 40  2 51 44  4 28 60 24 52 69  1 15 79], a_shuffle_aclus: [ 81  98  78  85  56  26  54  11  62 107  41  49  35  67 100  89  97  64   2  93  59  14  25  74  10  45  16  48  58  27  83  55  63  31  84  50   5  60  23 105  87  90  96  24  66  51  68  73  17 108  53  86 102  76  52  13  34   9  71  28   8  75  32  88  36  19  15  57   4  69  61   7  37  82  33  70  92   3  21 109]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [29 52  7  1 65  2 64 77 45 26 13 35 27 18 28 19  4  5 66 51 55  3 12 39 11 60 40 50 49 70  0 24 33 78 53 42 31 56 71 73 16 37 22 32 67 34 47 68  9 62 69 72 54 58 46 41 25 43 10 14 48 36  8 75 61 23 15 79 44 63 76 30 59 38  6 57 74 21 20 17], a_shuffle_aclus: [ 41  70  10   3  87   4  86 107  62  35  17  52  36  25  37  26   7   8  88  69  74   5  16  56  15  82  57  68  67  93   2  33  50 108  71  59  48  75  96  98  23  54  31  49  89  51  64  90  13  84  92  97  73  78  63  58  34  60  14  19  66  53  11 102  83  32  21 109  61  85 105  45  81  55   9  76 100  28  27  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [16 15 18 68  6 31 38 66 71 22 42 73 14 63 45 23 12  7 54 17 29 30 11  0  2 57 70 41 77 21  9 72 24 28 61 34 25 49 40 48  5  3  4 43 26 20 78 53 59 47 65  1 56 62 69 74 52 50 35 79 36 75 39 51 13 37 46 58 27  8 32 55 10 76 33 44 19 60 67 64], a_shuffle_aclus: [ 23  21  25  90   9  48  55  88  96  31  59  98  19  85  62  32  16  10  73  24  41  45  15   2   4  76  93  58 107  28  13  97  33  37  83  51  34  67  57  66   8   5   7  60  35  27 108  71  81  64  87   3  75  84  92 100  70  68  52 109  53 102  56  69  17  54  63  78  36  11  49  74  14 105  50  61  26  82  89  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 49 45 25 34  7  8 62 35 61 16 69 53 10 68 33 64 75 54 58 50 18 28 29 65 26 43  4 42 41  5 52  9 19 71 22 48 47 51 40 32 27 24 66 20 67 21 13 77 44 76 59 11 30  3 12 72 70 36 31 38 74 17 79  6 78 39 60 73 56 23 55 63 15 14 57  2 37  1  0], a_shuffle_aclus: [ 63  67  62  34  51  10  11  84  52  83  23  92  71  14  90  50  86 102  73  78  68  25  37  41  87  35  60   7  59  58   8  70  13  26  96  31  66  64  69  57  49  36  33  88  27  89  28  17 107  61 105  81  15  45   5  16  97  93  53  48  55 100  24 109   9 108  56  82  98  75  32  74  85  21  19  76   4  54   3   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 65 30 73 46 16  3 59  4 32 51 44 62 45 17 37 74 36 78 22 40 11 29 75  0 50 49 42 38 18 58 33 31 43 79 52  2 56 23 20 21 12 60 13 69 26 66  6 14 67 39 15  5 57 34 71 48 55 35 64 76  9 54 28 27  1 63 53 24  8 41 47 61  7 19 25 77 72 68 70], a_shuffle_aclus: [ 14  87  45  98  63  23   5  81   7  49  69  61  84  62  24  54 100  53 108  31  57  15  41 102   2  68  67  59  55  25  78  50  48  60 109  70   4  75  32  27  28  16  82  17  92  35  88   9  19  89  56  21   8  76  51  96  66  74  52  86 105  13  73  37  36   3  85  71  33  11  58  64  83  10  26  34 107  97  90  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 27  8 23 25 52 20 36 61 65 70 50 28 29 75 51 11 10 57 66 69 63 12 14  2 22 78 13 55 49 26 17 21 77 15 42  9  1 71  4 53 73 40 48 16 64 43 30  5 68 18 58 45 59  6 31 44 62  0 76 34 60  7 46 37 19 39  3 74 38 72 33 24 56 32 47 54 79 67 35], a_shuffle_aclus: [ 58  36  11  32  34  70  27  53  83  87  93  68  37  41 102  69  15  14  76  88  92  85  16  19   4  31 108  17  74  67  35  24  28 107  21  59  13   3  96   7  71  98  57  66  23  86  60  45   8  90  25  78  62  81   9  48  61  84   2 105  51  82  10  63  54  26  56   5 100  55  97  50  33  75  49  64  73 109  89  52]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [76 41 70 39 38 36 47 31 25 57  3 60 17  0  7 51 37 44 12 53 78 18 20 22 64 45 59 56 52 63 72 75 71 11 24 13 46 21 55 54 26 74 73  8 19 43  1 69 48 29  2 27 23  6 10 79 40 32 34 42  4 49 50 77 62 28 15 68 16 14  5 61 65 35 33 58 67 30  9 66], a_shuffle_aclus: [105  58  93  56  55  53  64  48  34  76   5  82  24   2  10  69  54  61  16  71 108  25  27  31  86  62  81  75  70  85  97 102  96  15  33  17  63  28  74  73  35 100  98  11  26  60   3  92  66  41   4  36  32   9  14 109  57  49  51  59   7  67  68 107  84  37  21  90  23  19   8  83  87  52  50  78  89  45  13  88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [25 12 40 59 51 52 50 39  5  9 77 72 55 58 34 79 65 69 67 49 44 26 30 73 24 16 11  0 53  4 76 62 64 21  1  7 63 37 66 61 74 71 70 54 41 48 19 60 15 56 13 38  3  6 47 35 32 23 45 20 36 42  8 31 28 78 68 43 27 14 33 46 22  2 75 29 10 57 18 17], a_shuffle_aclus: [ 34  16  57  81  69  70  68  56   8  13 107  97  74  78  51 109  87  92  89  67  61  35  45  98  33  23  15   2  71   7 105  84  86  28   3  10  85  54  88  83 100  96  93  73  58  66  26  82  21  75  17  55   5   9  64  52  49  32  62  27  53  59  11  48  37 108  90  60  36  19  50  63  31   4 102  41  14  76  25  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65  2 54 56 43 59 38 20 12 36 28 79  3 69 51 24 72  9 70 52 32 45 21  8 10 53 55 75  4 77 66 57 71 17  5 26 33 35 11  0 44 58 50 64 40 23 62 18 19 78 49 39 73 25 67 60 34 15 13 16 76 48  7 74  1 22 61 68 37 27 63 30 42 46 29 14 47 41 31  6], a_shuffle_aclus: [ 87   4  73  75  60  81  55  27  16  53  37 109   5  92  69  33  97  13  93  70  49  62  28  11  14  71  74 102   7 107  88  76  96  24   8  35  50  52  15   2  61  78  68  86  57  32  84  25  26 108  67  56  98  34  89  82  51  21  17  23 105  66  10 100   3  31  83  90  54  36  85  45  59  63  41  19  64  58  48   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60 66 61 64 16 59 19  7 31 10 73  4 46 35 27 37 67 21 54 52 39 12 62  1 74 34 17 51 22 79  2 57 65 70 15 77 25 36 53 41 75 13 20 50 32 63  0 48 78 24 76 71 55 23 69 56 58 29 26 49 30 38 47  6 45 44 43 40 28  3 14 11 68 18  8 33 72  5 42  9], a_shuffle_aclus: [ 82  88  83  86  23  81  26  10  48  14  98   7  63  52  36  54  89  28  73  70  56  16  84   3 100  51  24  69  31 109   4  76  87  93  21 107  34  53  71  58 102  17  27  68  49  85   2  66 108  33 105  96  74  32  92  75  78  41  35  67  45  55  64   9  62  61  60  57  37   5  19  15  90  25  11  50  97   8  59  13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24  5 28 41 47 37 15 26 69 67 63 52 59 72 39 10 36 53 66 57 45 50 43 75  3 18  8 77 14 11 78 34 64 22 68 35 30 46  7 49 74 60  1 73 33 65 79 29  6  4 19 44 42 20 40 55 71 21 12 25  2 61 32 54 70 16 27 58 48 51 13 17 62 76 31 23 56 38  9  0], a_shuffle_aclus: [ 33   8  37  58  64  54  21  35  92  89  85  70  81  97  56  14  53  71  88  76  62  68  60 102   5  25  11 107  19  15 108  51  86  31  90  52  45  63  10  67 100  82   3  98  50  87 109  41   9   7  26  61  59  27  57  74  96  28  16  34   4  83  49  73  93  23  36  78  66  69  17  24  84 105  48  32  75  55  13   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 53 28 73 50 20 23 10 22 37 66 52 13 51 12 69 16 25 49 63 36 41 61 64 74 67 31 44 45 54  6 55  2 60 32 75 39 70  8 71 27 76 34 77 14 40  7 72 15 17 56 43 30 58 24  9 78  0 11 35 79 18 59 46 47 38 68  5 19 48 26 57 62 42 21  4  1 33  3 29], a_shuffle_aclus: [ 87  71  37  98  68  27  32  14  31  54  88  70  17  69  16  92  23  34  67  85  53  58  83  86 100  89  48  61  62  73   9  74   4  82  49 102  56  93  11  96  36 105  51 107  19  57  10  97  21  24  75  60  45  78  33  13 108   2  15  52 109  25  81  63  64  55  90   8  26  66  35  76  84  59  28   7   3  50   5  41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 25 14 35 33 17 76 78 69 31 68 36 57 79 53 43  9  5 29 26 24 19 65 13 34 55 66 74 46 38 64 12 56 70 50  2 73 58 45 15  8 47 21 54  7 49 60 28 30 10 42 63 51 39  0 52 77 48 72  1 41 20 59 23 75 61 18 40 11 27 22 67 62  6 16 37 44 32 71  4], a_shuffle_aclus: [  5  34  19  52  50  24 105 108  92  48  90  53  76 109  71  60  13   8  41  35  33  26  87  17  51  74  88 100  63  55  86  16  75  93  68   4  98  78  62  21  11  64  28  73  10  67  82  37  45  14  59  85  69  56   2  70 107  66  97   3  58  27  81  32 102  83  25  57  15  36  31  89  84   9  23  54  61  49  96   7]\n",
      "a_shuffle_IDXs: [ 7 63 76 27 34 72 11 48 42 30 52 19 16  2 33 65 24 70 69 54 36  0 38 50 64 74 57 59 23 75 62 20 78 31  1  5  9 17 28 21 61 68 37 47 77  8 26 58  4 29  3 79 40 45 10 25 60 41 39 73 32 71 22 49 43 35 12 14 15 53 56 44 18 13 51 66  6 67 46 55], a_shuffle_aclus: [ 10  85 105  36  51  97  15  66  59  45  70  26  23   4  50  87  33  93  92  73  53   2  55  68  86 100  76  81  32 102  84  27 108  48   3   8  13  24  37  28  83  90  54  64 107  11  35  78   7  41   5 109  57  62  14  34  82  58  56  98  49  96  31  67  60  52  16  19  21  71  75  61  25  17  69  88   9  89  63  74]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 26 45 46  6 16 57 31 69 72 29 38 30 32 62  0 22 15 27 20 70 67 49 43 44 55 34 79 13 51 37  9 76  7 19 58 56 21 35 10 25 63 71  8 11 53 61 24 60 64 33 41 47 77 40 68  3 78 52 66 36 17 23 59 12 39 74  4  2 18 48  1 65  5 54 42 75 14 28 73], a_shuffle_aclus: [ 68  35  62  63   9  23  76  48  92  97  41  55  45  49  84   2  31  21  36  27  93  89  67  60  61  74  51 109  17  69  54  13 105  10  26  78  75  28  52  14  34  85  96  11  15  71  83  33  82  86  50  58  64 107  57  90   5 108  70  88  53  24  32  81  16  56 100   7   4  25  66   3  87   8  73  59 102  19  37  98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [40  7 66 62 73 23 12 75 46 61 24 69 74 13 36  5 70 45 67  8 31 68 60 15 32 22 27  6 52 37 18 58 49 51 34 30 26 16 79 14  1 29  4  2 77 54 39 19 48  0 71 78 41 57 21  9 72 25 10 64 55 33 20 28 63 42 76 35 59 44 43 56 50 38  3 53 11 17 65 47], a_shuffle_aclus: [ 57  10  88  84  98  32  16 102  63  83  33  92 100  17  53   8  93  62  89  11  48  90  82  21  49  31  36   9  70  54  25  78  67  69  51  45  35  23 109  19   3  41   7   4 107  73  56  26  66   2  96 108  58  76  28  13  97  34  14  86  74  50  27  37  85  59 105  52  81  61  60  75  68  55   5  71  15  24  87  64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 27 28 77 60 68 26 72 22 30 58 38 46 32 55 10 33 23 62 61  4 71 34  1 56  6 54 21 65 45 31 75 12 39  3  9 50 43 29 14  8 70 18 73 40 79 37 36 53 25 19 41 76 35 24 59 13 69 42  0 15 48 57 63 74 16 66 47 67 51  5 49 52 78  2  7 17 20 44 11], a_shuffle_aclus: [ 86  36  37 107  82  90  35  97  31  45  78  55  63  49  74  14  50  32  84  83   7  96  51   3  75   9  73  28  87  62  48 102  16  56   5  13  68  60  41  19  11  93  25  98  57 109  54  53  71  34  26  58 105  52  33  81  17  92  59   2  21  66  76  85 100  23  88  64  89  69   8  67  70 108   4  10  24  27  61  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [20 51 59 40 75 30 11 64 36  0  7 22 54 62  2 24 34  5 70 10 71 23 72 74 28  3 65 67 50  6 66 18 19 60 29 39  8 38 77 43 73 76 16 47 44 56 31 41 42 12 55 57 45 25 69 53 46  9 26  4 58 17 68 37 32 52 15 35 14 48  1 13 79 49 78 33 61 21 27 63], a_shuffle_aclus: [ 27  69  81  57 102  45  15  86  53   2  10  31  73  84   4  33  51   8  93  14  96  32  97 100  37   5  87  89  68   9  88  25  26  82  41  56  11  55 107  60  98 105  23  64  61  75  48  58  59  16  74  76  62  34  92  71  63  13  35   7  78  24  90  54  49  70  21  52  19  66   3  17 109  67 108  50  83  28  36  85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 59  9 40 58 26 23 43 27 67  0 12 37 14 70 54 46 34 74 73  3 21  6 44 31 10 75  4 62 25 32 78 45 33 52 30 28 63 51 68  2 36 76 53 22 69  7 42 50 39 60 56  5  8 35 17 49 48 38 24 13  1 79 72 71 15 55 19 11 29 20 18 57 66 47 16 64 77 61 41], a_shuffle_aclus: [ 87  81  13  57  78  35  32  60  36  89   2  16  54  19  93  73  63  51 100  98   5  28   9  61  48  14 102   7  84  34  49 108  62  50  70  45  37  85  69  90   4  53 105  71  31  92  10  59  68  56  82  75   8  11  52  24  67  66  55  33  17   3 109  97  96  21  74  26  15  41  27  25  76  88  64  23  86 107  83  58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46 49 51 63 74 39 64 52  2 24 54 18  9 73 17 37  6 14 61 77 47 35 65 20  1 23  8 41  4 10 30 56 72 13 50 66 38 71 62 40 59 32 45 12 44 67 33 58  0 60 28 70 78 16 53 25 29 27 68  3 55 79 69 22 19 48  5 31 11 21 57 15 34  7 75 26 76 36 42 43], a_shuffle_aclus: [ 63  67  69  85 100  56  86  70   4  33  73  25  13  98  24  54   9  19  83 107  64  52  87  27   3  32  11  58   7  14  45  75  97  17  68  88  55  96  84  57  81  49  62  16  61  89  50  78   2  82  37  93 108  23  71  34  41  36  90   5  74 109  92  31  26  66   8  48  15  28  76  21  51  10 102  35 105  53  59  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [42 33 29 11 46 69  8 54 76 52 61  1 32 62  9 70 15  6 74 77 64 73 26 53 14 66 59  3 41 38 30 60 20 35  2  0 55 45 68  5 79 75 21 51 18 28 10 72 25 63 23 71 16 67 13 39 37 36 31  7 12 27 65 40 47 58 57 49 34 19 48 22 24 43 56  4 78 50 44 17], a_shuffle_aclus: [ 59  50  41  15  63  92  11  73 105  70  83   3  49  84  13  93  21   9 100 107  86  98  35  71  19  88  81   5  58  55  45  82  27  52   4   2  74  62  90   8 109 102  28  69  25  37  14  97  34  85  32  96  23  89  17  56  54  53  48  10  16  36  87  57  64  78  76  67  51  26  66  31  33  60  75   7 108  68  61  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [78  9 38 40 45 62 71 58  0 31 60 66 77 24 39 18 17 48 13 25 30 27 67 57 19 28  2 61 46 63 73 69 22 11  1 64 50 59  8 42 23 21  3 34 79  7 26 15  5 75 29 52 41 53 37 54 35  4 32  6 12 20 43 44 55 68 70 47 33 56 51 16 76 49 14 10 65 72 36 74], a_shuffle_aclus: [108  13  55  57  62  84  96  78   2  48  82  88 107  33  56  25  24  66  17  34  45  36  89  76  26  37   4  83  63  85  98  92  31  15   3  86  68  81  11  59  32  28   5  51 109  10  35  21   8 102  41  70  58  71  54  73  52   7  49   9  16  27  60  61  74  90  93  64  50  75  69  23 105  67  19  14  87  97  53 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7 34 58 24  6 79 76 65 54  1 39 29 69 43 21 52  3 61 32  8 15 28 78 25  5 40 33 67 17 49 68 47 51 36 77 37 45 38 20 59 14 53  0 75 44 19  2 57 13 18 60 71  4 66 63 35 41  9 55 11 42 73 48 72 46 64 50 22 70 74 30 27 62 12 16 56 23 26 10 31], a_shuffle_aclus: [ 10  51  78  33   9 109 105  87  73   3  56  41  92  60  28  70   5  83  49  11  21  37 108  34   8  57  50  89  24  67  90  64  69  53 107  54  62  55  27  81  19  71   2 102  61  26   4  76  17  25  82  96   7  88  85  52  58  13  74  15  59  98  66  97  63  86  68  31  93 100  45  36  84  16  23  75  32  35  14  48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 55 60 34 64 48 68 12 19 24 57 41 39 71 25  3 14 51 31 61 28 74 13 15 47 63 46 16 32 69 10 43 67 44 18 50 35 27 72 79 59 77 65 73 37 62 70  4 54  1  9 26  2 56 66 58  0 38 22 42  6 30  5 21 20 53 40 36 76  7 11 33 29 75 49 52 23 45 17 78], a_shuffle_aclus: [ 11  74  82  51  86  66  90  16  26  33  76  58  56  96  34   5  19  69  48  83  37 100  17  21  64  85  63  23  49  92  14  60  89  61  25  68  52  36  97 109  81 107  87  98  54  84  93   7  73   3  13  35   4  75  88  78   2  55  31  59   9  45   8  28  27  71  57  53 105  10  15  50  41 102  67  70  32  62  24 108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [69 70 72 45 17 60 37  3 58 21 19 13 55 65 25 67 52 53 63 39 78 46  9 79  6 34 14 24 47 27  5  4 54 48 68 62 15 71 75 50 35 66 36 18 30 51 29 28 20 33 42 49 16 56 10 11 77  7 23 61  0 43 41 38 44 73 76  1  2 32 57 74 64 26 12 31 59 40  8 22], a_shuffle_aclus: [ 92  93  97  62  24  82  54   5  78  28  26  17  74  87  34  89  70  71  85  56 108  63  13 109   9  51  19  33  64  36   8   7  73  66  90  84  21  96 102  68  52  88  53  25  45  69  41  37  27  50  59  67  23  75  14  15 107  10  32  83   2  60  58  55  61  98 105   3   4  49  76 100  86  35  16  48  81  57  11  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 45 74 22 58  8 67 51 65 72  9 52 62 40 26 42 31 23 60 63 77 33  0 43 49 73 34 68  5 55 19  6  7 12 75 17 16 15 70  1 66 64 56  2 18 76 38 27 78 46 29 21 69 13 24 48 20 36 54 28  4 11 61 41 59 39 30 37 79 71 35  3 53 10 32 50 47 25 14 44], a_shuffle_aclus: [ 76  62 100  31  78  11  89  69  87  97  13  70  84  57  35  59  48  32  82  85 107  50   2  60  67  98  51  90   8  74  26   9  10  16 102  24  23  21  93   3  88  86  75   4  25 105  55  36 108  63  41  28  92  17  33  66  27  53  73  37   7  15  83  58  81  56  45  54 109  96  52   5  71  14  49  68  64  34  19  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [32 43 50 29 73 54 24 52 76  6  7 47 26 18  1 75 56  0 65 22 57 39 42 27 30 19  8 69 33 49 17 48 37  2 70 20 23 16 59 71 25 46 67 41 36 61 62 45 21 77  5  4 13 66 51 58  9 12 63 60 64 28 79 15 31 68 55  3 14 72 74 44 38 34 40 78 11 35 10 53], a_shuffle_aclus: [ 49  60  68  41  98  73  33  70 105   9  10  64  35  25   3 102  75   2  87  31  76  56  59  36  45  26  11  92  50  67  24  66  54   4  93  27  32  23  81  96  34  63  89  58  53  83  84  62  28 107   8   7  17  88  69  78  13  16  85  82  86  37 109  21  48  90  74   5  19  97 100  61  55  51  57 108  15  52  14  71]\n",
      "a_shuffle_IDXs: [14 23 74 61 26 40 68 53 77  8 75 20 28 55 38 76  3  4 18 59  7 21 15 11 24 12 27 67 50 65 63 56 44 16 48 39 78  5 72 42 49 32 30 22 25 71 31  1 41 60 79 52 57  9 46 47  6 73 58  2 51 62 64 70 29 35 10 17 36 33 54 19 66 37 13  0 69 45 43 34], a_shuffle_aclus: [ 19  32 100  83  35  57  90  71 107  11 102  27  37  74  55 105   5   7  25  81  10  28  21  15  33  16  36  89  68  87  85  75  61  23  66  56 108   8  97  59  67  49  45  31  34  96  48   3  58  82 109  70  76  13  63  64   9  98  78   4  69  84  86  93  41  52  14  24  53  50  73  26  88  54  17   2  92  62  60  51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [40 73 16 60 33 34 74 19 66  2 48 18 57 59 53 10 62 42 12 51 54 29 26 13 23  9 65 17 56 61 75 35 21 49 69 76 47 44 45 58 36  0 70 64  1 30 11 72 28  4 63 38  6 79 46 43 67 50 68 20  5 52 41 27  8 22 37 78 24 32 39 25 71 14 31 55  3 15  7 77], a_shuffle_aclus: [ 57  98  23  82  50  51 100  26  88   4  66  25  76  81  71  14  84  59  16  69  73  41  35  17  32  13  87  24  75  83 102  52  28  67  92 105  64  61  62  78  53   2  93  86   3  45  15  97  37   7  85  55   9 109  63  60  89  68  90  27   8  70  58  36  11  31  54 108  33  49  56  34  96  19  48  74   5  21  10 107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:391: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ey = np.nansum(arr * y_mat) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:392: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ex = np.nansum(arr * x_mat) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:393: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_xy = np.nansum(arr * (y_mat - ey) * (x_mat - ex)) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:394: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_yy = np.nansum(arr * (y_mat - ey) ** 2) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:395: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cov_xx = np.nansum(arr * (x_mat - ex) ** 2) / arr_sum\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 66 47 61  5 31 35 39 15 49 78 10 28 37 52  9 56 11 79 36 48 57 72 60 44  8 20 22 54 30  1 71 18 67  4 40 33 29 23 46 69 45 12 50 32 19 68  6 16  2  3 74 17 65 14 53 75 25  0 59 64 73 27 42 13 70 34  7 77 51 55 76 38 62 58 24 26 41 43 21], a_shuffle_aclus: [ 85  88  64  83   8  48  52  56  21  67 108  14  37  54  70  13  75  15 109  53  66  76  97  82  61  11  27  31  73  45   3  96  25  89   7  57  50  41  32  63  92  62  16  68  49  26  90   9  23   4   5 100  24  87  19  71 102  34   2  81  86  98  36  59  17  93  51  10 107  69  74 105  55  84  78  33  35  58  60  28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 43 38 16 14 11 68 57 27 20 66  6 31 13 56 36 55 42 75  5 17 52 28  2 29 33 79 35 25 67 63  0 59 78 45 61 12 32 23 37 21 54 24 77 34  8 62 22 50  9 72 58 18 40 44  3 71 73  1 64  7 51 26 60 47 69  4 10 76 48 15 39 41 30 46 19 49 53 70 74], a_shuffle_aclus: [ 87  60  55  23  19  15  90  76  36  27  88   9  48  17  75  53  74  59 102   8  24  70  37   4  41  50 109  52  34  89  85   2  81 108  62  83  16  49  32  54  28  73  33 107  51  11  84  31  68  13  97  78  25  57  61   5  96  98   3  86  10  69  35  82  64  92   7  14 105  66  21  56  58  45  63  26  67  71  93 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 68 33 54 13  5 61 25 56 15 69 21 19 47 40 73 11 28 12 20 74 10 79 58 55 50 76  6  2 43 51 26 16 29  7 31  9  4  3 53 37 67 17 34 72 62 60 57  8 27 65 32 24 70 42 18 77 46 63 22 49 35 44 38 36 66  1 71 23 45 48 64 14 59  0 39 75 52 30 78], a_shuffle_aclus: [ 58  90  50  73  17   8  83  34  75  21  92  28  26  64  57  98  15  37  16  27 100  14 109  78  74  68 105   9   4  60  69  35  23  41  10  48  13   7   5  71  54  89  24  51  97  84  82  76  11  36  87  49  33  93  59  25 107  63  85  31  67  52  61  55  53  88   3  96  32  62  66  86  19  81   2  56 102  70  45 108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 58 13 79 62 61  1 14 48  9 11 77 43 22 33 49 16 34 51 72 21 45 37 55 47 64 12 56 18 27 50 63  4 59  6 65 36 39 32  5 30 70 23 52 71  2 25 31 54 19 67 35 73 42 44 78  0 24 17 75  3 28 20 53 41 68 57 40 76 15 38 66 26  8 60 29 46 69  7 74], a_shuffle_aclus: [ 14  78  17 109  84  83   3  19  66  13  15 107  60  31  50  67  23  51  69  97  28  62  54  74  64  86  16  75  25  36  68  85   7  81   9  87  53  56  49   8  45  93  32  70  96   4  34  48  73  26  89  52  98  59  61 108   2  33  24 102   5  37  27  71  58  90  76  57 105  21  55  88  35  11  82  41  63  92  10 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 3 23 30 54 75 67 25  2 48 33 69 34 59 17 73 28 51 29  5 39 71 18 72 43 37 16 12 56 68 65 41 74 52 70 24  0  9 63 78 60 40  8 66 42 79  7 57 26 45  6 76 53 11 62 58 64 35 21  1 49 10 14 27 15 13 32 77 55 19 36 31 22  4 47 38 50 20 61 46 44], a_shuffle_aclus: [  5  32  45  73 102  89  34   4  66  50  92  51  81  24  98  37  69  41   8  56  96  25  97  60  54  23  16  75  90  87  58 100  70  93  33   2  13  85 108  82  57  11  88  59 109  10  76  35  62   9 105  71  15  84  78  86  52  28   3  67  14  19  36  21  17  49 107  74  26  53  48  31   7  64  55  68  27  83  63  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65  7 73 49 67 39  2 30 36  5 13 20  3 51 34 77 42 72 45 21 47  8 15 44 61 33  6  1 71 23 58 28 68 76 26 10 57 31 14 48 54 38 24 41 70 55 79 11 75 69 64 63 25 66 32 18  0 62 19 59  4 17 35 43 60 37 50 46 40 12 16 27 78  9 74 22 53 52 29 56], a_shuffle_aclus: [ 87  10  98  67  89  56   4  45  53   8  17  27   5  69  51 107  59  97  62  28  64  11  21  61  83  50   9   3  96  32  78  37  90 105  35  14  76  48  19  66  73  55  33  58  93  74 109  15 102  92  86  85  34  88  49  25   2  84  26  81   7  24  52  60  82  54  68  63  57  16  23  36 108  13 100  31  71  70  41  75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [69 27 79 72 54 41 24 31 58 52 29 65 56 60 57 53 13 78 22 50 49 55 45 16 40 73  2  8  4 70 48  5  9 23 37 66 14 21 71 75  0 39 68  3 30 67 47 15 42  1  6 25 26 35 12 59 28 51 62 33 36 76 10 18 63 77  7 20 46 19 43 17 44 11 61 64 34 38 74 32], a_shuffle_aclus: [ 92  36 109  97  73  58  33  48  78  70  41  87  75  82  76  71  17 108  31  68  67  74  62  23  57  98   4  11   7  93  66   8  13  32  54  88  19  28  96 102   2  56  90   5  45  89  64  21  59   3   9  34  35  52  16  81  37  69  84  50  53 105  14  25  85 107  10  27  63  26  60  24  61  15  83  86  51  55 100  49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [23 61 46 36 26 28 48 25 39 24 76 70 66  8 64 43  6 57  7  5 22 19  1  2 52 67 49 72 69 60 62 78 79 77 73  0 38 35 65 50 47 27 68 29 41 51 34 63 40 75  3 54 20 10 45 30 74 33 37 15 53 32 42  9 59 31 58 12 18 21 17 14 56 55 71  4 16 11 13 44], a_shuffle_aclus: [ 32  83  63  53  35  37  66  34  56  33 105  93  88  11  86  60   9  76  10   8  31  26   3   4  70  89  67  97  92  82  84 108 109 107  98   2  55  52  87  68  64  36  90  41  58  69  51  85  57 102   5  73  27  14  62  45 100  50  54  21  71  49  59  13  81  48  78  16  25  28  24  19  75  74  96   7  23  15  17  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 19 79 74 75 26 37 36 61  9 22 56 49 21 18 15 45 31 11 54 33 60  6 34 58 44 27 78 59 67 16 29 38  3 28 25  1 52  8  5 72 68 42 70 66 24 35 30 63 77 17 53 71  4 41 69 48 43  0 62 12  2 39 23 32 14 46 50 47 76 13 40 51 20 55  7 65 57 64 73], a_shuffle_aclus: [ 14  26 109 100 102  35  54  53  83  13  31  75  67  28  25  21  62  48  15  73  50  82   9  51  78  61  36 108  81  89  23  41  55   5  37  34   3  70  11   8  97  90  59  93  88  33  52  45  85 107  24  71  96   7  58  92  66  60   2  84  16   4  56  32  49  19  63  68  64 105  17  57  69  27  74  10  87  76  86  98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 22 49 48 24 46 20 14 54  8 18  3 19 13 27 62 55 41 66 67 76 71 31  0 63  1  9 30 78 39 51 25 34 73 21 77  5 61 33 15 69  7 50 79 52 53 59 17 57 70 68 38 40 23 36 72  6 37 74 75 28 56 32 65 11 43 64  4  2 44 45 35 60 42 29 10 26 16 58 12], a_shuffle_aclus: [ 64  31  67  66  33  63  27  19  73  11  25   5  26  17  36  84  74  58  88  89 105  96  48   2  85   3  13  45 108  56  69  34  51  98  28 107   8  83  50  21  92  10  68 109  70  71  81  24  76  93  90  55  57  32  53  97   9  54 100 102  37  75  49  87  15  60  86   7   4  61  62  52  82  59  41  14  35  23  78  16]\n",
      "a_shuffle_IDXs: [68  4 15 11 62 65 54 76 77 40 30 67  5 47  2 36 53 20 75  6 39  3  1 37 18 78 73 69 14 33  7 64 51 43 32 61 55 56 38 27 25 31 24 34 63 10 41 66 23 44  9 17 74 29 48 45 49 59 26  8 50 16 19 60 70 42 22  0 72 35 57 21 28 12 58 13 46 71 79 52], a_shuffle_aclus: [ 90   7  21  15  84  87  73 105 107  57  45  89   8  64   4  53  71  27 102   9  56   5   3  54  25 108  98  92  19  50  10  86  69  60  49  83  74  75  55  36  34  48  33  51  85  14  58  88  32  61  13  24 100  41  66  62  67  81  35  11  68  23  26  82  93  59  31   2  97  52  76  28  37  16  78  17  63  96 109  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73 22  6 37 40 59 64 19 77 44 45 53 23 61 75 17 27  7 55 34 38 15 31 14 24 54 74  3 18 71 16 57 35 50 76  8 36 46 56 13 21 10 70 62 29 25 41  2 66 39 28 30 11  5 51 48 32 52  0 63 58 43 12  9 33 49 42 67 47 68 72  4  1 78 26 65 20 69 79 60], a_shuffle_aclus: [ 98  31   9  54  57  81  86  26 107  61  62  71  32  83 102  24  36  10  74  51  55  21  48  19  33  73 100   5  25  96  23  76  52  68 105  11  53  63  75  17  28  14  93  84  41  34  58   4  88  56  37  45  15   8  69  66  49  70   2  85  78  60  16  13  50  67  59  89  64  90  97   7   3 108  35  87  27  92 109  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41  6 67 54 44 33 64 68 17 55  9 56 53 22 12 29 40 25 51 59 50 31 72 48 36 27 28 58 69 42 62 49 16 37 10 65 71 60 21 73 78 14 26 70 63  1  8 43  0 79 18  3 20 34 15 46 66 39 24  2 19 45 77 61 35 30 11 23 32  7  4 74  5 76 57 75 47 38 52 13], a_shuffle_aclus: [ 58   9  89  73  61  50  86  90  24  74  13  75  71  31  16  41  57  34  69  81  68  48  97  66  53  36  37  78  92  59  84  67  23  54  14  87  96  82  28  98 108  19  35  93  85   3  11  60   2 109  25   5  27  51  21  63  88  56  33   4  26  62 107  83  52  45  15  32  49  10   7 100   8 105  76 102  64  55  70  17]\n",
      "a_shuffle_IDXs: [42 60 50 51 12 72  4 56 24 68 62 20 49 23 71 18 75 25 15  5 19 74 73 57  6 78 21  9  1 32 63 64 27 14 38 65  7 67 66 61 79 54 30 36 77 76  0 59 34 31 16 40 44 33 58 69  8 22 35 29 47 39  3 26 43 11 28 52 55 13 53 10 70 48 46 17  2 41 45 37], a_shuffle_aclus: [ 59  82  68  69  16  97   7  75  33  90  84  27  67  32  96  25 102  34  21   8  26 100  98  76   9 108  28  13   3  49  85  86  36  19  55  87  10  89  88  83 109  73  45  53 107 105   2  81  51  48  23  57  61  50  78  92  11  31  52  41  64  56   5  35  60  15  37  70  74  17  71  14  93  66  63  24   4  58  62  54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [42  5 18 40 49 19 56 39  6  4 47 35 30 38 28 62  0 67 14 77 23 25 68 24 51 29 34 12 13 27  8 59 11 64 72  1 53 37 55 50 43  3 15 57 66 33 26 45  7 44 22 36 41 17 73 79 32  2 71 69 76 21 65 60 48 70 63 74 10 54 46 20 52 58 31  9 78 16 61 75], a_shuffle_aclus: [ 59   8  25  57  67  26  75  56   9   7  64  52  45  55  37  84   2  89  19 107  32  34  90  33  69  41  51  16  17  36  11  81  15  86  97   3  71  54  74  68  60   5  21  76  88  50  35  62  10  61  31  53  58  24  98 109  49   4  96  92 105  28  87  82  66  93  85 100  14  73  63  27  70  78  48  13 108  23  83 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28  1 18 29 75 63 71 31 24 47  4 13 48 65  5 11 66  9 23 10 15 58 36 49 43 26  6 79 44  8 27 34 67 35 46 21 72 61  2 19 50 16 51 56 12 52 59 38 73 68 53 70 57 77 42 64 30 55 22  0 25 20 17 39 54 41 40 33 74  3 60  7 32 45 78 37 62 76 14 69], a_shuffle_aclus: [ 37   3  25  41 102  85  96  48  33  64   7  17  66  87   8  15  88  13  32  14  21  78  53  67  60  35   9 109  61  11  36  51  89  52  63  28  97  83   4  26  68  23  69  75  16  70  81  55  98  90  71  93  76 107  59  86  45  74  31   2  34  27  24  56  73  58  57  50 100   5  82  10  49  62 108  54  84 105  19  92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51  2  9 63 71  4  3 33 10 49 20 58 38 11 45 43 52  0 60 40 73 31 29 39 76 54 19 62 68 46 47  1 56 37 22 14  6 66 78 12  7 79 13 24 53 17 28 74 64 30 59 77 65 41 15 35 50  8 16 25 23 75 42 32 26 70 48 21 18 27 34 67 57  5 36 55 72 44 69 61], a_shuffle_aclus: [ 69   4  13  85  96   7   5  50  14  67  27  78  55  15  62  60  70   2  82  57  98  48  41  56 105  73  26  84  90  63  64   3  75  54  31  19   9  88 108  16  10 109  17  33  71  24  37 100  86  45  81 107  87  58  21  52  68  11  23  34  32 102  59  49  35  93  66  28  25  36  51  89  76   8  53  74  97  61  92  83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [54 74 18 40 11 63 61 70 29 22 12  5 10 69 15 43 56 41 13 30 64  6 34 53 73 77 25 44 42 31 28 50 48 32 45 65 26 66 75 67  4 59 24 72 39 78 52 14 49  3  0  2 76 19 16 36 33  7 55 68  1 35 23 17 38 47 71 58 57 27 46 62 79 60  9 21 37 51  8 20], a_shuffle_aclus: [ 73 100  25  57  15  85  83  93  41  31  16   8  14  92  21  60  75  58  17  45  86   9  51  71  98 107  34  61  59  48  37  68  66  49  62  87  35  88 102  89   7  81  33  97  56 108  70  19  67   5   2   4 105  26  23  53  50  10  74  90   3  52  32  24  55  64  96  78  76  36  63  84 109  82  13  28  54  69  11  27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [22 76 78 79 66 23 65 50 47 34  7  2 19 18 52 42 13 51 75 54 62 74 10 56  6 60 53 64  0 16 77 32 70 33 30 36 57 21 15 17  8 39 55 26 14 29 73 20  3 71 72 40 31 49 63 37 44 28  5 43 59 27 61 25 46  4 35 38 69  1 67 68 48 12 24 11 45  9 58 41], a_shuffle_aclus: [ 31 105 108 109  88  32  87  68  64  51  10   4  26  25  70  59  17  69 102  73  84 100  14  75   9  82  71  86   2  23 107  49  93  50  45  53  76  28  21  24  11  56  74  35  19  41  98  27   5  96  97  57  48  67  85  54  61  37   8  60  81  36  83  34  63   7  52  55  92   3  89  90  66  16  33  15  62  13  78  58]\n",
      "a_shuffle_IDXs: [78  3 61  4 31 18 29 62 32 54 67  8 25 19 45 13 77 65 33 15  2 41 70 66 35 58 22 30 68 27 64 26 46 63 14 17 76 21 51 37 47 43 69 12  7  0 59 71 50 16 55 39 28 11 52 34 36 74  6 49 24 79 72 40 60 56  1 20 73 48  9 57 10 75 23 42 53 44  5 38], a_shuffle_aclus: [108   5  83   7  48  25  41  84  49  73  89  11  34  26  62  17 107  87  50  21   4  58  93  88  52  78  31  45  90  36  86  35  63  85  19  24 105  28  69  54  64  60  92  16  10   2  81  96  68  23  74  56  37  15  70  51  53 100   9  67  33 109  97  57  82  75   3  27  98  66  13  76  14 102  32  59  71  61   8  55]\n",
      "a_shuffle_IDXs: [74 61 53 48 43 29 77 54 68 50  5 62 11 10 28 63 35 13 31 33 78 66 40 71 67 34 49 47  6 19 72 73 39 30 21 14 59  9  3 75 26 18  0 76 37 79  1 32 22 65 12 23 27 52 15 17  4 64 24 51 41 55 38 16 25 44 58 56 45 70  8 20 46  2 69 36 42 57  7 60], a_shuffle_aclus: [100  83  71  66  60  41 107  73  90  68   8  84  15  14  37  85  52  17  48  50 108  88  57  96  89  51  67  64   9  26  97  98  56  45  28  19  81  13   5 102  35  25   2 105  54 109   3  49  31  87  16  32  36  70  21  24   7  86  33  69  58  74  55  23  34  61  78  75  62  93  11  27  63   4  92  53  59  76  10  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 24 39 41 61 63 30 28 66 32 78 33 35 77 52 59 26 70 57 73 56 27 40 29 19 16 76  1 11  9  3 53 65 45 13 62 34 31 46 58 64  7 23  8 37 50 42 15 25 75 21 36 67 72 14 44 48 43  2 79  5 49 55 47 69  6 54 38 60 68 22 18 74  0 71 12 20  4 51 17], a_shuffle_aclus: [ 14  33  56  58  83  85  45  37  88  49 108  50  52 107  70  81  35  93  76  98  75  36  57  41  26  23 105   3  15  13   5  71  87  62  17  84  51  48  63  78  86  10  32  11  54  68  59  21  34 102  28  53  89  97  19  61  66  60   4 109   8  67  74  64  92   9  73  55  82  90  31  25 100   2  96  16  27   7  69  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [39  5  6 10 14 34 30 57 60 21 77 72 65 61 76 73 62 63 29  1 32 56 12 15 47 46 22 37 64 44 59 13 71  9 33 36  8 28  7 11  4 17 70 58 25 41 31 43 40 75 45 74 79 78  2 24 23 26 19 38 55 20 27 51 42 67 54 18 35 68 52 53 16 48 69 50  0 66  3 49], a_shuffle_aclus: [ 56   8   9  14  19  51  45  76  82  28 107  97  87  83 105  98  84  85  41   3  49  75  16  21  64  63  31  54  86  61  81  17  96  13  50  53  11  37  10  15   7  24  93  78  34  58  48  60  57 102  62 100 109 108   4  33  32  35  26  55  74  27  36  69  59  89  73  25  52  90  70  71  23  66  92  68   2  88   5  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [33 54 78 30 77 65 68 21 24 67 31 48 22 59 23 38 45 37 15 27  7 14 36 69 72  6 18 53  8 40 16 39 61 43 79 74 73 64 11 47 35 26 57 34 56  9 28 51 58 50 20 29  2 42 25 13 49 32 41 19 63 55  5 46 71  1 44 75 17  4 52 62 12 70  3 66 76 10 60  0], a_shuffle_aclus: [ 50  73 108  45 107  87  90  28  33  89  48  66  31  81  32  55  62  54  21  36  10  19  53  92  97   9  25  71  11  57  23  56  83  60 109 100  98  86  15  64  52  35  76  51  75  13  37  69  78  68  27  41   4  59  34  17  67  49  58  26  85  74   8  63  96   3  61 102  24   7  70  84  16  93   5  88 105  14  82   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24 11 46 36 55 69 27 53 66  4 42 49 37 58 16 77 60 23 47 13 14 57 38 63 21 15 34 70  6  2 12 72 52 68 76 56 61 75  8 67 25  0 39  5 54 32 78  1 18 17 59  7 62 33  3 64 20 74 73 44 19 10 30 29 31 50 28 45  9 51 71 41 65 79 48 26 43 40 35 22], a_shuffle_aclus: [ 33  15  63  53  74  92  36  71  88   7  59  67  54  78  23 107  82  32  64  17  19  76  55  85  28  21  51  93   9   4  16  97  70  90 105  75  83 102  11  89  34   2  56   8  73  49 108   3  25  24  81  10  84  50   5  86  27 100  98  61  26  14  45  41  48  68  37  62  13  69  96  58  87 109  66  35  60  57  52  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73 66 65  9 42 28 68 17 20 19  8 30 53 41 44 46 50 55 79 58  0 38 11  7 31 35  6 67 59 37 34 54 49 48  3 52 25 57 78 33 61 26 27  1 72 22 32 77 75 23 15 18 14 56 64  4 29 76 63 13 71 10  2 69 21 45 62 24 39 40 74 12  5 47 51 36 70 60 43 16], a_shuffle_aclus: [ 98  88  87  13  59  37  90  24  27  26  11  45  71  58  61  63  68  74 109  78   2  55  15  10  48  52   9  89  81  54  51  73  67  66   5  70  34  76 108  50  83  35  36   3  97  31  49 107 102  32  21  25  19  75  86   7  41 105  85  17  96  14   4  92  28  62  84  33  56  57 100  16   8  64  69  53  93  82  60  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47  0  6  4 52  9 32 45 65 46 21 36  7 60 49 38 74 76 13 71 73 11 42 29 75 30 44 54 10 24  2 43 69 68 51  3  5 34 16 66 63 78 28 62 55 35 79 61  1 40 31 56 26 25 64 19 37 39 70 20 18 27 57 12 53 72 59 67 23 15 14 41 58  8 22 77 33 48 17 50], a_shuffle_aclus: [ 64   2   9   7  70  13  49  62  87  63  28  53  10  82  67  55 100 105  17  96  98  15  59  41 102  45  61  73  14  33   4  60  92  90  69   5   8  51  23  88  85 108  37  84  74  52 109  83   3  57  48  75  35  34  86  26  54  56  93  27  25  36  76  16  71  97  81  89  32  21  19  58  78  11  31 107  50  66  24  68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [38 63 10 57 46 35 74 47 71 72 27 33 28 17 37 36 41 49 20 19 55 13  2 39 53 31 62 76 42  3  9  7 21 77  5 23 51 56 75 32 64 61 29 34 30 24 22  0  8 15 44 70 52 45 69 78 14 43 11 58 25 50 16 59 73 12 65 40 54 48  1 60 18  4 67 26 66 68 79  6], a_shuffle_aclus: [ 55  85  14  76  63  52 100  64  96  97  36  50  37  24  54  53  58  67  27  26  74  17   4  56  71  48  84 105  59   5  13  10  28 107   8  32  69  75 102  49  86  83  41  51  45  33  31   2  11  21  61  93  70  62  92 108  19  60  15  78  34  68  23  81  98  16  87  57  73  66   3  82  25   7  89  35  88  90 109   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [19 69 22 53 27 52 30 17 33 23 47 64 13 12 73 14  0 49 25 36 26 74 46  9 66 61  8 35  4 28 18 43 58  1 20 62 24 16 77 21 44 41 48 76 55 40 34 15  5 45 54 78 79 57  6 56 51 37 68 65 50 67  7 72 42 31 32 39 59  3 63  2 38 70 71 29 60 11 10 75], a_shuffle_aclus: [ 26  92  31  71  36  70  45  24  50  32  64  86  17  16  98  19   2  67  34  53  35 100  63  13  88  83  11  52   7  37  25  60  78   3  27  84  33  23 107  28  61  58  66 105  74  57  51  21   8  62  73 108 109  76   9  75  69  54  90  87  68  89  10  97  59  48  49  56  81   5  85   4  55  93  96  41  82  15  14 102]\n",
      "a_shuffle_IDXs: [26 61 16 42 48 24 68 58 38  6 57 10 18 55 77  9 79  5 67 19  8 53 33 12 76 60 74 75 17 72  0 28 64 62 54 41 39 51 35 66 63 46 29 49 32  3 43 31  1 30 23 40 70 50 73 34 78 71 47 22 52 25 45 56 15  2 13 27 37  4  7 36 21 20 59 69 11 44 14 65], a_shuffle_aclus: [ 35  83  23  59  66  33  90  78  55   9  76  14  25  74 107  13 109   8  89  26  11  71  50  16 105  82 100 102  24  97   2  37  86  84  73  58  56  69  52  88  85  63  41  67  49   5  60  48   3  45  32  57  93  68  98  51 108  96  64  31  70  34  62  75  21   4  17  36  54   7  10  53  28  27  81  92  15  61  19  87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 41 74 30 15  2 27 35 18 14 33 72 77 37 48 62 54 21 22 46  0 69 63 73 10 12 71 47 52 36 66 61  6 13 23 45 20  8 64 25 55 78 43 58 19 65 42 44  1 68 34 59 67 40 60 17 76  3 28 29 51 26 24  5  9 79 49  4 53 57 32 39 50  7 38 16 11 31 75 70], a_shuffle_aclus: [ 75  58 100  45  21   4  36  52  25  19  50  97 107  54  66  84  73  28  31  63   2  92  85  98  14  16  96  64  70  53  88  83   9  17  32  62  27  11  86  34  74 108  60  78  26  87  59  61   3  90  51  81  89  57  82  24 105   5  37  41  69  35  33   8  13 109  67   7  71  76  49  56  68  10  55  23  15  48 102  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [78 43 11 69 48 67  2 25 17 10 23 35 29 77  0 49 60 66 14 40 28 38 72 64 42 70  8 34 75  7 58 24 32  4 62 20 59 46 57 73 55 18 31 19  5 39 61 41 52 79 16 15 22 74 26  6 51 27 36 13 65  3 56 63 30 71 54 47 68 76 44 45 33  9 53  1 37 50 12 21], a_shuffle_aclus: [108  60  15  92  66  89   4  34  24  14  32  52  41 107   2  67  82  88  19  57  37  55  97  86  59  93  11  51 102  10  78  33  49   7  84  27  81  63  76  98  74  25  48  26   8  56  83  58  70 109  23  21  31 100  35   9  69  36  53  17  87   5  75  85  45  96  73  64  90 105  61  62  50  13  71   3  54  68  16  28]\n",
      "a_shuffle_IDXs: [71 16 73 74 49 26 59 29 54 23 53 12  4 79  9  8 21 66 36 72 76 78 68 35 19 46 75 57 37 63 14 28 51 48 69 61 10 47  7 31 27 56  1 44 34 40 67 62 60 65 11 25 24 18 41 38  3 50  2  6 30 17 22 77  0 33 55 43 32 39 58 13 15  5 52 70 45 20 64 42], a_shuffle_aclus: [ 96  23  98 100  67  35  81  41  73  32  71  16   7 109  13  11  28  88  53  97 105 108  90  52  26  63 102  76  54  85  19  37  69  66  92  83  14  64  10  48  36  75   3  61  51  57  89  84  82  87  15  34  33  25  58  55   5  68   4   9  45  24  31 107   2  50  74  60  49  56  78  17  21   8  70  93  62  27  86  59]\n",
      "a_shuffle_IDXs: [70 33 16 58 52 50 72 21 46 10 37  5  6 57 25 60 62 12 68 77 11 45 41 49 69 29  0 35  4  3 67 14 24 71 18 40 47 39 55 22 30 36 59  1 79 61 53 15  7 76 38 26 66 56 73 64 43 75 34 27 42 23  9 54 13 65  2 78  8 74 17 51 63 28 44 31 32 19 48 20], a_shuffle_aclus: [ 93  50  23  78  70  68  97  28  63  14  54   8   9  76  34  82  84  16  90 107  15  62  58  67  92  41   2  52   7   5  89  19  33  96  25  57  64  56  74  31  45  53  81   3 109  83  71  21  10 105  55  35  88  75  98  86  60 102  51  36  59  32  13  73  17  87   4 108  11 100  24  69  85  37  61  48  49  26  66  27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [33 56 74 53 64  3 30 59 23 77 48  8 65  5 10 79  4 49  1 70 21 40 39 68 60 31 61 35 38 47 32 75 25  7 34 29 36 18 50 67 26 78 73 54 14 57 42  2 45 19 71 62 27  0 43 37 44 52 28 51 55 22 58 12 41 13 46 17 76 66 15 69 72  6 16 63 11  9 20 24], a_shuffle_aclus: [ 50  75 100  71  86   5  45  81  32 107  66  11  87   8  14 109   7  67   3  93  28  57  56  90  82  48  83  52  55  64  49 102  34  10  51  41  53  25  68  89  35 108  98  73  19  76  59   4  62  26  96  84  36   2  60  54  61  70  37  69  74  31  78  16  58  17  63  24 105  88  21  92  97   9  23  85  15  13  27  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 58 44  2  9 68 28 78 73 48 76 66 53 33 18  3 41 26 10 22 29 77 79 32 75 27 65 42 17 63 16  0 62 20 59 51 43 71 61 64 13  8 37  5 46 15 14  7 54 12 40 35 69  6 19 11 67 23 52 21 47 24 72 31 60 57 55 50  4 39 74 25 30 49 45  1 38 70 34 36], a_shuffle_aclus: [ 75  78  61   4  13  90  37 108  98  66 105  88  71  50  25   5  58  35  14  31  41 107 109  49 102  36  87  59  24  85  23   2  84  27  81  69  60  96  83  86  17  11  54   8  63  21  19  10  73  16  57  52  92   9  26  15  89  32  70  28  64  33  97  48  82  76  74  68   7  56 100  34  45  67  62   3  55  93  51  53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 29 72 23  2 46 42 57 40 36 39 54 52 70 17 50 13 76  1 48 10 28  9 64 14 59 20 49 68 69 53 75 62 77 12 71 11 67 15 33 78 18 25 21 26 65 16 24 47 61 66  4  0  6 31 55 19 38 45 44  8 34 58 60  5 43 73 37 22 74 56 41 51  7 32 79 35 63  3 30], a_shuffle_aclus: [ 36  41  97  32   4  63  59  76  57  53  56  73  70  93  24  68  17 105   3  66  14  37  13  86  19  81  27  67  90  92  71 102  84 107  16  96  15  89  21  50 108  25  34  28  35  87  23  33  64  83  88   7   2   9  48  74  26  55  62  61  11  51  78  82   8  60  98  54  31 100  75  58  69  10  49 109  52  85   5  45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 23 79 39 46  9 72 53 73  8 61 22 42 68 47 55  4 32 76 43 10 50 34 56 26 71  1 20 24 70 31 16 67 78 17 60 13 12 48 30 77 40 52 74 49 33 64 65  6 69 44 18  3  5 21 58 19 25 62 35 14 63 54 41 36 45  2 51 29 37 15 75  0 11 27 66 59 57 38  7], a_shuffle_aclus: [ 37  32 109  56  63  13  97  71  98  11  83  31  59  90  64  74   7  49 105  60  14  68  51  75  35  96   3  27  33  93  48  23  89 108  24  82  17  16  66  45 107  57  70 100  67  50  86  87   9  92  61  25   5   8  28  78  26  34  84  52  19  85  73  58  53  62   4  69  41  54  21 102   2  15  36  88  81  76  55  10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [15 28 41 73 67  0 52 29 77 58  3 18 64 34 71  9 33 24 65  7 30 66 39 37 44  1 54 61 38 68  8 35 19 36 22 57 63 79 60 32 78 51 40 50 74 23 46 47 17 48 12 75 72 45 69 62 59 43 49 42 25 31  4 27 21  2 10 70  6  5 53 20 11 56 14 55 26 76 13 16], a_shuffle_aclus: [ 21  37  58  98  89   2  70  41 107  78   5  25  86  51  96  13  50  33  87  10  45  88  56  54  61   3  73  83  55  90  11  52  26  53  31  76  85 109  82  49 108  69  57  68 100  32  63  64  24  66  16 102  97  62  92  84  81  60  67  59  34  48   7  36  28   4  14  93   9   8  71  27  15  75  19  74  35 105  17  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [21 33  0  5  6 27 14 11 73 69  3 68 40 20 26 49 43 22 15 17 51 28 42 13 50 18 75 25 12 66 31 56 74 78 36 32 60 65 72 58 71  9 62 63 10 41 54 64 52 79 23  8 67 35 77 61  2 44 24 55 29 70 19  7  4 53 34 16 48 59 76 57 30 46 38 45 37  1 47 39], a_shuffle_aclus: [ 28  50   2   8   9  36  19  15  98  92   5  90  57  27  35  67  60  31  21  24  69  37  59  17  68  25 102  34  16  88  48  75 100 108  53  49  82  87  97  78  96  13  84  85  14  58  73  86  70 109  32  11  89  52 107  83   4  61  33  74  41  93  26  10   7  71  51  23  66  81 105  76  45  63  55  62  54   3  64  56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 35 28 22 25 38 77 61 50 29 54 11  0 14 45 46 27 42 70 64 19 34 47 43 63 20 72 78 53 23 51 49  5 26 62 21 10  4 65 68 48  8 13 36 66 15 60  3 44 59 79 73 76 74 31  2 58 37 41 33 69 16  9 30 39 56 18 71 75 67  7 55  1 32  6 52 57 24 40 12], a_shuffle_aclus: [ 24  52  37  31  34  55 107  83  68  41  73  15   2  19  62  63  36  59  93  86  26  51  64  60  85  27  97 108  71  32  69  67   8  35  84  28  14   7  87  90  66  11  17  53  88  21  82   5  61  81 109  98 105 100  48   4  78  54  58  50  92  23  13  45  56  75  25  96 102  89  10  74   3  49   9  70  76  33  57  16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [26 19 27 62 36 54 40 34  4  0  2 24  6 20  5 14 48 17 51 65  1 77 76 30 58 32 69 61 37 53 56 73 64  7 45 29 10 18 38  3 23 59 28  9 41 46 74 22 72 12 52 60 33 55 39 78 70 71 57 11 25  8 67 31 79 42 15 21 16 75 13 63 49 47 43 35 44 66 68 50], a_shuffle_aclus: [ 35  26  36  84  53  73  57  51   7   2   4  33   9  27   8  19  66  24  69  87   3 107 105  45  78  49  92  83  54  71  75  98  86  10  62  41  14  25  55   5  32  81  37  13  58  63 100  31  97  16  70  82  50  74  56 108  93  96  76  15  34  11  89  48 109  59  21  28  23 102  17  85  67  64  60  52  61  88  90  68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [77  3 50 20 68 79 12 44 66 43 74 37 49 59  0 56 72 41  9 67 18 47 52 39 14  7 16  5 60 70 76 57 21 65 53 40 34 29 78 33 58  2 11 23 19 15 22 42 61 62  4 26 31 63 25 64 28 35 69  1 27 13 51  8 73 54 71 38 32 55 10  6 24 36 30 17 46 48 45 75], a_shuffle_aclus: [107   5  68  27  90 109  16  61  88  60 100  54  67  81   2  75  97  58  13  89  25  64  70  56  19  10  23   8  82  93 105  76  28  87  71  57  51  41 108  50  78   4  15  32  26  21  31  59  83  84   7  35  48  85  34  86  37  52  92   3  36  17  69  11  98  73  96  55  49  74  14   9  33  53  45  24  63  66  62 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [74 16 60 11 62  7 19  2 53 10 63 71 45 18 20 48 51 72 31  9 29 61 36 28 38 37 43 26 22  3  4 50 15 41 75 55 46 33  8 39 44 78 57  6 30 66 70 73 65 14 42 13 12 32  0 40 17 25 21 54 49 58 24 79 52 64 77  5  1 67 34 76 56 47 27 68 69 59 35 23], a_shuffle_aclus: [100  23  82  15  84  10  26   4  71  14  85  96  62  25  27  66  69  97  48  13  41  83  53  37  55  54  60  35  31   5   7  68  21  58 102  74  63  50  11  56  61 108  76   9  45  88  93  98  87  19  59  17  16  49   2  57  24  34  28  73  67  78  33 109  70  86 107   8   3  89  51 105  75  64  36  90  92  81  52  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [23 35 66 40 67 46 75 77 43  5 78 59 55 70 64 73 65 50 39 28 17 48 36 16 49  8 45 27 25 68 15 57 52  7  6 71  3 51 34  2 21 54 41 11 53 33 31 22 60 14 72 56 69 58  0 26 30 13 63 32 62 61 29 12 76 42  1 20 74 10 24 18 37 47  4 19 79  9 38 44], a_shuffle_aclus: [ 32  52  88  57  89  63 102 107  60   8 108  81  74  93  86  98  87  68  56  37  24  66  53  23  67  11  62  36  34  90  21  76  70  10   9  96   5  69  51   4  28  73  58  15  71  50  48  31  82  19  97  75  92  78   2  35  45  17  85  49  84  83  41  16 105  59   3  27 100  14  33  25  54  64   7  26 109  13  55  61]\n",
      "a_shuffle_IDXs: [58 63 20 48  6 13 16 43 26 45  0 73 65 31 51  9 44 40 22 78  7  3 10 72  4 36 17 14 79 52 46 18 38 33 76 62 64 37 75 56 34  5 54 32  2 15 21 24 61 23 28 11 66 69 27  1 71 57 67 55 12  8 42 70 68 30 74 47 35 25 49 77 29 53 41 39 60 19 50 59], a_shuffle_aclus: [ 78  85  27  66   9  17  23  60  35  62   2  98  87  48  69  13  61  57  31 108  10   5  14  97   7  53  24  19 109  70  63  25  55  50 105  84  86  54 102  75  51   8  73  49   4  21  28  33  83  32  37  15  88  92  36   3  96  76  89  74  16  11  59  93  90  45 100  64  52  34  67 107  41  71  58  56  82  26  68  81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [14 10 69  6 23  1 63 20 12 65 54 78 60 26 15 31 40 53  7 32 27 17 52 16 73  0 44 51 24 42  3  4 36  8 37  5 30 74  2 29 43 45 75 67 22 18 79 57 66 77 11 28 72 25 62 49 64 46 41 68 21 19 70 59 34 35 47 39 50  9 76 56 58 38 55 71 33 48 61 13], a_shuffle_aclus: [ 19  14  92   9  32   3  85  27  16  87  73 108  82  35  21  48  57  71  10  49  36  24  70  23  98   2  61  69  33  59   5   7  53  11  54   8  45 100   4  41  60  62 102  89  31  25 109  76  88 107  15  37  97  34  84  67  86  63  58  90  28  26  93  81  51  52  64  56  68  13 105  75  78  55  74  96  50  66  83  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73 61 14 27 77 62 75 57 67 25 45 74 68 65 26 22 63 37 33 24 72 47 13  3 51 16  7 41  9 50 19 30 66 43 39 48 20  0 32 49  5 18 52  2  6 58 11 69 46 78 53 44 28 15 56 40 17 29 35 21 54 36 42 10 64 79 12  8 55 31 59 23  1 34 76  4 38 60 70 71], a_shuffle_aclus: [ 98  83  19  36 107  84 102  76  89  34  62 100  90  87  35  31  85  54  50  33  97  64  17   5  69  23  10  58  13  68  26  45  88  60  56  66  27   2  49  67   8  25  70   4   9  78  15  92  63 108  71  61  37  21  75  57  24  41  52  28  73  53  59  14  86 109  16  11  74  48  81  32   3  51 105   7  55  82  93  96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [71 44 36 40 19  8 22  4 43 34 56  0 39 49 51 58 11 67 73 61 77 62 30 37 72 41 50 27 70 65 32 12 13  7 35 53 69 79 33 76 55 46 21 48 18 60 47 52 26 54 24 10 17 45 25 57  6 38  2 75 78  1 16 15 42  5 23 20 59  3  9 66 14 74 28 64 29 31 63 68], a_shuffle_aclus: [ 96  61  53  57  26  11  31   7  60  51  75   2  56  67  69  78  15  89  98  83 107  84  45  54  97  58  68  36  93  87  49  16  17  10  52  71  92 109  50 105  74  63  28  66  25  82  64  70  35  73  33  14  24  62  34  76   9  55   4 102 108   3  23  21  59   8  32  27  81   5  13  88  19 100  37  86  41  48  85  90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [34  4 45 22 38 48 28 17 44  3 71 43 16 32 36 53 27 15 46  9 31 73 14 30 59 70 55 69 56 39  1 41 60 35  7 20 21  8 13 54 65 74 47 63 68  0 77 18  2 23 72  6 61 66 33 12 19 67 25 57 50 42 29 52 10 75 37 58 64 49 79 11  5 76 40 62 51 24 78 26], a_shuffle_aclus: [ 51   7  62  31  55  66  37  24  61   5  96  60  23  49  53  71  36  21  63  13  48  98  19  45  81  93  74  92  75  56   3  58  82  52  10  27  28  11  17  73  87 100  64  85  90   2 107  25   4  32  97   9  83  88  50  16  26  89  34  76  68  59  41  70  14 102  54  78  86  67 109  15   8 105  57  84  69  33 108  35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [14 31 77 19 41 76 17 20 61  4 35 22 40 42 11 71 56 36 33 39 38 70 53 45 24 69 57 43 60 74  9 65 10 59  1 47 52 13 51 54  5  6 50 29 48 66 55 37 21 78 23 27 72 18 49 25  7 64  3 44 63  8 58 32 12 30 68  2 79 28 67 34  0 62 46 16 26 73 75 15], a_shuffle_aclus: [ 19  48 107  26  58 105  24  27  83   7  52  31  57  59  15  96  75  53  50  56  55  93  71  62  33  92  76  60  82 100  13  87  14  81   3  64  70  17  69  73   8   9  68  41  66  88  74  54  28 108  32  36  97  25  67  34  10  86   5  61  85  11  78  49  16  45  90   4 109  37  89  51   2  84  63  23  35  98 102  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 26 50 69 55 44 63 61 13 42 41 34 16 76 11  0  6 75 65 17 74 64  8 73 60 31 78 15 29 71 54 32 67 33 70 66 35 59 43  1 56 14 25 57 47 51 40  9 20  7  4 72  3 22 10 77 38 58  2 23 18 62 28 39 49 21  5 24 12 46 37 53 19 36 52 68 48 30 79 45], a_shuffle_aclus: [ 36  35  68  92  74  61  85  83  17  59  58  51  23 105  15   2   9 102  87  24 100  86  11  98  82  48 108  21  41  96  73  49  89  50  93  88  52  81  60   3  75  19  34  76  64  69  57  13  27  10   7  97   5  31  14 107  55  78   4  32  25  84  37  56  67  28   8  33  16  63  54  71  26  53  70  90  66  45 109  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [66  1 16  4 55 69 23 73 67 14  2 57 51 79 37 65 27 26 59 29 11 13 34 43 74 44 42 36 28 46 30 32 10 76 47 62 64 75 68 54 45 40 61  9  6 24 25 50 31  8 38 20 56 78 60 48 72 63 49 33 39 12  0 17 52  7 15 77 19 71 22 41 53 58  3 70 18 21 35  5], a_shuffle_aclus: [ 88   3  23   7  74  92  32  98  89  19   4  76  69 109  54  87  36  35  81  41  15  17  51  60 100  61  59  53  37  63  45  49  14 105  64  84  86 102  90  73  62  57  83  13   9  33  34  68  48  11  55  27  75 108  82  66  97  85  67  50  56  16   2  24  70  10  21 107  26  96  31  58  71  78   5  93  25  28  52   8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [30 61  1 11 49 42 37 19  4 35 17 31 48 73 43 25 36  2 13  6 54 78 67 28  5 75 39 22 58 60 14 33 68 41 79  7 72 10 18 77 23 64 46 63 59 47 71 29  0 57 53  8 74 16 50 70 15 55 62 12 76 21 51 34 66 45 27 65 69 52 32 24 26 20 44  9 40 38 56  3], a_shuffle_aclus: [ 45  83   3  15  67  59  54  26   7  52  24  48  66  98  60  34  53   4  17   9  73 108  89  37   8 102  56  31  78  82  19  50  90  58 109  10  97  14  25 107  32  86  63  85  81  64  96  41   2  76  71  11 100  23  68  93  21  74  84  16 105  28  69  51  88  62  36  87  92  70  49  33  35  27  61  13  57  55  75   5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 11  2 46  6  8 66 73 18 15 25 41 21 10 28 76 59 55  1 44 39 34 78 60 26 22 62 38 17 14 13 54 37 53  5  7 61  0 31 27  9 35 47 79 32  4 36 20 29 74  3 23 57 63 72 65 19 48 77 64 42 51 68 43 49 56 70 58 75 52 16 33 69 24 40 67 30 71 45 12], a_shuffle_aclus: [ 68  15   4  63   9  11  88  98  25  21  34  58  28  14  37 105  81  74   3  61  56  51 108  82  35  31  84  55  24  19  17  73  54  71   8  10  83   2  48  36  13  52  64 109  49   7  53  27  41 100   5  32  76  85  97  87  26  66 107  86  59  69  90  60  67  75  93  78 102  70  23  50  92  33  57  89  45  96  62  16]\n",
      "a_shuffle_IDXs: [36 76 22 68 77 53 50 61 67 26 64 27 70 28 40 44 69 12 14 32 58  6  4 49 79 66 47 65  5 75 45 30 52 17  2 56 43  1 55  8 39 38 60 18 63 71 73 16  0  7 33 29 78 20 13 31 19 25 24 51 11 42 54 15 35 57  9  3 46 74 59 72 37 34 21 62 10 48 41 23], a_shuffle_aclus: [ 53 105  31  90 107  71  68  83  89  35  86  36  93  37  57  61  92  16  19  49  78   9   7  67 109  88  64  87   8 102  62  45  70  24   4  75  60   3  74  11  56  55  82  25  85  96  98  23   2  10  50  41 108  27  17  48  26  34  33  69  15  59  73  21  52  76  13   5  63 100  81  97  54  51  28  84  14  66  58  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [33 63 12 77 13  7 14 36 16  9 61 22 41 59 38 21 79 34 48  0 78 73 75 45 53 47 27 20 50 56 72  4 37 18 70 28 15 39 44 32 67  8 76 68 71 43 58 55  2 42 40 19 69 52 51 10 25 66 31  3 11 54 24 64  5 57 29 17 60  6  1 49 30 62 65 74 35 46 23 26], a_shuffle_aclus: [ 50  85  16 107  17  10  19  53  23  13  83  31  58  81  55  28 109  51  66   2 108  98 102  62  71  64  36  27  68  75  97   7  54  25  93  37  21  56  61  49  89  11 105  90  96  60  78  74   4  59  57  26  92  70  69  14  34  88  48   5  15  73  33  86   8  76  41  24  82   9   3  67  45  84  87 100  52  63  32  35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [77 70 60 29 43 26 23 57 11 79  3 64 68 32 12 65 39 18 67 50  8 58 21 22 52 28 55 63 24 69 25 38 33 48 62 19 17 54 53  9 27 42 37  5 56 72 78 45 44 76 15 36 13 20 41 47 35 34 49  4  7 10 66 75 61  1  2 59 51  6 14 71 40 31 16 46 30 73  0 74], a_shuffle_aclus: [107  93  82  41  60  35  32  76  15 109   5  86  90  49  16  87  56  25  89  68  11  78  28  31  70  37  74  85  33  92  34  55  50  66  84  26  24  73  71  13  36  59  54   8  75  97 108  62  61 105  21  53  17  27  58  64  52  51  67   7  10  14  88 102  83   3   4  81  69   9  19  96  57  48  23  63  45  98   2 100]\n",
      "a_shuffle_IDXs: [17 10 36 78 21 62 34 60 37 71 24 47 30 50 45 52 54 38 55 77  5 61 49 59 26  0 53 12 18  8 66 32 14 58 13 28 79 69 20 33 74 64 39 23 57 35 15  6 65 48 31 73 16  2 42 19  7 63 11  3 44 56 22 76 27 67  4 25  1 72 70 29 75 46 41 68  9 51 43 40], a_shuffle_aclus: [ 24  14  53 108  28  84  51  82  54  96  33  64  45  68  62  70  73  55  74 107   8  83  67  81  35   2  71  16  25  11  88  49  19  78  17  37 109  92  27  50 100  86  56  32  76  52  21   9  87  66  48  98  23   4  59  26  10  85  15   5  61  75  31 105  36  89   7  34   3  97  93  41 102  63  58  90  13  69  60  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 65  4 49 38 76 47 59 19 35 50 21 53 79 67 33 27 74  1 58 45 15 24 26 20 14 29 57 72 30 17 12 41 43 66 70 34 25 77 16 37  8 23 75  0  6 48 62 31 64 55 73 36 42 63 51  5 32 60 46 10 61 68  9 11 18 52 56  7  3 71 22 40 13 54 44 39 69  2 78], a_shuffle_aclus: [ 37  87   7  67  55 105  64  81  26  52  68  28  71 109  89  50  36 100   3  78  62  21  33  35  27  19  41  76  97  45  24  16  58  60  88  93  51  34 107  23  54  11  32 102   2   9  66  84  48  86  74  98  53  59  85  69   8  49  82  63  14  83  90  13  15  25  70  75  10   5  96  31  57  17  73  61  56  92   4 108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 38 65 33 14 56 46 37 44 66 30 61  3 40  6 69 74 76 31 79 39 67 51 15  0 63 48  4 21 71 68 47 73 52 41 36 35  1 75 32 29 72 16 28 24 34 23 77 54 43 59 19 78 26 10 18 12 20 42 11 45 60 17 55 70 53 49 50  9  2 58  8 62  7 27 22 13 25 64  5], a_shuffle_aclus: [ 76  55  87  50  19  75  63  54  61  88  45  83   5  57   9  92 100 105  48 109  56  89  69  21   2  85  66   7  28  96  90  64  98  70  58  53  52   3 102  49  41  97  23  37  33  51  32 107  73  60  81  26 108  35  14  25  16  27  59  15  62  82  24  74  93  71  67  68  13   4  78  11  84  10  36  31  17  34  86   8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [70 69 76 47 48 35 34 19 28 66 52 24 65  9 33 50 68 23  4 14 40 12 31 22 62 55  3 73 53 56 67  2 15 26  1 58 13 39 25 41  8  0 71 79 20 43 78 18  5 16 21 11 74 60 37 54 59 36 29 51 63 42 32 57 17 77 27 49  6  7 38 64 10 45 61 44 30 46 72 75], a_shuffle_aclus: [ 93  92 105  64  66  52  51  26  37  88  70  33  87  13  50  68  90  32   7  19  57  16  48  31  84  74   5  98  71  75  89   4  21  35   3  78  17  56  34  58  11   2  96 109  27  60 108  25   8  23  28  15 100  82  54  73  81  53  41  69  85  59  49  76  24 107  36  67   9  10  55  86  14  62  83  61  45  63  97 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 7 55 18 58 60  8 31  4  0 49  6 30 51 29 71 24 19 77 68 43 53 37 56 72  3 35 78 12 54  5 42 17 40 76 57 74 11 45 13 33  9 21 38 50 39 62 27 16  2 10 67 59 26 23 47 66 34 20 15 63 61 28 44 46 64 79 69 73 32 36 65 41  1 48 22 75 52 70 14 25], a_shuffle_aclus: [ 10  74  25  78  82  11  48   7   2  67   9  45  69  41  96  33  26 107  90  60  71  54  75  97   5  52 108  16  73   8  59  24  57 105  76 100  15  62  17  50  13  28  55  68  56  84  36  23   4  14  89  81  35  32  64  88  51  27  21  85  83  37  61  63  86 109  92  98  49  53  87  58   3  66  31 102  70  93  19  34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [78 45  7 72 38 70  6 10 32 28  1 68 44 46  4 27 13 12 16 34 53 59 31 48 61 39 75 67 23  8 41 50 73  2 20 76 63 11 49 43 37 33  0 15 65 24 17 22 40 57 26  9 58 51 14 52 54 29 77 79  5 47 42 62 18 30  3 21 55 36 66 25 35 64 60 56 69 74 19 71], a_shuffle_aclus: [108  62  10  97  55  93   9  14  49  37   3  90  61  63   7  36  17  16  23  51  71  81  48  66  83  56 102  89  32  11  58  68  98   4  27 105  85  15  67  60  54  50   2  21  87  33  24  31  57  76  35  13  78  69  19  70  73  41 107 109   8  64  59  84  25  45   5  28  74  53  88  34  52  86  82  75  92 100  26  96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [27 58 67 18 38 57 53 68 61  0 52 66  3 59 12 54  8 32 34  5 56 64  6 45 39 47  9 72 15 65 22 24 63 49 48  1 44 69  4 76 25 41 10 26 55 37 28 79 73 75 17 50 20 74 30 70 43 21 36 60 29 11 51 62 33 42  2 23 78 46 13 14 31 35 77  7 16 19 40 71], a_shuffle_aclus: [ 36  78  89  25  55  76  71  90  83   2  70  88   5  81  16  73  11  49  51   8  75  86   9  62  56  64  13  97  21  87  31  33  85  67  66   3  61  92   7 105  34  58  14  35  74  54  37 109  98 102  24  68  27 100  45  93  60  28  53  82  41  15  69  84  50  59   4  32 108  63  17  19  48  52 107  10  23  26  57  96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 40 48  7 33 19 43  0  6 42 45 35 71 53 59 20 16 18 24 56 68 65 39 63  3 72  5  4 30 75 21 28 15 36 14 61 66 54 22 26 31 49 69 32 29 77  2 34 12 52 60  1 47 46 55 44  9 17 50 38 41 74 67 10 62 70 25 11 78 64 37 58 57 51 13 23 79 73 76 27], a_shuffle_aclus: [ 11  57  66  10  50  26  60   2   9  59  62  52  96  71  81  27  23  25  33  75  90  87  56  85   5  97   8   7  45 102  28  37  21  53  19  83  88  73  31  35  48  67  92  49  41 107   4  51  16  70  82   3  64  63  74  61  13  24  68  55  58 100  89  14  84  93  34  15 108  86  54  78  76  69  17  32 109  98 105  36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [45 35  9 29 56 69 47 32 55 71 16 37 70 76  7 43 48 33 63 72  0 14 40 19 77 22 73 34 61 66  1 30  6 62 74 64 13 39 31 41 54 42 17 36 67 21 23 52 20 12  5 59 38 15 28 44 50 57 58 46  2 79 26 24 68 60 11  3 10 51  8 27 78 25 65  4 53 18 75 49], a_shuffle_aclus: [ 62  52  13  41  75  92  64  49  74  96  23  54  93 105  10  60  66  50  85  97   2  19  57  26 107  31  98  51  83  88   3  45   9  84 100  86  17  56  48  58  73  59  24  53  89  28  32  70  27  16   8  81  55  21  37  61  68  76  78  63   4 109  35  33  90  82  15   5  14  69  11  36 108  34  87   7  71  25 102  67]\n",
      "a_shuffle_IDXs: [30 31 62 44 60 33 11 14 27  2  1 28 58 46 41 32 68 74 24  0 15 76 63  3 18 78  4 56  6 53 39 43 75 71 50 21  5 26 17 54 77 64 13 51 38 55 12 47 66 20 34 25 37 67 36  9 69 35 57 40 73 70 61 65 16 42  8 29 48 72 10 59 79 23  7 52 19 22 49 45], a_shuffle_aclus: [ 45  48  84  61  82  50  15  19  36   4   3  37  78  63  58  49  90 100  33   2  21 105  85   5  25 108   7  75   9  71  56  60 102  96  68  28   8  35  24  73 107  86  17  69  55  74  16  64  88  27  51  34  54  89  53  13  92  52  76  57  98  93  83  87  23  59  11  41  66  97  14  81 109  32  10  70  26  31  67  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [46  8  4 77 49 27 10  7 68 75 55 67 56 57 16  6 64  5 37 51 12 45 40 13 61 36 43 11 17 20 69 15 70 18 74 39 34 26 31 25 52 63 30  9 65 59 53 32 24  3 73  1 58 21 41 78 76 42 50 72 23 29 66 71 48 38 44 28 54 35 47  0  2 60 62 79 33 14 22 19], a_shuffle_aclus: [ 63  11   7 107  67  36  14  10  90 102  74  89  75  76  23   9  86   8  54  69  16  62  57  17  83  53  60  15  24  27  92  21  93  25 100  56  51  35  48  34  70  85  45  13  87  81  71  49  33   5  98   3  78  28  58 108 105  59  68  97  32  41  88  96  66  55  61  37  73  52  64   2   4  82  84 109  50  19  31  26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60 47 76  1  9 31 48 79 19 71 62 75 33 57 49 11 38 25 56  4 13 55 53 34 24 17 69 77 45 74 10 54 63 16 23 67 27 43  0 72 42 21 78 44 14 50  2  3 28 35  7  8 32 20 46 66 73 61 26 39 22 29 18 64 59 41 12 65 58 40 52  5 70 37  6 51 15 36 68 30], a_shuffle_aclus: [ 82  64 105   3  13  48  66 109  26  96  84 102  50  76  67  15  55  34  75   7  17  74  71  51  33  24  92 107  62 100  14  73  85  23  32  89  36  60   2  97  59  28 108  61  19  68   4   5  37  52  10  11  49  27  63  88  98  83  35  56  31  41  25  86  81  58  16  87  78  57  70   8  93  54   9  69  21  53  90  45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [26 48 14 52 62 10 50  4 57 76 34  1  9 32 30 65  7 39 73 56 40  0 66 71  3 69 16 25 44 54 36 77 75 55 51  6 19 33 28 47 46 31 37 45 13 74  5  8 15 38 22 20 23 79 70 42  2 17 72 27 18 67 61 12 43 68 78 59 21 29 60 24 49 53 35 41 63 64 58 11], a_shuffle_aclus: [ 35  66  19  70  84  14  68   7  76 105  51   3  13  49  45  87  10  56  98  75  57   2  88  96   5  92  23  34  61  73  53 107 102  74  69   9  26  50  37  64  63  48  54  62  17 100   8  11  21  55  31  27  32 109  93  59   4  24  97  36  25  89  83  16  60  90 108  81  28  41  82  33  67  71  52  58  85  86  78  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [25 55 48  7 20 65 44 45 11 29 43 58  3  8 12 26 64 36 27 70  6 66  5 54 28 18 22 16 31  1 34  2 39 38 52 49 74 15 35 40  9 67 24 33 32 17 59 37 14 21 50 68 75 72 42  0 13  4 47 56 46 23 62 63 78 61 57 77 79 71 30 53 69 41 73 10 19 51 76 60], a_shuffle_aclus: [ 34  74  66  10  27  87  61  62  15  41  60  78   5  11  16  35  86  53  36  93   9  88   8  73  37  25  31  23  48   3  51   4  56  55  70  67 100  21  52  57  13  89  33  50  49  24  81  54  19  28  68  90 102  97  59   2  17   7  64  75  63  32  84  85 108  83  76 107 109  96  45  71  92  58  98  14  26  69 105  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [48 61 75 66 70  8 51 63 30 47 43 76 60 34  2 10 58 28  7 32 17 57 52 56 59 25 36 54 42 21  1 65  6 38 18 27 33  3 29 37 50 41 19 74 45 77 13 46 49 72 20 24 64 78 67 12  4 23 22  9 62  5 15 35 73 39 79 31  0 14 26 44 11 71 68 40 69 53 55 16], a_shuffle_aclus: [ 66  83 102  88  93  11  69  85  45  64  60 105  82  51   4  14  78  37  10  49  24  76  70  75  81  34  53  73  59  28   3  87   9  55  25  36  50   5  41  54  68  58  26 100  62 107  17  63  67  97  27  33  86 108  89  16   7  32  31  13  84   8  21  52  98  56 109  48   2  19  35  61  15  96  90  57  92  71  74  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 23 42 57 29 39 19 71 53 75 59 25  2 52  3  7 64 54 65 36 27 78  6  0 56 46 12 37 76  8  1 24 41  5 21 63 69 45 55 33 35 14 40 61 22 10 13  9 72 17 60 31 51 30 38 73 62 47 28 70 43 16 79 68 26  4 34 77 32 18 20 15 44 50 67 66 48 74 58 49], a_shuffle_aclus: [ 15  32  59  76  41  56  26  96  71 102  81  34   4  70   5  10  86  73  87  53  36 108   9   2  75  63  16  54 105  11   3  33  58   8  28  85  92  62  74  50  52  19  57  83  31  14  17  13  97  24  82  48  69  45  55  98  84  64  37  93  60  23 109  90  35   7  51 107  49  25  27  21  61  68  89  88  66 100  78  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [69  0 25 73 54 11  1  5 10 63 46  7  9 45 67 16 37 70 44 50  4 62 36 31 23 15 38 24 72 75 27 76 33 57 65 42 47 52 60 32  8 43 77  2 28 21 74 22 13 64  6 78 40 19  3 39 18 66 41 51 55 59 61 35 17 58 71 26 20 79 29 30 53 68 56 12 48 49 34 14], a_shuffle_aclus: [ 92   2  34  98  73  15   3   8  14  85  63  10  13  62  89  23  54  93  61  68   7  84  53  48  32  21  55  33  97 102  36 105  50  76  87  59  64  70  82  49  11  60 107   4  37  28 100  31  17  86   9 108  57  26   5  56  25  88  58  69  74  81  83  52  24  78  96  35  27 109  41  45  71  90  75  16  66  67  51  19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 1 72 54 10 18 33 39 40 71 42 23 13 29 57 78 15 50  3 62 75 60 59 22 76 67 49 16 28  7 36 35 47 65 79 70  5 34 17 56 52  6 25 66  4 64 58 14 32 46 20 45 24 12 44 48 77  2 26 19 41  9 43 31 27  0 37  8 21 55 38 63 74 51 69 73 68 11 61 30 53], a_shuffle_aclus: [  3  97  73  14  25  50  56  57  96  59  32  17  41  76 108  21  68   5  84 102  82  81  31 105  89  67  23  37  10  53  52  64  87 109  93   8  51  24  75  70   9  34  88   7  86  78  19  49  63  27  62  33  16  61  66 107   4  35  26  58  13  60  48  36   2  54  11  28  74  55  85 100  69  92  98  90  15  83  45  71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 39 14 58 10 72 34 54 44 53 23 21 71 67 69  6  2 63 60 22 48 56 33 29 17 74 31 47  8 55 28 37 52 16 62 78 49 20 66 76 45 18 42 15  0 30 27 41 43 59 77 19 73 32 51 65 46 61 68 26 40 50  5 12 11 36 13  4  1 25 24 79  7 57 70  9 75 35  3 38], a_shuffle_aclus: [ 86  56  19  78  14  97  51  73  61  71  32  28  96  89  92   9   4  85  82  31  66  75  50  41  24 100  48  64  11  74  37  54  70  23  84 108  67  27  88 105  62  25  59  21   2  45  36  58  60  81 107  26  98  49  69  87  63  83  90  35  57  68   8  16  15  53  17   7   3  34  33 109  10  76  93  13 102  52   5  55]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 36 79 18 65 66  1 52 14 68 41 20 42 58 77 34  8 17 10 40 29 19  7 37 21 49 12 44 27 62 24 59 33  2 56 23 71 61 60 15 48  9 69 57 47 74  5 50 25 46 67 31 54 45  6 26 16 55  0 30 32 39 43 28 51  4 76 78 53 38 11 64 35 70 73 72 22 75  3 13], a_shuffle_aclus: [ 85  53 109  25  87  88   3  70  19  90  58  27  59  78 107  51  11  24  14  57  41  26  10  54  28  67  16  61  36  84  33  81  50   4  75  32  96  83  82  21  66  13  92  76  64 100   8  68  34  63  89  48  73  62   9  35  23  74   2  45  49  56  60  37  69   7 105 108  71  55  15  86  52  93  98  97  31 102   5  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [70  6 14 26 30 59  5 79 57 43 41 51 23  0 67 37 11  8 48 44 69 62 38 21 19 39 40 61 55 42  4 22 63 65 50  7 68 31 72 32 13 15 64 17 74 35 46 52 60 78 56 75  1 49 18 47 66 27 36 53 45 73 12 54  3 58 16 20  2 28 71 29 34 77 10  9 33 76 25 24], a_shuffle_aclus: [ 93   9  19  35  45  81   8 109  76  60  58  69  32   2  89  54  15  11  66  61  92  84  55  28  26  56  57  83  74  59   7  31  85  87  68  10  90  48  97  49  17  21  86  24 100  52  63  70  82 108  75 102   3  67  25  64  88  36  53  71  62  98  16  73   5  78  23  27   4  37  96  41  51 107  14  13  50 105  34  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6 15 56 55 35 79 52 17 68 59 20 78 39 58 24 72 13 48 70 37 25 28 50 16 67 62 51 38 61 31 54 77 47  8  9  7 73 75 27 33 71 12 60 74 49 65 36 34 53 46 76 29 69 64 14 41 19 63 10 57 45  3  4 30 18 22 26  0 42 21 43 40 11 44 32  1  2  5 23 66], a_shuffle_aclus: [  9  21  75  74  52 109  70  24  90  81  27 108  56  78  33  97  17  66  93  54  34  37  68  23  89  84  69  55  83  48  73 107  64  11  13  10  98 102  36  50  96  16  82 100  67  87  53  51  71  63 105  41  92  86  19  58  26  85  14  76  62   5   7  45  25  31  35   2  59  28  60  57  15  61  49   3   4   8  32  88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 52 39 62 33 23 51 12 34 10 22 47 42  7 58 32 20  4 19 73 79 25 14 65 29 45 11 13 72 71 43  2  0  8 48 35 37 77 61 64 49 68 63  9 38 55 67 27 44 46 17 60  5 75 30 26 16 70  6 54 56 36 40 41 24 31  1 57 74 78 69 76 21 28  3 59 53 66 18 15], a_shuffle_aclus: [ 68  70  56  84  50  32  69  16  51  14  31  64  59  10  78  49  27   7  26  98 109  34  19  87  41  62  15  17  97  96  60   4   2  11  66  52  54 107  83  86  67  90  85  13  55  74  89  36  61  63  24  82   8 102  45  35  23  93   9  73  75  53  57  58  33  48   3  76 100 108  92 105  28  37   5  81  71  88  25  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0  7 78 33 76 17 21  6 20 52 31 15 42 73 28 65 19 74 72 18 46 34 29 43 53  4 32 27 35 64 71 58 12 59 62  2 70 30 66  8 55 51 67 45 56 14 69 77 11  9  3 48 41 23 26 63 61 37 44 49 36 68 60 79 40 75 25 24 50 10 16  1 13 47 22 39 38 57  5 54], a_shuffle_aclus: [  2  10 108  50 105  24  28   9  27  70  48  21  59  98  37  87  26 100  97  25  63  51  41  60  71   7  49  36  52  86  96  78  16  81  84   4  93  45  88  11  74  69  89  62  75  19  92 107  15  13   5  66  58  32  35  85  83  54  61  67  53  90  82 109  57 102  34  33  68  14  23   3  17  64  31  56  55  76   8  73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [68 40 63 52 28 75 36  3 11 67 47  2 27 23 45 69 15 73 54  9 59 21 74 31 79 41 43 20 26 35 39 70 51 25  5  4 55 44 56 33 34 38 14 18 49 60 62 53 50 76 66 19 13 42 72 64 61 77 78  6 29 32 16 71  0  7 57 24 48  1 65 17 46 37 58 12 10  8 30 22], a_shuffle_aclus: [ 90  57  85  70  37 102  53   5  15  89  64   4  36  32  62  92  21  98  73  13  81  28 100  48 109  58  60  27  35  52  56  93  69  34   8   7  74  61  75  50  51  55  19  25  67  82  84  71  68 105  88  26  17  59  97  86  83 107 108   9  41  49  23  96   2  10  76  33  66   3  87  24  63  54  78  16  14  11  45  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [62 69 50 63  8 68 61 75 31 48 25 51  2 40 15 24 57 16 13 43 70 30 74 33 32  4 72 29 12 59 36 42 76 22 28 55 37 45 38 26  1 53  0 60 14 71 46  9 34  7 11 17 52 35 49 67 27 47 19  5  6 41 56 10 66 73 78  3 58 65 20 44 18 64 23 39 54 21 79 77], a_shuffle_aclus: [ 84  92  68  85  11  90  83 102  48  66  34  69   4  57  21  33  76  23  17  60  93  45 100  50  49   7  97  41  16  81  53  59 105  31  37  74  54  62  55  35   3  71   2  82  19  96  63  13  51  10  15  24  70  52  67  89  36  64  26   8   9  58  75  14  88  98 108   5  78  87  27  61  25  86  32  56  73  28 109 107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73 50 32 63 36 14 54  3 77 21 27 26 76 62 30 57 61 29 47 20  0 24 13 49 39 70 25 55 68 41 69 40  6 71 64 66 28 65  4 37 51 43  7 52 10 60 44 42  9 72 15 74  8 17 59 79 23 33 34 16 11 67 58 78 38 31 53 45  1 46 56 35  2 19 22 75  5 48 18 12], a_shuffle_aclus: [ 98  68  49  85  53  19  73   5 107  28  36  35 105  84  45  76  83  41  64  27   2  33  17  67  56  93  34  74  90  58  92  57   9  96  86  88  37  87   7  54  69  60  10  70  14  82  61  59  13  97  21 100  11  24  81 109  32  50  51  23  15  89  78 108  55  48  71  62   3  63  75  52   4  26  31 102   8  66  25  16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28  8 76 48 79 30 54 69 14  4 25 70 71 31 52 32  2 61 39 68 17  9 27 75 13 42 64 77 40 55  7 50 62 16  5  0 56 58 22 66  6 19 36 44 49 41 21 20 33 45  1 60 74 57 15 59  3 29 78 26 23 43 65 73 51 24 12 11 38 34 53 67 35 47 72 46 10 18 37 63], a_shuffle_aclus: [ 37  11 105  66 109  45  73  92  19   7  34  93  96  48  70  49   4  83  56  90  24  13  36 102  17  59  86 107  57  74  10  68  84  23   8   2  75  78  31  88   9  26  53  61  67  58  28  27  50  62   3  82 100  76  21  81   5  41 108  35  32  60  87  98  69  33  16  15  55  51  71  89  52  64  97  63  14  25  54  85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 10 23 74 70 31 36 57 24  7 42 45 59  0 68  9 38 77 30 32 28  2 14  6 65 60 25 22 44 54 71 43 52 76 39 63 79 16 37  4 58 50 46 15 56 55 49  1 48 17  5 78 13 51 11 53 20 64 41 18 47 40 34 33 19 73 12 26 66 72 69 27 62  3 21 61 35 75 67 29], a_shuffle_aclus: [ 11  14  32 100  93  48  53  76  33  10  59  62  81   2  90  13  55 107  45  49  37   4  19   9  87  82  34  31  61  73  96  60  70 105  56  85 109  23  54   7  78  68  63  21  75  74  67   3  66  24   8 108  17  69  15  71  27  86  58  25  64  57  51  50  26  98  16  35  88  97  92  36  84   5  28  83  52 102  89  41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [53 68 32 35 55 31 57 43 40  9 54  0 67 63  3 34 13 59 16 47 71 78  8 48 56  4 64 18 12 65 50 74 72 52 69 11 30 38 44 51 37 17 61 66 46 22 23 58 73  5  2 29 33 79  6  7 36 28 24 39 62 19 75 70 26 15 25 77 49 45 21  1 14 42 27 20 41 10 76 60], a_shuffle_aclus: [ 71  90  49  52  74  48  76  60  57  13  73   2  89  85   5  51  17  81  23  64  96 108  11  66  75   7  86  25  16  87  68 100  97  70  92  15  45  55  61  69  54  24  83  88  63  31  32  78  98   8   4  41  50 109   9  10  53  37  33  56  84  26 102  93  35  21  34 107  67  62  28   3  19  59  36  27  58  14 105  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 29 49 15  3 47  5 25 59 76 24 38 28 51 37  2 27 67 48 64 61 33 73 71 41 11 22 54 74 35 18 40 53 60 70 55 78 69 45 19  8 75 10 66 62 63 34  9 31 43 68 20 42  1 30 16 32 44 52  6 14 72 50 58 36 12 17 39 46  7 23 21 26  0 13 56  4 57 77 79], a_shuffle_aclus: [ 87  41  67  21   5  64   8  34  81 105  33  55  37  69  54   4  36  89  66  86  83  50  98  96  58  15  31  73 100  52  25  57  71  82  93  74 108  92  62  26  11 102  14  88  84  85  51  13  48  60  90  27  59   3  45  23  49  61  70   9  19  97  68  78  53  16  24  56  63  10  32  28  35   2  17  75   7  76 107 109]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [75  1  5 33 24 48 35 20 34 61 11  7 56 27 25 68 67 12 47 66 40 78 31  8 50 51 55 53  3 41 54 73 29  0 43 13 59 17 39 72 63 74 71 45 42 30 37 32  6 19 58 69 76 23  2 60 46 18 49 70 36  4 57 65 79 64 28 38 26 44 52 21 10 15 62 22 77 16  9 14], a_shuffle_aclus: [102   3   8  50  33  66  52  27  51  83  15  10  75  36  34  90  89  16  64  88  57 108  48  11  68  69  74  71   5  58  73  98  41   2  60  17  81  24  56  97  85 100  96  62  59  45  54  49   9  26  78  92 105  32   4  82  63  25  67  93  53   7  76  87 109  86  37  55  35  61  70  28  14  21  84  31 107  23  13  19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 59 65 64 43 52 44 25 15 70  3 14  2 57 23 72 19 78 13 26 73 68 34 60 47 22 33 11 32 75 38 56 17  7 30 16 62 40 53 71 69 24 21 67  4 31  5 46 48 27 76 35 36 28 55 20 10 51 77 58 39  6  8 63 18  1  0  9 50 41 45 66 54 37 29 12 61 49 42 74], a_shuffle_aclus: [109  81  87  86  60  70  61  34  21  93   5  19   4  76  32  97  26 108  17  35  98  90  51  82  64  31  50  15  49 102  55  75  24  10  45  23  84  57  71  96  92  33  28  89   7  48   8  63  66  36 105  52  53  37  74  27  14  69 107  78  56   9  11  85  25   3   2  13  68  58  62  88  73  54  41  16  83  67  59 100]\n",
      "a_shuffle_IDXs: [19 41 23  5 15 77 55 13 68 74  1 52 48 70 33 47 62 67 66 65 60 64 38 16 11 37 42 18 50  0 28 63 10 73 14 46 12 79 25 24 27 44 53 39  4 32  3 54  7 43 26 78 22 76 57 61 36 59 35 49 30 40  6 31 21 29 45 51  9  2 71 20 72 17 34 58 75 56 69  8], a_shuffle_aclus: [ 26  58  32   8  21 107  74  17  90 100   3  70  66  93  50  64  84  89  88  87  82  86  55  23  15  54  59  25  68   2  37  85  14  98  19  63  16 109  34  33  36  61  71  56   7  49   5  73  10  60  35 108  31 105  76  83  53  81  52  67  45  57   9  48  28  41  62  69  13   4  96  27  97  24  51  78 102  75  92  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 31 40 58 61 65 69  5 62 11 77 39 26 13 68  3 22 78 51 47 28 33 24 46 41 75  1  4 52 14 55 53 25 29 48 45 23 54 35 16 38  9 49 37 64 57 70  8 59 60 63  7 74 15 20 66 44  6 72 71 27 42 12 21 43 32 56 19 10 30 73 50 76 18 34 17 67 79 36  0], a_shuffle_aclus: [  4  48  57  78  83  87  92   8  84  15 107  56  35  17  90   5  31 108  69  64  37  50  33  63  58 102   3   7  70  19  74  71  34  41  66  62  32  73  52  23  55  13  67  54  86  76  93  11  81  82  85  10 100  21  27  88  61   9  97  96  36  59  16  28  60  49  75  26  14  45  98  68 105  25  51  24  89 109  53   2]\n",
      "a_shuffle_IDXs: [70 42  2 38 59 52 28 74 48 32 62 16 50 53 20 54 34 67  4 44 11 40 36 63 45 18 33 31 57 43  8 64 68 56 12 77 15 58 65 25  3 22  6  7 41 75 14 47 60  1 78 27 37 49 55 19 21 24 79 29 10 17 46 35 69 23 66 76 26 71 73 39 13 51  9 72  5  0 30 61], a_shuffle_aclus: [ 93  59   4  55  81  70  37 100  66  49  84  23  68  71  27  73  51  89   7  61  15  57  53  85  62  25  50  48  76  60  11  86  90  75  16 107  21  78  87  34   5  31   9  10  58 102  19  64  82   3 108  36  54  67  74  26  28  33 109  41  14  24  63  52  92  32  88 105  35  96  98  56  17  69  13  97   8   2  45  83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [43 54 22 51 39  9  7  4 29 76 56 48  0 60  3 61 49 27 77 26 30 58 72 41 42 59 14 47 19 69 18  1 28 53 20 11  6 73 62  8 36 75 34 74 44 16 78 31 17 52 40 57 50 46 67 10 13 79 64 32 24 33 68 23 66  5 38 15 65  2 70 25 37 63 21 35 55 12 71 45], a_shuffle_aclus: [ 60  73  31  69  56  13  10   7  41 105  75  66   2  82   5  83  67  36 107  35  45  78  97  58  59  81  19  64  26  92  25   3  37  71  27  15   9  98  84  11  53 102  51 100  61  23 108  48  24  70  57  76  68  63  89  14  17 109  86  49  33  50  90  32  88   8  55  21  87   4  93  34  54  85  28  52  74  16  96  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [70 50 36 79 26 54 33  6  0  5 10  3 78 75 32 22 29 62 63 55  2 35 39  9 53 69 11  1 44 14 28 43 15 73 24 52 57 72 56 20 61 13 12  7 31 58 38 30 68 66 48 65 41 40 64 49 60 77 37 74 76  8 67  4 51 17 27 21 18 45 71 19 25 23 46 34 59 16 47 42], a_shuffle_aclus: [ 93  68  53 109  35  73  50   9   2   8  14   5 108 102  49  31  41  84  85  74   4  52  56  13  71  92  15   3  61  19  37  60  21  98  33  70  76  97  75  27  83  17  16  10  48  78  55  45  90  88  66  87  58  57  86  67  82 107  54 100 105  11  89   7  69  24  36  28  25  62  96  26  34  32  63  51  81  23  64  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 24 17 20 44 69  9 75 74 54 13 59  5 27 35 52 64 30 46  2 72 42 48 60 22 55 49 50  6 41 29 19 26 63 61 66 53 36 65 10 67 45 56 31  8 73 37 11 38 78  1  3 70 34 40 18 51 71 16 39 14 58 68 62  4 43 79 76 32 15  0 25 23 77 57 21 33 12 28  7], a_shuffle_aclus: [ 64  33  24  27  61  92  13 102 100  73  17  81   8  36  52  70  86  45  63   4  97  59  66  82  31  74  67  68   9  58  41  26  35  85  83  88  71  53  87  14  89  62  75  48  11  98  54  15  55 108   3   5  93  51  57  25  69  96  23  56  19  78  90  84   7  60 109 105  49  21   2  34  32 107  76  28  50  16  37  10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0 55 38 57  7 51 17 63 42 16 23 73 79 68 58 59 49 75 62 71  9 76 67 21 24 27 74 30  6 40 36 28 18  3 54 46 19 48  1 53 61 15 47 70  4 31 72 50 34 64 39 44 20 65  8 22 11 14 13 35 29  2 10 41 43 66 12 26 77 52 69  5 60 45 37 33 78 32 25 56], a_shuffle_aclus: [  2  74  55  76  10  69  24  85  59  23  32  98 109  90  78  81  67 102  84  96  13 105  89  28  33  36 100  45   9  57  53  37  25   5  73  63  26  66   3  71  83  21  64  93   7  48  97  68  51  86  56  61  27  87  11  31  15  19  17  52  41   4  14  58  60  88  16  35 107  70  92   8  82  62  54  50 108  49  34  75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [37 67 50 39  1 75 38 73  4 32 11  5 68 21 58 46 71 28 51 31 24 60 56  0  2 17 33 30 54 19 29 53 52  8 63 59  9 14 47 27 22 48 62 76  6 61 74 23 15 77 35 25 44 69 18 10  7 43 65 36 45 57 55 34 64 16 42 20 49 12 41  3 70 72 13 40 78 66 79 26], a_shuffle_aclus: [ 54  89  68  56   3 102  55  98   7  49  15   8  90  28  78  63  96  37  69  48  33  82  75   2   4  24  50  45  73  26  41  71  70  11  85  81  13  19  64  36  31  66  84 105   9  83 100  32  21 107  52  34  61  92  25  14  10  60  87  53  62  76  74  51  86  23  59  27  67  16  58   5  93  97  17  57 108  88 109  35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 62 56 38 70  4 22 30 29 50 24 60 53 26 58 21 42 20 37 68 67 32 57  3 47  5 14 69 61 34 54 13 71 17 31 75 19 48 35 52 18 74 72 45  2 64 25 59 27 39 78  7 65  9 23 63 10 66 16 76 55  1 11 79 77 46 28  8 73 15 49 33 43 40  6 36 44 12 51  0], a_shuffle_aclus: [ 58  84  75  55  93   7  31  45  41  68  33  82  71  35  78  28  59  27  54  90  89  49  76   5  64   8  19  92  83  51  73  17  96  24  48 102  26  66  52  70  25 100  97  62   4  86  34  81  36  56 108  10  87  13  32  85  14  88  23 105  74   3  15 109 107  63  37  11  98  21  67  50  60  57   9  53  61  16  69   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [53 60 18 48 65 11 78 26 30 55 70  3 32  6 46  4 21 23 16 36 22  5 62 71 69 74 75 41 72 66 29 15 50 20 31 45 19 63 77 57  2 42 17 79 33 64 54 44 47 68 49 24 52 59 38  0 58 25  1 34  9 12 13 67 61 35 43 76 39 37 73 14  7 10 51 28 27 40  8 56], a_shuffle_aclus: [ 71  82  25  66  87  15 108  35  45  74  93   5  49   9  63   7  28  32  23  53  31   8  84  96  92 100 102  58  97  88  41  21  68  27  48  62  26  85 107  76   4  59  24 109  50  86  73  61  64  90  67  33  70  81  55   2  78  34   3  51  13  16  17  89  83  52  60 105  56  54  98  19  10  14  69  37  36  57  11  75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 4 70 21 35 46 61 53 72 58 11 39 56 30  9 32 28 18 27 42  8 73  2 41 10 57 40 16 75 49 23 65 15 24 44 50 71  1 17 51 14  6 31 37 77 22 59 45 52 26 78 55 47  0 25 48 33 20 69 79  7 62 38 68 43 76 19  5 36 74 67 54  3 29 60 66 12 34 64 13 63], a_shuffle_aclus: [  7  93  28  52  63  83  71  97  78  15  56  75  45  13  49  37  25  36  59  11  98   4  58  14  76  57  23 102  67  32  87  21  33  61  68  96   3  24  69  19   9  48  54 107  31  81  62  70  35 108  74  64   2  34  66  50  27  92 109  10  84  55  90  60 105  26   8  53 100  89  73   5  41  82  88  16  51  86  17  85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6 26 35 69 19 43 10 24 12 60 11 49 50 31 15 36  7  9 27 78 29 75 40 45 61 72 51 48 33 22 41 30 38 58 53 52 55 28 74 34 14  1  4 23 73 18 37 21  8 39 65 16 25 59 57 32  0  5 56 77  2 44 66 71 76 64 63 42 13 47 17 79 54 20 67 62  3 46 68 70], a_shuffle_aclus: [  9  35  52  92  26  60  14  33  16  82  15  67  68  48  21  53  10  13  36 108  41 102  57  62  83  97  69  66  50  31  58  45  55  78  71  70  74  37 100  51  19   3   7  32  98  25  54  28  11  56  87  23  34  81  76  49   2   8  75 107   4  61  88  96 105  86  85  59  17  64  24 109  73  27  89  84   5  63  90  93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [25 41 14 78 55 71 37 63 76 20 22  6 49 32 79 52 21 51 11 13 29 54 23 36 62 64 43 70 45 39 24 38 40 16 68 65 61 67  0 58 47 75  7 59 53  4 34  5 35 72 10 66  9 48 56 15 44 57  1  2 50 31 18 26 30 28 60 73 12 27 33  8 74 17  3 46 77 69 19 42], a_shuffle_aclus: [ 34  58  19 108  74  96  54  85 105  27  31   9  67  49 109  70  28  69  15  17  41  73  32  53  84  86  60  93  62  56  33  55  57  23  90  87  83  89   2  78  64 102  10  81  71   7  51   8  52  97  14  88  13  66  75  21  61  76   3   4  68  48  25  35  45  37  82  98  16  36  50  11 100  24   5  63 107  92  26  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 75 63 77 18  8 48 40 72 68 19 52  3 64 15  2 41 27 26 70 37 53  4 28 39 36 29 69 22 21 67 12 55 51 60 49 59 78 79  0 71 25 57 34 43 76 31 33 42  5 20 54  1 45 13 46 32  6 17 66 24 35 58 11 44 61 16 23 65 14 47 10 73 74 38 56 50  7 62 30], a_shuffle_aclus: [ 13 102  85 107  25  11  66  57  97  90  26  70   5  86  21   4  58  36  35  93  54  71   7  37  56  53  41  92  31  28  89  16  74  69  82  67  81 108 109   2  96  34  76  51  60 105  48  50  59   8  27  73   3  62  17  63  49   9  24  88  33  52  78  15  61  83  23  32  87  19  64  14  98 100  55  75  68  10  84  45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [10 76 79  6 52 33 45  0 19 37 66 15 57  4 32 68 18 61 67 56 46 69 78 54 27 22 47 70 73 59 17 36 26  8 53 50 25 40  9 51 74 12 14  2 20 49 31  7 13 42 41 60 35 43 58 72 77 34 21 62 16 29 75 65 71 39 23 44 11  5  1 48 55 30 63 64 24  3 38 28], a_shuffle_aclus: [ 14 105 109   9  70  50  62   2  26  54  88  21  76   7  49  90  25  83  89  75  63  92 108  73  36  31  64  93  98  81  24  53  35  11  71  68  34  57  13  69 100  16  19   4  27  67  48  10  17  59  58  82  52  60  78  97 107  51  28  84  23  41 102  87  96  56  32  61  15   8   3  66  74  45  85  86  33   5  55  37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [15 53 28 12 67 14 68 18 69 48 76  8 33 64 49 50 62 44 32 13  3 22 37 25 56 77 46 52 39  9 73 10 38 61 63 41 75 26 66 43 65 79  7 54  0 20 42 29 45  6  1  4 27 34 21  5 60 59 71 17 11 35 31 74 72 23 47 55 19 16 51 78 57 58  2 36 24 30 70 40], a_shuffle_aclus: [ 21  71  37  16  89  19  90  25  92  66 105  11  50  86  67  68  84  61  49  17   5  31  54  34  75 107  63  70  56  13  98  14  55  83  85  58 102  35  88  60  87 109  10  73   2  27  59  41  62   9   3   7  36  51  28   8  82  81  96  24  15  52  48 100  97  32  64  74  26  23  69 108  76  78   4  53  33  45  93  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [76  8 45 73 15 51 18 50 75 46 36 32 54 34 59 19  5 40 11 43 41 69 66  9 48 10 20 35 71 77 21 52 38 29 31 53  4 57 25  2 60 24 39 74 64  6 55 23 62 65 63 70 16  3  0 14 61 79 42  7 26 47 12 37 58 68 30 67 56 78 72 33 49 27 28 22  1 17 44 13], a_shuffle_aclus: [105  11  62  98  21  69  25  68 102  63  53  49  73  51  81  26   8  57  15  60  58  92  88  13  66  14  27  52  96 107  28  70  55  41  48  71   7  76  34   4  82  33  56 100  86   9  74  32  84  87  85  93  23   5   2  19  83 109  59  10  35  64  16  54  78  90  45  89  75 108  97  50  67  36  37  31   3  24  61  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 25 10 56 48 33  8 15 76  2  0  1 12 67  4 13 49 30 58 59 40 77 22 20 28 50  7 36 41 63 78 21 57 47 11  3 75  5 18 44 61 43 37 34 74 31 35 46 52 16 70 29 55 66 54 27 23 62 14 19 38  9 73 45 39  6 71 69 60 68 24 17 72 32 79 64 26 42 53 51], a_shuffle_aclus: [ 87  34  14  75  66  50  11  21 105   4   2   3  16  89   7  17  67  45  78  81  57 107  31  27  37  68  10  53  58  85 108  28  76  64  15   5 102   8  25  61  83  60  54  51 100  48  52  63  70  23  93  41  74  88  73  36  32  84  19  26  55  13  98  62  56   9  96  92  82  90  33  24  97  49 109  86  35  59  71  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [79 52 17 39 14 36 12 21 54 13 50 46 49 24  7 30 53 61 67 16 78 32 72 40 66 65 51 20 55 27 35 58 77 48 33  2 37 71 18  4  3 76  9 31 23 15  6 69 10 26 44 75 25 41 56 28 59 29 63 22 64 60 74 70 45 73  5 57 19  0 34  1 47 62 11 68 42 38  8 43], a_shuffle_aclus: [109  70  24  56  19  53  16  28  73  17  68  63  67  33  10  45  71  83  89  23 108  49  97  57  88  87  69  27  74  36  52  78 107  66  50   4  54  96  25   7   5 105  13  48  32  21   9  92  14  35  61 102  34  58  75  37  81  41  85  31  86  82 100  93  62  98   8  76  26   2  51   3  64  84  15  90  59  55  11  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [34 54 40 45 67 37  6 29 31 58 65 74 76 33 62 24 27 18 10 75 63 11 22 53 19  4 64 78 39 48 12 38 20 79 47 17  5 43 41 68  2 61  1 25 56 46 16 57 32 36 23 50 70 35 30 15 72 26 60 52 55 69 51 21 13 44 77  7 73 49  9  3 42  0 71 66 28 14 59  8], a_shuffle_aclus: [ 51  73  57  62  89  54   9  41  48  78  87 100 105  50  84  33  36  25  14 102  85  15  31  71  26   7  86 108  56  66  16  55  27 109  64  24   8  60  58  90   4  83   3  34  75  63  23  76  49  53  32  68  93  52  45  21  97  35  82  70  74  92  69  28  17  61 107  10  98  67  13   5  59   2  96  88  37  19  81  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [51  5  3 44  8 57 45 30 72 66 25 34 35 17 53 52 60 70 13 71 31 40 74 58 61 73 20 26 15 59 14 41 16 19 37 47 29 54 68  6 22  7 39 65  2 48 27 24 56 10  9  0 77 11 18 69 38 49 79 28 21 78  1 55 76 32  4 75 43 33 63 67 42 64 62 50 46 12 36 23], a_shuffle_aclus: [ 69   8   5  61  11  76  62  45  97  88  34  51  52  24  71  70  82  93  17  96  48  57 100  78  83  98  27  35  21  81  19  58  23  26  54  64  41  73  90   9  31  10  56  87   4  66  36  33  75  14  13   2 107  15  25  92  55  67 109  37  28 108   3  74 105  49   7 102  60  50  85  89  59  86  84  68  63  16  53  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [78 57 39 13  1 69 77 44 30 62 41 15 51 47 45  0 49 11 50 66 28 26 60 34 71 24 73 17 16 58 14  9 42 12  7 36 76  5  4 72 56 19 25 74 40 61 63 54 64 46 48 38 23 29 22 35 32 27 59 55 21 37 68 65 75 33 43 70  2 53 20 67 31 52 10  3  6 18 79  8], a_shuffle_aclus: [108  76  56  17   3  92 107  61  45  84  58  21  69  64  62   2  67  15  68  88  37  35  82  51  96  33  98  24  23  78  19  13  59  16  10  53 105   8   7  97  75  26  34 100  57  83  85  73  86  63  66  55  32  41  31  52  49  36  81  74  28  54  90  87 102  50  60  93   4  71  27  89  48  70  14   5   9  25 109  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [44 22 63 31 36 15 43 38 60  5 69 23 79  8 25 13 52  1 62 48 59 61 55  0 29 45 27 68  3 12 21 64 47 11 70 53 17 72 26 10  6 16 46 74 76 28 20 42 54 75 32 39 58 40 66  7 73 51 77 65 57 35 19 50  4 18  2 56 71 41 30  9 67 14 78 33 49 37 34 24], a_shuffle_aclus: [ 61  31  85  48  53  21  60  55  82   8  92  32 109  11  34  17  70   3  84  66  81  83  74   2  41  62  36  90   5  16  28  86  64  15  93  71  24  97  35  14   9  23  63 100 105  37  27  59  73 102  49  56  78  57  88  10  98  69 107  87  76  52  26  68   7  25   4  75  96  58  45  13  89  19 108  50  67  54  51  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [12 14 77 70 40 34 15 51  8 20 60  4  5 24 58 37 36  2  6 25  1 33  0 16 21 32 42 78 76 44 49 27  9 38 19 68 65 53 64 71 48 13 52 43 59 50 73 69 66 11 39 67 46 55 47 74 26 79 23 75 61 57 63 62 10 28  7 29 56 30 72 35 45 18 31 41 54  3 17 22], a_shuffle_aclus: [ 16  19 107  93  57  51  21  69  11  27  82   7   8  33  78  54  53   4   9  34   3  50   2  23  28  49  59 108 105  61  67  36  13  55  26  90  87  71  86  96  66  17  70  60  81  68  98  92  88  15  56  89  63  74  64 100  35 109  32 102  83  76  85  84  14  37  10  41  75  45  97  52  62  25  48  58  73   5  24  31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 63 21 41  0  6 17 68 16 73 48 44 18 37 24 53  3 49 11 45 79 76 46 77 38 36  5 13 67 75  4 25 52 78 31  2 30 51 60  8 10 28 64 34 69 26 43 56 27 71 54 61 23 14 33 62 58  9 55  7 39 29 72 40 22 20 32 65 35 70 15 50  1 19 12 47 59 42 74 66], a_shuffle_aclus: [ 76  85  28  58   2   9  24  90  23  98  66  61  25  54  33  71   5  67  15  62 109 105  63 107  55  53   8  17  89 102   7  34  70 108  48   4  45  69  82  11  14  37  86  51  92  35  60  75  36  96  73  83  32  19  50  84  78  13  74  10  56  41  97  57  31  27  49  87  52  93  21  68   3  26  16  64  81  59 100  88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [35 50 68 10 59 79 72  0 45 77 20  6 16 69 64 76 14  9 26 24 58  3 44 38 29  7 28 61 18 70 13 25 49 65 57 48 17 78 39 42 15  5 21 23 31 71 55  2 54 33 74 41 62  8 67 73 52  4 46 43 36 34  1 75 19 40 11 32 30 63 60 47 22 12 56 37 53 66 27 51], a_shuffle_aclus: [ 52  68  90  14  81 109  97   2  62 107  27   9  23  92  86 105  19  13  35  33  78   5  61  55  41  10  37  83  25  93  17  34  67  87  76  66  24 108  56  59  21   8  28  32  48  96  74   4  73  50 100  58  84  11  89  98  70   7  63  60  53  51   3 102  26  57  15  49  45  85  82  64  31  16  75  54  71  88  36  69]\n",
      "a_shuffle_IDXs: [39 30 72 63 58 22 54 52 44 71 65 15 59 60 26 51 36 20  1 17 23 43 40 16 62 48 38 75 27  4 61  3 34 79 56 74 57 55 53  5  9 28 46 10 25 41 13 21 29  6 45 77 24 19 31 68 76 37  7 69  0 11 64  2 49 78 12 67 35 50 47 14 42 73 18  8 33 70 32 66], a_shuffle_aclus: [ 56  45  97  85  78  31  73  70  61  96  87  21  81  82  35  69  53  27   3  24  32  60  57  23  84  66  55 102  36   7  83   5  51 109  75 100  76  74  71   8  13  37  63  14  34  58  17  28  41   9  62 107  33  26  48  90 105  54  10  92   2  15  86   4  67 108  16  89  52  68  64  19  59  98  25  11  50  93  49  88]\n",
      "a_shuffle_IDXs: [70 22 39 26 15 75 10 49 59 61 50 30 58 35 14 52 18 68 29  3 65 73 72  5 63  2  0 20  6 79 16 48 60 27 53 19 34 23 43 56  8 36 54 51 44 74 67 57  1 32 38 64  7 40 77 55 31  9 41 13 25 45 69 76 12 46 47 37 71 24 62 66 42  4 78 28 33 11 17 21], a_shuffle_aclus: [ 93  31  56  35  21 102  14  67  81  83  68  45  78  52  19  70  25  90  41   5  87  98  97   8  85   4   2  27   9 109  23  66  82  36  71  26  51  32  60  75  11  53  73  69  61 100  89  76   3  49  55  86  10  57 107  74  48  13  58  17  34  62  92 105  16  63  64  54  96  33  84  88  59   7 108  37  50  15  24  28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [35 19 57 34 20 78 53  5 67 39 68 49  8 29 48 55 42 79 62  2 22 56 24 50 16 38 77 43 27 36  9 46 47 58 74 72 76 30  6 59 13 23 54  0 37 40 63  1 11 73 10 45 71 66  3  4 69 52 44 21 65 33 12  7 70 28 25 14 75 15 18 32 60 61 41 51 64 17 26 31], a_shuffle_aclus: [ 52  26  76  51  27 108  71   8  89  56  90  67  11  41  66  74  59 109  84   4  31  75  33  68  23  55 107  60  36  53  13  63  64  78 100  97 105  45   9  81  17  32  73   2  54  57  85   3  15  98  14  62  96  88   5   7  92  70  61  28  87  50  16  10  93  37  34  19 102  21  25  49  82  83  58  69  86  24  35  48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 6 29 74 43 77 36 18 65 63 33 41 56 27 64 49 50 10 70 23 46 13 51 25 14 73 17 61 45 47 71  5 48  8 21 62 57  0 72 39 58 11 55 40 44 69 60 75  9 32  7 28 38 37 22 52 15 26 66 31 35 30 34 24  1  3 54 79 67 68  2 76 42 19 78  4 16 53 59 12 20], a_shuffle_aclus: [  9  41 100  60 107  53  25  87  85  50  58  75  36  86  67  68  14  93  32  63  17  69  34  19  98  24  83  62  64  96   8  66  11  28  84  76   2  97  56  78  15  74  57  61  92  82 102  13  49  10  37  55  54  31  70  21  35  88  48  52  45  51  33   3   5  73 109  89  90   4 105  59  26 108   7  23  71  81  16  27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [35 64 68 14 26 24 67 78 62  9 53 47 72 36 20 32 79 48 33 31 46 49 12 74  6 11 27 41 58 69 17 73 38 40 15  0 16  1 63 10  5 65 28 21  3 29 13 59 34 52 75  2  7 51 70 71 30 61 56 54 25 23 57 45 37 19  8 60 50 22 18  4 55 66 44 39 43 76 77 42], a_shuffle_aclus: [ 52  86  90  19  35  33  89 108  84  13  71  64  97  53  27  49 109  66  50  48  63  67  16 100   9  15  36  58  78  92  24  98  55  57  21   2  23   3  85  14   8  87  37  28   5  41  17  81  51  70 102   4  10  69  93  96  45  83  75  73  34  32  76  62  54  26  11  82  68  31  25   7  74  88  61  56  60 105 107  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [11 75 19 14 35 73 15 50  6  5 17 28 42 36 24 57 66 21 13 32 61 56 26 34 51 62 47 43 12 68 44  9 20 58 71 53 69 59  4 72 31 30 55 70 40 38 63 76 45 33 79 39  7  0 49 22 78 52  3  1 16 77 48 65 46 67 27 10 18 41 25 54  8 23 60 37 74 64  2 29], a_shuffle_aclus: [ 15 102  26  19  52  98  21  68   9   8  24  37  59  53  33  76  88  28  17  49  83  75  35  51  69  84  64  60  16  90  61  13  27  78  96  71  92  81   7  97  48  45  74  93  57  55  85 105  62  50 109  56  10   2  67  31 108  70   5   3  23 107  66  87  63  89  36  14  25  58  34  73  11  32  82  54 100  86   4  41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [48 40 55 43 19 28 41 57 50 20 45 74 29 76 18 21 56  7 67 72  0 27 53  4 15 17 31 38  9 25  1 63 42 13 75 32 71 59 34 69 16 11 68 73 14 61 23 77 79 24 64 66 36 47 46 60  2 51 44  3 22 62 65 37 26 49 70 35 52 39  5 12  8 30 78  6 54 58 10 33], a_shuffle_aclus: [ 66  57  74  60  26  37  58  76  68  27  62 100  41 105  25  28  75  10  89  97   2  36  71   7  21  24  48  55  13  34   3  85  59  17 102  49  96  81  51  92  23  15  90  98  19  83  32 107 109  33  86  88  53  64  63  82   4  69  61   5  31  84  87  54  35  67  93  52  70  56   8  16  11  45 108   9  73  78  14  50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [34 28 19 69 78 22 41 38 11 72 73 35 32 55 74 33 29 71  7  5 48 60 30 66 26 40 61 54 49  8 21 65 27 14 23  3 77 67 76 63 37 53 57 56 51 18 52 75 25 31 36  9 50 24 68 44 42  1 58 13 39 20 62  0 10 17 47  6  2 12 16 59 79  4 43 45 70 46 15 64], a_shuffle_aclus: [ 51  37  26  92 108  31  58  55  15  97  98  52  49  74 100  50  41  96  10   8  66  82  45  88  35  57  83  73  67  11  28  87  36  19  32   5 107  89 105  85  54  71  76  75  69  25  70 102  34  48  53  13  68  33  90  61  59   3  78  17  56  27  84   2  14  24  64   9   4  16  23  81 109   7  60  62  93  63  21  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60 31 10 50  8  4  3 72 61 16  2 28 63 19 26 40 66 47 64 58 56 17 75 74 69 62 35 78 46 49 59 37 29 79 57  9 23 36 71 65 70 48 54 38 52 11 43 55 39 77 18  7 45  5  1 21  6 12 67 73 76 42 30 24 51 14 25 33 68 13 34 15 27 32 53 41 20 22 44  0], a_shuffle_aclus: [ 82  48  14  68  11   7   5  97  83  23   4  37  85  26  35  57  88  64  86  78  75  24 102 100  92  84  52 108  63  67  81  54  41 109  76  13  32  53  96  87  93  66  73  55  70  15  60  74  56 107  25  10  62   8   3  28   9  16  89  98 105  59  45  33  69  19  34  50  90  17  51  21  36  49  71  58  27  31  61   2]\n",
      "a_shuffle_IDXs: [26 30 63 60 18  2 55 68 10 45  6 16 58 11 75 51 50  5 37 40 28 41 21 20 76 38 43 56 15 64 12  8 42 72 36 78 57 66 27  3 61  0 53 32 33 29 67 74 49  9 46 34  1 69 73 23 24 44 39 48 19 31 62 25  4 54 14 79  7 52 47 13 70 59 71 77 65 35 22 17], a_shuffle_aclus: [ 35  45  85  82  25   4  74  90  14  62   9  23  78  15 102  69  68   8  54  57  37  58  28  27 105  55  60  75  21  86  16  11  59  97  53 108  76  88  36   5  83   2  71  49  50  41  89 100  67  13  63  51   3  92  98  32  33  61  56  66  26  48  84  34   7  73  19 109  10  70  64  17  93  81  96 107  87  52  31  24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [49 43 40 19  0 37 32 22 41 66 10 36 31 68 53 27 58 46 52 16  8 38 23 77 63 70 14 57 44 12 24 78 13 72 67 54 11 59 76 74 64  9  7 79  4 56 48 60 47 69 71 75 45  2 17 15 28 61 50 29 33  5 42 34  6 25 35  3 73 18 39 62 65  1 30 20 55 26 51 21], a_shuffle_aclus: [ 67  60  57  26   2  54  49  31  58  88  14  53  48  90  71  36  78  63  70  23  11  55  32 107  85  93  19  76  61  16  33 108  17  97  89  73  15  81 105 100  86  13  10 109   7  75  66  82  64  92  96 102  62   4  24  21  37  83  68  41  50   8  59  51   9  34  52   5  98  25  56  84  87   3  45  27  74  35  69  28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [17 25  3 73 44 30 68 41 38  8 77 67 14 20 36 29 59 45 55  4 12 39 27 54 31 24 43  0 32 47 66 49 78 64 21 22 18 51 16 79 19 69 37 33 74 72 57 34 35 52 42 65 28 15 58 23 48 62 60 26  9 50 53 75 61 11 71 63 76 70 13  2  6  1 46 56 10  5  7 40], a_shuffle_aclus: [ 24  34   5  98  61  45  90  58  55  11 107  89  19  27  53  41  81  62  74   7  16  56  36  73  48  33  60   2  49  64  88  67 108  86  28  31  25  69  23 109  26  92  54  50 100  97  76  51  52  70  59  87  37  21  78  32  66  84  82  35  13  68  71 102  83  15  96  85 105  93  17   4   9   3  63  75  14   8  10  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 66  3  1 78 69 21 75 39 57 17 44  5 58 72 20 30 48 18 61 56 70 40 65 25 49 28 15  0  2 14 77 10 23 51 16 13 37 63  6 76 47 43 12 74 64 53 68  7 22 11 19 36 27 41 31  9 52 32 60 38 55 54 71 45  4 33 46 79 62 24 73 26 29  8 35 42 34 67 59], a_shuffle_aclus: [ 68  88   5   3 108  92  28 102  56  76  24  61   8  78  97  27  45  66  25  83  75  93  57  87  34  67  37  21   2   4  19 107  14  32  69  23  17  54  85   9 105  64  60  16 100  86  71  90  10  31  15  26  53  36  58  48  13  70  49  82  55  74  73  96  62   7  50  63 109  84  33  98  35  41  11  52  59  51  89  81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 8 26  7 40 31 47 73 56 71 68 14  3 67 27 42 12 33 45 65 30 13 15 24 49 74 69 37 23  0  5 18 22 38 54 20 21 64 79 58 35  9 48 51 75 78 72 16 10 36 43 44 32 11 53 66 55 39 19 50 52  2 34 63 29 41 70  6 57 28 61  1  4 25 59 76 62 17 77 46 60], a_shuffle_aclus: [ 11  35  10  57  48  64  98  75  96  90  19   5  89  36  59  16  50  62  87  45  17  21  33  67 100  92  54  32   2   8  25  31  55  73  27  28  86 109  78  52  13  66  69 102 108  97  23  14  53  60  61  49  15  71  88  74  56  26  68  70   4  51  85  41  58  93   9  76  37  83   3   7  34  81 105  84  24 107  63  82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [52 50 41 33 74 42 19 79 73  3 23 66 12  9 29 22 38 43  5 68 31 70  0 67 61  8 15  7 32 72 55 16  2 65 21 78 17 28 69  1 76 60  4 56 14 75 40 18 48 24 36 49 54 13 30 62 37 59 58 63 53 77 26 45 20 11 34 39 10 35 47 27 44 71 46 64  6 25 57 51], a_shuffle_aclus: [ 70  68  58  50 100  59  26 109  98   5  32  88  16  13  41  31  55  60   8  90  48  93   2  89  83  11  21  10  49  97  74  23   4  87  28 108  24  37  92   3 105  82   7  75  19 102  57  25  66  33  53  67  73  17  45  84  54  81  78  85  71 107  35  62  27  15  51  56  14  52  64  36  61  96  63  86   9  34  76  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [24 36 30 10 75 20 57 59 55 70 34 28 42 58 19 11 26 68 64 51 73  2 32 79 66 48  9  7 67 21 18 46 54 25  4 15 22 60 44 16 47 61 17 65 29 12 31 39 43 78 53 76 72 27 38 45 37 41 33 35  6 13 74 63 69 77 49 14  5 23 40  1 52  3 62 56 71 50  0  8], a_shuffle_aclus: [ 33  53  45  14 102  27  76  81  74  93  51  37  59  78  26  15  35  90  86  69  98   4  49 109  88  66  13  10  89  28  25  63  73  34   7  21  31  82  61  23  64  83  24  87  41  16  48  56  60 108  71 105  97  36  55  62  54  58  50  52   9  17 100  85  92 107  67  19   8  32  57   3  70   5  84  75  96  68   2  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 1 39 45 23 74 24  3 29 42 71 69 70 33 17 47 37 31 14 73 11 62  5 54 16 22 40 48 61 72 75 53 46 12 32 25 30 35 78 50 36 79 64 27 59  9 28 65 67 18  4 21 19 51  0 77 41 15  2 76  8  6 66 68 58 56 20  7 10 60 34 38 44 63 57 43 55 52 49 13 26], a_shuffle_aclus: [  3  56  62  32 100  33   5  41  59  96  92  93  50  24  64  54  48  19  98  15  84   8  73  23  31  57  66  83  97 102  71  63  16  49  34  45  52 108  68  53 109  86  36  81  13  37  87  89  25   7  28  26  69   2 107  58  21   4 105  11   9  88  90  78  75  27  10  14  82  51  55  61  85  76  60  74  70  67  17  35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [47 37 23 52 17 74 35 58 15 50 18 55 77 62 70  3 42 73 11 21 68 46 65  2 16 66 78 51 53 64  0 24 38  7 79 59 48 36 56 72 49 71  8 76 54 20 31 75 63 39 14 41 26 34  4  9 60 19  1 12 40 28 43 10 57 27 45  5 25 67 61 29 32 22 30 13 33  6 44 69], a_shuffle_aclus: [ 64  54  32  70  24 100  52  78  21  68  25  74 107  84  93   5  59  98  15  28  90  63  87   4  23  88 108  69  71  86   2  33  55  10 109  81  66  53  75  97  67  96  11 105  73  27  48 102  85  56  19  58  35  51   7  13  82  26   3  16  57  37  60  14  76  36  62   8  34  89  83  41  49  31  45  17  50   9  61  92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [33 78  2 26 67 58 17 45 12 55 72 37 48 38 68 50 19 36 32  9 73 64 10  5 21 14 16 60 44 24 31  7 69 52 46 43 39 27  0 29 70 51 22 74 23 35 42 57 40 65 53  4  3 11 62 49 20 18 47 61 59  8 77 34 75 63 76  1  6 28 71 66 13 41 56 25 30 79 54 15], a_shuffle_aclus: [ 50 108   4  35  89  78  24  62  16  74  97  54  66  55  90  68  26  53  49  13  98  86  14   8  28  19  23  82  61  33  48  10  92  70  63  60  56  36   2  41  93  69  31 100  32  52  59  76  57  87  71   7   5  15  84  67  27  25  64  83  81  11 107  51 102  85 105   3   9  37  96  88  17  58  75  34  45 109  73  21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 21 37 61  4 58 69 29 12 73 11 14 75 56 71 78  0 54 59 53 35 25 38  5 42 34 60 43 36  2  6 68 48 18  9 55 17 16 22 70 23  3 13 31 46 24 79 45 62 28 44  8 47 65 15 51 19 76 27  7 20 66 26 52 41 74 67 49 32 33 30 77 40 64  1 50 10 57 72 39], a_shuffle_aclus: [ 85  28  54  83   7  78  92  41  16  98  15  19 102  75  96 108   2  73  81  71  52  34  55   8  59  51  82  60  53   4   9  90  66  25  13  74  24  23  31  93  32   5  17  48  63  33 109  62  84  37  61  11  64  87  21  69  26 105  36  10  27  88  35  70  58 100  89  67  49  50  45 107  57  86   3  68  14  76  97  56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [22 50 19 52 47 30 70 26 63 56 35 61  2 33  5  1 23 60 76 78 15 45 68 57 64 32 66 18 42 71 44 27 79 40 16 69 11 75 13 54 37 29  0 53  7 24 12 65 62 72 25 10  8 38 73  3 51 14 17 34 31 41 21 67 39 46 36 49 58 28 20 43 59  9  4 74 48 55  6 77], a_shuffle_aclus: [ 31  68  26  70  64  45  93  35  85  75  52  83   4  50   8   3  32  82 105 108  21  62  90  76  86  49  88  25  59  96  61  36 109  57  23  92  15 102  17  73  54  41   2  71  10  33  16  87  84  97  34  14  11  55  98   5  69  19  24  51  48  58  28  89  56  63  53  67  78  37  27  60  81  13   7 100  66  74   9 107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 58 45 39 57 50 59 33 72 13 65 78 25 61 62 14  3 56  0 66  7 18 21 52 79 70 37 11 30 27 19 55 12 36  8  1 77 54 76 24 31 74 47 15 32  2  5 48 73 43 69 28 42 51 67 44 17 41 64 10 34  4 22 75 26 29 38 68 53 35 16 40 60 49 23 20  9  6 46 71], a_shuffle_aclus: [ 85  78  62  56  76  68  81  50  97  17  87 108  34  83  84  19   5  75   2  88  10  25  28  70 109  93  54  15  45  36  26  74  16  53  11   3 107  73 105  33  48 100  64  21  49   4   8  66  98  60  92  37  59  69  89  61  24  58  86  14  51   7  31 102  35  41  55  90  71  52  23  57  82  67  32  27  13   9  63  96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 2 52 60 24 32  1 44 17 72 70 74 33 16 28 27 49 41 63 56 46 11 23 43 65 50 25 10 37 30 39  7 55  4 58 22 69 77  9 15 40 20 19 53  0 34 29 64 13 48 21 54 18 71 42 36 66 79  6 67 51 12 76 75  5 45 26 14 38  3 73 62 68 59 78 31 35 47 61 57  8], a_shuffle_aclus: [  4  70  82  33  49   3  61  24  97  93 100  50  23  37  36  67  58  85  75  63  15  32  60  87  68  34  14  54  45  56  10  74   7  78  31  92 107  13  21  57  27  26  71   2  51  41  86  17  66  28  73  25  96  59  53  88 109   9  89  69  16 105 102   8  62  35  19  55   5  98  84  90  81 108  48  52  64  83  76  11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 1 10 26 43 62 21 58 34 37 42 53 78 40 38 73 79 12 27 50 63 56 65 15 11  6 54 76 57  4 20 25 22 77 49 74  5  8 29 67 70 28 31 41 32 60 68 72 44 17 18 19 16 71 51 24 23 61  3  0 64 75 13  7 14 35 47 52 59 39 69 55  2 46 30  9 48 33 45 66 36], a_shuffle_aclus: [  3  14  35  60  84  28  78  51  54  59  71 108  57  55  98 109  16  36  68  85  75  87  21  15   9  73 105  76   7  27  34  31 107  67 100   8  11  41  89  93  37  48  58  49  82  90  97  61  24  25  26  23  96  69  33  32  83   5   2  86 102  17  10  19  52  64  70  81  56  92  74   4  63  45  13  66  50  62  88  53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [60 36 63 49 20 69 13 15 28  7 37 42 48 70 10 29 51 23 58 16  1 30 78 11 14 79 64 65 62 61 46  0 32 67  6 40 68 50 21 18 31  3 19 26 44 56 72 66 59 17 57  2 47 41 74 52 76 54 39 53 22 43 35 33  8  4 34 24  9 27 12 73 55 75  5 38 45 77 71 25], a_shuffle_aclus: [ 82  53  85  67  27  92  17  21  37  10  54  59  66  93  14  41  69  32  78  23   3  45 108  15  19 109  86  87  84  83  63   2  49  89   9  57  90  68  28  25  48   5  26  35  61  75  97  88  81  24  76   4  64  58 100  70 105  73  56  71  31  60  52  50  11   7  51  33  13  36  16  98  74 102   8  55  62 107  96  34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [52 25 59 74 20 76 39 45 33 68 14 57  5 71 40  0 30 26 70 61 56 15 79 12 32 53 44 78  6 72  2 65 37  8 13 18 49 36 67 69 51 73 16 47 27 28 75 19  3 21 24  9 60 48 35 66 38  7 22 42 77 23 50 63 58  4 54 29 46 34  1 41 17 31 11 55 62 64 10 43], a_shuffle_aclus: [ 70  34  81 100  27 105  56  62  50  90  19  76   8  96  57   2  45  35  93  83  75  21 109  16  49  71  61 108   9  97   4  87  54  11  17  25  67  53  89  92  69  98  23  64  36  37 102  26   5  28  33  13  82  66  52  88  55  10  31  59 107  32  68  85  78   7  73  41  63  51   3  58  24  48  15  74  84  86  14  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [41 79  7 54 20 55 74 73  5 15 50 58 71 33 44 65 12 49 63 14 67 57 17 64 66 34 10 30 51 32 19  0 70 68 48 47 11  3 46 69 27 77 23  9 21 26 60 28 52 61 29 31  2 53 24 72 76 18 42 62 22 16 43  6  4 37 35 40 56 38  1 25 13 36 39 75 78  8 59 45], a_shuffle_aclus: [ 58 109  10  73  27  74 100  98   8  21  68  78  96  50  61  87  16  67  85  19  89  76  24  86  88  51  14  45  69  49  26   2  93  90  66  64  15   5  63  92  36 107  32  13  28  35  82  37  70  83  41  48   4  71  33  97 105  25  59  84  31  23  60   9   7  54  52  57  75  55   3  34  17  53  56 102 108  11  81  62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [49 46 26 66 27 31 25 19 44 78  3 47 15 60 76 34  4  1  9 30 56 58 48 62 57 63 50 38  2 55  8 45 74 29 42 79 65 75  5 51 77 23 73  7 41 11  0 28 61 70 53 16 40 67 71 14 39 37 35 59 32 36 69 43  6 64 18 52 17 12 54 24 22 10 20 21 33 72 68 13], a_shuffle_aclus: [ 67  63  35  88  36  48  34  26  61 108   5  64  21  82 105  51   7   3  13  45  75  78  66  84  76  85  68  55   4  74  11  62 100  41  59 109  87 102   8  69 107  32  98  10  58  15   2  37  83  93  71  23  57  89  96  19  56  54  52  81  49  53  92  60   9  86  25  70  24  16  73  33  31  14  27  28  50  97  90  17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [26 13 70  1  8 65 61 58 23 68 33 45 25 28 74  9 18  3 41  0 20 79 54 59  2 29 57 27 77 44  7 71  6 55 51 17 47 52 36 63 34 19 37 46 39 15  5 76 32 75 10 12 78 60 35 48 56 11 62 53 14 40 50 72 30  4 64 43 22 67 38 66 31 69 49 16 24 21 73 42], a_shuffle_aclus: [ 35  17  93   3  11  87  83  78  32  90  50  62  34  37 100  13  25   5  58   2  27 109  73  81   4  41  76  36 107  61  10  96   9  74  69  24  64  70  53  85  51  26  54  63  56  21   8 105  49 102  14  16 108  82  52  66  75  15  84  71  19  57  68  97  45   7  86  60  31  89  55  88  48  92  67  23  33  28  98  59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [28 78 16 29 51 58 70 57 52 53 30  8 26 49  3  1 74 34 76 77 22  0 23 54 62 45 17  9  4 43 11 50  6  7 66 19 14 37 55  5 60 31 12 32 59 25 67 65 13  2 48 10 27 47 61 69 44 41 36 15 63 73 40 38 68 42 56 21 18 71 35 75 24 72 39 79 20 46 33 64], a_shuffle_aclus: [ 37 108  23  41  69  78  93  76  70  71  45  11  35  67   5   3 100  51 105 107  31   2  32  73  84  62  24  13   7  60  15  68   9  10  88  26  19  54  74   8  82  48  16  49  81  34  89  87  17   4  66  14  36  64  83  92  61  58  53  21  85  98  57  55  90  59  75  28  25  96  52 102  33  97  56 109  27  63  50  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 62 16 74 44 58 20 68 35 48 72 79 42 31 19 38 59 23 49  5 57 26 40  4 75 65  7 18 60 54 32 41 55 71 67 39 11 45 21 28 61  2 43 64 63 33 73 69 14 78 56 30 17 10 27 53 76 66 34 70 12 46 25 13 29  8  0 15 51 47 36  1 77  6 22 24 37 52  3 50], a_shuffle_aclus: [ 13  84  23 100  61  78  27  90  52  66  97 109  59  48  26  55  81  32  67   8  76  35  57   7 102  87  10  25  82  73  49  58  74  96  89  56  15  62  28  37  83   4  60  86  85  50  98  92  19 108  75  45  24  14  36  71 105  88  51  93  16  63  34  17  41  11   2  21  69  64  53   3 107   9  31  33  54  70   5  68]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 0 36 13 24 48  6 35 46 39 40 30 70 26 57  8 18  5 61 68 52 42 53  9 28  1 78 17 77  3 62 41 73 79 31 71  2  4  7 63 34 47 32 22 76 69 60 75 14 64 66 50 19 54 58 25 15 55 49 11 37 29 74 33 59 23 27 10 72 56 67 65 38 12 51 16 45 44 20 21 43], a_shuffle_aclus: [  2  53  17  33  66   9  52  63  56  57  45  93  35  76  11  25   8  83  90  70  59  71  13  37   3 108  24 107   5  84  58  98 109  48  96   4   7  10  85  51  64  49  31 105  92  82 102  19  86  88  68  26  73  78  34  21  74  67  15  54  41 100  50  81  32  36  14  97  75  89  87  55  16  69  23  62  61  27  28  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [63 50 18 68 38 71 33 54 62 28 79 27 39 69 14 44  6 58 57 15 72 42  0 55 56 35  3 77 43 40 53 49  1 26 70 78 65 45 25 67 66 52 22 60 37  8 16 29 13 48 10  2 17 34 76 11 12  4 47 24 21 46 59 61 20 64 31  5  9 36 32 30 19 51 41 75 74 73  7 23], a_shuffle_aclus: [ 85  68  25  90  55  96  50  73  84  37 109  36  56  92  19  61   9  78  76  21  97  59   2  74  75  52   5 107  60  57  71  67   3  35  93 108  87  62  34  89  88  70  31  82  54  11  23  41  17  66  14   4  24  51 105  15  16   7  64  33  28  63  81  83  27  86  48   8  13  53  49  45  26  69  58 102 100  98  10  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 71 47 50  2 59 19 21 74 33 39 44 66 52 79 43 17 72 55 11 60 41 67 65 22 34 68  5  6 31  4 40 49 37 35 18 69 20 64 36 13 62 16 23 78 56 63 25 73 42 58 15 45 38  7 29  1 53 61 10 70  8 26 30 24  3 77 54 46 75 12 14 48 76 27 32 28  0  9 51], a_shuffle_aclus: [ 76  96  64  68   4  81  26  28 100  50  56  61  88  70 109  60  24  97  74  15  82  58  89  87  31  51  90   8   9  48   7  57  67  54  52  25  92  27  86  53  17  84  23  32 108  75  85  34  98  59  78  21  62  55  10  41   3  71  83  14  93  11  35  45  33   5 107  73  63 102  16  19  66 105  36  49  37   2  13  69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [ 9 69 56 25 48 53 24 62 68 57 59  0  7 72 66 28 29 50 15 36 16 21 26 27  6 17  4 30 74  3 13  1 73  8 18 58  5 31 70 67 14 44 37 35 32 38 47 64 79 63 61 42 46 71 43 12 60 49 51 75 77 65 45 39 34 20 78 76 10 41 55 19 52 40 33 54  2 11 22 23], a_shuffle_aclus: [ 13  92  75  34  66  71  33  84  90  76  81   2  10  97  88  37  41  68  21  53  23  28  35  36   9  24   7  45 100   5  17   3  98  11  25  78   8  48  93  89  19  61  54  52  49  55  64  86 109  85  83  59  63  96  60  16  82  67  69 102 107  87  62  56  51  27 108 105  14  58  74  26  70  57  50  73   4  15  31  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [33 76 43 70 63 42 36 14 77 75 66 74 61 16 72 20  5 21 65 54 56 15 64  1  7 24 62  6 37 53 49 44 25 57 23  4 19 10 18 46 17 71  3 55 41 79 45 12 30 51  0 32 29 40 31 73  8 13 48 69 34 78 59 67 39 50  9 60 47 35  2 22 58 27 11 68 38 26 52 28], a_shuffle_aclus: [ 50 105  60  93  85  59  53  19 107 102  88 100  83  23  97  27   8  28  87  73  75  21  86   3  10  33  84   9  54  71  67  61  34  76  32   7  26  14  25  63  24  96   5  74  58 109  62  16  45  69   2  49  41  57  48  98  11  17  66  92  51 108  81  89  56  68  13  82  64  52   4  31  78  36  15  90  55  35  70  37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [66  4 29 23 72 32 75 30 25 31 54 47 64 46  0 15 76 55 27 73 53 78 13 49 45 65 19 11 79 63 71 51 42 50 35 24 41 38  5 44 43  3 20 57 67 56 17  8 22 18  7 33 68 70 74 61 62  6 40 77 21 48  9 16 34 12 10 28 69  2 37  1 52 36 26 60 58 59 39 14], a_shuffle_aclus: [ 88   7  41  32  97  49 102  45  34  48  73  64  86  63   2  21 105  74  36  98  71 108  17  67  62  87  26  15 109  85  96  69  59  68  52  33  58  55   8  61  60   5  27  76  89  75  24  11  31  25  10  50  90  93 100  83  84   9  57 107  28  66  13  23  51  16  14  37  92   4  54   3  70  53  35  82  78  81  56  19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [48 28 40 45  5 49 60 43 13 58 69 77  0  2 22 20 16 41 78 29  9 70 14 31 11 57 73  6 67  4 34 55 74 62  8 72 38 51 36 24 46 30 61 25 75 66 47  7  1 79  3 68 27 56 26 59 65 52 64 37 44 39 54 23 50 33 71 12 32 17 76 10 42 63 53 18 21 35 19 15], a_shuffle_aclus: [ 66  37  57  62   8  67  82  60  17  78  92 107   2   4  31  27  23  58 108  41  13  93  19  48  15  76  98   9  89   7  51  74 100  84  11  97  55  69  53  33  63  45  83  34 102  88  64  10   3 109   5  90  36  75  35  81  87  70  86  54  61  56  73  32  68  50  96  16  49  24 105  14  59  85  71  25  28  52  26  21]\n",
      "a_shuffle_IDXs: [76 77 54 71 78 56 23 39  2 20  3 15 73 31 17  6 45 64 42 75 55 10 11  9 16 34 38 48 50 63 61 51 52 19 60 18 29 66 74 13 43 12 58 35 67  0 57 40 69 22 65 62 47 70 59 26 25 37 32  5 28 36  4 21 68 53 14 33 30 41 79  8  7 49  1 27 24 46 72 44], a_shuffle_aclus: [105 107  73  96 108  75  32  56   4  27   5  21  98  48  24   9  62  86  59 102  74  14  15  13  23  51  55  66  68  85  83  69  70  26  82  25  41  88 100  17  60  16  78  52  89   2  76  57  92  31  87  84  64  93  81  35  34  54  49   8  37  53   7  28  90  71  19  50  45  58 109  11  10  67   3  36  33  63  97  61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [32 35 19 13 78 42 38 28 48 40 34 52 44 12 27 21 51 46 43  0 17 24 71 53 37 50 66 47  2 63 20 49 70 33 58 77  1 39 30  9 69 73  6 79  5 55 64 68 59 29 54 45 25 57 18 75 14 23 10 60 72  3 16 36 31 41 74 22 11 26 61 62  7 15 76 67 65 56  8  4], a_shuffle_aclus: [ 49  52  26  17 108  59  55  37  66  57  51  70  61  16  36  28  69  63  60   2  24  33  96  71  54  68  88  64   4  85  27  67  93  50  78 107   3  56  45  13  92  98   9 109   8  74  86  90  81  41  73  62  34  76  25 102  19  32  14  82  97   5  23  53  48  58 100  31  15  35  83  84  10  21 105  89  87  75  11   7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [48  1 62 64 66 63 72 42 47 16 58 23 30  5 32 36 50 75 22 49  4 12 24 60 40 19 28 79 10 26 14 51 68 39 56 53 54 20 70 76 65 43 38 69 15 41 35 37 31 17  9  0 77 25 74 73 34 52 67  2 55 27 11  8 59 46  7 33 29 18 13 57 61 45 71 44 21 78  3  6], a_shuffle_aclus: [ 66   3  84  86  88  85  97  59  64  23  78  32  45   8  49  53  68 102  31  67   7  16  33  82  57  26  37 109  14  35  19  69  90  56  75  71  73  27  93 105  87  60  55  92  21  58  52  54  48  24  13   2 107  34 100  98  51  70  89   4  74  36  15  11  81  63  10  50  41  25  17  76  83  62  96  61  28 108   5   9]\n",
      "a_shuffle_IDXs: [64 15 31 17 12 37  3 33  7 45 58 41  0 63 21 27 68 10 19 67  1 52 14  6 54 72 71 25 65 44 29 38 62 36 49 43 35 40 18 26 61 22 73 16 75 24 79 39 53 78 59  4  5 51 76 57 69 28 46 20  8 47 66 23 70 42 34 74 48 11  9 55 30  2 56 60 77 13 50 32], a_shuffle_aclus: [ 86  21  48  24  16  54   5  50  10  62  78  58   2  85  28  36  90  14  26  89   3  70  19   9  73  97  96  34  87  61  41  55  84  53  67  60  52  57  25  35  83  31  98  23 102  33 109  56  71 108  81   7   8  69 105  76  92  37  63  27  11  64  88  32  93  59  51 100  66  15  13  74  45   4  75  82 107  17  68  49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [74 46 31  9 73 11 17  4 39 24  6 26 25 13 69 43 48 54 27 50 33 79 56 12 61 78 76 58  2 35 15 63 75 70 22  1 41 44 14 38 42 20 30 36  3 40 66 62 53 32 18 10 45  5  8 34 55 64 29 72 77 59 37 52  7 51 67 57  0 71 65 16 19 60 23 47 49 68 28 21], a_shuffle_aclus: [100  63  48  13  98  15  24   7  56  33   9  35  34  17  92  60  66  73  36  68  50 109  75  16  83 108 105  78   4  52  21  85 102  93  31   3  58  61  19  55  59  27  45  53   5  57  88  84  71  49  25  14  62   8  11  51  74  86  41  97 107  81  54  70  10  69  89  76   2  96  87  23  26  82  32  64  67  90  37  28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [68 47 72 17 73 55 13 79 39 46 15 76 43  8 77 52 61 66 11 56 36 33 78 26 50 71 12 45 29 62 63 67 57 14 58 51 70  6 48 53 10  5 54 37  0 34  3 74 21 22 49 27 40 28 35  4  1 30 20 41  2 65 25  9 23 44  7 38 19 42 64 31 18 69 59 16 24 32 60 75], a_shuffle_aclus: [ 90  64  97  24  98  74  17 109  56  63  21 105  60  11 107  70  83  88  15  75  53  50 108  35  68  96  16  62  41  84  85  89  76  19  78  69  93   9  66  71  14   8  73  54   2  51   5 100  28  31  67  36  57  37  52   7   3  45  27  58   4  87  34  13  32  61  10  55  26  59  86  48  25  92  81  23  33  49  82 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [43  3 53 23 11 41 31 30 42 66  4 65 64 79 77 71 39 46 58 19 62 20 63 60 15 70 52 27  5 57 68 36 45 21 56 33 37 73  9 17 25 14 50 55  7 54 44  6 10  1 16 51 34 69 75 72 35 32 22 78 59  8 48 28 49 12 18 67 38 13 61  0 26 24 76 40 74 47  2 29], a_shuffle_aclus: [ 60   5  71  32  15  58  48  45  59  88   7  87  86 109 107  96  56  63  78  26  84  27  85  82  21  93  70  36   8  76  90  53  62  28  75  50  54  98  13  24  34  19  68  74  10  73  61   9  14   3  23  69  51  92 102  97  52  49  31 108  81  11  66  37  67  16  25  89  55  17  83   2  35  33 105  57 100  64   4  41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [59 22 62 72 56 73 23 55 13 61 24 77 63  4 18 39 54 17 33 10 36 16 34 51  3 32 28 58 26 67 48 41 19 12 25  5 71  9  0 21  7 79 64 65 52 20 35 49 38 60  8 57 27 74 76  1 30 14 37 45  2 29 75 53 47 43 44 11 46 50  6 69 42 31 15 70 40 68 78 66], a_shuffle_aclus: [ 81  31  84  97  75  98  32  74  17  83  33 107  85   7  25  56  73  24  50  14  53  23  51  69   5  49  37  78  35  89  66  58  26  16  34   8  96  13   2  28  10 109  86  87  70  27  52  67  55  82  11  76  36 100 105   3  45  19  54  62   4  41 102  71  64  60  61  15  63  68   9  92  59  48  21  93  57  90 108  88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [65 52 59 63  3 57 50 17 25 27 51 26 37 41 70 49 75 61  0 73 53 15 79 45 14  8 68 24 28 29 62 64  9 16 69 11 58 77  5 31  1  6 72 10 33 48 66 44 56 12 21 34 78 30 36 18 76 54 32 42 20 71 13 35  4 19 38 55 39 40 47  7 46 74  2 60 23 67 22 43], a_shuffle_aclus: [ 87  70  81  85   5  76  68  24  34  36  69  35  54  58  93  67 102  83   2  98  71  21 109  62  19  11  90  33  37  41  84  86  13  23  92  15  78 107   8  48   3   9  97  14  50  66  88  61  75  16  28  51 108  45  53  25 105  73  49  59  27  96  17  52   7  26  55  74  56  57  64  10  63 100   4  82  32  89  31  60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [64 39 28  8 25 13  6 56 15 23 17 21 78 20  9  7  5 46 44 11 19 69 59 26 63 48 67 31 33 14  0 50 29 18 43  1 65 24 72 60 16 42 52 73 62 12 38 66 61 41 79 55 57 77 40 45 32 58 68 71 76 34 54 22 36 10 27 53  4 70 35 30  2 37 47 51  3 75 49 74], a_shuffle_aclus: [ 86  56  37  11  34  17   9  75  21  32  24  28 108  27  13  10   8  63  61  15  26  92  81  35  85  66  89  48  50  19   2  68  41  25  60   3  87  33  97  82  23  59  70  98  84  16  55  88  83  58 109  74  76 107  57  62  49  78  90  96 105  51  73  31  53  14  36  71   7  93  52  45   4  54  64  69   5 102  67 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [73 30 44 13 79 29 37 33  0 66  3 39 36 49 63  2 31 51 57 43 35 61 27 68 62 23 59  6 54 24 65 34 20 40 25 32 21 72 76 56 53 78 42  1  4 15 77 58 47 19 11 10 67 60 12 46 28 71 52 38 48 17 16 22  9 75 64 14 74 45  8 26 70  5  7 41 18 69 50 55], a_shuffle_aclus: [ 98  45  61  17 109  41  54  50   2  88   5  56  53  67  85   4  48  69  76  60  52  83  36  90  84  32  81   9  73  33  87  51  27  57  34  49  28  97 105  75  71 108  59   3   7  21 107  78  64  26  15  14  89  82  16  63  37  96  70  55  66  24  23  31  13 102  86  19 100  62  11  35  93   8  10  58  25  92  68  74]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [29 44 74 15  3 46 20 24 22 36 54 37 31 68 13 75 52 53 60 51 71 39 28 76 12 11 49 65 50 23  8  9 27 47 17 41 38 59 14 69 35 56 34 63  5 55 57 10 70  6  0 72 79 77 42  4 25 40 62 19 30 58 26 33 48  1 67 66 73 16  7 21 45 43 78  2 64 61 18 32], a_shuffle_aclus: [ 41  61 100  21   5  63  27  33  31  53  73  54  48  90  17 102  70  71  82  69  96  56  37 105  16  15  67  87  68  32  11  13  36  64  24  58  55  81  19  92  52  75  51  85   8  74  76  14  93   9   2  97 109 107  59   7  34  57  84  26  45  78  35  50  66   3  89  88  98  23  10  28  62  60 108   4  86  83  25  49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/NeuroPy/neuropy/analyses/decoders.py:397: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [40 78 47  2 13 12 38 21 42 59 56 60 49 10 55 51  5 61 53  3 46 43 35 19 14 58 30 32 66  8 48  7 76 20 33 36 68 34 44 57 50  0 18 73  1 74 28 15 65 71 79  4 37 23 11 39  9 69 75 26 77  6 52 64 62 22 29 63 17 54 24 70 27 41 31 45 67 72 16 25], a_shuffle_aclus: [ 57 108  64   4  17  16  55  28  59  81  75  82  67  14  74  69   8  83  71   5  63  60  52  26  19  78  45  49  88  11  66  10 105  27  50  53  90  51  61  76  68   2  25  98   3 100  37  21  87  96 109   7  54  32  15  56  13  92 102  35 107   9  70  86  84  31  41  85  24  73  33  93  36  58  48  62  89  97  23  34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [59 78 79 37 38 63 10 75 12 11 69 72 16 34 31 13  0 19 44 27 64  3 65 45  7 14 73 50 18 76 74 60 21 17  4 24 15 30 47 67 58 68 46  5 39 28 49 53 61 57  2 70 77 22 55 25 23 29  9 33 36 42  8 54  6 20 52 35 40 48 41 71  1 43 56 62 66 51 26 32], a_shuffle_aclus: [ 81 108 109  54  55  85  14 102  16  15  92  97  23  51  48  17   2  26  61  36  86   5  87  62  10  19  98  68  25 105 100  82  28  24   7  33  21  45  64  89  78  90  63   8  56  37  67  71  83  76   4  93 107  31  74  34  32  41  13  50  53  59  11  73   9  27  70  52  57  66  58  96   3  60  75  84  88  69  35  49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [67 38 49 74  6 22 26 62 34  8 52 54 39 10 55 20 36 65 56 18 37 51  1 57 21 30 46 53 42 78 59 27  0 50 64 23 70 32  4 66 61  7  3  5 69 68 15 43 72 45 35 60 16 13 28 11 75 40 76 24 33 48 17 44 31 25  9 58 41 14 12 77 19 73 79 71  2 63 29 47], a_shuffle_aclus: [ 89  55  67 100   9  31  35  84  51  11  70  73  56  14  74  27  53  87  75  25  54  69   3  76  28  45  63  71  59 108  81  36   2  68  86  32  93  49   7  88  83  10   5   8  92  90  21  60  97  62  52  82  23  17  37  15 102  57 105  33  50  66  24  61  48  34  13  78  58  19  16 107  26  98 109  96   4  85  41  64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [57 78  5  4 46 76 13 14  2 36  6  0 63 21 59 55 15 72 49 44 25 24  3  7 33 32 68 50 56  8  9 10 42 18 37 38 16 71 41 28 47 11 19 67 17 70 23 29 45  1 65 12 53 34 77 66 52 43 39 64 22 75 58 61 62 30 48 51 26 69 35 40 79 27 54 31 60 74 20 73], a_shuffle_aclus: [ 76 108   8   7  63 105  17  19   4  53   9   2  85  28  81  74  21  97  67  61  34  33   5  10  50  49  90  68  75  11  13  14  59  25  54  55  23  96  58  37  64  15  26  89  24  93  32  41  62   3  87  16  71  51 107  88  70  60  56  86  31 102  78  83  84  45  66  69  35  92  52  57 109  36  73  48  82 100  27  98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [62 30 38 68 71 13  0 18 76 59 20 55 66 63 54 45 67 39 17 74 41 32 61 48 16 77 46 56 26 43 36 15 42  2 33  4 64 29 21  5 27  8 35 65 28 11  3  7 40 58 44 73 34 49 37 31 78  9 14 79 24 53  6  1 69 12 19 72 10 52 70 60 22 75 51 57 25 47 50 23], a_shuffle_aclus: [ 84  45  55  90  96  17   2  25 105  81  27  74  88  85  73  62  89  56  24 100  58  49  83  66  23 107  63  75  35  60  53  21  59   4  50   7  86  41  28   8  36  11  52  87  37  15   5  10  57  78  61  98  51  67  54  48 108  13  19 109  33  71   9   3  92  16  26  97  14  70  93  82  31 102  69  76  34  64  68  32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [50 37 66 36 24 55 77 34 19 30 47  2 20 29 45  1 23 26 18 62 59 40 56 14 16 64 43 17  0 11 42 54 10 12 67 78 31 72 38 76 53 68  4 48 15 71 49 74 79 33 75 35 52 60 39  9  7 69 61 27 13 32 22 73 63 57 65 28 46 58  5 44 41 70  3 21 51  8 25  6], a_shuffle_aclus: [ 68  54  88  53  33  74 107  51  26  45  64   4  27  41  62   3  32  35  25  84  81  57  75  19  23  86  60  24   2  15  59  73  14  16  89 108  48  97  55 105  71  90   7  66  21  96  67 100 109  50 102  52  70  82  56  13  10  92  83  36  17  49  31  98  85  76  87  37  63  78   8  61  58  93   5  28  69  11  34   9]\n",
      "a_shuffle_IDXs: [68 75 11 42 12  4 23 46 19 13  3 28 32 58 71 60 27 26 77 25 76 50 33  5 15 67 69  8  2  9 74 17 55 72 10 43 53  7 54 14 79 37 73 31 21 20 36 63 78 16 65 49 44 66 59  1 35  0 29 22 56  6 51 62 41 48 47 30 57 61 52 18 38 39 34 70 64 45 24 40], a_shuffle_aclus: [ 90 102  15  59  16   7  32  63  26  17   5  37  49  78  96  82  36  35 107  34 105  68  50   8  21  89  92  11   4  13 100  24  74  97  14  60  71  10  73  19 109  54  98  48  28  27  53  85 108  23  87  67  61  88  81   3  52   2  41  31  75   9  69  84  58  66  64  45  76  83  70  25  55  56  51  93  86  62  33  57]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [56 67 51 29 60 77 61 16 41 12  6  9 15 39 65 31 49 36 70 62 71 52 47 25 74 24 26 34 35 55 21  2 64 27 33 69 78  0 75 13  4  5 54 48 11 37 43 18 58 28 76 63  3 57 23 79 45 53 68 44 73 40 30 10  7 20 14  8 50 46 22 32 17 66 38 72 42 59 19  1], a_shuffle_aclus: [ 75  89  69  41  82 107  83  23  58  16   9  13  21  56  87  48  67  53  93  84  96  70  64  34 100  33  35  51  52  74  28   4  86  36  50  92 108   2 102  17   7   8  73  66  15  54  60  25  78  37 105  85   5  76  32 109  62  71  90  61  98  57  45  14  10  27  19  11  68  63  31  49  24  88  55  97  59  81  26   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 40  8 35 36  4  9 67 44 25 30 22 76  5 51 52 79 63 71 11 43 33 31 50 15 72 69 54 17 77  0 29 41 66 78 20 18 55 24 45  3 27 74 19 32 49 38 75 42 16 37 12  1 48 34 39 46 58 21 68 53 70 65 10 13  2 47 59 57 26 62  7  6 73 56 28 23 60 14 64], a_shuffle_aclus: [ 83  57  11  52  53   7  13  89  61  34  45  31 105   8  69  70 109  85  96  15  60  50  48  68  21  97  92  73  24 107   2  41  58  88 108  27  25  74  33  62   5  36 100  26  49  67  55 102  59  23  54  16   3  66  51  56  63  78  28  90  71  93  87  14  17   4  64  81  76  35  84  10   9  98  75  37  32  82  19  86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [61 73 46 17 37 44 15 49 67 68 69 12 78 60 21 58 34 55 76 16 36 48 71 13 22 18 23  5 20 66  8  3 74 11 10  1 64 70 50 19 62 28 56 79 41 33 42  4 51 32 35  2  0  6 72 77 59 25  9 45 26  7 27 63 30 40 29 43 38 31 54 24 47 39 65 53 57 75 52 14], a_shuffle_aclus: [ 83  98  63  24  54  61  21  67  89  90  92  16 108  82  28  78  51  74 105  23  53  66  96  17  31  25  32   8  27  88  11   5 100  15  14   3  86  93  68  26  84  37  75 109  58  50  59   7  69  49  52   4   2   9  97 107  81  34  13  62  35  10  36  85  45  57  41  60  55  48  73  33  64  56  87  71  76 102  70  19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_shuffle_IDXs: [71 38 57 23  0 25 47 17  9 13 50 56 78 64 35 33 18  1 46 49 69  4 72 32 77 43 14 48 58 26 52 22 24 28 27 76 70 74 41 44 59  7  3 68 12 10 30 34 20 40 11 63  2 19 36 54 79 53  8 31 66 39 37 55 65 29 42 61 21 75 45  5 60  6 16 15 62 51 67 73], a_shuffle_aclus: [ 96  55  76  32   2  34  64  24  13  17  68  75 108  86  52  50  25   3  63  67  92   7  97  49 107  60  19  66  78  35  70  31  33  37  36 105  93 100  58  61  81  10   5  90  16  14  45  51  27  57  15  85   4  26  53  73 109  71  11  48  88  56  54  74  87  41  59  83  28 102  62   8  82   9  23  21  84  69  89  98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:344: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n"
     ]
    }
   ],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['perform_wcorr_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 600}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88707195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6500a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_problem_result_names = ['directional_decoders_evaluate_epochs', 'directional_decoders_decode_continuous']\n",
    "# pickle_problem_global_result_key_names = ['DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations']\n",
    "# pickle_problem_global_result_key_names = ['SequenceBased',]\n",
    "# pickle_problem_global_result_key_names = ['DirectionalLaps', 'DirectionalMergedDecoders', 'RankOrder', 'DirectionalDecodersDecoded']\n",
    "# global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=pickle_problem_global_result_key_names, debug_print=True)\n",
    "# global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=['DirectionalLaps', 'DirectionalMergedDecoders', 'DirectionalDecodersEpochsEvaluations', 'DirectionalDecodersDecoded'], debug_print=True)\n",
    "global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=['SequenceBased', 'RankOrder', 'long_short_fr_indicies_analysis', 'long_short_leave_one_out_decoding_analysis', 'jonathan_firing_rate_analysis', 'DirectionalMergedDecoders', 'DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations', 'DirectionalDecodersDecoded'], debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6412c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=['SequenceBased', 'RankOrder', 'long_short_fr_indicies_analysis', 'long_short_leave_one_out_decoding_analysis', 'jonathan_firing_rate_analysis', 'DirectionalMergedDecoders', 'DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations', 'DirectionalDecodersDecoded'], debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f69245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import overwrite_replay_epochs_and_recompute\n",
    "\n",
    "overwrite_replay_epochs_and_recompute(curr_active_pipeline)\n",
    "\n",
    "# Looking at what I have left I can actually make it until tomorrow without a bridge if Dr. Tellez can refill it in the morning. It's cutting it close though, so the only concern is that there is no wiggle room in case of delays at the pharmacy or with insurance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b18a50",
   "metadata": {},
   "source": [
    "## 2024-06-25 - Save all custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_suffix: str = '_withNewComputedReplays'\n",
    "custom_suffix: str = '_withNewKamranExportedReplays'\n",
    "custom_save_filenames = {\n",
    "    'pipeline_pkl':f'loadedSessPickle{custom_suffix}.pkl',\n",
    "    'global_computation_pkl':f\"global_computation_results{custom_suffix}.pkl\",\n",
    "    'pipeline_h5':f'pipeline{custom_suffix}.h5',\n",
    "}\n",
    "print(f'custom_save_filenames: {custom_save_filenames}')\n",
    "custom_save_filepaths = {k:v for k, v in custom_save_filenames.items()}\n",
    "# custom_save_filepaths['pipeline_pkl'] = Path('W:/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_withNewKamranExportedReplays.pkl').resolve()\n",
    "# custom_save_filepaths['global_computation_pkl'] = Path(r'W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\output\\global_computation_results_withNewKamranExportedReplays.pkl').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c264e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_save_filepaths['pipeline_pkl'] = curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE, active_pickle_filename=custom_save_filenames['pipeline_pkl'])\n",
    "custom_save_filepaths['pipeline_pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_save_filepaths['global_computation_pkl'] = curr_active_pipeline.save_global_computation_results(override_global_pickle_filename=custom_save_filenames['global_computation_pkl'])\n",
    "custom_save_filepaths['global_computation_pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a658dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_save_filepaths['pipeline_h5'] = curr_active_pipeline.export_pipeline_to_h5(override_filename=custom_save_filenames['pipeline_h5'])\n",
    "custom_save_filepaths['pipeline_h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157058e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_black",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
