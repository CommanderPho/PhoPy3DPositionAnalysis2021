{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionTables, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult, BatchSessionCompletionHandler\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_file_metadata\n",
    "from pyphocorehelpers.Filesystem.path_helpers import discover_data_files, generate_copydict, copy_movedict, copy_file, save_copydict_to_text_file, read_copydict_from_text_file, invert_filedict\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import get_file_str_if_file_exists\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import check_output_h5_files, copy_files_in_filelist_to_dest\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import ConcreteSessionFolder, BackupMethods\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots, BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import build_merged_neuron_firing_rate_indicies\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsHelpers\n",
    "\n",
    "# BATCH_DATE_TO_USE = '2023-10-18_Apogee' # used for filenames throught the notebook\n",
    "# BATCH_DATE_TO_USE = '2023-11-15_Lab' # used for filenames throught the notebook\n",
    "BATCH_DATE_TO_USE = '2023-12-13' # used for filenames throught the notebook\n",
    "\n",
    "# scripts_output_path = Path('/home/halechr/cloud/turbo/Data/Output/').resolve() # Greatlakes\n",
    "# scripts_output_path = Path('output/generated_slurm_scripts/').resolve() # Apogee\n",
    "scripts_output_path = Path('/home/halechr/FastData/gen_scripts/').resolve() # Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d958aaf",
   "metadata": {},
   "source": [
    "## Build Processing Scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb673d3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_one_2006-6-08_14-26-15\\\\run_kdiba_gor01_one_2006-6-08_14-26-15.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_one_2006-6-08_14-26-15\\\\figures_kdiba_gor01_one_2006-6-08_14-26-15.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_one_2006-6-09_1-22-43\\\\run_kdiba_gor01_one_2006-6-09_1-22-43.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_one_2006-6-09_1-22-43\\\\figures_kdiba_gor01_one_2006-6-09_1-22-43.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_one_2006-6-12_15-55-31\\\\run_kdiba_gor01_one_2006-6-12_15-55-31.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_one_2006-6-12_15-55-31\\\\figures_kdiba_gor01_one_2006-6-12_15-55-31.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-07_16-40-19\\\\run_kdiba_gor01_two_2006-6-07_16-40-19.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-07_16-40-19\\\\figures_kdiba_gor01_two_2006-6-07_16-40-19.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-08_21-16-25\\\\run_kdiba_gor01_two_2006-6-08_21-16-25.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-08_21-16-25\\\\figures_kdiba_gor01_two_2006-6-08_21-16-25.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-09_22-24-40\\\\run_kdiba_gor01_two_2006-6-09_22-24-40.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-09_22-24-40\\\\figures_kdiba_gor01_two_2006-6-09_22-24-40.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-12_16-53-46\\\\run_kdiba_gor01_two_2006-6-12_16-53-46.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_gor01_two_2006-6-12_16-53-46\\\\figures_kdiba_gor01_two_2006-6-12_16-53-46.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_one_2006-4-09_17-29-30\\\\run_kdiba_vvp01_one_2006-4-09_17-29-30.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_one_2006-4-09_17-29-30\\\\figures_kdiba_vvp01_one_2006-4-09_17-29-30.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_one_2006-4-10_12-25-50\\\\run_kdiba_vvp01_one_2006-4-10_12-25-50.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_one_2006-4-10_12-25-50\\\\figures_kdiba_vvp01_one_2006-4-10_12-25-50.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_two_2006-4-09_16-40-54\\\\run_kdiba_vvp01_two_2006-4-09_16-40-54.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_two_2006-4-09_16-40-54\\\\figures_kdiba_vvp01_two_2006-4-09_16-40-54.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_two_2006-4-10_12-58-3\\\\run_kdiba_vvp01_two_2006-4-10_12-58-3.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_vvp01_two_2006-4-10_12-58-3\\\\figures_kdiba_vvp01_two_2006-4-10_12-58-3.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_11-02_17-46-44\\\\run_kdiba_pin01_one_11-02_17-46-44.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_11-02_17-46-44\\\\figures_kdiba_pin01_one_11-02_17-46-44.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_11-02_19-28-0\\\\run_kdiba_pin01_one_11-02_19-28-0.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_11-02_19-28-0\\\\figures_kdiba_pin01_one_11-02_19-28-0.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_11-03_12-3-25\\\\run_kdiba_pin01_one_11-03_12-3-25.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_11-03_12-3-25\\\\figures_kdiba_pin01_one_11-03_12-3-25.py'),\n",
       " ('C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_fet11-01_12-58-54\\\\run_kdiba_pin01_one_fet11-01_12-58-54.py',\n",
       "  'C:\\\\home\\\\halechr\\\\FastData\\\\gen_scripts\\\\run_kdiba_pin01_one_fet11-01_12-58-54\\\\figures_kdiba_pin01_one_fet11-01_12-58-54.py')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import generate_batch_single_session_scripts\n",
    "\n",
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "separate_execute_and_figure_gen_scripts: bool = True\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data')] # , Path(r'/home/halechr/FastData'), Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/home/halechr/turbo/Data'), \n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "\n",
    "## Build Slurm Scripts:\n",
    "session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "included_session_contexts, output_python_scripts, output_slurm_scripts = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts,\n",
    "                                                                                                               output_directory=scripts_output_path,\n",
    "    use_separate_run_directories=True, should_perform_figure_generation_to_file=False, enable_hdf5_output=True, separate_execute_and_figure_gen_scripts=separate_execute_and_figure_gen_scripts)\n",
    "\n",
    "if separate_execute_and_figure_gen_scripts:\n",
    "    computation_script_paths = [x[0] for x in output_python_scripts]\n",
    "    generate_figures_script_paths = [x[1] for x in output_python_scripts]\n",
    "    \n",
    "display(output_python_scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6b61a8",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 3\n",
    "# List of your script paths\n",
    "script_paths = computation_script_paths\n",
    "# script_paths = generate_figures_script_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f98344",
   "metadata": {},
   "source": [
    "## Execute the generated scripts in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e294b06",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdece07dd16442981660fcf7c7268d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Running:', max=15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_gor01_one_2006-6-12_15-55-31/run_kdiba_gor01_one_2006-6-12_15-55-31.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 3 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_gor01_one_2006-6-09_1-22-43/run_kdiba_gor01_one_2006-6-09_1-22-43.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_gor01_one_2006-6-08_14-26-15/run_kdiba_gor01_one_2006-6-08_14-26-15.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2558: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_gor01_two_2006-6-07_16-40-19/run_kdiba_gor01_two_2006-6-07_16-40-19.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_gor01_two_2006-6-08_21-16-25/run_kdiba_gor01_two_2006-6-08_21-16-25.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_gor01_two_2006-6-12_16-53-46/run_kdiba_gor01_two_2006-6-12_16-53-46.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_vvp01_one_2006-4-09_17-29-30/run_kdiba_vvp01_one_2006-4-09_17-29-30.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_vvp01_one_2006-4-10_12-25-50/run_kdiba_vvp01_one_2006-4-10_12-25-50.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/2006-4-10_12-25-50.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2558: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_vvp01_two_2006-4-09_16-40-54/run_kdiba_vvp01_two_2006-4-09_16-40-54.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/2006-4-09_16-40-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2558: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_vvp01_two_2006-4-10_12-58-3/run_kdiba_vvp01_two_2006-4-10_12-58-3.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/2006-4-10_12-58-3.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 3 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:330: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_pin01_one_11-02_17-46-44/run_kdiba_pin01_one_11-02_17-46-44.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-02_17-46-44/11-02_17-46-44.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2558: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_pin01_one_11-02_19-28-0/run_kdiba_pin01_one_11-02_19-28-0.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-02_19-28-0/11-02_19-28-0.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_pin01_one_11-03_12-3-25/run_kdiba_pin01_one_11-03_12-3-25.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/pin01/one/11-03_12-3-25/11-03_12-3-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 3 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_gor01_two_2006-6-09_22-24-40/run_kdiba_gor01_two_2006-6-09_22-24-40.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2558: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2620: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_overlap_dot_product = pf_overlap_dot_product_curve / overlap_area\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n",
      "Error in /home/halechr/FastData/gen_scripts/run_kdiba_pin01_one_fet11-01_12-58-54/run_kdiba_pin01_one_fet11-01_12-58-54.py: /home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/cloud/turbo/Data/KDIBA/pin01/one/fet11-01_12-58-54/fet11-01_12-58-54.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2558: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "## recieves: max_parallel_executions, script_paths\n",
    "\"\"\"\n",
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 5\n",
    "# List of your script paths\n",
    "script_paths = output_python_scripts\n",
    "\"\"\"\n",
    "\n",
    "# Function to execute a script\n",
    "def run_script(script_path):\n",
    "    try:\n",
    "        result = subprocess.run(['python', script_path], capture_output=True, text=True)\n",
    "        return script_path, result.stdout, result.stderr\n",
    "    except Exception as e:\n",
    "        return script_path, None, str(e)\n",
    "\n",
    "# Create a progress bar\n",
    "progress_bar = widgets.IntProgress(value=0, min=0, max=len(script_paths), description='Running:', bar_style='info')\n",
    "display(progress_bar)\n",
    "\n",
    "# Run scripts in parallel with a limit on the number of parallel instances\n",
    "with ProcessPoolExecutor(max_workers=max_parallel_executions) as executor:\n",
    "    futures = {executor.submit(run_script, path): path for path in script_paths}\n",
    "    for future in as_completed(futures):\n",
    "        script, stdout, stderr = future.result()\n",
    "        progress_bar.value += 1  # Update the progress bar\n",
    "        if stderr:\n",
    "            print(f\"Error in {script}: {stderr}\")\n",
    "        else:\n",
    "            print(f\"Completed {script}\")\n",
    "\n",
    "# Progress bar will automatically update as scripts complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdcbbf",
   "metadata": {},
   "source": [
    "## using `dask` for parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "## recieves: max_parallel_executions, script_paths\n",
    "\"\"\"\n",
    "# Maximum number of parallel script executions\n",
    "max_parallel_executions = 5\n",
    "# List of your script paths\n",
    "script_paths = output_python_scripts\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a Dask Client\n",
    "client = Client()\n",
    "\n",
    "# Function to execute a script\n",
    "def run_script(script_path):\n",
    "    try:\n",
    "        result = subprocess.run(['python', script_path], capture_output=True, text=True)\n",
    "        return script_path, result.stdout, result.stderr\n",
    "    except Exception as e:\n",
    "        return script_path, None, str(e)\n",
    "\n",
    "# Create a list of delayed tasks\n",
    "tasks = [dask.delayed(run_script)(path) for path in script_paths]\n",
    "\n",
    "# Run the tasks in parallel\n",
    "results = dask.compute(*tasks, scheduler='processes', num_workers=max_parallel_executions)\n",
    "\n",
    "# Process and display the results\n",
    "for script, stdout, stderr in results:\n",
    "    if stderr:\n",
    "        print(f\"Error in {script}: {stderr}\")\n",
    "    else:\n",
    "        print(f\"Completed {script}\")\n",
    "\n",
    "# Shutdown Dask client\n",
    "client.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f583772",
   "metadata": {},
   "source": [
    "## Resume normal stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data')] # , Path(r'/home/halechr/FastData'), Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/home/halechr/turbo/Data'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = UserAnnotationsManager.get_hardcoded_good_sessions()\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "\n",
    "\n",
    "# Output Paths:\n",
    "included_h5_paths = [get_file_str_if_file_exists(v.pipeline_results_h5) for v in good_session_concrete_folders]\n",
    "copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix=BATCH_DATE_TO_USE, only_include_file_types=['local_pkl', 'global_pkl'])\n",
    "check_output_h5_files(included_h5_paths)\n",
    "# from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import generate_batch_single_session_scripts\n",
    "\n",
    "# ## Build Slurm Scripts:\n",
    "# session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "# included_session_contexts, output_python_scripts, output_slurm_scripts = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts, output_directory=Path('output/generated_slurm_scripts/').resolve(), use_separate_run_directories=True, should_perform_figure_generation_to_file=False)\n",
    "# display(output_python_scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Output File Processing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cb2d8-65b3-405b-a41b-22b2fa7cb28e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_dict_h5_files = copy_movedict(copy_dict)\n",
    "moved_files_dict_h5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e3954-15a4-449c-b927-56d5d79153c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_copydict_output_filename=f'backed_up_files_copydict_{BATCH_DATE_TO_USE}.csv'\n",
    "moved_files_copydict_file_path = Path(global_data_root_parent_path).joinpath(moved_files_copydict_output_filename).resolve() # Use Default\n",
    "print(f'moved_files_copydict_file_path: {moved_files_copydict_file_path}')\n",
    "\n",
    "_out_string, filedict_out_path = save_copydict_to_text_file(moved_files_dict_h5_files, moved_files_copydict_file_path, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab869730",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_moved_files_dict_files = read_copydict_from_text_file(moved_files_copydict_file_path, debug_print=False)\n",
    "read_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_moved_files_dict_files\n",
    "restore_moved_files_dict_files = invert_filedict(read_moved_files_dict_files)\n",
    "restore_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0953",
   "metadata": {},
   "source": [
    "## Extract `across_sessions_instantaneous_fr_dict` from the computation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "# neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True, )\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_and_save_all_combined_tables(included_session_contexts, included_h5_paths, override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'{BATCH_DATE_TO_USE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "# _out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "# _out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership','is_refined_LxC', 'is_refined_SxC'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'is_refined_LxC', 'is_refined_SxC', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d9a7",
   "metadata": {},
   "source": [
    "# 2023-10-06 - `joined_neruon_fri_df` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE = '2023-10-05_NewParameters'\n",
    "BATCH_DATE_TO_USE = '2023-10-07'\n",
    "all_sessions_joined_neruon_fri_df, out_path = build_and_merge_all_sessions_joined_neruon_fri_df(global_data_root_parent_path, BATCH_DATE_TO_USE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_neruon_fri_df_basename = f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df'\n",
    "AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=joined_neruon_fri_df_basename, include_csv=False)\n",
    "print(f'>>\\t done with {output_file_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-10-04 - Load Saved across-sessions-data, process, and produce figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "graphics_output_dict = AcrossSessionsResults.post_compute_all_sessions_processing(global_data_root_parent_path=global_data_root_parent_path, BATCH_DATE_TO_USE=BATCH_DATE_TO_USE, plotting_enabled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9987dc",
   "metadata": {},
   "source": [
    " #TODO 2023-10-05 11:40: - [ ] Extract the \"contrarian cells\", the ones that have a strong exclusivity on the laps but the opposite tendency on the replays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d829b90",
   "metadata": {},
   "source": [
    "## 2023-11-15 - For manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load the saved across-session results:\n",
    "# Outputs: across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list, neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table\n",
    "\n",
    "inst_fr_output_filename: str = f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "## Load all across-session tables from the pickles:\n",
    "output_path_suffix: str = f'{BATCH_DATE_TO_USE}'\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.load_all_combined_tables(override_output_parent_path=global_data_root_parent_path, output_path_suffix=output_path_suffix) # output_path_suffix=f'2023-10-04-GL-Recomp'\n",
    "num_sessions = len(neuron_replay_stats_table.session_uid.unique().to_numpy())\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "# neuron_replay_stats_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike3d-poetry",
   "language": "python",
   "name": "spike3d-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
