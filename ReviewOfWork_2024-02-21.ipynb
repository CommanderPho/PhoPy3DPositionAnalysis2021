{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:20.608442900Z",
     "start_time": "2023-11-16T23:21:20.217442100Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "doc_output_parent_folder: C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\EXTERNAL\\DEVELOPER_NOTES\\DataStructureDocumentation\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n",
      "DAY_DATE_STR: 2024-02-21, DAY_DATE_TO_USE: 2024-02-21\n",
      "NOW_DATETIME: 2024-02-21_0405PM, NOW_DATETIME_TO_USE: 2024-02-21_0405PM\n",
      "global_data_root_parent_path changed to W:\\Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0367ebba1fe4eb69755fc8652d353bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Data Root:', layout=Layout(width='auto'), options=(WindowsPath('W:/Data'),), style=ToggleButtonsStyle(button_width='max-content'), tooltip='global_data_root_parent_path', value=WindowsPath('W:/Data'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext viztracer\n",
    "from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "text_formatter: PlainTextFormatter = IPython.get_ipython().display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, CapturedException, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables, CapturedException\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer, RankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path(r'/media/MAX/Data'), Path(r'/media/halechr/MAX/Data'), Path(r'/home/halechr/FastData'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')]\n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "\tglobal global_data_root_parent_path\n",
    "\tnew_global_data_root_parent_path = new_path.resolve()\n",
    "\tglobal_data_root_parent_path = new_global_data_root_parent_path\n",
    "\tprint(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "\tassert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\t\t\t\n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {},
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:com.PhoHale.Spike3D.pipeline:NeuropyPipeline.__setstate__(state=\"{'pipeline_name': 'kdiba_pipeline', 'session_data_type': 'kdiba', '_stage': <pyphoplacecellanalysis.General.Pipeline.Stages.Display.DisplayPipelineStage object at 0x000002B108AAE370>}\")\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed, using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:NeuropyPipeline.on_stage_changed(new_stage=\"PipelineStage.Displayed\")\n",
      "WARNING:com.PhoHale.Spike3D.pipeline:WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context: IdentifyingContext = good_contexts_list[1] # select the session from all of the good sessions here.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # DONE. Very good. Many good Pfs, many good replays.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # DONE, might be the BEST SESSION, good example session with lots of place cells, clean replays, and clear bar graphs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # DONE, Good Pfs but no good replays ---- VERY weird effect of the replays, a sharp drop to strongly negative values more than 3/4 through the experiment.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6') # BAD, 2023-07-14, unsure why still.\n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # DONE, GREAT, both good Pfs and replays! Interesting see-saw!\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25') # DONE, Added replay selections. Very \"jumpy\" between the starts and ends of the track.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40') # 2024-01-10 new RANKORDER APOGEE | DONE, Added replay selections. A TON of putative replays in general, most bad, but some good. LOOKIN GOOD!\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='twolong_LR_pf1Dsession_name='2006-4-12_15-25-59') # BAD, No Epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-16_18-47-52')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-17_12-52-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-25_13-20-55')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-28_12-38-13')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44') # DONE, good. Many good pfs, many good replays. Noticed very strange jumping off the track in the 3D behavior/spikes viewer. Is there something wrong with this session?\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0') # DONE, good?, replays selected, few --- \"ZeroDivisionError: float division by zero\"\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # DONE, very few replays\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_12-15-3') ### KeyError: 'maze1_odd'\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_22-4-5') ### \n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # DONE, replays selected, quite a few replays but few are very good.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "# \n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation',\n",
    "                                            #    'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step', \n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            # 'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DirectionalLaps',\n",
       " 'DirectionalMergedDecoders',\n",
       " 'RankOrder',\n",
       " 'DirectionalDecodersDecoded',\n",
       " 'long_short_leave_one_out_decoding_analysis',\n",
       " 'short_long_pf_overlap_analyses',\n",
       " 'long_short_fr_indicies_analysis',\n",
       " 'jonathan_firing_rate_analysis',\n",
       " 'long_short_post_decoding',\n",
       " 'long_short_endcap']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(curr_active_pipeline.global_computation_results.computed_data.keys())\n",
    "\n",
    "\n",
    "# 2024-01-22 ERROR: when the pipeline is manually saved, its global_computations seem to be saved to the pickle too. After modifying how global computations are loaded from pickle, the following global computations code block no longer appropriately overwrites the existing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=['DirectionalLaps', 'DirectionalMergedDecoders', 'RankOrder', 'DirectionalDecodersDecoded'], debug_print=True)\n",
    "# global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=[k for k in list(curr_active_pipeline.global_computation_results.computed_data.keys())], debug_print=True) # drop all global keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_problem_result_names = ['directional_decoders_evaluate_epochs', 'directional_decoders_decode_continuous']\n",
    "pickle_problem_global_result_key_names = ['DirectionalDecodersDecoded', 'DirectionalDecodersEpochsEvaluations']\n",
    "global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=pickle_problem_global_result_key_names, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba46b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.574268400Z",
     "start_time": "2023-11-16T23:21:35.966373700Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\output\\global_computation_results.pkl... done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['DirectionalLaps',\n",
       "  'DirectionalMergedDecoders',\n",
       "  'RankOrder',\n",
       "  'DirectionalDecodersDecoded',\n",
       "  'long_short_leave_one_out_decoding_analysis',\n",
       "  'short_long_pf_overlap_analyses',\n",
       "  'long_short_fr_indicies_analysis',\n",
       "  'jonathan_firing_rate_analysis',\n",
       "  'long_short_post_decoding',\n",
       "  'long_short_endcap'],\n",
       " ['DirectionalLaps',\n",
       "  'DirectionalMergedDecoders',\n",
       "  'RankOrder',\n",
       "  'DirectionalDecodersDecoded',\n",
       "  'long_short_leave_one_out_decoding_analysis',\n",
       "  'short_long_pf_overlap_analyses',\n",
       "  'long_short_fr_indicies_analysis',\n",
       "  'jonathan_firing_rate_analysis',\n",
       "  'long_short_post_decoding',\n",
       "  'long_short_endcap'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "missing required value, so we don't need to call .validate_computation_test(...) to know it isn't valid!\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.02})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: ERROR: time_bin_size_seconds (0.02) > (t_stop - t_start) (0.012687353417277336). Reduce the bin size or exclude this epoch.\n",
      "Exception occured while validating (`validate_computation_test(...)`) after recomputation:\n",
      " Validation exception: 'long_short_inst_spike_rate_groups'\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('lap_direction_determination', 'maze_any'), ('split_to_directional_laps', 'maze_any'), ('merged_directional_placefields', 'maze_any'), ('rank_order_shuffle_analysis', 'maze_any'), ('long_short_decoding_analyses', 'maze_any'), ('short_long_pf_overlap_analyses', 'maze_any'), ('long_short_fr_indicies_analyses', 'maze_any'), ('jonathan_firing_rate_analysis', 'maze_any'), ('long_short_post_decoding', 'maze_any'), ('long_short_endcap_analysis', 'maze_any')].\n",
      "\n",
      "\n",
      "!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"\n",
      "\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### GLOBAL COMPUTATIONS:\n",
    "extended_computations_include_includelist=['lap_direction_determination', #'pf_computation', 'firing_rate_trends',# 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', \n",
    "    'long_short_post_decoding', # #TODO 2024-01-19 05:49: - [ ] `'long_short_post_decoding' is broken for some reason `AttributeError: 'NoneType' object has no attribute 'active_filter_epochs'``\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "    # 'directional_decoders_evaluate_epochs'\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'] # , 'directional_decoders_decode_continuous'\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_decode_continuous'] # \n",
    "\n",
    "\n",
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # curr_active_pipeline.load_pickled_global_computation_results()\n",
    "        curr_active_pipeline.load_pickled_global_computation_results(allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist) # is new\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results: {e}')\n",
    "        raise\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     exception_info = sys.exc_info()\n",
    "#     e = CapturedException(e, exception_info)\n",
    "#     print(f'second half threw: {e}')\n",
    "\n",
    "# 4m 5.2s for inst fr computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47820977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "    # 'directional_decoders_evaluate_epochs',\n",
    "] # do only specified\n",
    "\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps',\n",
    "#     # 'merged_directional_placefields',\n",
    "#     # 'directional_decoders_decode_continuous',\n",
    "# ]\n",
    "force_recompute_override_computations_includelist = None\n",
    "\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "newly_computed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# force_recompute_override_computations_includelist = ['_decode_continuous_using_directional_decoders']\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(extended_computations_include_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.02}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_decode_continuous'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f372737e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_bin_size: 3.775693131596347\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.775693131596347\n",
      "pos_bin_size: 3.775693131596347\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025, pos_bin_size: 3.775693131596347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\analyses\\decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\analyses\\decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\analyses\\decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\analyses\\decoders.py:175: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: WCorr:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 43/81\n",
      "\tagreeing_rows_ratio: 0.5308641975308642\n",
      "\tPerformance: Ripple: WCorr\n",
      "agreeing_rows_count/num_total_epochs: 224/718\n",
      "\tagreeing_rows_ratio: 0.31197771587743733\n",
      "Performance: Simple PF PearsonR:\n",
      "\tLaps:\n",
      "agreeing_rows_count/num_total_epochs: 71/81\n",
      "\tagreeing_rows_ratio: 0.8765432098765432\n",
      "\tPerformance: Ripple: Simple PF PearsonR\n",
      "agreeing_rows_count/num_total_epochs: 197/718\n",
      "\tagreeing_rows_ratio: 0.2743732590529248\n"
     ]
    }
   ],
   "source": [
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_evaluate_epochs'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a1538b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_LR</th>\n",
       "      <th>P_RL</th>\n",
       "      <th>P_Long</th>\n",
       "      <th>P_Short</th>\n",
       "      <th>ripple_idx</th>\n",
       "      <th>ripple_start_t</th>\n",
       "      <th>P_Long_LR</th>\n",
       "      <th>P_Long_RL</th>\n",
       "      <th>P_Short_LR</th>\n",
       "      <th>P_Short_RL</th>\n",
       "      <th>most_likely_decoder_index</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>long_LR_pf_peak_x_pearsonr</th>\n",
       "      <th>long_RL_pf_peak_x_pearsonr</th>\n",
       "      <th>short_LR_pf_peak_x_pearsonr</th>\n",
       "      <th>short_RL_pf_peak_x_pearsonr</th>\n",
       "      <th>best_decoder_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.437230</td>\n",
       "      <td>0.562770</td>\n",
       "      <td>0.697740</td>\n",
       "      <td>0.302260</td>\n",
       "      <td>0</td>\n",
       "      <td>49.107308</td>\n",
       "      <td>0.305073</td>\n",
       "      <td>0.392667</td>\n",
       "      <td>0.132157</td>\n",
       "      <td>0.170103</td>\n",
       "      <td>1</td>\n",
       "      <td>49.107308</td>\n",
       "      <td>49.285484</td>\n",
       "      <td>2</td>\n",
       "      <td>0.178176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.855048</td>\n",
       "      <td>0.144952</td>\n",
       "      <td>0.836530</td>\n",
       "      <td>0.163470</td>\n",
       "      <td>1</td>\n",
       "      <td>64.083055</td>\n",
       "      <td>0.715273</td>\n",
       "      <td>0.121257</td>\n",
       "      <td>0.139775</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>0</td>\n",
       "      <td>64.083055</td>\n",
       "      <td>64.198900</td>\n",
       "      <td>3</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.477629</td>\n",
       "      <td>0.522371</td>\n",
       "      <td>0.798243</td>\n",
       "      <td>0.201757</td>\n",
       "      <td>2</td>\n",
       "      <td>64.805681</td>\n",
       "      <td>0.381265</td>\n",
       "      <td>0.416979</td>\n",
       "      <td>0.096365</td>\n",
       "      <td>0.105392</td>\n",
       "      <td>1</td>\n",
       "      <td>64.805681</td>\n",
       "      <td>64.968527</td>\n",
       "      <td>4</td>\n",
       "      <td>0.162847</td>\n",
       "      <td>-0.242318</td>\n",
       "      <td>0.087645</td>\n",
       "      <td>-0.084831</td>\n",
       "      <td>-0.013088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.514560</td>\n",
       "      <td>0.485440</td>\n",
       "      <td>0.780462</td>\n",
       "      <td>0.219538</td>\n",
       "      <td>3</td>\n",
       "      <td>66.661598</td>\n",
       "      <td>0.401595</td>\n",
       "      <td>0.378868</td>\n",
       "      <td>0.112965</td>\n",
       "      <td>0.106572</td>\n",
       "      <td>0</td>\n",
       "      <td>66.661598</td>\n",
       "      <td>66.779040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.117442</td>\n",
       "      <td>-0.486672</td>\n",
       "      <td>-0.566053</td>\n",
       "      <td>-0.442575</td>\n",
       "      <td>-0.512350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.541998</td>\n",
       "      <td>0.458002</td>\n",
       "      <td>0.460950</td>\n",
       "      <td>0.539050</td>\n",
       "      <td>4</td>\n",
       "      <td>67.966460</td>\n",
       "      <td>0.249834</td>\n",
       "      <td>0.211116</td>\n",
       "      <td>0.292164</td>\n",
       "      <td>0.246886</td>\n",
       "      <td>2</td>\n",
       "      <td>67.966460</td>\n",
       "      <td>68.108171</td>\n",
       "      <td>6</td>\n",
       "      <td>0.141711</td>\n",
       "      <td>-0.111701</td>\n",
       "      <td>-0.355286</td>\n",
       "      <td>-0.130743</td>\n",
       "      <td>-0.314293</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.587526</td>\n",
       "      <td>0.412474</td>\n",
       "      <td>0.507546</td>\n",
       "      <td>0.492454</td>\n",
       "      <td>5</td>\n",
       "      <td>68.571951</td>\n",
       "      <td>0.298197</td>\n",
       "      <td>0.209349</td>\n",
       "      <td>0.289330</td>\n",
       "      <td>0.203124</td>\n",
       "      <td>0</td>\n",
       "      <td>68.571951</td>\n",
       "      <td>68.764442</td>\n",
       "      <td>7</td>\n",
       "      <td>0.192491</td>\n",
       "      <td>-0.254988</td>\n",
       "      <td>-0.532460</td>\n",
       "      <td>-0.200078</td>\n",
       "      <td>-0.391718</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>0.444446</td>\n",
       "      <td>0.555554</td>\n",
       "      <td>0.641664</td>\n",
       "      <td>0.358336</td>\n",
       "      <td>712</td>\n",
       "      <td>2553.543550</td>\n",
       "      <td>0.285185</td>\n",
       "      <td>0.356479</td>\n",
       "      <td>0.159261</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>1</td>\n",
       "      <td>2553.543550</td>\n",
       "      <td>2553.857447</td>\n",
       "      <td>741</td>\n",
       "      <td>0.313897</td>\n",
       "      <td>-0.356764</td>\n",
       "      <td>-0.450427</td>\n",
       "      <td>-0.124920</td>\n",
       "      <td>-0.634293</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>0.658599</td>\n",
       "      <td>0.341401</td>\n",
       "      <td>0.461012</td>\n",
       "      <td>0.538988</td>\n",
       "      <td>713</td>\n",
       "      <td>2556.111956</td>\n",
       "      <td>0.303622</td>\n",
       "      <td>0.157390</td>\n",
       "      <td>0.354977</td>\n",
       "      <td>0.184011</td>\n",
       "      <td>2</td>\n",
       "      <td>2556.111956</td>\n",
       "      <td>2556.382507</td>\n",
       "      <td>742</td>\n",
       "      <td>0.270551</td>\n",
       "      <td>-0.389353</td>\n",
       "      <td>0.541274</td>\n",
       "      <td>-0.311317</td>\n",
       "      <td>0.435791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>0.479827</td>\n",
       "      <td>0.520173</td>\n",
       "      <td>0.515796</td>\n",
       "      <td>0.484204</td>\n",
       "      <td>714</td>\n",
       "      <td>2556.596748</td>\n",
       "      <td>0.247493</td>\n",
       "      <td>0.268303</td>\n",
       "      <td>0.232334</td>\n",
       "      <td>0.251870</td>\n",
       "      <td>1</td>\n",
       "      <td>2556.596748</td>\n",
       "      <td>2556.918202</td>\n",
       "      <td>743</td>\n",
       "      <td>0.321454</td>\n",
       "      <td>0.169430</td>\n",
       "      <td>-0.013886</td>\n",
       "      <td>0.026681</td>\n",
       "      <td>-0.055447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.454819</td>\n",
       "      <td>0.686218</td>\n",
       "      <td>0.313782</td>\n",
       "      <td>715</td>\n",
       "      <td>2559.389348</td>\n",
       "      <td>0.374113</td>\n",
       "      <td>0.312105</td>\n",
       "      <td>0.171068</td>\n",
       "      <td>0.142714</td>\n",
       "      <td>0</td>\n",
       "      <td>2559.389348</td>\n",
       "      <td>2559.530322</td>\n",
       "      <td>744</td>\n",
       "      <td>0.140974</td>\n",
       "      <td>-0.682938</td>\n",
       "      <td>-0.639413</td>\n",
       "      <td>-0.640998</td>\n",
       "      <td>-0.614757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0.389175</td>\n",
       "      <td>0.610825</td>\n",
       "      <td>0.641664</td>\n",
       "      <td>0.358336</td>\n",
       "      <td>716</td>\n",
       "      <td>2585.469325</td>\n",
       "      <td>0.249719</td>\n",
       "      <td>0.391945</td>\n",
       "      <td>0.139455</td>\n",
       "      <td>0.218881</td>\n",
       "      <td>1</td>\n",
       "      <td>2585.469325</td>\n",
       "      <td>2585.533929</td>\n",
       "      <td>746</td>\n",
       "      <td>0.064604</td>\n",
       "      <td>0.624351</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.676045</td>\n",
       "      <td>0.195345</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.220087</td>\n",
       "      <td>0.779913</td>\n",
       "      <td>0.619707</td>\n",
       "      <td>0.380293</td>\n",
       "      <td>717</td>\n",
       "      <td>2586.462870</td>\n",
       "      <td>0.136390</td>\n",
       "      <td>0.483317</td>\n",
       "      <td>0.083698</td>\n",
       "      <td>0.296596</td>\n",
       "      <td>1</td>\n",
       "      <td>2586.462870</td>\n",
       "      <td>2586.589959</td>\n",
       "      <td>747</td>\n",
       "      <td>0.127089</td>\n",
       "      <td>0.730089</td>\n",
       "      <td>0.150765</td>\n",
       "      <td>0.716675</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         P_LR      P_RL    P_Long   P_Short  ripple_idx  ripple_start_t  P_Long_LR  P_Long_RL  P_Short_LR  P_Short_RL  most_likely_decoder_index        start         stop label  duration  long_LR_pf_peak_x_pearsonr  long_RL_pf_peak_x_pearsonr  short_LR_pf_peak_x_pearsonr  short_RL_pf_peak_x_pearsonr  best_decoder_index\n",
       "0    0.437230  0.562770  0.697740  0.302260           0       49.107308   0.305073   0.392667    0.132157    0.170103                          1    49.107308    49.285484     2  0.178176                         NaN                         NaN                          NaN                          NaN                   0\n",
       "1    0.855048  0.144952  0.836530  0.163470           1       64.083055   0.715273   0.121257    0.139775    0.023695                          0    64.083055    64.198900     3  0.115845                         NaN                         NaN                          NaN                          NaN                   0\n",
       "2    0.477629  0.522371  0.798243  0.201757           2       64.805681   0.381265   0.416979    0.096365    0.105392                          1    64.805681    64.968527     4  0.162847                   -0.242318                    0.087645                    -0.084831                    -0.013088                   0\n",
       "3    0.514560  0.485440  0.780462  0.219538           3       66.661598   0.401595   0.378868    0.112965    0.106572                          0    66.661598    66.779040     5  0.117442                   -0.486672                   -0.566053                    -0.442575                    -0.512350                   1\n",
       "4    0.541998  0.458002  0.460950  0.539050           4       67.966460   0.249834   0.211116    0.292164    0.246886                          2    67.966460    68.108171     6  0.141711                   -0.111701                   -0.355286                    -0.130743                    -0.314293                   1\n",
       "5    0.587526  0.412474  0.507546  0.492454           5       68.571951   0.298197   0.209349    0.289330    0.203124                          0    68.571951    68.764442     7  0.192491                   -0.254988                   -0.532460                    -0.200078                    -0.391718                   1\n",
       "..        ...       ...       ...       ...         ...             ...        ...        ...         ...         ...                        ...          ...          ...   ...       ...                         ...                         ...                          ...                          ...                 ...\n",
       "712  0.444446  0.555554  0.641664  0.358336         712     2553.543550   0.285185   0.356479    0.159261    0.199075                          1  2553.543550  2553.857447   741  0.313897                   -0.356764                   -0.450427                    -0.124920                    -0.634293                   3\n",
       "713  0.658599  0.341401  0.461012  0.538988         713     2556.111956   0.303622   0.157390    0.354977    0.184011                          2  2556.111956  2556.382507   742  0.270551                   -0.389353                    0.541274                    -0.311317                     0.435791                   1\n",
       "714  0.479827  0.520173  0.515796  0.484204         714     2556.596748   0.247493   0.268303    0.232334    0.251870                          1  2556.596748  2556.918202   743  0.321454                    0.169430                   -0.013886                     0.026681                    -0.055447                   0\n",
       "715  0.545181  0.454819  0.686218  0.313782         715     2559.389348   0.374113   0.312105    0.171068    0.142714                          0  2559.389348  2559.530322   744  0.140974                   -0.682938                   -0.639413                    -0.640998                    -0.614757                   0\n",
       "716  0.389175  0.610825  0.641664  0.358336         716     2585.469325   0.249719   0.391945    0.139455    0.218881                          1  2585.469325  2585.533929   746  0.064604                    0.624351                    0.004253                     0.676045                     0.195345                   2\n",
       "717  0.220087  0.779913  0.619707  0.380293         717     2586.462870   0.136390   0.483317    0.083698    0.296596                          1  2586.462870  2586.589959   747  0.127089                    0.730089                    0.150765                     0.716675                     0.137800                   0\n",
       "\n",
       "[718 rows x 20 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "ripple_decoding_time_bin_size = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "decoder_laps_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "\n",
    "# New items:\n",
    "decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "# Weighted correlations:\n",
    "laps_weighted_corr_merged_df = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "decoder_laps_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "decoder_ripple_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "# Pearson's correlations:\n",
    "laps_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "# laps_simple_pf_pearson_merged_df\n",
    "ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a95450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_csvs\n",
    "# collected_outputs_path = Path('/nfs/turbo/umms-kdiba/Data/Output/collected_outputs').resolve() # Linux\n",
    "# collected_outputs_path: Path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs').resolve() # GreatLakes\n",
    "collected_outputs_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs').resolve() # Apogee\n",
    "assert collected_outputs_path.exists()\n",
    "\n",
    "\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_output_csv_paths = directional_decoders_epochs_decode_result.export_csvs(parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta)\n",
    "_output_csv_paths\n",
    "\n",
    "# {'laps_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'laps_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_simple_pf_pearson_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_simple_pf_pearson_merged_df)_tbin-0.025.csv')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalLapsResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results() # newly_computed_values: [('pfdt_computation', 'maze_any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7af96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = curr_active_pipeline.save_split_global_computation_results(debug_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77babf98",
   "metadata": {},
   "source": [
    "## Continue Saving/Exporting stuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a869b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n",
    "curr_active_pipeline.clear_registered_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE) ## #TODO 2024-02-16 14:25: - [ ] PicklingError: Can't pickle <function make_set_closure_cell.<locals>.set_closure_cell at 0x7fd35e66b700>: it's not found as attr._compat.make_set_closure_cell.<locals>.set_closure_cell\n",
    "# curr_active_pipeline.save_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {},
   "source": [
    "# Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d86b27889e34c0983f3ebb578256ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Box(children=(Label(value='session path:', layout=Layout(width='auto')), Label(value='W:\\\\Data\\\\KDIBA\\\\gor01\\\\two\\\\2006-6-07_16-40-19\\\\output', layout=Layout(flex='1 1 auto', margin='2px', width='auto')), Button(button_style='info', description='Copy', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Copy to Clipboard'), Button(button_style='info', description='Reveal', icon='folder-tree', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Reveal in System Explorer')), layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='70%')), HBox(children=(Button(description='Output Folder', style=ButtonStyle()), Button(description='global pickle', style=ButtonStyle()), Button(description='pipeline pickle', style=ButtonStyle()), Button(description='.h5 export', style=ButtonStyle()), Button(description='TEST - Dialog', style=ButtonStyle()), Button(description='Save Pipeline', style=ButtonStyle()))), HBox(children=(Button(description='Reload display functions...', style=ButtonStyle()), Button(description='Reload computation functions...', style=ButtonStyle()))), ToggleButton(value=True, description='Figures Displaying')))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {},
   "source": [
    "# End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a533ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.700275900Z",
     "start_time": "2023-11-16T23:21:40.584273Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1236.2662453636294, 2587.801681999932)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "t_start, t_delta, t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have several python variables I want to print: t_start, t_delta, t_end\n",
    "# I want to generate a print statement that explicitly lists the variable name prior to its value like `print(f't_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')`\n",
    "# Currently I have to t_start, t_delta, t_end\n",
    "curr_active_pipeline.get_session_context()\n",
    "\n",
    "print(f'{curr_active_pipeline.session_name}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9071e94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:43.601382Z",
     "start_time": "2023-11-16T23:21:40.702275600Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "WARNING: PAPER_FIGURE_figure_1_add_replay_epoch_rasters(...): no user-assigned manually labeled replay epochs. Reeturning all epochs.\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n"
     ]
    }
   ],
   "source": [
    "## long_short_decoding_analyses:\n",
    "from attrs import astuple\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import LeaveOneOutDecodingAnalysis\n",
    "\n",
    "curr_long_short_decoding_analyses: LeaveOneOutDecodingAnalysis = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba8f0cf",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyphoplacecellanalysis.Analysis.Decoder.decoder_result.LeaveOneOutDecodingAnalysisResult object at 0x000002B4BA5AF2B0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_long_short_decoding_analyses.long_results_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83acf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_v_observed_result.observed_from_expected_diff_ptp_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49f5d4f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7104fc37",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult, DirectionalLapsResult, DirectionalDecodersDecodedResult\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']   \n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: float = rank_order_results.included_qclu_values\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912656a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "\n",
    "## UNPACK HERE via direct property access:\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "ripple_decoding_time_bin_size = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "decoder_laps_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "\n",
    "# New items:\n",
    "decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "# Weighted correlations:\n",
    "laps_weighted_corr_merged_df = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "decoder_laps_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "decoder_ripple_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "# Pearson's correlations:\n",
    "laps_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "238f67cb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previously_decoded time_bin_sizes: [0.01]\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersDecodedResult\n",
    "\n",
    "directional_decoders_decode_result: DirectionalDecodersDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "spikes_df = directional_decoders_decode_result.spikes_df\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "\n",
    "most_recent_time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# most_recent_time_bin_size\n",
    "most_recent_continuously_decoded_dict = deepcopy(directional_decoders_decode_result.most_recent_continuously_decoded_dict)\n",
    "# most_recent_continuously_decoded_dict\n",
    "\n",
    "## Adds in the 'pseudo2D' decoder in:\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# time_bin_size: float = 0.01\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = continuously_decoded_result_cache_dict[time_bin_size]\n",
    "pseudo2D_decoder_continuously_decoded_result = continuously_decoded_dict.get('pseudo2D', None)\n",
    "if pseudo2D_decoder_continuously_decoded_result is None:\n",
    "\t# compute here...\n",
    "\t## Currently used for both cases to decode:\n",
    "\tt_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\tsingle_global_epoch_df: pd.DataFrame = pd.DataFrame({'start': [t_start], 'stop': [t_end], 'label': [0]}) # Build an Epoch object containing a single epoch, corresponding to the global epoch for the entire session:\n",
    "\tsingle_global_epoch: Epoch = Epoch(single_global_epoch_df)\n",
    "\tspikes_df = directional_decoders_decode_result.spikes_df\n",
    "\tpseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = pseudo2D_decoder.decode_specific_epochs(spikes_df=deepcopy(spikes_df), filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "\tcontinuously_decoded_dict['pseudo2D'] = pseudo2D_decoder_continuously_decoded_result\n",
    "\tcontinuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2dc5fe",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "# `LongShortStatsItem` form (2024-01-02):\n",
    "# LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "# RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260739a4f36c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "# active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1c6006aba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Entropy/Surprise Results:\n",
    "active_extended_stats = global_results['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['pf_dt_sequential_surprise'] # DynamicParameters\n",
    "historical_snapshots = active_relative_entropy_results['historical_snapshots']\n",
    "post_update_times: np.ndarray = active_relative_entropy_results['post_update_times'] # (4152,) = (n_post_update_times,)\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals: np.ndarray = active_relative_entropy_results['time_intervals']\n",
    "surprise_time_bin_duration = (post_update_times[2]-post_update_times[1])\n",
    "long_short_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['long_short_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "short_long_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['short_long_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "flat_relative_entropy_results: np.ndarray = active_relative_entropy_results['flat_relative_entropy_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_results: np.ndarray = active_relative_entropy_results['flat_jensen_shannon_distance_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_across_all_positions: np.ndarray = np.sum(np.abs(flat_jensen_shannon_distance_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "flat_surprise_across_all_positions: np.ndarray = np.sum(np.abs(flat_relative_entropy_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "\n",
    "## Get the placefield dt matrix:\n",
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\t## Compute it if missing:\n",
    "\toccupancy_weighted_tuning_maps_over_time = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in historical_snapshots.values()])\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = occupancy_weighted_tuning_maps_over_time\n",
    "else:\n",
    "\toccupancy_weighted_tuning_maps_over_time = active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] # (n_post_update_times, n_neurons, n_xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d3bf5955d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c62d5c18c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "# appearing_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c292d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Saving/Loading `DirectionalLaps_2Hz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6c50e",
   "metadata": {
    "tags": [
     "save",
     "persistance"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import save_rank_order_results\n",
    "\n",
    "# DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "# DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "# print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "# NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "# NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "# print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "formatted_time = get_now_rounded_time_str()\n",
    "print(formatted_time)\n",
    "save_rank_order_results(curr_active_pipeline, day_date=f\"{formatted_time}\") # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm\n",
    "# '2024-01-09_0125PM-minimum_inclusion_fr-5-included_qclu_values-[1, 2]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_path = Path('/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/').resolve()\n",
    "# /media/halechr/MAX/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/2024-02-16_0427PM-minimum_inclusion_fr-5-included_qclu_values-[1, 2]DirectionalMergedDecoders.pkl\n",
    "sorted(search_path.glob(f\"{DAY_DATE_TO_USE}*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da8a7c",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import SaveStringGenerator\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadData\n",
    "\n",
    "# Load the data from a file into the pipeline:\n",
    "# out_filename_str: str = '2023-12-11-minimum_inclusion_fr_Hz_2_included_qclu_values_1-2_' # specific\n",
    "\n",
    "minimum_inclusion_fr_Hz: float = 5.0\n",
    "included_qclu_values: List[int] = [1,2]\n",
    "out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date=f'{DAY_DATE_TO_USE}_11am') # '2023-12-21_349am'\n",
    "# out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date='2023-12-22_312pm') # '2023-12-21_349am'\n",
    "print(f'out_filename_str: \"{out_filename_str}\"')\n",
    "# day_date_str: str = '2023-12-11_with_tuple_newer_'\n",
    "# day_date_str: str = ''\n",
    "directional_laps_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}DirectionalLaps.pkl').resolve()\n",
    "assert directional_laps_output_path.exists()\n",
    "# loaded_directional_laps, loaded_rank_order = loadData(directional_laps_output_path)\n",
    "loaded_directional_laps = loadData(directional_laps_output_path)\n",
    "assert (loaded_directional_laps is not None)\n",
    "# assert (loaded_rank_order is not None)\n",
    "\n",
    "rank_order_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}RankOrder.pkl').resolve()\n",
    "loaded_rank_order = loadData(rank_order_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the loaded data to the pipeline:\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'], curr_active_pipeline.global_computation_results.computed_data['RankOrder'] = loaded_directional_laps, loaded_rank_order\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd521ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec24467335a760",
   "metadata": {},
   "source": [
    "# POST-Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "728c46e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": [
     "unwrap"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multi_sort_raster_browser\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons, paired_incremental_sort_neurons # _display_directional_template_debugger\n",
    "from neuropy.utils.indexing_helpers import paired_incremental_sorting, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ScrollBarWithSpinBox.ScrollBarWithSpinBox import ScrollBarWithSpinBox\n",
    "\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "## Display Testing\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph import QtGui\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import pyqtplot_build_image_bounds_extent, pyqtplot_plot_image\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "# ripple_result_tuple\n",
    "\n",
    "## Unpacks `rank_order_results`: \n",
    "# global_replays = Epoch(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# active_replay_epochs, active_epochs_df, active_selected_spikes_df = combine_rank_order_results(rank_order_results, global_replays, track_templates=track_templates)\n",
    "# active_epochs_df\n",
    "\n",
    "# ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices\n",
    "dir_index_to_direction_name_map: Dict[int, str] = {0:'LR', 1:\"RL\"}\n",
    "\n",
    "\n",
    "## All three DataFrames are the same number of rows, each with one row corresponding to an Epoch:\n",
    "active_replay_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "# active_replay_epochs_df\n",
    "\n",
    "# Change column type to int8 for columns: 'long_best_direction_indices', 'short_best_direction_indices'\n",
    "# directional_likelihoods_df = pd.DataFrame.from_dict(ripple_result_tuple.directional_likelihoods_tuple._asdict()).astype({'long_best_direction_indices': 'int8', 'short_best_direction_indices': 'int8'})\n",
    "directional_likelihoods_df = ripple_result_tuple.directional_likelihoods_df\n",
    "# directional_likelihoods_df\n",
    "\n",
    "# 2023-12-15 - Newest method:\n",
    "# laps_combined_epoch_stats_df = rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "# ripple_combined_epoch_stats_df: pd.DataFrame  = rank_order_results.ripple_combined_epoch_stats_df\n",
    "# ripple_combined_epoch_stats_df\n",
    "\n",
    "\n",
    "# # Concatenate the three DataFrames along the columns axis:\n",
    "# # Assert that all DataFrames have the same number of rows:\n",
    "# assert len(active_replay_epochs_df) == len(directional_likelihoods_df) == len(ripple_combined_epoch_stats_df), \"DataFrames have different numbers of rows.\"\n",
    "# # Assert that all DataFrames have at least one row:\n",
    "# assert len(active_replay_epochs_df) > 0, \"active_replay_epochs_df is empty.\"\n",
    "# assert len(directional_likelihoods_df) > 0, \"directional_likelihoods_df is empty.\"\n",
    "# assert len(ripple_combined_epoch_stats_df) > 0, \"ripple_combined_epoch_stats_df is empty.\"\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = pd.concat([active_replay_epochs_df.reset_index(drop=True, inplace=False), directional_likelihoods_df.reset_index(drop=True, inplace=False), ripple_combined_epoch_stats_df.reset_index(drop=True, inplace=False)], axis=1)\n",
    "# merged_complete_epoch_stats_df = merged_complete_epoch_stats_df.set_index(active_replay_epochs_df.index, inplace=False)\n",
    "\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "# merged_complete_epoch_stats_df.to_csv('output/2023-12-21_merged_complete_epoch_stats_df.csv')\n",
    "# merged_complete_epoch_stats_df\n",
    "\n",
    "laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional_merged_decoders_result.all_directional_pf1D_Decoder.pf.spikes_df\n",
    "# all_directional_pf1D_Decoder_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripple_merged_complete_epoch_stats_df\n",
    "laps_merged_complete_epoch_stats_df\n",
    "['long_best_direction_indices', 'short_best_direction_indices', 'combined_best_direction_indicies', 'long_relative_direction_likelihoods', 'short_relative_direction_likelihoods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time series of Long-likely events\n",
    "# type(long_RL_results) # DynamicParameters\n",
    "long_LR_pf1D_Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920bfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_directional_decoder_dict_value)\n",
    "list(all_directional_decoder_dict_value.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df\n",
    "laps_most_likely_direction_from_decoder\n",
    "long_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdabd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ripple_result_tuple) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.DirectionalRankOrderResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "\n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps, partial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def register_type_display(func_to_register, type_to_register):\n",
    "\t\"\"\" adds the display function (`func_to_register`) it decorates to the class (`type_to_register) as a method\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\t@wraps(func_to_register)\n",
    "\tdef wrapper(*args, **kwargs):\n",
    "\t\treturn func_to_register(*args, **kwargs)\n",
    "\n",
    "\tfunction_name: str = func_to_register.__name__ # get the name of the function to be added as the property\n",
    "\tsetattr(type_to_register, function_name, wrapper) # set the function as a method with the same name as the decorated function on objects of the class.\t\n",
    "\treturn wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots \n",
    "\n",
    "# @register_type_display(DirectionalRankOrderResult)\n",
    "def plot_histograms(self: DirectionalRankOrderResult, **kwargs) -> \"MatplotlibRenderPlots\":\n",
    "\t\"\"\" \n",
    "\tnum='RipplesRankOrderZscore'\n",
    "\t\"\"\"\n",
    "\tprint(f'.plot_histograms(..., kwargs: {kwargs})')\n",
    "\tfig = plt.figure(layout=\"constrained\", **kwargs)\n",
    "\tax_dict = fig.subplot_mosaic(\n",
    "\t\t[\n",
    "\t\t\t[\"long_short_best_z_score_diff\", \"long_short_best_z_score_diff\"],\n",
    "\t\t\t[\"long_best_z_scores\", \"short_best_z_scores\"],\n",
    "\t\t],\n",
    "\t)\n",
    "\tplots = (pd.DataFrame({'long_best_z_scores': self.long_best_dir_z_score_values}).hist(ax=ax_dict['long_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'short_best_z_scores': self.short_best_dir_z_score_values}).hist(ax=ax_dict['short_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'long_short_best_z_score_diff': self.long_short_best_dir_z_score_diff_values}).hist(ax=ax_dict['long_short_best_z_score_diff'], bins=21, alpha=0.8),\n",
    "\t)\n",
    "\treturn MatplotlibRenderPlots(name='plot_histogram_figure', figures=[fig], axes=ax_dict)\n",
    "\n",
    "\n",
    "register_type_display(plot_histograms, DirectionalRankOrderResult)\n",
    "## Call the newly added `plot_histograms` function on the `ripple_result_tuple` object which is of type `DirectionalRankOrderResult`:\n",
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\t try saving to CSV...')\n",
    "merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "merged_complete_epoch_stats_df\n",
    "merged_complete_ripple_epoch_stats_df_output_path = curr_active_pipeline.get_output_path().joinpath(f'{DAY_DATE_TO_USE}_1247pm_merged_complete_epoch_stats_df.csv').resolve()\n",
    "merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
    "print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732743e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import reorder_columns\n",
    "\n",
    "dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4))\n",
    "reorder_columns(merged_complete_epoch_stats_df, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dceda30",
   "metadata": {},
   "source": [
    "## 2023-12-21 - Computing Spearman Percentiles as an alternative to the Z-score from shuffling, which does not seem to work for small numbers of active cells in an event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45aa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values, shuffled_results_output_dict, combined_variable_names, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles = rank_order_results.ripple_new_output_tuple\n",
    "# shuffled_results_output_dict['short_LR_pearson_Z']\n",
    "print(list(shuffled_results_output_dict.keys())) # ['short_LR_pearson_Z', 'short_LR_spearman_Z', 'short_RL_pearson_Z', 'short_RL_spearman_Z', 'long_LR_pearson_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_spearman_Z']\n",
    "\n",
    "['long_LR_pearson_Z', 'long_RL_pearson_Z', 'short_LR_pearson_Z', 'short_RL_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b40bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-12-22 - Add the LR-LR, RL-RL differences\n",
    "merged_complete_epoch_stats_df['LongShort_LR_quantile_diff'] = merged_complete_epoch_stats_df['LR_Long_rank_percentile'] - merged_complete_epoch_stats_df['LR_Short_rank_percentile']\n",
    "merged_complete_epoch_stats_df['LongShort_RL_quantile_diff'] = merged_complete_epoch_stats_df['RL_Long_rank_percentile'] - merged_complete_epoch_stats_df['RL_Short_rank_percentile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import _plot_significant_event_quantile_fig\n",
    "\n",
    "# active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "# if isinstance(global_events, pd.DataFrame):\n",
    "#     active_replay_epochs = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df())\n",
    "\n",
    "\n",
    "# _out = _plot_significant_event_quantile_fig(curr_active_pipeline, significant_ripple_combined_epoch_stats_df=significant_ripple_combined_epoch_stats_df)\n",
    "# _out\n",
    "\n",
    "marker_style = dict(linestyle='None', color='#ff7f0eff', markersize=6, markerfacecolor='#ff7f0eb4', markeredgecolor='#ff7f0eff')\n",
    "\n",
    "    # dict(facecolor='#ff7f0eb4', size=8.0)\n",
    "    # fignum='best_quantiles'\n",
    "\n",
    "# ripple_combined_epoch_stats_df['combined_best_direction_indicies']\n",
    "\n",
    "_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y='LongShort_BestDir_quantile_diff', title='Sig. (>0.95) Best Quantile Diff', **marker_style, marker='o')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd89199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "perform_update_title_subtitle(fig=fig_long_pf_1D, ax=ax_long_pf_1D, title_string=title_string, subtitle_string=subtitle_string, active_context=active_context, use_flexitext_titles=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neuropy.utils.matplotlib_helpers import draw_epoch_regions\n",
    "epochs_collection, epoch_labels = draw_epoch_regions(curr_active_pipeline.sess.epochs, ax, defer_render=False, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecb4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(significant_BestDir_quantile_stats_df.columns))\n",
    "['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']\n",
    "\n",
    "for a_name in ['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']:\n",
    "\t_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y=a_name, title=f'Sig. (>0.95) {a_name}', **marker_style, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "\n",
    "df = quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].copy()\n",
    "# Create the subplots and loop through columns\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 10))\n",
    "for i, col in enumerate(df.columns):\n",
    "    df[col].plot.hist(ax=axes[i], bins=21)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Adjust layout and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd61a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsLayoutWidget(show=True)\n",
    "win.resize(800,350)\n",
    "win.setWindowTitle('Z-Scorer: Histogram')\n",
    "plt1 = win.addPlot()\n",
    "vals = quantile_results_df.LR_Long_rank_percentile\n",
    "fisher_z_transformed_vals = np.arctanh(vals)\n",
    "\n",
    "## compute standard histogram\n",
    "y, x = np.histogram(vals) # , bins=np.linspace(-3, 8, 40)\n",
    "# fisher_z_transformed_y, x = np.histogram(fisher_z_transformed_vals, bins=x)\n",
    "\n",
    "## Using stepMode=\"center\" causes the plot to draw two lines for each sample.\n",
    "## notice that len(x) == len(y)+1\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "# plt1.plot(x, fisher_z_transformed_y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,255,100,50), name='fisher_z_values')\n",
    "\n",
    "# ## Now draw all points as a nicely-spaced scatter plot\n",
    "y = pg.pseudoScatter(vals, spacing=0.15)\n",
    "# #plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5)\n",
    "plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5, symbolPen=(255,255,255,200), symbolBrush=(0,0,255,150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30cb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.concat((ripple_combined_epoch_stats_df, ripple_p_values_epoch_stats_df), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43327521",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_not(np.isnan(rank_order_results.ripple_combined_epoch_stats_df.index).any())\n",
    "# ripple_combined_epoch_stats_df.label.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.label).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.index).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60749347",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdone. building global result.\n"
     ]
    }
   ],
   "source": [
    "print(f'\\tdone. building global result.')\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_output_tuple (output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles) = ripple_new_output_tuple\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "unique_Probe_Epoch_IDs\n",
    "for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "\t# probe_epoch_df = active_selected_spikes_df[a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id']]\n",
    "\t# epoch_unique_aclus = probe_epoch_df.aclu.unique()\n",
    "\tmask = (a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id'])\n",
    "\t# epoch_unique_aclus = active_selected_spikes_df.loc[mask, 'aclu'].unique()\n",
    "\tfor a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "\t\t# Shuffle aclus here:\n",
    "\t\tactive_selected_spikes_df.loc[mask, 'aclu'] = active_selected_spikes_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\t\tactive_selected_spikes_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# ## Shuffle aclus here:\n",
    "\t\t# # probe_epoch_df.aclu.sample(1000)\n",
    "\t\t# # a_aclu_peak_map\n",
    "\t\t# # Assuming 'df' is your DataFrame and 'column_name' is the column you want to shuffle\n",
    "\t\t# probe_epoch_df['aclu'] = probe_epoch_df['aclu'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\t# probe_epoch_df[f'{a_decoder_name}_pf_peak_x'] = probe_epoch_df.aclu.map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.aclu.map(a_aclu_peak_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c306ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-01-09 - More Efficient\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df: pd.DataFrame):\n",
    "    \"\"\" 2024-01-09 - Candidate for moving into RankOrderComputations \n",
    "    captures: decoder_names\n",
    "    \n",
    "    Usage:\n",
    "    \n",
    "    shuffled_dfs = _perform_efficient_shuffle(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "    \n",
    "    \"\"\"\n",
    "    decoder_names = track_templates.get_decoder_names()\n",
    "    \n",
    "    ## Compute real values here:\n",
    "    epoch_id_grouped_selected_spikes_df = active_selected_spikes_df.groupby('Probe_Epoch_id') # I can even compute this outside the loop?\n",
    "\n",
    "    # spearman_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='spearman', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "    # pearson_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='pearson', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "\n",
    "    # real_stats_df = pd.concat((spearman_correlations, pearson_correlations), axis='columns')\n",
    "    # real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()] # drop duplicated 'Probe_Epoch_id' column\n",
    "    # # Change column type to uint64 for column: 'Probe_Epoch_id'\n",
    "    # real_stats_df = real_stats_df.astype({'Probe_Epoch_id': 'uint64'})\n",
    "    # # Rename column 'Probe_Epoch_id' to 'label'\n",
    "    # real_stats_df = real_stats_df.rename(columns={'Probe_Epoch_id': 'label'})\n",
    "    \n",
    "    # Parallelize correlation computations if required\n",
    "    correlations = []\n",
    "    for method in ['spearman', 'pearson']:\n",
    "        correlations.append(\n",
    "            epoch_id_grouped_selected_spikes_df.apply(\n",
    "                lambda group: RankOrderAnalyses._subfn_calculate_correlations(\n",
    "                    group, method=method, decoder_names=decoder_names)\n",
    "            )\n",
    "        )\n",
    "  \n",
    "    # Adjust and join all calculated correlations\n",
    "    real_stats_df = pd.concat(correlations, axis='columns').reset_index()\n",
    "    real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()]\n",
    "\n",
    "    real_stats_df.rename(columns={'Probe_Epoch_id': 'label'}, inplace=True)\n",
    "    real_stats_df['label'] = real_stats_df['label'].astype('uint64')  # in-place type casting\n",
    "    \n",
    "    return real_stats_df\n",
    "\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "def _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    \"\"\" 2024-01-09 - Performs the shuffles in a simple way\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "\n",
    "    # Create a list to hold the shuffled dataframes\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        # Working on a copy of the DataFrame\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "            \n",
    "            # Shuffle 'aclu' values\n",
    "            shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "            \n",
    "            # # Apply aclu peak map dictionary to 'aclu' column\n",
    "            # for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            #     shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "            \n",
    "\n",
    "        # end `for a_probe_epoch_ID`\n",
    "        # Once done, apply the aclu peak maps to shuffled_df's 'aclu' column:\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df[f'{a_decoder_name}_pf_peak_x'] = shuffled_df.aclu.map(a_aclu_peak_map)\n",
    "            \n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "        \n",
    "        # Adding the shuffled DataFrame to the list\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "        \n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "def _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles: int = 5):\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    def map_dict_to_group(group, a_dict, column):\n",
    "        group[column] = group[column].map(a_dict)\n",
    "        return group\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'] = shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df = shuffled_df.groupby('Probe_Epoch_id').apply(map_dict_to_group, a_dict=a_aclu_peak_map, column=f'{a_decoder_name}_pf_peak_x')\n",
    "\n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "\n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-suggested_perform_efficient_shuffle.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "shuffled_dfs, shuffled_stats_dfs = _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 50, 1m 21.2s, 10, 16.1s\n",
    "# shuffled_dfs, shuffled_stats_dfs = _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 10, 12.8s\n",
    "\n",
    "\n",
    "shuffled_dfs\n",
    "shuffled_stats_dfs\n",
    "# 5, 4.1 sec\n",
    "# 0.5s!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values = shuffled_stats_dfs\n",
    "# Build the output `stacked_arrays`: _________________________________________________________________________________ #\n",
    "\n",
    "stacked_arrays = np.stack([a_shuffle_real_stats_df[combined_variable_names].to_numpy() for a_shuffle_real_stats_df in output_active_epoch_computed_values], axis=0) # for compatibility: .shape (n_shuffles, n_epochs, n_columns)\n",
    "# stacked_df = pd.concat(output_active_epoch_computed_values, axis='index')\n",
    "\n",
    "## Drop any shuffle indicies where NaNs are returned for any of the stats values.\n",
    "is_valid_row = np.logical_not(np.isnan(stacked_arrays)).all(axis=(1,2)) # row [0, 66, :] is bad, ... so is [1, 66, :], ... [20, 66, :], ... they are repeated!!\n",
    "n_valid_shuffles = np.sum(is_valid_row)\n",
    "if debug_print:\n",
    "\tprint(f'n_valid_shuffles: {n_valid_shuffles}')\n",
    "valid_stacked_arrays = stacked_arrays[is_valid_row] ## Get only the rows where all elements along both axis (1, 2) are True\n",
    "\n",
    "# Need: valid_stacked_arrays, real_stacked_arrays, combined_variable_names\n",
    "combined_epoch_stats_df: pd.DataFrame = pd.DataFrame(real_stacked_arrays, columns=combined_variable_names)\n",
    "combined_variable_z_score_column_names = [f\"{a_name}_Z\" for a_name in combined_variable_names] # combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']\n",
    "\n",
    "## Extract the stats values for each shuffle from `valid_stacked_arrays`:\n",
    "n_epochs = np.shape(real_stacked_arrays)[0]\n",
    "n_variables = np.shape(real_stacked_arrays)[1]\n",
    "\n",
    "# valid_stacked_arrays.shape: (n_shuffles, n_epochs, n_variables)\n",
    "assert n_epochs == np.shape(valid_stacked_arrays)[-2]\n",
    "assert n_variables == np.shape(valid_stacked_arrays)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "num_shuffles = 5\n",
    "\n",
    "# Define the operation to be run in parallel for a shuffle iteration\n",
    "def shuffle_iteration(i):\n",
    "    # Working on a copy of the DataFrame\n",
    "    shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "    for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "        mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "\n",
    "        # Shuffle 'aclu' values\n",
    "        shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        # Apply aclu peak map dictionary to 'aclu' column\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    # Return the shuffled DataFrame\n",
    "    return shuffled_df\n",
    "\n",
    "# Create a list to hold the shuffled dataframes\n",
    "shuffled_dfs = Parallel(n_jobs=-1)(delayed(shuffle_iteration)(i) for i in range(num_shuffles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()]\n",
    "print(peak_column_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    # Apply aclu peak map dictionary to each decoder name\n",
    "    for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "        active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df['aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffles = {}\n",
    "    for i in range(num_shuffles):\n",
    "        shuffles[i] = active_selected_spikes_df.copy()\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffles[i]['Probe_Epoch_id'])\n",
    "            # Shuffle multiple columns here:\n",
    "            for a_decoder_name in decoder_aclu_peak_map_dict.keys():\n",
    "                shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'].sample(frac=1).values\n",
    "    return shuffles\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "shuffled_dfs = _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "# shuffled_dfs\n",
    "# 5, 1.5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle 'aclu' values\n",
    "shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "\n",
    "# Shuffle aclu and their corresponding peaks: ['aclu', 'long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()] # ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']] = shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']].sample(frac=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d641689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(output_active_epoch_computed_values) # 0.946189 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #TODO 2023-12-13 02:07: - [ ] Figure out how 'Probe_Epoch_id' maps to `ripple_result_tuple.active_epochs`\n",
    "ripple_result_tuple.active_epochs\n",
    "rank_order_results.LR_ripple.ranked_aclus_stats_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the pf_x information for each aclu:\n",
    "## 2023-10-11 - Get the long/short peak locations\n",
    "# decoder_peak_coms_list = [a_decoder.pf.ratemap.peak_tuning_curve_center_of_masses[is_good_aclus] for a_decoder in decoder_args]\n",
    "decoder_aclu_peak_location_dict_list = [dict(zip(neuron_IDs, peak_locations)) for neuron_IDs, peak_locations in zip(track_templates.decoder_neuron_IDs_list, track_templates.decoder_peak_location_list)]\n",
    "decoder_aclu_peak_location_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de234ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_tuning_curve_center_of_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1305ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.decoder_LR_pf_peak_ranks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b179f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replays:\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "if isinstance(global_replays, pd.DataFrame):\n",
    "\tglobal_replays = Epoch(global_replays.epochs.get_valid_df())\n",
    "\n",
    "# get the aligned epochs and the z-scores aligned to them:\n",
    "active_replay_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_replays.to_dataframe().copy(), is_laps=False)\n",
    "active_replay_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6384a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laps:\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps).trimmed_to_non_overlapping()\n",
    "active_laps_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_laps.to_dataframe(), is_laps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find only the significant events (|z| > 1.96):\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "filtered_z_score_df, (n_events, n_significant_events, percent_significant_events) = RankOrderAnalyses.find_only_significant_events(rank_order_results, high_z_criteria=1.96)\n",
    "filtered_z_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_z_score_df.index.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86532662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-11-20 - Finding high-significance periods for Kamran:\n",
    "z_threshold = 1.96\n",
    "is_greater_than_z_threshold_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) > z_threshold)\n",
    "is_greater_than_z_threshold_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) > z_threshold)\n",
    "is_significant_either = np.logical_or(is_greater_than_z_threshold_long, is_greater_than_z_threshold_short)\n",
    "is_significant_either\n",
    "\n",
    "# is_greater_than_3std_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) >= 3.0)\n",
    "# is_greater_than_3std_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) >= 3.0)\n",
    "# is_significant_either = np.logical_or(is_greater_than_3std_long, is_greater_than_3std_short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f925cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_ripple_epochs = deepcopy(Epoch(ripple_result_tuple.active_epochs)).boolean_indicies_slice(is_significant_either)\n",
    "# significant_ripple_epochs = deepcopy(global_replays).boolean_indicies_slice(is_significant_either)\n",
    "significant_ripple_epochs.to_dataframe()\n",
    "\n",
    "# significant_ripple_epochs.filename = Path(f'output/2023-11-27_SignificantReplayRipples').resolve()\n",
    "# significant_ripple_epochs.to_neuroscope()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8beece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_epochs = ripple_result_tuple.active_epochs\n",
    "active_epochs: Epoch = rank_order_results.RL_ripple.epochs_df # Epoch(rank_order_results.RL_ripple.epochs_df)\n",
    "# type(active_epochs)\n",
    "active_epochs.n_epochs\n",
    "# rank_order_results.RL_ripple.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ffe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.epochs_df\n",
    "rank_order_results.LR_ripple.spikes_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39525572",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_variable_names: ['LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson']\n",
    "combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f47973",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.build_display_context_for_filtered_session(filtered_session_name='maze_any', display_fn_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b319650",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ebf52",
   "metadata": {},
   "source": [
    "#### Iterates through the epochs (via the slider) and saves out the images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f73ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = Path(r'C:\\Users\\pho\\Desktop\\2023-12-19 Exports').resolve()\n",
    "all_save_paths = _out_rank_order_event_raster_debugger.export_figure_all_slider_values(export_path=export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_y_values_dict = {_active_plot_identifier:{int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs} for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()}\n",
    "aclu_max_y_values_dict = {_active_plot_identifier:np.max(list({int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs}.values())) for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()} # {'long_LR': 51.48039215686274, 'long_RL': 53.5, 'short_LR': 51.48039215686274, 'short_RL': 53.5}\n",
    "global_max_y_value = np.max(list(aclu_max_y_values_dict.values()))\n",
    "global_max_y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01518ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_neurons = np.max([len(v) for v in _out_rank_order_event_raster_debugger.plots_data.unsorted_original_neuron_IDs_lists])\n",
    "max_n_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots.all_separate_plots['long_LR']['root_plot']\n",
    "\n",
    "\n",
    "root_plots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_alt_directional_merged_decoders_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d02ad",
   "metadata": {},
   "source": [
    "## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab95d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dabb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive-mode parameters:\n",
    "_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=True, scrollable_figure=True, defer_render=False)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_curr_interaction_mode_kwargs = _interactive_mode_kwargs # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interactive:\n",
    "_non_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=False, scrollable_figure=False, defer_render=True)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=False, backend='AGG')\n",
    "_curr_interaction_mode_kwargs = _non_interactive_mode_kwargs # non-interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727307e",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Marginal Scatter Plots from `alt_directional_merged_decoders_result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "use_single_time_bin_per_epoch = False\n",
    "active_display_context = curr_active_pipeline.build_display_context_for_session('plot_all_epoch_bins_marginal_predictions', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size) # \n",
    "if use_single_time_bin_per_epoch:\n",
    "\tactive_display_context = active_display_context.adding_context_if_missing(use_single_time_bin_per_epoch=use_single_time_bin_per_epoch)\n",
    "\n",
    "# 'directional_decoded_epochs_marginals'\n",
    "collector_decoded_epoch_marginals = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs_marginals', curr_active_pipeline.get_session_context(), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=active_display_context,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsave_figure=True, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdirectional_merged_decoders_result=alt_directional_merged_decoders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae85fcb",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Marginal Yellow-Blue Plots from `alt_directional_merged_decoders_result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5876c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_context = owning_pipeline_reference.sess.get_context()\n",
    "# Build the active context directly:\n",
    "active_display_context: IdentifyingContext = curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size)\n",
    "if use_single_time_bin_per_epoch:\n",
    "\tactive_display_context = active_display_context.adding_context_if_missing(use_single_time_bin_per_epoch=use_single_time_bin_per_epoch)\n",
    "active_display_context\n",
    "\n",
    "## Plot the decoded epoch bins of the custom result:\n",
    "_out_decoded_epochs = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', curr_active_pipeline.get_session_context(), #active_display_context,\n",
    "\tmax_num_lap_epochs = 80, max_num_ripple_epochs = 100,\n",
    "\t# render_directional_marginal_laps=True, render_directional_marginal_ripples=True, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=True,\n",
    "\trender_directional_marginal_laps=False, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=False, render_track_identity_marginal_ripples=True,\n",
    "\t# constrained_layout=True, # layout='none',\n",
    "\t# build_fn='basic_view', constrained_layout=True, # 25.5s\n",
    "\tbuild_fn='insets_view', constrained_layout=True, #constrained_layout=None, layout='none', # , constrained_layout=False constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast, 4.2s\n",
    "\t**_curr_interaction_mode_kwargs, # interactive mode\n",
    "\tskip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=True, \n",
    "\tdirectional_merged_decoders_result=alt_directional_merged_decoders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t)\n",
    "collector_decoded_epochs = _out_decoded_epochs['collector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_only_keys = [item for item in active_display_context.keys() if 'lap' in item] # items exclusive to laps: ['laps_t_bin']\n",
    "ripple_only_keys = [item for item in active_display_context.keys() if 'ripple' in item]\n",
    "laps_context = active_display_context.get_subset(subset_excludelist=ripple_only_keys) # laps specific context filtering out the ripple keys\n",
    "ripple_context = active_display_context.get_subset(subset_excludelist=laps_only_keys) # ripple specific context filtering out the laps keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4169a",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Build General Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## `alt_directional_merged_decoders_result`\n",
    "from PendingNotebookCode import test_build_new_marginals_df\n",
    "\n",
    "# `alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result`\n",
    "\n",
    "# laps_time_bin_marginals_df = test_build_new_marginals_df(alt_directional_merged_decoders_result)\n",
    "laps_time_bin_marginals_df: pd.DataFrame = test_build_new_marginals_df(a_decoder_result=deepcopy(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t a_track_identity_marginals=alt_directional_merged_decoders_result.laps_directional_marginals_tuple[0]\n",
    "\t\t\t\t\t\t\t )\n",
    "laps_time_bin_marginals_df\n",
    "\n",
    "ripple_time_bin_marginals_df: pd.DataFrame = test_build_new_marginals_df(a_decoder_result=deepcopy(alt_directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t a_track_identity_marginals=alt_directional_merged_decoders_result.ripple_directional_marginals_tuple[0]\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "ripple_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "\n",
    "perform_write_to_file_callback = None\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = deepcopy(laps_time_bin_marginals_df)\n",
    "ripple_all_epoch_bins_marginals_df = deepcopy(ripple_time_bin_marginals_df)\n",
    "\n",
    "if active_context is not None:\n",
    "\tdisplay_context = active_context.adding_context('display_fn', display_fn_name='plot_all_epoch_bins_marginal_predictions')\n",
    "\t\n",
    "# These subset contexts are used to filter out lap/ripple only keys.\n",
    "# e.g. active_context=curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size)\n",
    "\t# only want laps_t_bin on the laps plot and ripple_t_bin on the ripples plot\n",
    "laps_only_keys = [item for item in display_context.keys() if 'lap' in item] # items exclusive to laps: ['laps_t_bin']\n",
    "ripple_only_keys = [item for item in display_context.keys() if 'ripple' in item]\n",
    "laps_display_context = display_context.get_subset(subset_excludelist=ripple_only_keys) # laps specific context filtering out the ripple keys\n",
    "ripple_display_context = display_context.get_subset(subset_excludelist=laps_only_keys) # ripple specific context filtering out the laps keys\n",
    "\n",
    "\n",
    "with mpl.rc_context({'figure.figsize': (12.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42,\n",
    "\t\t\t\t\t\t\"axes.spines.left\": False, \"axes.spines.right\": False, \"axes.spines.bottom\": False, \"axes.spines.top\": False,\n",
    "\t\t\t\t\t\t\"axes.edgecolor\": \"none\", \"xtick.bottom\": False, \"xtick.top\": False, \"ytick.left\": False, \"ytick.right\": False}):\n",
    "\t# Create a FigureCollector instance\n",
    "\twith FigureCollector(name='plot_all_epoch_bins_marginal_predictions', base_context=display_context) as collector:\n",
    "\n",
    "\t\t## Define common operations to do after making the figure:\n",
    "\t\tdef setup_common_after_creation(a_collector, fig, axes, sub_context, title=f'<size:22> Sig. (>0.95) <weight:bold>Best</> <weight:bold>Quantile Diff</></>'):\n",
    "\t\t\t\"\"\" Captures:\n",
    "\n",
    "\t\t\tt_split, t_start, t_end)\n",
    "\t\t\t\"\"\"\n",
    "\t\t\ta_collector.contexts.append(sub_context)\n",
    "\t\t\t\n",
    "\t\t\tfor ax in (axes if isinstance(axes, Iterable) else [axes]):\n",
    "\t\t\t\t# Update the xlimits with the new bounds\n",
    "\t\t\t\tax.set_ylim(0.0, 1.0)\n",
    "\t\t\t\t# Add epoch indicators\n",
    "\t\t\t\t_tmp_output_dict = PlottingHelpers.helper_matplotlib_add_long_short_epoch_indicator_regions(ax=ax, t_split=t_delta, t_start=t_start, t_end=t_end)\n",
    "\t\t\t\t# Update the xlimits with the new bounds\n",
    "\t\t\t\tax.set_xlim(t_start, t_end)\n",
    "\t\t\t\t# Draw a horizontal line at y=0.5\n",
    "\t\t\t\tax.axhline(y=0.5, color=(0,0,0,1)) # , linestyle='--'\n",
    "\t\t\t\t## This is figure level stuff and only needs to be done once:\n",
    "\t\t\t\t# `flexitext` version:\n",
    "\t\t\t\ttext_formatter = FormattedFigureText()\n",
    "\t\t\t\tax.set_title('')\n",
    "\t\t\t\tfig.suptitle('')\n",
    "\t\t\t\t# top=0.84, bottom=0.125, left=0.07, right=0.97,\n",
    "\t\t\t\t# text_formatter.setup_margins(fig, top_margin=1.0, left_margin=0.0, right_margin=1.0, bottom_margin=0.05)\n",
    "\t\t\t\ttext_formatter.setup_margins(fig, top_margin=0.84, left_margin=0.07, right_margin=0.97, bottom_margin=0.125)\n",
    "\t\t\t\t# fig.subplots_adjust(top=top_margin, left=left_margin, right=right_margin, bottom=bottom_margin)\n",
    "\t\t\t\t# title_text_obj = flexitext(text_formatter.left_margin, text_formatter.top_margin, title, va=\"bottom\", xycoords=\"figure fraction\")\n",
    "\t\t\t\ttitle_text_obj = flexitext(text_formatter.left_margin, 0.98, title, va=\"top\", xycoords=\"figure fraction\") # 0.98, va=\"top\" means the top edge of the title will be aligned to the fig_y=0.98 mark of the figure.\n",
    "\t\t\t\t# footer_text_obj = flexitext((text_formatter.left_margin * 0.1), (text_formatter.bottom_margin * 0.25),\n",
    "\t\t\t\t#                             text_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t#                             va=\"top\", xycoords=\"figure fraction\")\n",
    "\n",
    "\t\t\t\tfooter_text_obj = flexitext((text_formatter.left_margin * 0.1), (0.0025), ## (va=\"bottom\", (0.0025)) - this means that the bottom edge of the footer text is aligned with the fig_y=0.0025 in figure space\n",
    "\t\t\t\t\t\t\t\t\t\t\ttext_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t\t\t\t\t\t\t\tva=\"bottom\", xycoords=\"figure fraction\")\n",
    "\t\t\n",
    "\t\t\tif ((perform_write_to_file_callback is not None) and (sub_context is not None)):\n",
    "\t\t\t\tperform_write_to_file_callback(sub_context, fig)\n",
    "\t\t\t\n",
    "\t\t# Plot for BestDir\n",
    "\t\tfig, ax = collector.subplots(num='Laps_Marginal', clear=True)\n",
    "\t\t_out_Laps = sns.scatterplot(\n",
    "\t\t\tax=ax,\n",
    "\t\t\tdata=laps_all_epoch_bins_marginals_df,\n",
    "\t\t\tx='t_bin_center',\n",
    "\t\t\ty='P_Long',\n",
    "\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t)\n",
    "\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=laps_display_context.adding_context('subplot', subplot_name='Laps all_epoch_binned Marginals'), \n",
    "\t\t\t\t\t\t\t\t\ttitle=f'<size:22> Laps <weight:bold>all_epoch_binned</> Marginals</>')\n",
    "\t\t\n",
    "\t\tfig, ax = collector.subplots(num='Ripple_Marginal', clear=True)\n",
    "\t\t_out_Ripple = sns.scatterplot(\n",
    "\t\t\tax=ax,\n",
    "\t\t\tdata=ripple_all_epoch_bins_marginals_df,\n",
    "\t\t\tx='t_bin_center',\n",
    "\t\t\ty='P_Long',\n",
    "\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t)\n",
    "\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=ripple_display_context.adding_context('subplot', subplot_name='Ripple all_epoch_binned Marginals'), \n",
    "\t\t\t\t\t\ttitle=f'<size:22> Ripple <weight:bold>all_epoch_binned</> Marginals</>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d655560",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_time_bin_marginals_df['lap_idx'] = laps_time_bin_marginals_df.index.to_numpy()\n",
    "laps_time_bin_marginals_df['lap_start_t'] = laps_epochs_df['start'].to_numpy()\n",
    "laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3800d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-01-19 - Can decode position from the pseudo2D posterior directly, or by using the pseudo2D decoder to determine the best direction and track_id and use the corresponding 1D decoder's predicted position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f76dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-01-19 - Export All Epoch Time bin marginals to CSV also\n",
    "## Laps:\n",
    "laps_epochs_df: pd.DataFrame = deepcopy(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs).to_dataframe()\n",
    "laps_directional_marginals_tuple = DirectionalMergedDecodersResult.determine_directional_likelihoods(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals = DirectionalMergedDecodersResult.determine_long_short_likelihoods(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "track_identity_marginals, track_identity_all_epoch_bins_marginal, most_likely_track_identity_from_decoder, is_most_likely_track_identity_Long = laps_track_identity_marginals\n",
    "\n",
    "laps_marginals_df: pd.DataFrame = pd.DataFrame(np.hstack((laps_directional_all_epoch_bins_marginal, track_identity_all_epoch_bins_marginal)), columns=['P_LR', 'P_RL', 'P_Long', 'P_Short'])\n",
    "laps_marginals_df['lap_idx'] = laps_marginals_df.index.to_numpy()\n",
    "laps_marginals_df['lap_start_t'] = laps_epochs_df['start'].to_numpy()\n",
    "laps_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78fbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(laps_marginals_df)\n",
    "laps_marginals_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local computation: check laps\n",
    "laps = curr_active_pipeline.sess.laps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(start=0.030, step=0.01, stop=0.10) # [0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc018065",
   "metadata": {},
   "source": [
    "# Call perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c6522e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "\n",
    "BATCH_DATE_TO_USE: str = '2024-02-16_Lab' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "collected_outputs_path = Path('/nfs/turbo/umms-kdiba/Data/Output/collected_outputs').resolve() # Linux\n",
    "# collected_outputs_path: Path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs').resolve() # GreatLakes\n",
    "# collected_outputs_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs').resolve() # Apogee\n",
    "\n",
    "\n",
    "def perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict, save_hdf=True, save_csvs=True) -> dict:\n",
    "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    print(f'perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...,across_session_results_extended_dict: {across_session_results_extended_dict})')\n",
    "    from copy import deepcopy\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from neuropy.utils.debug_helpers import parameter_sweeps\n",
    "    from neuropy.core.laps import Laps\n",
    "    from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _check_result_laps_epochs_df_performance\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "    from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "    # Export CSVs:\n",
    "    def export_marginals_df_csv(marginals_df: pd.DataFrame, data_identifier_str: str, parent_output_path: Path, active_context):\n",
    "        \"\"\" captures nothing\n",
    "        \"\"\"\n",
    "        # output_date_str: str = get_now_rounded_time_str()\n",
    "        output_date_str: str = get_now_day_str()\n",
    "        # parent_output_path: Path = Path('output').resolve()\n",
    "        # active_context = curr_active_pipeline.get_session_context()\n",
    "        session_identifier_str: str = active_context.get_description()\n",
    "        assert output_date_str is not None\n",
    "        out_basename = '-'.join([output_date_str, session_identifier_str, data_identifier_str]) # '2024-01-04|kdiba_gor01_one_2006-6-09_1-22-43|(laps_marginals_df).csv'\n",
    "        out_filename = f\"{out_basename}.csv\"\n",
    "        out_path = parent_output_path.joinpath(out_filename).resolve()\n",
    "        marginals_df.to_csv(out_path)\n",
    "        return out_path \n",
    "\n",
    "\n",
    "    def _subfn_process_time_bin_swept_results(curr_active_pipeline, output_extracted_result_tuples):\n",
    "        \"\"\" After the sweeps are complete and multiple (one for each time_bin_size swept) indepdnent dfs are had with the four results types this function concatenates each of the four into a single dataframe for all time_bin_size values with a column 'time_bin_size'. \n",
    "        It also saves them out to CSVs in a manner similar to what `compute_and_export_marginals_dfs_completion_function` did to be compatible with `2024-01-23 - Across Session Point and YellowBlue Marginal CSV Exports.ipynb`\n",
    "        Captures: save_csvs\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        several_time_bin_sizes_laps_time_bin_marginals_df_list = []\n",
    "        several_time_bin_sizes_laps_per_epoch_marginals_df_list = []\n",
    "\n",
    "        several_time_bin_sizes_ripple_time_bin_marginals_df_list = []\n",
    "        several_time_bin_sizes_ripple_per_epoch_marginals_df_list = []\n",
    "\n",
    "\n",
    "        # for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
    "        for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df, a_ripple_time_bin_marginals_df, a_ripple_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
    "            a_sweep_dict = dict(a_sweep_tuple)\n",
    "            \n",
    "            # Shared\n",
    "            desired_laps_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
    "            desired_ripple_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
    "            \n",
    "            # a_laps_time_bin_marginals_df.\n",
    "            df = a_laps_time_bin_marginals_df\n",
    "            df['time_bin_size'] = desired_laps_decoding_time_bin_size # desired_laps_decoding_time_bin_size\n",
    "            # df['session_name'] = session_name\n",
    "            df = a_laps_all_epoch_bins_marginals_df\n",
    "            df['time_bin_size'] = desired_laps_decoding_time_bin_size\n",
    "\n",
    "            df = a_ripple_time_bin_marginals_df\n",
    "            df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
    "            df = a_ripple_all_epoch_bins_marginals_df\n",
    "            df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
    "\n",
    "            several_time_bin_sizes_laps_time_bin_marginals_df_list.append(a_laps_time_bin_marginals_df)\n",
    "            several_time_bin_sizes_laps_per_epoch_marginals_df_list.append(a_laps_all_epoch_bins_marginals_df)\n",
    "            \n",
    "            several_time_bin_sizes_ripple_time_bin_marginals_df_list.append(a_ripple_time_bin_marginals_df)\n",
    "            several_time_bin_sizes_ripple_per_epoch_marginals_df_list.append(a_ripple_all_epoch_bins_marginals_df)\n",
    "\n",
    "\n",
    "        ## Build across_sessions join dataframes:\n",
    "        several_time_bin_sizes_time_bin_laps_df: pd.DataFrame = pd.concat(several_time_bin_sizes_laps_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
    "        several_time_bin_sizes_laps_df: pd.DataFrame = pd.concat(several_time_bin_sizes_laps_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
    "\n",
    "        several_time_bin_sizes_time_bin_ripple_df: pd.DataFrame = pd.concat(several_time_bin_sizes_ripple_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
    "        several_time_bin_sizes_ripple_df: pd.DataFrame = pd.concat(several_time_bin_sizes_ripple_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
    "\n",
    "        # Export time_bin_swept results to CSVs:\n",
    "        if save_csvs:\n",
    "            assert collected_outputs_path.exists()\n",
    "            active_context = curr_active_pipeline.get_session_context()\n",
    "            laps_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_laps_df, data_identifier_str=f'(laps_time_bin_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            laps_out_path = export_marginals_df_csv(several_time_bin_sizes_laps_df, data_identifier_str=f'(laps_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            ripple_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_ripple_df, data_identifier_str=f'(ripple_time_bin_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            ripple_out_path = export_marginals_df_csv(several_time_bin_sizes_ripple_df, data_identifier_str=f'(ripple_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "        else:\n",
    "            laps_time_bin_marginals_out_path, laps_out_path, ripple_time_bin_marginals_out_path, ripple_out_path = None, None, None, None\n",
    "            \n",
    "        return (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
    "        # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
    "        \n",
    "\n",
    "\n",
    "    def add_session_df_columns(df: pd.DataFrame, session_name: str, curr_session_t_delta: Optional[float], time_col: str) -> pd.DataFrame:\n",
    "        \"\"\" adds session-specific information to the marginal dataframes \"\"\"\n",
    "        df['session_name'] = session_name \n",
    "        if curr_session_t_delta is not None:\n",
    "            df['delta_aligned_start_t'] = df[time_col] - curr_session_t_delta\n",
    "        return df\n",
    "\n",
    "\n",
    "    ## Single decode:\n",
    "    def _try_single_decode(owning_pipeline_reference, directional_merged_decoders_result, use_single_time_bin_per_epoch: bool, desired_laps_decoding_time_bin_size: Optional[float]=None, desired_ripple_decoding_time_bin_size: Optional[float]=None, desired_shared_decoding_time_bin_size: Optional[float]=None, minimum_event_duration: Optional[float]=None):\n",
    "        \"\"\" decodes laps and ripples for a single bin size. \n",
    "        \n",
    "        minimum_event_duration: if provided, excludes all events shorter than minimum_event_duration\n",
    "        \"\"\"\n",
    "        if desired_shared_decoding_time_bin_size is not None:\n",
    "            assert desired_laps_decoding_time_bin_size is None\n",
    "            assert desired_ripple_decoding_time_bin_size is None\n",
    "            desired_laps_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
    "            desired_ripple_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
    "            \n",
    "\n",
    "        ## Decode Laps:\n",
    "        laps_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs)\n",
    "        if not isinstance(laps_epochs_df, pd.DataFrame):\n",
    "            laps_epochs_df = laps_epochs_df.to_dataframe()\n",
    "        # global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any' (? same as global_epoch_name?)\n",
    "        min_possible_laps_time_bin_size: float = find_minimum_time_bin_duration(laps_epochs_df['duration'].to_numpy())\n",
    "        min_bounded_laps_decoding_time_bin_size: float = min(desired_laps_decoding_time_bin_size, min_possible_laps_time_bin_size) # 10ms # 0.002\n",
    "        if desired_laps_decoding_time_bin_size < min_bounded_laps_decoding_time_bin_size:\n",
    "            print(f'WARN: desired_laps_decoding_time_bin_size: {desired_laps_decoding_time_bin_size} < min_bounded_laps_decoding_time_bin_size: {min_bounded_laps_decoding_time_bin_size}... hopefully it works.')\n",
    "        laps_decoding_time_bin_size: float = desired_laps_decoding_time_bin_size # allow direct use\n",
    "        if use_single_time_bin_per_epoch:\n",
    "            laps_decoding_time_bin_size = None\n",
    "        directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=laps_epochs_df,\n",
    "                                                                                                                                                        decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "        \n",
    "\n",
    "        ## Decode Ripples:\n",
    "        if desired_ripple_decoding_time_bin_size is not None:\n",
    "            # global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(owning_pipeline_reference.filtered_sessions[global_epoch_name].replay))\n",
    "            replay_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs)\n",
    "            if not isinstance(replay_epochs_df, pd.DataFrame):\n",
    "                replay_epochs_df = replay_epochs_df.to_dataframe()\n",
    "            # min_possible_ripple_time_bin_size: float = find_minimum_time_bin_duration(replay_epochs_df['duration'].to_numpy())\n",
    "            # min_bounded_ripple_decoding_time_bin_size: float = min(desired_ripple_decoding_time_bin_size, min_possible_ripple_time_bin_size) # 10ms # 0.002\n",
    "            # if desired_ripple_decoding_time_bin_size < min_bounded_ripple_decoding_time_bin_size:\n",
    "            #     print(f'WARN: desired_ripple_decoding_time_bin_size: {desired_ripple_decoding_time_bin_size} < min_bounded_ripple_decoding_time_bin_size: {min_bounded_ripple_decoding_time_bin_size}... hopefully it works.')\n",
    "            ripple_decoding_time_bin_size: float = desired_ripple_decoding_time_bin_size # allow direct use            \n",
    "            ## Drop those less than the time bin duration\n",
    "            print(f'DropShorterMode:')\n",
    "            pre_drop_n_epochs = len(replay_epochs_df)\n",
    "            if minimum_event_duration is not None:                \n",
    "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > minimum_event_duration]\n",
    "                post_drop_n_epochs = len(replay_epochs_df)\n",
    "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
    "                print(f'\\tminimum_event_duration present (minimum_event_duration={minimum_event_duration}).\\n\\tdropping {n_dropped_epochs} that are shorter than our minimum_event_duration of {minimum_event_duration}.', end='\\t')\n",
    "            else:\n",
    "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > desired_ripple_decoding_time_bin_size]\n",
    "                post_drop_n_epochs = len(replay_epochs_df)\n",
    "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
    "                print(f'\\tdropping {n_dropped_epochs} that are shorter than our ripple decoding time bin size of {desired_ripple_decoding_time_bin_size}', end='\\t') \n",
    "\n",
    "            print(f'{post_drop_n_epochs} remain.')\n",
    "            directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=replay_epochs_df,\n",
    "                                                                                                                                                                                            decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "\n",
    "        directional_merged_decoders_result.perform_compute_marginals()\n",
    "        return directional_merged_decoders_result\n",
    "        \n",
    "\n",
    "    def _update_result_laps(a_result: DecodedFilterEpochsResult, laps_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" captures nothing. Can reusing the same laps_df as it makes no modifications to it. \n",
    "        \n",
    "        e.g. a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple]\n",
    "        \"\"\"\n",
    "        result_laps_epochs_df: pd.DataFrame = a_result.laps_epochs_df\n",
    "        ## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions\n",
    "        result_laps_epochs_df['maze_id'] = laps_df['maze_id'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
    "        ## add the 'is_LR_dir' groud-truth column in:\n",
    "        result_laps_epochs_df['is_LR_dir'] = laps_df['is_LR_dir'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
    "        \n",
    "        laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir = a_result.laps_directional_marginals_tuple\n",
    "        laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = a_result.laps_track_identity_marginals_tuple\n",
    "        ## Add the decoded results to the laps df:\n",
    "        result_laps_epochs_df['is_most_likely_track_identity_Long'] = laps_is_most_likely_track_identity_Long\n",
    "        result_laps_epochs_df['is_most_likely_direction_LR'] = laps_is_most_likely_direction_LR_dir\n",
    "        return result_laps_epochs_df\n",
    "\n",
    "    # BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "    assert collected_outputs_path.exists()\n",
    "    curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "    CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "    print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "    active_context = curr_active_pipeline.get_session_context()\n",
    "    session_ctxt_key:str = active_context.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "    \n",
    "    ## INPUT PARAMETER: time_bin_size sweep paraemters\n",
    "    desired_shared_decoding_time_bin_size = np.linspace(start=0.030, stop=0.10, num=6)\n",
    "    \n",
    "    # Shared time bin sizes\n",
    "    # all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_sizes, use_single_time_bin_per_epoch=[False], desired_ripple_decoding_time_bin_size=[None])\n",
    "    all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_shared_decoding_time_bin_size=desired_shared_decoding_time_bin_size, use_single_time_bin_per_epoch=[False], minimum_event_duration=[desired_shared_decoding_time_bin_size[-1]]) # with Ripples\n",
    "    # len(all_param_sweep_options)\n",
    "    \n",
    "    ## Perfrom the computations:\n",
    "\n",
    "    # DirectionalMergedDecoders: Get the result after computation:\n",
    "    ## Copy the default result:\n",
    "    directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "    alt_directional_merged_decoders_result: DirectionalMergedDecodersResult = deepcopy(directional_merged_decoders_result)\n",
    "\n",
    "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size-{laps_decoding_time_bin_size}_{data_identifier_str}\"\n",
    "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size_sweep_results\"\n",
    "    out_path_basename_str: str = f\"{CURR_BATCH_OUTPUT_PREFIX}_time_bin_size_sweep_results\"\n",
    "    # out_path_filenname_str: str = f\"{out_path_basename_str}.csv\"\n",
    "\n",
    "    out_path_filenname_str: str = f\"{out_path_basename_str}.h5\"\n",
    "    out_path: Path = collected_outputs_path.resolve().joinpath(out_path_filenname_str).resolve()\n",
    "    print(f'\\out_path_str: \"{out_path_filenname_str}\"')\n",
    "    print(f'\\tout_path: \"{out_path}\"')\n",
    "    \n",
    "    # Ensure it has the 'lap_track' column\n",
    "    ## Compute the ground-truth information using the position information:\n",
    "    # adds columns: ['maze_id', 'is_LR_dir']\n",
    "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "    laps_obj: Laps = curr_active_pipeline.sess.laps\n",
    "    laps_obj.update_lap_dir_from_smoothed_velocity(pos_input=curr_active_pipeline.sess.position)\n",
    "    laps_obj.update_maze_id_if_needed(t_start=t_start, t_delta=t_delta, t_end=t_end)\n",
    "    laps_df = laps_obj.to_dataframe()\n",
    "    \n",
    "    # Uses: session_ctxt_key, all_param_sweep_options\n",
    "    output_alt_directional_merged_decoders_result = {} # empty dict\n",
    "    output_laps_decoding_accuracy_results_dict = {} # empty dict\n",
    "    output_extracted_result_tuples = {}\n",
    "\n",
    "    for a_sweep_dict in all_param_sweep_options:\n",
    "        a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
    "        print(f'a_sweep_dict: {a_sweep_dict}')\n",
    "        # Convert parameters to string because Parquet supports metadata as string\n",
    "        a_sweep_str_params = {key: str(value) for key, value in a_sweep_dict.items() if value is not None}\n",
    "        \n",
    "        output_alt_directional_merged_decoders_result[a_sweep_tuple] = _try_single_decode(curr_active_pipeline, alt_directional_merged_decoders_result, **a_sweep_dict)\n",
    "\n",
    "        laps_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_time_bin_marginals_df.copy()\n",
    "        laps_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_all_epoch_bins_marginals_df.copy()\n",
    "        \n",
    "        ## Ripples:\n",
    "        ripple_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_time_bin_marginals_df.copy()\n",
    "        ripple_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_all_epoch_bins_marginals_df.copy()\n",
    "\n",
    "        session_name = curr_session_name\n",
    "        curr_session_t_delta = t_delta\n",
    "        \n",
    "        for a_df, a_time_bin_column_name in zip((laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df), ('t_bin_center', 'lap_start_t', 't_bin_center', 'ripple_start_t')):\n",
    "            ## Add the session-specific columns:\n",
    "            a_df = add_session_df_columns(a_df, session_name, curr_session_t_delta, a_time_bin_column_name)\n",
    "\n",
    "        ## Build the output tuple:\n",
    "        output_extracted_result_tuples[a_sweep_tuple] = (laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df)\n",
    "        \n",
    "        # desired_laps_decoding_time_bin_size_str: str = a_sweep_str_params.get('desired_laps_decoding_time_bin_size', None)\n",
    "        laps_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_decoding_time_bin_size\n",
    "        # ripple_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_decoding_time_bin_size\n",
    "        actual_laps_decoding_time_bin_size_str: str = str(laps_decoding_time_bin_size)\n",
    "        if save_hdf and (actual_laps_decoding_time_bin_size_str is not None):\n",
    "            laps_time_bin_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_time_bin_marginals_df', format='table', data_columns=True)\n",
    "            laps_all_epoch_bins_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_all_epoch_bins_marginals_df', format='table', data_columns=True)\n",
    "\n",
    "        ## TODO: output ripple .h5 here if desired.\n",
    "            \n",
    "\n",
    "        # get the current lap object and determine the percentage correct:\n",
    "        result_laps_epochs_df: pd.DataFrame = _update_result_laps(a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple], laps_df=laps_df)\n",
    "        (is_decoded_track_correct, is_decoded_dir_correct, are_both_decoded_properties_correct), (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly) = _check_result_laps_epochs_df_performance(result_laps_epochs_df)\n",
    "        output_laps_decoding_accuracy_results_dict[laps_decoding_time_bin_size] = (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly)\n",
    "        \n",
    "\n",
    "    ## Output the performance:\n",
    "    output_laps_decoding_accuracy_results_df: pd.DataFrame = pd.DataFrame(output_laps_decoding_accuracy_results_dict.values(), index=output_laps_decoding_accuracy_results_dict.keys(), \n",
    "                    columns=['percent_laps_track_identity_estimated_correctly',\n",
    "                            'percent_laps_direction_estimated_correctly',\n",
    "                            'percent_laps_estimated_correctly'])\n",
    "    output_laps_decoding_accuracy_results_df.index.name = 'laps_decoding_time_bin_size'\n",
    "    ## Save out the laps peformance result\n",
    "    if save_hdf:\n",
    "        output_laps_decoding_accuracy_results_df.to_hdf(out_path, key=f'{session_ctxt_key}/laps_decoding_accuracy_results', format='table', data_columns=True)\n",
    "\n",
    "    ## Call the subfunction to process the time_bin_size swept result and produce combined output dataframes:\n",
    "    combined_multi_timebin_outputs_tuple = _subfn_process_time_bin_swept_results(curr_active_pipeline, output_extracted_result_tuples)\n",
    "    # Unpacking:    \n",
    "    # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
    "\n",
    "    # add to output dict\n",
    "    # across_session_results_extended_dict['compute_and_export_marginals_dfs_completion_function'] = _out\n",
    "    across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'] = (out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple)\n",
    "    # can unpack like:\n",
    "    (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
    "\n",
    "    print(f'>>\\t done with {curr_session_context}')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "    return across_session_results_extended_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773e9f51",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f6b31",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(None, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, save_hdf=False)\n",
    "out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a71abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file_pat\n",
    "collected_outputs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_laps_decoding_accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_histograms( data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str ) -> None:\n",
    "#     # get the pre-delta epochs\n",
    "#     pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "#     post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "#     descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "#     # plot pre-delta histogram\n",
    "#     pre_delta_df.hist(column='P_Long')\n",
    "#     plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "#     plt.show()\n",
    "\n",
    "#     # plot post-delta histogram\n",
    "#     post_delta_df.hist(column='P_Long')\n",
    "#     plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "def plot_histograms(data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str) -> None:\n",
    "    \"\"\" plots a stacked histogram of the many time-bin sizes \"\"\"\n",
    "    # get the pre-delta epochs\n",
    "    pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "    post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "    descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "    # plot pre-delta histogram\n",
    "    time_bin_sizes = pre_delta_df['time_bin_size'].unique()\n",
    "    \n",
    "    figure_identifier: str = f\"{descriptor_str}_preDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = pre_delta_df[pre_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot post-delta histogram\n",
    "    time_bin_sizes = post_delta_df['time_bin_size'].unique()\n",
    "    figure_identifier: str = f\"{descriptor_str}_postDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = post_delta_df[post_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# # You can use it like this:\n",
    "# plot_histograms('Laps', 'All Sessions', all_sessions_laps_time_bin_df, \"75 ms\")\n",
    "# plot_histograms('Ripples', 'All Sessions', all_sessions_ripple_time_bin_df, \"75 ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from neuropy.utils.matplotlib_helpers import pho_jointplot\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# def pho_jointplot(*args, **kwargs):\n",
    "# \t\"\"\" wraps sns.jointplot to allow adding titles/axis labels/etc.\"\"\"\n",
    "# \ttitle = kwargs.pop('title', None)\n",
    "# \t_out = sns.jointplot(*args, **kwargs)\n",
    "# \tif title is not None:\n",
    "# \t\tplt.suptitle(title)\n",
    "# \treturn _out\n",
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "plot_histograms('Ripples', 'One Session', several_time_bin_sizes_time_bin_ripple_df, \"several\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "several_time_bin_sizes_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(\n",
    "#     several_time_bin_sizes_laps_df, x=\"P_Long\", col=\"species\", row=\"time_bin_size\",\n",
    "#     binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    "# )\n",
    "\n",
    "sns.displot(\n",
    "    several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', row=\"time_bin_size\",\n",
    "    binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be351c18",
   "metadata": {},
   "source": [
    "# 2024-01-31 - Reinvestigation regarding remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "911d7495",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncationCheckingResults(_VersionedResultMixin_version='2024.01.10_0', disappearing_endcap_aclus=Int64Index([21], dtype='int64'), non_disappearing_endcap_aclus=Int64Index([15, 20, 23, 25, 32, 33, 39, 41, 43, 44, 46, 50, 51, 52, 54, 60, 63, 64], dtype='int64'), significant_distant_remapping_endcap_aclus=Int64Index([25, 44, 46, 64], dtype='int64'), minor_remapping_endcap_aclus=Int64Index([15, 20, 23, 32, 33, 39, 41, 43, 50, 51, 52, 54, 60, 63], dtype='int64'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d9b54",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## From Jonathan Long/Short Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3641aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df = deepcopy(jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "\n",
    "## try to add the 2D peak information to the cells in `neuron_replay_stats_df`:\n",
    "neuron_replay_stats_df['long_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['long_pf2D_peak_y'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_y'] = pd.NA\n",
    "\n",
    "# flat_peaks_df: pd.DataFrame = deepcopy(active_peak_prominence_2d_results['flat_peaks_df']).reset_index(drop=True)\n",
    "long_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[long_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "short_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[short_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), long_filtered_flat_peaks_df.neuron_id.to_numpy()), ['long_pf2D_peak_x', 'long_pf2D_peak_y']] = long_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), short_filtered_flat_peaks_df.neuron_id.to_numpy()), ['short_pf2D_peak_x', 'short_pf2D_peak_y']] = short_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "\n",
    "both_included_neuron_stats_df = deepcopy(neuron_replay_stats_df[neuron_replay_stats_df['LS_pf_peak_x_diff'].notnull()]).drop(columns=['track_membership', 'neuron_type'])\n",
    "both_included_neuron_stats_df\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# _out_scatter = sns.scatterplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', y='aclu') # , hue='aclu'\n",
    "# _out_scatter.show()\n",
    "# _out_hist = sns.histplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', bins=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_long_pf].to_numpy()\n",
    "short_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_short_pf].to_numpy()\n",
    "\n",
    "long_pf_aclus, short_pf_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18cfba",
   "metadata": {},
   "source": [
    "## 2024-02-06 - `directional_compute_trial_by_trial_correlation_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c383e8ea",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  6,  7,  8, 10, 11, 15, 17, 20, 21, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 39, 40, 41, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 60, 62, 63, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import compute_spatial_binned_activity_via_pfdt, compute_trial_by_trial_correlation_matrix\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import TrialByTrialActivity\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs\n",
    "\n",
    "# track_templates.shared_LR_aclus_only_neuron_IDs\n",
    "# track_templates.shared_RL_aclus_only_neuron_IDs\n",
    "\n",
    "\n",
    "## Directional Trial-by-Trial Activity:\n",
    "if 'pf1D_dt' not in curr_active_pipeline.computation_results[global_epoch_name].computed_data:\n",
    "\t# if `KeyError: 'pf1D_dt'` recompute\n",
    "\tcurr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "\n",
    "active_pf_1D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D_dt'])\n",
    "active_pf_2D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D_dt'])\n",
    "\n",
    "active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_1D_dt)\n",
    "# active_pf_dt.res\n",
    "# Limit only to the placefield aclus:\n",
    "active_pf_dt = active_pf_dt.get_by_id(ids=any_decoder_neuron_IDs)\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_2D_dt) # 2D\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c34fd9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aclu</th>\n",
       "      <th>subpeak_idx</th>\n",
       "      <th>long_LR_peak</th>\n",
       "      <th>long_RL_peak</th>\n",
       "      <th>short_LR_peak</th>\n",
       "      <th>short_RL_peak</th>\n",
       "      <th>long_LR_peak_height</th>\n",
       "      <th>long_RL_peak_height</th>\n",
       "      <th>short_LR_peak_height</th>\n",
       "      <th>short_RL_peak_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>101.686577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.789350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203.630292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.976133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.546281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214.957371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.311789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.362471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.340736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.667815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.327902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>207.405985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165.873360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.527519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169.649053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165.873360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.854598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.796006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.910272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.892122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.634851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.013656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.513969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.378259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.385979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>230.060143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.686577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>71.481032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196.078905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.371909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80065</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aclu  subpeak_idx  long_LR_peak  long_RL_peak  short_LR_peak  short_RL_peak  long_LR_peak_height  long_RL_peak_height  short_LR_peak_height  short_RL_peak_height\n",
       "0       5            0    101.686577           NaN     116.789350            NaN             1.000000                  NaN               1.00000                   NaN\n",
       "1       6            0           NaN    203.630292            NaN     180.976133                  NaN             1.000000                   NaN              1.000000\n",
       "2       6            1           NaN    154.546281            NaN     214.957371                  NaN             0.311789                   NaN              0.362471\n",
       "3       6            2           NaN    124.340736            NaN     135.667815                  NaN             0.216879                   NaN              0.327902\n",
       "4       6            0    207.405985           NaN     165.873360            NaN             1.000000                  NaN               1.00000                   NaN\n",
       "5       7            0           NaN    188.527519            NaN     169.649053                  NaN             1.000000                   NaN              1.000000\n",
       "..    ...          ...           ...           ...            ...            ...                  ...                  ...                   ...                   ...\n",
       "118    64            1           NaN    165.873360            NaN     199.854598                  NaN             0.796006                   NaN              0.910272\n",
       "119    64            2           NaN    131.892122            NaN            NaN                  NaN             0.634851                   NaN                   NaN\n",
       "120    64            3           NaN    113.013656            NaN            NaN                  NaN             0.513969                   NaN                   NaN\n",
       "121    64            4           NaN     56.378259            NaN            NaN                  NaN             0.385979                   NaN                   NaN\n",
       "122    64            0    230.060143           NaN     101.686577            NaN             1.000000                  NaN               1.00000                   NaN\n",
       "123    64            1     71.481032           NaN     196.078905            NaN             0.371909                  NaN               0.80065                   NaN\n",
       "\n",
       "[124 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decoder_aclu_peak_location_df_merged = deepcopy(track_templates.get_decoders_aclu_peak_location_df(width=None)).drop(columns=['series_idx', 'LR_peak_diff', 'RL_peak_diff'])\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "decoder_aclu_peak_location_df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b01804af",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aclu</th>\n",
       "      <th>long_LR_num_peaks</th>\n",
       "      <th>long_RL_num_peaks</th>\n",
       "      <th>short_LR_num_peaks</th>\n",
       "      <th>short_RL_num_peaks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aclu  long_LR_num_peaks  long_RL_num_peaks  short_LR_num_peaks  short_RL_num_peaks\n",
       "0      5                  1                  0                   1                   0\n",
       "1      6                  1                  2                   1                   3\n",
       "2      7                  2                  2                   2                   2\n",
       "3      8                  2                  3                   1                   1\n",
       "4     10                  0                  1                   0                   1\n",
       "5     11                  1                  0                   1                   0\n",
       "..   ...                ...                ...                 ...                 ...\n",
       "35    55                  3                  2                   1                   2\n",
       "36    56                  1                  1                   1                   1\n",
       "37    60                  1                  1                   1                   2\n",
       "38    62                  2                  2                   2                   2\n",
       "39    63                  1                  1                   1                   1\n",
       "40    64                  2                  4                   2                   2\n",
       "\n",
       "[41 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_aclu_num_peaks_df: pd.DataFrame = track_templates.get_decoders_aclu_num_peaks_df()\n",
    "decoder_aclu_num_peaks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b90dc",
   "metadata": {},
   "source": [
    "## 2024-02-08 - Filter to find only the clear remap examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import TrialByTrialActivity\n",
    "from pyphocorehelpers.indexing_helpers import dict_to_full_array\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0cf56",
   "metadata": {},
   "source": [
    "### Get num peaks exclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_ids_dict = {k:v.neuron_ids for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# neuron_ids_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42a8d",
   "metadata": {},
   "source": [
    "### Get stability for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ec2d7",
   "metadata": {},
   "source": [
    "#### 2024-02-08 - 3pm - new stability dataframe to look at stability of each cell across decoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57869e27",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aclu</th>\n",
       "      <th>long_LR</th>\n",
       "      <th>long_RL</th>\n",
       "      <th>short_LR</th>\n",
       "      <th>short_RL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.874069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910657</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.875489</td>\n",
       "      <td>0.215731</td>\n",
       "      <td>0.846430</td>\n",
       "      <td>0.620833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.446878</td>\n",
       "      <td>0.688336</td>\n",
       "      <td>0.452010</td>\n",
       "      <td>0.879564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.662679</td>\n",
       "      <td>-0.083122</td>\n",
       "      <td>0.743684</td>\n",
       "      <td>0.363851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154582</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.743192</td>\n",
       "      <td>0.456506</td>\n",
       "      <td>0.923773</td>\n",
       "      <td>0.697694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185272</td>\n",
       "      <td>0.343337</td>\n",
       "      <td>0.618398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>56</td>\n",
       "      <td>0.875790</td>\n",
       "      <td>0.752575</td>\n",
       "      <td>0.975669</td>\n",
       "      <td>0.963018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>60</td>\n",
       "      <td>0.849974</td>\n",
       "      <td>0.553614</td>\n",
       "      <td>0.985093</td>\n",
       "      <td>0.533796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688237</td>\n",
       "      <td>0.519924</td>\n",
       "      <td>0.629818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>63</td>\n",
       "      <td>0.269881</td>\n",
       "      <td>0.765199</td>\n",
       "      <td>0.354431</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64</td>\n",
       "      <td>0.782534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.713814</td>\n",
       "      <td>0.728734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aclu   long_LR   long_RL  short_LR  short_RL\n",
       "0      5  0.874069  0.000000  0.910657  0.000000\n",
       "1      6  0.875489  0.215731  0.846430  0.620833\n",
       "2      7  0.446878  0.688336  0.452010  0.879564\n",
       "3      8  0.662679 -0.083122  0.743684  0.363851\n",
       "4     10  0.000000  0.000000  0.154582  0.000000\n",
       "5     11  0.743192  0.456506  0.923773  0.697694\n",
       "..   ...       ...       ...       ...       ...\n",
       "35    55  0.000000  0.185272  0.343337  0.618398\n",
       "36    56  0.875790  0.752575  0.975669  0.963018\n",
       "37    60  0.849974  0.553614  0.985093  0.533796\n",
       "38    62  0.000000  0.688237  0.519924  0.629818\n",
       "39    63  0.269881  0.765199  0.354431  0.000000\n",
       "40    64  0.782534  0.000000  0.713814  0.728734\n",
       "\n",
       "[41 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k,v in directional_active_lap_pf_results_dicts.items():\n",
    "# stability_dict = {k:v.aclu_to_stability_score_dict for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict = {k:dict_to_full_array(v.aclu_to_stability_score_dict, full_indicies=any_decoder_neuron_IDs, fill_value=0.0) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "\n",
    "\n",
    "# list(stability_dict.values())\n",
    "\n",
    "stability_dict = {k:list(v.aclu_to_stability_score_dict.values()) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "## all the same size hopefully!\n",
    "# [len(v) for v in list(stability_dict.values())]\n",
    "\n",
    "stability_df: pd.DataFrame = pd.DataFrame({'aclu': any_decoder_neuron_IDs, **stability_dict})\n",
    "# stability_df.rename(dict(zip([], [])))\n",
    "stability_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3308ff",
   "metadata": {},
   "source": [
    "### Ensure that we're only getting the location of the maximum peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d1dcd",
   "metadata": {
    "tags": [
     "df_method_bad",
     "bad"
    ]
   },
   "outputs": [],
   "source": [
    "decoder_aclu_MAX_peak_location_df_merged = decoder_aclu_peak_location_df_merged[decoder_aclu_peak_location_df_merged['subpeak_idx'] == 0]\n",
    "decoder_aclu_MAX_peak_location_df_merged\n",
    "# decoder_aclu_MAX_peak_location_df_merged.columns # ['aclu', 'subpeak_idx', 'long_LR_peak', 'long_RL_peak', 'short_LR_peak', 'short_RL_peak', 'long_LR_peak_height', 'long_RL_peak_height', 'short_LR_peak_height', 'short_RL_peak_height', 'LR_peak_diff', 'RL_peak_diff']\n",
    "\n",
    "common_drop_column_names = ['subpeak_idx', 'LR_peak_diff', 'RL_peak_diff']\n",
    "RL_column_names = [col for col in list(decoder_aclu_MAX_peak_location_df_merged.columns) if (str(col).find('RL_') != -1)] # ['long_RL_peak', 'short_RL_peak', 'long_RL_peak_height', 'short_RL_peak_height', 'RL_peak_diff']\n",
    "LR_column_names = [col for col in list(decoder_aclu_MAX_peak_location_df_merged.columns) if (str(col).find('LR_') != -1)] # ['long_LR_peak', 'short_LR_peak', 'long_LR_peak_height', 'short_LR_peak_height', 'LR_peak_diff']\n",
    "\n",
    "LR_only_decoder_aclu_MAX_peak_location_df_merged = decoder_aclu_MAX_peak_location_df_merged.drop(columns=(RL_column_names+common_drop_column_names), inplace=False).dropna(axis='index')\n",
    "LR_only_decoder_aclu_MAX_peak_location_df_merged\n",
    "\n",
    "\n",
    "## OUTPUTS: decoder_aclu_MAX_peak_location_df_merged, LR_only_decoder_aclu_MAX_peak_location_df_merged, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_aclu_peak_location_df_merged\n",
    "\n",
    "valid_aclus = deepcopy(decoder_aclu_peak_location_df_merged.aclu.unique())\n",
    "peaks_df_subset: pd.DataFrame = decoder_aclu_peak_location_df_merged.copy()\n",
    "\n",
    "# active_IDX = 1\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def integer_slider(update_func):\n",
    "    \"\"\" Captures: valid_aclus\n",
    "    \"\"\"\n",
    "    slider = widgets.IntSlider(description='cell_IDX:', min=0, max=len(valid_aclus)-1, value=0)\n",
    "    def on_slider_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            # Call the user-provided update function with the current slider index\n",
    "            update_func(change['new'])\n",
    "    slider.observe(on_slider_change)\n",
    "    display(slider)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_function(index):\n",
    "    \"\"\" Define an update function that will be called with the current slider index \n",
    "    Captures decoder_aclu_peak_location_df_merged, valid_aclus\n",
    "    \"\"\"\n",
    "    global peaks_df_subset\n",
    "    print(f'Slider index: {index}')\n",
    "    active_aclu = int(valid_aclus[int(index)])\n",
    "    peaks_df_subset = decoder_aclu_peak_location_df_merged[decoder_aclu_peak_location_df_merged.aclu == active_aclu].copy()\n",
    "    display(peaks_df_subset)\n",
    "\n",
    "\n",
    "# timebinned_neuron_info = long_results_obj.timebinned_neuron_info\n",
    "# active_fig_obj, update_function = DiagnosticDistanceMetricFigure.build_interactive_diagnostic_distance_metric_figure(long_results_obj, timebinned_neuron_info, result)\n",
    "\n",
    "\n",
    "# Call the integer_slider function with the update function\n",
    "integer_slider(update_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539eb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_df_subset[['long_LR_peak', 'short_LR_peak']]\n",
    "peaks_df_subset[['long_LR_peak_height', 'short_LR_peak_height']]\n",
    "peaks_df_subset['LR_peak_diff']\n",
    "\n",
    "\n",
    "## #TODO 2024-02-16 06:50: - [ ] ERROR discovered in `decoder_aclu_peak_location_df_merged` - the columns 'LR_peak_diff', 'RL_peak_diff' are incorrect as they aren't comparing the maximum peak (supposed to be at `subpeak_idx == 0`, but better given by `height == 1.0`) of long decoder to maximum peak of short. The comparison logic is wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8a0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f287c",
   "metadata": {
    "tags": [
     "good",
     "active",
     "proper"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import NumpyHelpers\n",
    "from neuropy.utils.indexing_helpers import intersection_of_arrays, union_of_arrays\n",
    "\n",
    "def unwrap_single_item(lst):\n",
    "    return lst[0] if len(lst) == 1 else None\n",
    "\n",
    "# decoder_aclu_peak_maps_dict = {a_name:deepcopy(dict(zip(a_decoder.neuron_IDs, a_decoder.peak_locations))) for a_name, a_decoder in track_templates.get_decoders_dict().items()}\n",
    "decoder_aclu_peak_maps_dict = {a_name:deepcopy(dict(zip(a_decoder.neuron_IDs, a_decoder.get_tuning_curve_peak_positions(peak_mode='peaks')))) for a_name, a_decoder in track_templates.get_decoders_dict().items()}\n",
    "\n",
    "# ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "LR_decoder_names = ['long_LR', 'short_LR']\n",
    "RL_decoder_names = ['long_RL', 'short_RL']\n",
    "\n",
    "## Only the maximums:\n",
    "decoder_aclu_MAX_peak_maps_dict = {a_name:{k:unwrap_single_item(v) for k, v in deepcopy(dict(zip(a_decoder.neuron_IDs, a_decoder.get_tuning_curve_peak_positions(peak_mode='peaks', height=1)))).items()} for a_name, a_decoder in track_templates.get_decoders_dict().items()}\n",
    "# decoder_aclu_MAX_peak_maps_dict = {a_name:deepcopy(dict(zip(a_decoder.neuron_IDs, unwrap_single_item(a_decoder.get_tuning_curve_peak_positions(peak_mode='peaks', height=1))))) for a_name, a_decoder in track_templates.get_decoders_dict().items()}\n",
    "# decoder_aclu_MAX_peak_maps_dict\n",
    "\n",
    "# decoder_aclu_MAX_peak_maps_df = pd.DataFrame(decoder_aclu_MAX_peak_maps_dict)\n",
    "LR_only_decoder_aclu_MAX_peak_maps_df = pd.DataFrame({k:v for k,v in decoder_aclu_MAX_peak_maps_dict.items() if k in LR_decoder_names})\n",
    "RL_only_decoder_aclu_MAX_peak_maps_df = pd.DataFrame({k:v for k,v in decoder_aclu_MAX_peak_maps_dict.items() if k in RL_decoder_names})\n",
    "\n",
    "## Drop row if either long/short is missing a value:\n",
    "LR_only_decoder_aclu_MAX_peak_maps_df = LR_only_decoder_aclu_MAX_peak_maps_df.dropna(axis=0, how='any')\n",
    "RL_only_decoder_aclu_MAX_peak_maps_df = RL_only_decoder_aclu_MAX_peak_maps_df.dropna(axis=0, how='any')\n",
    "\n",
    "## Compute the difference between the Long/Short peaks:\n",
    "LR_only_decoder_aclu_MAX_peak_maps_df['peak_diff'] = LR_only_decoder_aclu_MAX_peak_maps_df.diff(axis='columns').to_numpy()[:, -1]\n",
    "RL_only_decoder_aclu_MAX_peak_maps_df['peak_diff'] = RL_only_decoder_aclu_MAX_peak_maps_df.diff(axis='columns').to_numpy()[:, -1]\n",
    "\n",
    "LR_only_decoder_aclu_MAX_peak_maps_df\n",
    "RL_only_decoder_aclu_MAX_peak_maps_df\n",
    "\n",
    "\n",
    "long_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['long_LR'].to_numpy()\n",
    "short_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['short_LR'].to_numpy()\n",
    "peak_x_diff = LR_only_decoder_aclu_MAX_peak_maps_df['peak_diff'].to_numpy()\n",
    "# decoder_aclu_peak_maps_dict\n",
    "\n",
    "\n",
    "## OUTPUTS: LR_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n",
    "## OUTPUTS: RL_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# maximal_peak_only_decoder_aclu_peak_location_df_merged = deepcopy(decoder_aclu_peak_location_df_merged)[decoder_aclu_peak_location_df_merged['long_LR_peak_height'] == 1.0]\n",
    "\n",
    "LR_height_column_names = ['long_LR_peak_height', 'short_LR_peak_height']\n",
    "\n",
    "\n",
    "\n",
    "# [decoder_aclu_peak_location_df_merged[a_name] == 1.0 for a_name in LR_height_column_names]\n",
    "\n",
    "LR_max_peak_dfs = [deepcopy(decoder_aclu_peak_location_df_merged)[decoder_aclu_peak_location_df_merged[a_name] == 1.0].drop(columns=['subpeak_idx', 'series_idx', 'LR_peak_diff', 'RL_peak_diff', a_name]) for a_name in LR_height_column_names]\n",
    "\n",
    "aclus_with_LR_peaks = intersection_of_arrays(*[a_df.aclu.unique() for a_df in LR_max_peak_dfs])\n",
    "aclus_with_LR_peaks\n",
    "\n",
    "\n",
    "## Align them now:\n",
    "LR_max_peak_dfs = [a_df[a_df.aclu.isin(aclus_with_LR_peaks)] for a_df in LR_max_peak_dfs]\n",
    "LR_max_peak_dfs\n",
    "\n",
    "# aclus_with_LR_peaks = aclu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096dda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_peak_only_decoder_aclu_peak_location_df_merged = deepcopy(decoder_aclu_peak_location_df_merged)[decoder_aclu_peak_location_df_merged[LR_height_column_names] == 1.0]\n",
    "maximal_peak_only_decoder_aclu_peak_location_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bbdbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_location_df_merged[['LR_peak_diff', 'RL_peak_diff']].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024-02-08 - Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5de5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_peak_heatmap_test\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent\n",
    "\n",
    "# INPUTS: directional_active_lap_pf_results_dicts, test_aclu: int = 26, xbin_centers, decoder_aclu_peak_location_df_merged\n",
    "\n",
    "def plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged: pd.DataFrame, aclu: int = 26, **kwargs):\n",
    "    \"\"\" 2024-02-06 - Plot all four decoders for a single aclu, with overlayed red lines for the detected peaks. \n",
    "    \n",
    "    plot_single_heatmap_set_with_points\n",
    "\n",
    "    plot_cell_position_binned_activity_over_time\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## TEst: Look at a single aclu value\n",
    "    # test_aclu: int = 26\n",
    "    # test_aclu: int = 28\n",
    "    \n",
    "    active_context: IdentifyingContext = kwargs.get('active_context', IdentifyingContext())\n",
    "    active_context = active_context.overwriting_context(aclu=aclu)\n",
    "\n",
    "    decoders_tuning_curves_dict = kwargs.get('decoders_tuning_curves_dict', None)\n",
    "    \n",
    "    matching_aclu_df = decoder_aclu_peak_location_df_merged[decoder_aclu_peak_location_df_merged.aclu == aclu].copy()\n",
    "    assert len(matching_aclu_df) > 0, f\"matching_aclu_df: {matching_aclu_df} for aclu == {aclu}\"\n",
    "    new_peaks_dict: Dict = list(matching_aclu_df.itertuples(index=False))[0]._asdict() # {'aclu': 28, 'long_LR_peak': 185.29063638457257, 'long_RL_peak': nan, 'short_LR_peak': 176.75276643746625, 'short_RL_peak': nan, 'LR_peak_diff': 8.537869947106316, 'RL_peak_diff': nan}\n",
    "        \n",
    "    # long_LR_name, long_RL_name, short_LR_name, short_RL_name\n",
    "    curr_aclu_z_scored_tuning_map_matrix_dict = {}\n",
    "    curr_aclu_mean_epoch_peak_location_dict = {}\n",
    "    curr_aclu_median_peak_location_dict = {}\n",
    "    curr_aclu_extracted_decoder_peak_locations_dict = {}\n",
    "\n",
    "    ## Find the peak location for each epoch:\n",
    "    for a_name, a_decoder_directional_active_lap_pf_result in directional_active_lap_pf_results_dicts.items():\n",
    "        # print(f'a_name: {a_name}')\n",
    "        matrix_idx = a_decoder_directional_active_lap_pf_result.aclu_to_matrix_IDX_map[aclu]\n",
    "        curr_aclu_z_scored_tuning_map_matrix = a_decoder_directional_active_lap_pf_result.z_scored_tuning_map_matrix[:,matrix_idx,:] # .shape (22, 80, 56)\n",
    "        curr_aclu_z_scored_tuning_map_matrix_dict[a_name] = curr_aclu_z_scored_tuning_map_matrix\n",
    "\n",
    "        # curr_aclu_mean_epoch_peak_location_dict[a_name] = np.nanmax(curr_aclu_z_scored_tuning_map_matrix, axis=-1)\n",
    "        assert np.shape(curr_aclu_z_scored_tuning_map_matrix)[-1] == len(xbin_centers), f\"np.shape(curr_aclu_z_scored_tuning_map_matrix)[-1]: {np.shape(curr_aclu_z_scored_tuning_map_matrix)} != len(xbin_centers): {len(xbin_centers)}\"\n",
    "        curr_peak_value = new_peaks_dict[f'{a_name}_peak']\n",
    "        # print(f'curr_peak_value: {curr_peak_value}')\n",
    "        curr_aclu_extracted_decoder_peak_locations_dict[a_name] = curr_peak_value\n",
    "\n",
    "        curr_aclu_mean_epoch_peak_location_dict[a_name] = np.nanargmax(curr_aclu_z_scored_tuning_map_matrix, axis=-1)\n",
    "        curr_aclu_mean_epoch_peak_location_dict[a_name] = xbin_centers[curr_aclu_mean_epoch_peak_location_dict[a_name]] # convert to actual positions instead of indicies\n",
    "        curr_aclu_median_peak_location_dict[a_name] = np.nanmedian(curr_aclu_mean_epoch_peak_location_dict[a_name])\n",
    "\n",
    "    # curr_aclu_mean_epoch_peak_location_dict # {'maze1_odd': array([ 0, 55, 54, 55, 55, 53, 50, 55, 52, 52, 55, 53, 53, 52, 51, 52, 55, 55, 53, 55, 55, 54], dtype=int64), 'maze2_odd': array([46, 45, 43, 46, 45, 46, 46, 46, 45, 45, 44, 46, 44, 45, 46, 45, 44, 44, 45, 45], dtype=int64)}\n",
    "\n",
    "\n",
    "    if decoders_tuning_curves_dict is not None:\n",
    "        curr_aclu_tuning_curves_dict = {name:v.get(aclu, None) for name, v in decoders_tuning_curves_dict.items()}\n",
    "    else:\n",
    "        curr_aclu_tuning_curves_dict = None\n",
    "                \n",
    "    # point_value = curr_aclu_median_peak_location_dict\n",
    "    point_value = curr_aclu_extracted_decoder_peak_locations_dict\n",
    "    fig, ax_dict = plot_peak_heatmap_test(curr_aclu_z_scored_tuning_map_matrix_dict, xbin=xbin, point_dict=point_value, tuning_curves_dict=curr_aclu_tuning_curves_dict, include_tuning_curves=True)\n",
    "    # Set window title and plot title\n",
    "    perform_update_title_subtitle(fig=fig, ax=None, title_string=f\"Position-Binned Activity per Lap - aclu {aclu}\", subtitle_string=None, active_context=active_context, use_flexitext_titles=True)\n",
    "\n",
    "    # fig, ax_dict = plot_peak_heatmap_test(curr_aclu_z_scored_tuning_map_matrix_dict, xbin=xbin, point_dict=curr_aclu_extracted_decoder_peak_locations_dict) # , defer_show=True\n",
    "    \n",
    "    # fig.show()\n",
    "    return fig, ax_dict\n",
    "\n",
    "\n",
    "\n",
    "decoders_tuning_curves_dict = track_templates.decoder_normalized_tuning_curves_dict_dict.copy()\n",
    "\n",
    "extra_decoder_values_dict = {'tuning_curves': decoders_tuning_curves_dict, 'points': decoder_aclu_peak_location_df_merged}\n",
    "\n",
    "# decoders_tuning_curves_dict\n",
    "xbin_centers = deepcopy(active_pf_dt.xbin_centers)\n",
    "xbin = deepcopy(active_pf_dt.xbin)\n",
    "fig, ax_dict = plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, extra_decoder_values_dict=extra_decoder_values_dict, aclu=4, \n",
    "                                                   decoders_tuning_curves_dict=decoders_tuning_curves_dict, decoder_aclu_peak_location_df_merged=decoder_aclu_peak_location_df_merged,\n",
    "                                                    active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a399108",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot a couple\n",
    "# long_pf_aclus, short_pf_aclus\n",
    "\n",
    "# plot_aclus = [4,  5,  8,  9]\n",
    "plot_aclus = [11, 18, 68]\n",
    "# plot_aclus = [68]\n",
    "\n",
    "for test_aclu in plot_aclus:\n",
    "\tfig, ax_dict = plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, extra_decoder_values_dict=extra_decoder_values_dict, aclu=test_aclu, \n",
    "                                                   decoders_tuning_curves_dict=decoders_tuning_curves_dict, decoder_aclu_peak_location_df_merged=decoder_aclu_peak_location_df_merged,\n",
    "                                                    active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "\t\n",
    "\tfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_location_df_merged.aclu.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78872f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# def plot_heatmap_grid(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged: pd.DataFrame, aclu_list, **kwargs):\n",
    "#     \"\"\" 2024-02-06 - Plot all four decoders for a set of aclus, with overlayed red lines for the detected peaks. \"\"\"\n",
    "    \n",
    "#     ## calculate square root and round up to get number of rows and cols for subplots\n",
    "#     grid_size = math.ceil(math.sqrt(len(aclu_list)))\n",
    "\n",
    "#     ## Create a subplot grid\n",
    "#     fig, axs = plt.subplots(grid_size, grid_size)\n",
    "\n",
    "#     for i, aclu in enumerate(aclu_list):\n",
    "#         curr_ax = axs[i//grid_size, i%grid_size]\n",
    "\n",
    "#         ## Call your original function and pass in curr_ax as the 'ax' parameter\n",
    "#         plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged, aclu=aclu, ax=curr_ax, **kwargs)\n",
    "    \n",
    "#     return fig, axs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import TimedAnimation\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import ipywidgets as widgets  \n",
    "\n",
    "def plot_heatmap_tabs(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged: pd.DataFrame, aclu_list, **kwargs):\n",
    "    # Create widgets\n",
    "    out = widgets.Output()\n",
    "    tab = widgets.Tab(children = [out for aclu in aclu_list])\n",
    "   \n",
    "    for index, aclu in enumerate(aclu_list):\n",
    "        tab.set_title(index, f'ACLU {aclu}')\n",
    "        with tab.children[index]:\n",
    "            plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, \n",
    "                                                 decoder_aclu_peak_location_df_merged, aclu=aclu, **kwargs)\n",
    "            plt.show()\n",
    "   \n",
    "    display(tab)\n",
    "\n",
    "# Provide list of aclu's:\n",
    "# aclu_list = [11, 18, 68]\n",
    "aclu_list = track_templates.any_decoder_neuron_IDs[:5]\n",
    "# fig, axs = plot_heatmap_grid(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged, aclu_list=aclu_list, decoders_tuning_curves_dict=decoders_tuning_curves_dict, active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "plot_heatmap_tabs(directional_active_lap_pf_results_dicts, xbin_centers, xbin,\n",
    "                  decoder_aclu_peak_location_df_merged, aclu_list=aclu_list, \n",
    "                  decoders_tuning_curves_dict=decoders_tuning_curves_dict, \n",
    "                  active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f217aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = fig.get_layout_engine()\n",
    "layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f23ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these layout changes don't seem to take effect until the window containing the figure is resized.\n",
    "# fig.set_layout_engine('compressed') # TAKEWAY: Use 'compressed' instead of 'constrained'\n",
    "fig.set_layout_engine('none') # disabling layout engine. Strangely still allows window to resize and the plots scale, so I'm not sure what the layout engine is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(directional_active_lap_pf_results_dicts.keys()) # ['maze1_odd', 'maze1_even', 'maze2_odd', 'maze2_even']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef13541",
   "metadata": {},
   "source": [
    "# 2024-02-02 - napari_plot_directional_trial_by_trial_activity_viz Trial-by-trial Correlation Matrix C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd735e",
   "metadata": {},
   "source": [
    "### 🎨 Show Trial-by-trial Correlation Matrix C in `napari`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72df59",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import napari\n",
    "# import afinder\n",
    "from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_from_layers_dict\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_trial_by_trial_activity_viz\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_plot_directional_trial_by_trial_activity_viz\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_export_image_sequence\n",
    "\n",
    "## Directional\n",
    "directional_viewer, directional_image_layer_dict, custom_direction_split_layers_dict = napari_plot_directional_trial_by_trial_activity_viz(directional_active_lap_pf_results_dicts, include_trial_by_trial_correlation_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9934d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Global:\n",
    "viewer, image_layer_dict = napari_trial_by_trial_activity_viz(z_scored_tuning_map_matrix, C_trial_by_trial_correlation_matrix, title='Trial-by-trial Correlation Matrix C', axis_labels=('aclu', 'lap', 'xbin')) # GLOBAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f61fa",
   "metadata": {},
   "source": [
    "# Napari Plotting Long/Short Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80146cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import test_LinearTrackDimensions_2D_pyqtgraph, LinearTrackDimensions, LinearTrackInstance\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackDimensions, LinearTrackDimensions3D\n",
    "\n",
    "long_track_dims = LinearTrackDimensions(track_length=170.0)\n",
    "short_track_dims = LinearTrackDimensions(track_length=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_LR_pf1D.config.grid_bin_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf2D.config.grid_bin_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_LR_pf1D.position.has_linear_pos\n",
    "# long_LR_pf1D.position.compute_linearized_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get grid_bin_bounds:\n",
    "long_grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "short_grid_bin_bounds = deepcopy(short_pf2D.config.grid_bin_bounds)\n",
    "\n",
    "long_grid_bin_bounds\n",
    "short_grid_bin_bounds\n",
    "linear_track_instance = LinearTrackInstance.init_from_grid_bin_bounds(grid_bin_bounds=long_grid_bin_bounds, debug_print=True)\n",
    "linear_track_instance\n",
    "\n",
    "\n",
    "# a_track_dims, ideal_maze_pdata = LinearTrackDimensions.init_from_grid_bin_bounds(grid_bin_bounds, return_geoemtry=True)\n",
    "linear_track_instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4ec86",
   "metadata": {},
   "source": [
    "# 2023-09-07 - Track Graphics Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07993325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.analyses.placefields import compute_grid_bin_bounds\n",
    "from pyphocorehelpers.geometry_helpers import map_value\n",
    "\n",
    "pos_df = deepcopy(global_session.position).to_dataframe()\n",
    "# xlinear = deepcopy(global_session.position.linear_pos_obj.x)\n",
    "xlinear = deepcopy(global_session.position.to_dataframe()['lin_pos'].to_numpy())\n",
    "# xlinear = -1.0 * xlinear # flip over the y-axis first\n",
    "lin_pos_bounds = compute_grid_bin_bounds(xlinear)[0]\n",
    "x_bounds = compute_grid_bin_bounds(pos_df['x'].to_numpy())[0]\n",
    "print(f'lin_pos_bounds: {lin_pos_bounds}, x_bounds: {x_bounds}')\n",
    "xlinear = map_value(xlinear, lin_pos_bounds, x_bounds) # map xlinear from its current bounds range to the xbounds range\n",
    "\n",
    "## Confirmed they match: lin_pos_bounds: (20.53900014070859, 260.280278480539), x_bounds: (20.53900014070859, 260.280278480539)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app, w, cw, (ax0, ax1), (long_track_dims, long_rect_items, long_rects), (short_track_dims, short_rect_items, short_rects) = test_LinearTrackDimensions_2D_pyqtgraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b53254",
   "metadata": {},
   "source": [
    "## 🔝🖼️🎨 2024-02-16 - NOW - Working Track Remapping Diagram Figure!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb92e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import _build_track_1D_verticies\n",
    "\n",
    "def _plot_track_remapping_diagram(LR_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, short_peak_x, peak_x_diff, grid_bin_bounds):\n",
    "    \"\"\" \n",
    "    🔝🖼️🎨\n",
    "    \"\"\"\n",
    "    # BUILDS TRACK PROPERTIES ____________________________________________________________________________________________ #\n",
    "    from pyphocorehelpers.geometry_helpers import BoundsRect, point_tuple_mid_point, map_value\n",
    "    from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackDimensions\n",
    "    from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_vertical_track_bounds_lines, add_track_shapes, test_LinearTrackDimensions_2D_pyqtgraph\n",
    "    from neuropy.utils.mathutil import contiguous_regions, threshPeriods, compute_grid_bin_bounds, map_value\n",
    "    from pyphoplacecellanalysis.Pho2D.track_shape_drawing import test_LinearTrackDimensions_2D_Matplotlib\n",
    "    from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackInstance\n",
    "\n",
    "    grid_bin_bounds = BoundsRect.init_from_grid_bin_bounds(grid_bin_bounds)\n",
    "    display(grid_bin_bounds)\n",
    "\n",
    "    # long_track_dims = LinearTrackDimensions.init_from_grid_bin_bounds(grid_bin_bounds)\n",
    "    # short_track_dims = LinearTrackDimensions.init_from_grid_bin_bounds(grid_bin_bounds)\n",
    "\n",
    "    long_track_dims = LinearTrackDimensions(track_length=170.0)\n",
    "    short_track_dims = LinearTrackDimensions(track_length=100.0)\n",
    "\n",
    "    common_1D_platform_height = 0.25\n",
    "    common_1D_track_height = 0.1\n",
    "    long_track_dims.minor_axis_platform_side_width = common_1D_platform_height\n",
    "    long_track_dims.track_width = common_1D_track_height # (short_track_dims.minor_axis_platform_side_width\n",
    "\n",
    "    short_track_dims.minor_axis_platform_side_width = common_1D_platform_height\n",
    "    short_track_dims.track_width = common_1D_track_height # (short_track_dims.minor_axis_platform_side_width\n",
    "\n",
    "    # instances:\n",
    "    long_track = LinearTrackInstance(long_track_dims, grid_bin_bounds=grid_bin_bounds)\n",
    "    short_track = LinearTrackInstance(short_track_dims, grid_bin_bounds=grid_bin_bounds)\n",
    "\n",
    "    print(long_track_dims)\n",
    "    print(short_track_dims)\n",
    "\n",
    "    # BEGIN PLOTTING _____________________________________________________________________________________________________ #\n",
    "    long_path = _build_track_1D_verticies(platform_length=22.0, track_length=170.0, track_1D_height=1.0, platform_1D_height=1.1, track_center_midpoint_x=long_track.grid_bin_bounds.center_point[0], track_center_midpoint_y=-1.0, debug_print=True)\n",
    "    short_path = _build_track_1D_verticies(platform_length=22.0, track_length=100.0, track_1D_height=1.0, platform_1D_height=1.1, track_center_midpoint_x=short_track.grid_bin_bounds.center_point[0], track_center_midpoint_y=1.0, debug_print=True)\n",
    "\n",
    "    ## Create the remapping figure:\n",
    "    fig, ax = plt.subplots()\n",
    "    long_patch = patches.PathPatch(long_path, facecolor='orange', alpha=0.5, lw=2)\n",
    "    ax.add_patch(long_patch)\n",
    "\n",
    "    short_patch = patches.PathPatch(short_path, facecolor='green', alpha=0.5, lw=2)\n",
    "    ax.add_patch(short_patch)\n",
    "    ax.autoscale()\n",
    "\n",
    "    ## INPUTS: LR_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, short_peak_x, peak_x_diff\n",
    "\n",
    "    active_aclus = LR_only_decoder_aclu_MAX_peak_maps_df.index.to_numpy()\n",
    "\n",
    "    # Define a colormap to map your unique integer indices to colors\n",
    "    colormap = plt.cm.viridis  # or any other colormap\n",
    "    normalize = mcolors.Normalize(vmin=active_aclus.min(), vmax=active_aclus.max())\n",
    "    scalar_map = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "\n",
    "    random_y_jitter = np.random.ranf((np.shape(active_aclus)[0], )) * 0.05\n",
    "    # random_y_jitter = np.zeros((np.shape(active_aclus)[0], )) # no jitter\n",
    "\n",
    "    long_y = (np.full_like(long_peak_x, 0.1)+random_y_jitter)\n",
    "    short_y = (np.full_like(short_peak_x, 0.75)+random_y_jitter)\n",
    "\n",
    "    _out = ax.scatter(long_peak_x, y=long_y, c=scalar_map.to_rgba(active_aclus), alpha=0.9, label='LR long_peak_x')\n",
    "    _out2 = ax.scatter(short_peak_x, y=short_y, c=scalar_map.to_rgba(active_aclus), alpha=0.9, label='LR short_peak_x')\n",
    "\n",
    "    # Add text labels to scatter points\n",
    "    for i, aclu_val in enumerate(active_aclus):\n",
    "        ax.text(long_peak_x[i], long_y[i], str(aclu_val), color='black', fontsize=8)\n",
    "        ax.text(short_peak_x[i], short_y[i], str(aclu_val), color='black', fontsize=8)\n",
    "\n",
    "    # Draw arrows from the first set of points to the second set\n",
    "    arrows_output = {}\n",
    "    for idx in range(len(long_peak_x)):\n",
    "        # Starting point coordinates\n",
    "        start_x = long_peak_x[idx]\n",
    "        # start_y = 0.1 + random_y_jitter[idx]\n",
    "        start_y = long_y[idx]\n",
    "        # End point coordinates\n",
    "        end_x = short_peak_x[idx]\n",
    "        # end_y = 0.75 + random_y_jitter[idx]\n",
    "        end_y = short_y[idx]\n",
    "        # Calculate the change in x and y for the arrow\n",
    "        # dx = end_x - start_x\n",
    "        # dy = end_y - start_y\n",
    "\n",
    "        # Get the corresponding color for the current index using the colormap\n",
    "        arrow_color = scalar_map.to_rgba(active_aclus[idx])\n",
    "        \n",
    "        # Annotate the plot with arrows; adjust the properties according to your needs\n",
    "        arrows_output[idx] = ax.annotate('',\n",
    "                    xy=(end_x, end_y),\n",
    "                    xytext=(start_x, start_y),\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color=arrow_color, alpha=0.6))\n",
    "\n",
    "    # Show the plot\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "    return fig, ax, (arrows_output)\n",
    "\n",
    "\n",
    "# grid_bin_bounds = BoundsRect.init_from_grid_bin_bounds(global_pf2D.config.grid_bin_bounds)\n",
    "fix, ax, _outputs_tuple = _plot_track_remapping_diagram(LR_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, short_peak_x, peak_x_diff, grid_bin_bounds=long_pf2D.config.grid_bin_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out.remove()\n",
    "_out2.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef4700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax1, ax2 = test_LinearTrackDimensions_2D_Matplotlib()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f981611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_vertical_track_bounds_lines\n",
    "grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "long_track_line_collection, short_track_line_collection = add_vertical_track_bounds_lines(grid_bin_bounds=grid_bin_bounds, ax=ax1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aeb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Napari Shapes Layer Test:\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_napari_track_shapes_layer\n",
    "\n",
    "# add the image\n",
    "# viewer = napari.view_image(data.camera(), name='photographer')\n",
    "\n",
    "test_shapes_viewer = napari.Viewer() # name='Test Shapes Viewer'\n",
    "# add the tracks\n",
    "long_rectangles_poly_shapes_layer, short_rectangles_poly_shapes_layer = add_napari_track_shapes_layer(test_shapes_viewer, long_rect_items, short_rect_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_layer_info(long_rectangles_poly_shapes_layer)\n",
    "extract_layer_info(short_rectangles_poly_shapes_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_rectangles_poly_shapes_layer.bounding_box\n",
    "# long_rectangles_poly_shapes_layer.interaction_box\n",
    "long_rectangles_poly_shapes_layer.corner_pixels # np.array([[ 44, 124], [376, 161]])\n",
    "# long_rectangles_poly_shapes_layer.frame\n",
    "\n",
    "\n",
    "# long_rectangles_poly_shapes_layer.rotate = 90\n",
    "# data_to_world, world_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_rectangles_poly_shapes_layer.rotate = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af347488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #TODO 2024-02-02 22:31: - [ ] These need to be update for global support\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_add_aclu_slider\n",
    "\n",
    "def build_filename_from_viewer(viewer, desired_save_parent_path: Path, slider_axis_IDX: int = 0) -> Path:\n",
    "    \"\"\"\n",
    "    Captures: curr_active_pipeline, neuron_ids, global_any_name\n",
    "    \n",
    "     Usage:\n",
    "        file_out_path = build_filename_from_viewer(viewer)\n",
    "        viewer.screenshot(path=file_out_path, canvas_only=True, flash=False)\n",
    "\n",
    "    \"\"\"\n",
    "    # desired_save_parent_path = Path('/home/halechr/Desktop/test_napari_out').resolve()\n",
    "\n",
    "    matrix_aclu_IDX: int = int(viewer.dims.current_step[slider_axis_IDX])\n",
    "    # find the aclu value for this index:\n",
    "    aclu: int = int(neuron_ids[matrix_aclu_IDX])\n",
    "    curr_context = curr_active_pipeline.build_display_context_for_filtered_session(global_any_name, 'napari_trial_by_trial_activity_viz', aclu=str(aclu))\n",
    "    curr_context_string: str = curr_context.get_description() #.get_description(suffix_items=[f'aclu-{aclu}'])\n",
    "    filename_string: str = f\"{curr_context_string}.png\"\n",
    "\n",
    "    file_out_path = desired_save_parent_path.joinpath(filename_string).resolve()\n",
    "    return file_out_path\n",
    "\n",
    "\n",
    "# desired_save_parent_path = Path('/home/halechr/Desktop/test_napari_out').resolve()\n",
    "desired_save_parent_path = Path(r'C:\\Users\\pho\\Desktop\\test_napari_out').resolve()\n",
    "_connected_on_update_slider_event = napari_add_aclu_slider(viewer=directional_viewer, neuron_ids=neuron_ids)\n",
    "imageseries_output_directory = napari_export_image_sequence(viewer=viewer, imageseries_output_directory=desired_save_parent_path, slider_axis_IDX=0, build_filename_from_viewer_callback_fn=build_filename_from_viewer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional_viewer.Config\n",
    "directional_viewer.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {},
   "source": [
    "# 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "# active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_spike_rasters_pyqtplot_2D', 'maze_any') # 'maze_any'\n",
    "assert isinstance(_out_graphics_dict, dict)\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = _out_graphics_dict['spike_raster_plt_2d'], _out_graphics_dict['spike_raster_plt_3d'], _out_graphics_dict['spike_raster_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8eceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_neuron_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "# graphics_output_dict = curr_active_pipeline.display('_display_long_short_pf1D_comparison', active_identifying_session_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "\n",
    "graphics_output_dict = curr_active_pipeline.display('_display_long_short_pf1D_comparison', active_identifying_session_ctx,\n",
    "                                                     include_includelist=[long_LR_name, short_LR_name, global_LR_name], active_context=active_identifying_session_ctx.overwriting_context(dir='LR'), included_any_context_neuron_ids=LR_shift_df.aclu.unique())\n",
    "\n",
    "\n",
    "# fig, axs, plot_data = graphics_output_dict['fig'], graphics_output_dict['axs'], graphics_output_dict['plot_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51201e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "\n",
    "_out = TemplateDebugger.init_templates_debugger(track_templates) # , included_any_context_neuron_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feab852",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot.display_function_items\n",
    "\n",
    "# '_display_directional_template_debugger'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd064d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_directional_template_debugger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba36573",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_template_debugger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_track_template_pf1Ds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_display_directional_merged_pfs'\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pfs', plot_all_directions=False, plot_long_directional=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting on 2024-02-06 to display the LR/RL directions instead of the All/Long/Short pfs:\n",
    "def _display_directional_merged_pfs(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist=None, save_figure=True, included_any_context_neuron_ids=None,\n",
    "\t\t\t\t\t\t\t\t\tplot_all_directions=True, plot_long_directional=False, plot_short_directional=False, **kwargs):\n",
    "\t\"\"\" Plots the merged pseduo-2D pfs/ratemaps. Plots: All-Directions, Long-Directional, Short-Directional in seperate windows. \n",
    "\t\n",
    "\tHistory: this is the Post 2022-10-22 display_all_pf_2D_pyqtgraph_binned_image_rendering-based method:\n",
    "\t\"\"\"\n",
    "\tfrom pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "\tfrom pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow \n",
    "\t\n",
    "\n",
    "\tdefer_render = kwargs.pop('defer_render', False)\n",
    "\tdirectional_merged_decoders_result: DirectionalMergedDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\tactive_merged_pf_plots_data_dict = {} #empty dict\n",
    "\t\n",
    "\tif plot_all_directions:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='All-Directions', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.all_directional_pf1D_Decoder.pf # all-directions\n",
    "\tif plot_long_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Long-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.long_directional_pf1D_Decoder.pf # Long-only\n",
    "\tif plot_short_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Short-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.short_directional_pf1D_Decoder.pf # Short-only\n",
    "\n",
    "\tout_plots_dict = {}\n",
    "\t\n",
    "\tfor active_context, active_pf_2D in active_merged_pf_plots_data_dict.items():\n",
    "\t\t# figure_format_config = {} # empty dict for config\n",
    "\t\tfigure_format_config = {'scrollability_mode': LayoutScrollability.NON_SCROLLABLE} # kwargs # kwargs as default figure_format_config\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig: BasicBinnedImageRenderingWindow  = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow\n",
    "\t\n",
    "\t\t# Set the window title from the context\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.setWindowTitle(f'{active_context.get_description()}')\n",
    "\t\tout_plots_dict[active_context] = out_all_pf_2D_pyqtgraph_binned_image_fig\n",
    "\n",
    "\t\t# Tries to update the display of the item:\n",
    "\t\tnames_list = [v for v in list(out_all_pf_2D_pyqtgraph_binned_image_fig.plots.keys()) if v not in ('name', 'context')]\n",
    "\t\tfor a_name in names_list:\n",
    "\t\t\t# Adjust the size of the text for the item by passing formatted text\n",
    "\t\t\ta_plot: pg.PlotItem = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[a_name].mainPlotItem # PlotItem \n",
    "\t\t\t# no clue why 2 is a good value for this...\n",
    "\t\t\ta_plot.titleLabel.setMaximumHeight(2)\n",
    "\t\t\ta_plot.layout.setRowFixedHeight(0, 2)\n",
    "\t\t\t\n",
    "\n",
    "\t\tif not defer_render:\n",
    "\t\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.show()\n",
    "\n",
    "\treturn out_plots_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcc19a",
   "metadata": {},
   "source": [
    "# 2023-12-18 - Simpily detect bimodal cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc55f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import ContinuousPeakLocationRepresentingMixin\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from scipy.signal import find_peaks\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', sortby=None)\n",
    "\n",
    "# active_ratemap = deepcopy(long_pf1D.ratemap)\n",
    "active_ratemap: Ratemap = deepcopy(long_LR_pf1D.ratemap)\n",
    "peaks_dict, aclu_n_peaks_dict, peaks_results_df = active_ratemap.compute_tuning_curve_modes(height=0.2, width=None)\n",
    "\n",
    "\n",
    "included_columns = ['pos', 'peak_heights'] # the columns of interest that you want in the final dataframe.\n",
    "included_columns_renamed = dict(zip(included_columns, ['peak', 'peak_height']))\n",
    "decoder_peaks_results_dfs = [a_decoder.pf.ratemap.get_tuning_curve_peak_df(height=0.2, width=None) for a_decoder in (track_templates.long_LR_decoder, track_templates.long_RL_decoder, track_templates.short_LR_decoder, track_templates.short_RL_decoder)]\n",
    "prefix_names = [f'{a_decoder_name}_' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "all_included_columns = ['aclu', 'series_idx', 'subpeak_idx'] + included_columns # Used to filter out the unwanted columns from the output\n",
    "\n",
    "# [['aclu', 'series_idx', 'subpeak_idx', 'pos']]\n",
    "\n",
    "# rename_list_fn = lambda a_prefix: {'pos': f\"{a_prefix}pos\"}\n",
    "rename_list_fn = lambda a_prefix: {a_col_name:f\"{a_prefix}{included_columns_renamed[a_col_name]}\" for a_col_name in included_columns}\n",
    "\n",
    "# column_names = [f'{a_decoder_name}_peak' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "\n",
    "# dataFrames = decoder_peaks_results_dfs\n",
    "# names = self.get_decoder_names()\n",
    "\n",
    "# rename 'pos' column in each dataframe and then reduce to perform cumulative outer merge\n",
    "result_df = decoder_peaks_results_dfs[0][all_included_columns].rename(columns=rename_list_fn(prefix_names[0]))\n",
    "for df, a_prefix in zip(decoder_peaks_results_dfs[1:], prefix_names[1:]):\n",
    "    result_df = pd.merge(result_df, df[all_included_columns].rename(columns=rename_list_fn(a_prefix)), on=['aclu', 'series_idx', 'subpeak_idx'], how='outer')\n",
    "\n",
    "# result = reorder_columns(result, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "# list(filter(lambda column: column.endswith('_peak_heights'), result.columns))\n",
    "# result_df = reorder_columns(result_df, column_name_desired_index_dict=dict(zip(list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), np.arange(len(result_df.columns)-4, len(result_df.columns)))))\n",
    "# result_df\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "result_df = reorder_columns_relative(result_df, column_names=list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), relative_mode='end')\n",
    "result_df\n",
    "# print(list(result.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ccd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\" 2023-06-01 - \n",
    "        \n",
    "        ## TODO 2023-06-02 NOW, NEXT: this might not work in 'AGG' mode because it tries to render it with QT, but we can see.\n",
    "        \n",
    "        Usage:\n",
    "            (pagination_controller_L, pagination_controller_S), (fig_L, fig_S), (ax_L, ax_S), (final_context_L, final_context_S), (active_out_figure_paths_L, active_out_figure_paths_S) = _subfn_prepare_plot_long_and_short_stacked_epoch_slices(curr_active_pipeline, defer_render=False)\n",
    "        \"\"\"\n",
    "        from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices_paginated\n",
    "\n",
    "        ## long_short_decoding_analyses:\n",
    "        curr_long_short_decoding_analyses = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "        ## Extract variables from results object:\n",
    "        long_results_obj, short_results_obj = curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj\n",
    "        long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "        pagination_controller_L, active_out_figure_paths_L, final_context_L = plot_decoded_epoch_slices_paginated(curr_active_pipeline, long_results_obj, curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='replays', decoder='long_results_obj'), included_epoch_indicies=included_epoch_indicies, save_figure=save_figure, **kwargs)\n",
    "        fig_L = pagination_controller_L.plots.fig\n",
    "        ax_L = fig_L.get_axes()\n",
    "        if defer_render:\n",
    "            widget_L = pagination_controller_L.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "            widget_L.close()\n",
    "            pagination_controller_L = None\n",
    "\n",
    "        \n",
    "        pagination_controller_S, active_out_figure_paths_S, final_context_S = plot_decoded_epoch_slices_paginated(curr_active_pipeline, short_results_obj, curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='replays', decoder='short_results_obj'), included_epoch_indicies=included_epoch_indicies, save_figure=save_figure, **kwargs)\n",
    "        fig_S = pagination_controller_S.plots.fig\n",
    "        ax_S = fig_S.get_axes()\n",
    "        if defer_render:\n",
    "            widget_S = pagination_controller_S.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "            widget_S.close()\n",
    "            pagination_controller_S = None\n",
    "\n",
    "        return (pagination_controller_L, pagination_controller_S), (fig_L, fig_S), (ax_L, ax_S), (final_context_L, final_context_S), (active_out_figure_paths_L, active_out_figure_paths_S)\n",
    "\n",
    "peaks_results_df = track_templates.get_decoders_aclu_peak_location_df().sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True)\n",
    "peaks_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d377cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_n_peaks_dict: Dict = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index().set_index('aclu').to_dict()['subpeak_idx_count'] # number of peaks (\"models\" for each aclu)\n",
    "# aclu_n_peaks_dict\n",
    "\n",
    "# peaks_results_df = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index()\n",
    "\n",
    "# peaks_results_df[peaks_results_df.aclu == 5]\n",
    "# peaks_results_df.aclu.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeccfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ratemap.n_neurons\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=active_ratemap.neuron_ids, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aclu_n_peaks_dict\n",
    "unimodal_only_aclus = np.array(list(unimodal_peaks_dict.keys()))\n",
    "unimodal_only_aclus\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=unimodal_only_aclus, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aebbf1",
   "metadata": {},
   "source": [
    "# 🟪 2024-02-08 Directional Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_ripple_filter_epochs_decoder_result_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da36235",
   "metadata": {},
   "source": [
    "### 2024-02-09 - Recover Radon Transform info to confirm the Pseudo2D decoder-based detection of long/short replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2420d6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _recover_position_bin_size\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _compute_all_df_score_metrics\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _compute_matching_best_indicies\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_compute_custom_epoch_decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97b0fb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "source": [
    "#### Main Custom Decoder Computation:\n",
    "2024-02-15 - Appears to be best to refactor to the TrackTemplates object. __________________________________________ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_compute_custom_epoch_decoding\n",
    "\n",
    "decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict = _perform_compute_custom_epoch_decoding(curr_active_pipeline, directional_merged_decoders_result, track_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d06ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recompute the epoch scores/metrics such as radon transform and wcorr:\n",
    "(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict), merged_df_outputs_tuple, raw_dict_outputs_tuple = _compute_all_df_score_metrics(directional_merged_decoders_result, track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict, spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df),\n",
    "                                                                                                                                                                                     should_skip_radon_transform=True)\n",
    "laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df, laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df = merged_df_outputs_tuple\n",
    "decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict, decoder_ripple_radon_transform_extras_dict, decoder_laps_weighted_corr_df_dict, decoder_ripple_weighted_corr_df_dict = raw_dict_outputs_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399efd5",
   "metadata": {},
   "source": [
    "##  2024-02-13 - Saving manual decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc95ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "\n",
    "## Needed for saving either way:\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "pos_bin_size = _recover_position_bin_size(track_templates.get_decoders()[0]) # 3.793023081021702\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "\n",
    "\n",
    "override_output_parent_path = None\n",
    "out_filename_str: str = '-'.join([DAY_DATE_TO_USE]) #SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date=DAY_DATE_TO_USE)\n",
    "output_parent_path: Path = (override_output_parent_path or curr_active_pipeline.get_output_path()).resolve()\n",
    "try:\n",
    "\toutput_path = output_parent_path.joinpath(f'{out_filename_str}_CustomDecodingResults.pkl').resolve()\n",
    "\tprint(f'saving to \"{output_path}\"...')\n",
    "\tsaveData(output_path, ({'pos_bin_size': pos_bin_size, 'ripple_decoding_time_bin_size':ripple_decoding_time_bin_size, 'laps_decoding_time_bin_size':laps_decoding_time_bin_size, 'decoder_laps_filter_epochs_decoder_result_dict':decoder_laps_filter_epochs_decoder_result_dict,\n",
    "\t\t\t\t\t\t  'decoder_ripple_filter_epochs_decoder_result_dict':decoder_ripple_filter_epochs_decoder_result_dict, 'decoder_laps_radon_transform_df_dict':decoder_laps_radon_transform_df_dict, 'decoder_ripple_radon_transform_df_dict':decoder_ripple_radon_transform_df_dict,\n",
    "\t\t\t\t\t\t  'decoder_laps_radon_transform_extras_dict': decoder_laps_radon_transform_extras_dict, 'decoder_ripple_radon_transform_extras_dict': decoder_ripple_radon_transform_extras_dict,\n",
    "\t\t\t\t\t\t  'laps_weighted_corr_merged_df': laps_weighted_corr_merged_df, 'ripple_weighted_corr_merged_df': ripple_weighted_corr_merged_df, 'decoder_laps_weighted_corr_df_dict': decoder_laps_weighted_corr_df_dict, 'decoder_ripple_weighted_corr_df_dict': decoder_ripple_weighted_corr_df_dict,\n",
    "                          'laps_simple_pf_pearson_merged_df': laps_simple_pf_pearson_merged_df, 'ripple_simple_pf_pearson_merged_df': ripple_simple_pf_pearson_merged_df,\n",
    "                          }))\n",
    "except BaseException as e:\n",
    "\tprint(f'issue saving \"{output_path}\": error: {e}')\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadData\n",
    "\n",
    "# load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-02-13_CustomDecodingResults.pkl\").resolve()\n",
    "# load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-02-13_9pm_CustomDecodingResults.pkl\").resolve()\n",
    "# load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-02-14_CustomDecodingResults.pkl\").resolve()\n",
    "# load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\output\\2024-02-15-8pm_CustomDecodingResults.pkl\").resolve()\n",
    "# load_path = Path(r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-02-16_CustomDecodingResults.pkl\").resolve()\n",
    "load_path = Path(\"/media/halechr/MAX/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/2024-02-16_CustomDecodingResults.pkl\").resolve()\n",
    "\n",
    "assert load_path.exists()\n",
    "loaded_dict = loadData(load_path, debug_print=False)\n",
    "## UNPACK HERE:\n",
    "pos_bin_size: float = loaded_dict['pos_bin_size']\n",
    "ripple_decoding_time_bin_size = loaded_dict['ripple_decoding_time_bin_size']\n",
    "laps_decoding_time_bin_size = loaded_dict['laps_decoding_time_bin_size']\n",
    "decoder_laps_filter_epochs_decoder_result_dict = loaded_dict['decoder_laps_filter_epochs_decoder_result_dict']\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = loaded_dict['decoder_ripple_filter_epochs_decoder_result_dict']\n",
    "decoder_laps_radon_transform_df_dict = loaded_dict['decoder_laps_radon_transform_df_dict']\n",
    "decoder_ripple_radon_transform_df_dict = loaded_dict['decoder_ripple_radon_transform_df_dict']\n",
    "## New 2024-02-14 - Noon:\n",
    "decoder_laps_radon_transform_extras_dict = loaded_dict['decoder_laps_radon_transform_extras_dict']\n",
    "decoder_ripple_radon_transform_extras_dict = loaded_dict['decoder_ripple_radon_transform_extras_dict']\n",
    "## New 2024-02-16 _ Weighted Corr\n",
    "laps_weighted_corr_merged_df = loaded_dict['laps_weighted_corr_merged_df']\n",
    "ripple_weighted_corr_merged_df = loaded_dict['ripple_weighted_corr_merged_df']\n",
    "decoder_laps_weighted_corr_df_dict = loaded_dict['decoder_laps_weighted_corr_df_dict']\n",
    "decoder_ripple_weighted_corr_df_dict = loaded_dict['decoder_ripple_weighted_corr_df_dict']\n",
    "\n",
    "laps_simple_pf_pearson_merged_df = loaded_dict['laps_simple_pf_pearson_merged_df']\n",
    "ripple_simple_pf_pearson_merged_df = loaded_dict['ripple_simple_pf_pearson_merged_df']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_epochs_decode_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8161a",
   "metadata": {},
   "source": [
    "## 🖼️🎨 2024-02-08 - `PhoPaginatedMultiDecoderDecodedEpochsWindow` - Plot Radon Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d9fda21",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoDockAreaContainingWindow.GlobalConnectionManagerAccessingMixin_on_setup()\n",
      "PhoDockAreaContainingWindow.try_register_any_control_widgets()\n",
      "\tflat_widgets_list contains 0 items\n",
      "no radon transform columns present in the the active_filter_epochs_df. Skipping.\n",
      "no radon transform columns present in the the active_filter_epochs_df. Skipping.\n",
      "no radon transform columns present in the the active_filter_epochs_df. Skipping.\n",
      "no radon transform columns present in the the active_filter_epochs_df. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\DisplayFunctions\\DecoderPredictionError.py:306: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  ax.set_xlim((xmin, xmax))\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\DisplayFunctions\\DecoderPredictionError.py:306: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  ax.set_xlim((xmin, xmax))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecodedEpochSlicesPaginatedFigureController.on_selected_epochs_changed(...)\n",
      "current_page_idx = 0, found_data_index =1\n",
      "\tselection_indicies: [1]\n",
      "DecodedEpochSlicesPaginatedFigureController.on_selected_epochs_changed(...)\n",
      "current_page_idx = 0, found_data_index =2\n",
      "\tselection_indicies: [1 2]\n",
      "DecodedEpochSlicesPaginatedFigureController.on_selected_epochs_changed(...)\n",
      "current_page_idx = 0, found_data_index =1\n",
      "\tselection_indicies: [2]\n",
      "DecodedEpochSlicesPaginatedFigureController.on_selected_epochs_changed(...)\n",
      "current_page_idx = 0, found_data_index =1\n",
      "\tselection_indicies: [1 2]\n",
      "DecodedEpochSlicesPaginatedFigureController.on_selected_epochs_changed(...)\n",
      "current_page_idx = 0, found_data_index =1\n",
      "\tselection_indicies: [1]\n"
     ]
    }
   ],
   "source": [
    "from pyphocorehelpers.gui.PhoUIContainer import PhoUIContainer\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.DockAreaWrapper import PhoDockAreaContainingWindow\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import DockAreaWrapper\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices_paginated\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import DecodedEpochSlicesPaginatedFigureController\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.PaginationCtrl.PaginationControlWidget import PaginationControlWidget\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import RadonTransformPlotDataProvider, WeightedCorrelationPaginatedPlotDataProvider\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "# pagination_controller_dict =  PhoPaginatedMultiDecoderDecodedEpochsWindow._subfn_prepare_plot_multi_decoders_stacked_epoch_slices(curr_active_pipeline, track_templates, decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', included_epoch_indicies=None, defer_render=False, save_figure=False)\n",
    "pagination_controller_dict =  PhoPaginatedMultiDecoderDecodedEpochsWindow._subfn_prepare_plot_multi_decoders_stacked_epoch_slices(curr_active_pipeline, track_templates, decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple', included_epoch_indicies=None, defer_render=False, save_figure=False)\n",
    "app, root_dockAreaWindow = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_pagination_controller_dict(pagination_controller_dict) # Combine to a single figure\n",
    "root_dockAreaWindow.add_data_overlays(track_templates, decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9af6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d86c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(decoder_laps_filter_epochs_decoder_result_dict.keys())\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555deda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the figure from the axes:\n",
    "a_fig = ax.get_figure()\n",
    "a_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller = root_dockAreaWindow.contents.pagination_controllers['long_LR'] # DecodedEpochSlicesPaginatedFigureController\n",
    "a_pagination_controller_figure_widget = paginator_controller_widget = a_controlling_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "paginator_controller_widget = a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget # PaginationControlWidget\n",
    "# paginator_controller_widget\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r\"\"\"\n",
    "DecodedEpochSlicesPaginatedFigureController.on_selected_epochs_changed(...)\n",
    "current_page_idx = 0, found_data_index =array([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
    "Traceback (most recent call last):\n",
    "  File \"k:\\FastSwap\\AppData\\VSCode\\yellow\\.venv_yellow\\lib\\site-packages\\matplotlib\\cbook.py\", line 298, in process\n",
    "    func(*args, **kwargs)\n",
    "  File \"C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Pho2D\\stacked_epoch_slices.py\", line 642, in on_selected_epochs_changed\n",
    "    self.on_click(event=event)\n",
    "  File \"C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Pho2D\\stacked_epoch_slices.py\", line 803, in on_click\n",
    "    self.params.is_selected[found_data_index] = not self.params.is_selected.get(found_data_index, False) # if never set before, assume that it's not selected\n",
    "TypeError: unhashable type: 'numpy.ndarray'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def on_click(event):\n",
    "    \"\"\" called when an axis is clicked to toggle the selection.\n",
    "    Captures: a_controlling_pagination_controller\n",
    "    \"\"\"\n",
    "    if a_controlling_pagination_controller.params.debug_print:\n",
    "        print(f'DecodedEpochSlicesPaginatedFigureController.on_click(...) OVERRIDE:')\n",
    "    # Get the clicked Axes object\n",
    "    ax = event.inaxes\n",
    "    # Find the axes\n",
    "    found_index = safe_find_index_in_list(a_controlling_pagination_controller.plots.axs, ax) # find the index on the page of the ax that was clicked\n",
    "    # print(f'{found_index = }')\n",
    "    current_page_idx = a_controlling_pagination_controller.current_page_idx\n",
    "    curr_page_data_indicies = a_controlling_pagination_controller.paginator.get_page_data(page_idx=current_page_idx)[0] # the [0] returns only the indicies and not the data\n",
    "    found_data_index = curr_page_data_indicies[found_index]\n",
    "    print(f'{current_page_idx = }, {found_data_index =}') # array([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
    "    # Toggle the selection status of the clicked Axes\n",
    "    a_controlling_pagination_controller.params.is_selected[found_data_index] = not a_controlling_pagination_controller.params.is_selected.get(found_data_index, False) # if never set before, assume that it's not selected\n",
    "    ## Update visual apperance of axis:\n",
    "    a_controlling_pagination_controller.perform_update_ax_selected_state(ax=ax, is_selected=a_controlling_pagination_controller.params.is_selected[found_data_index])\n",
    "\n",
    "\n",
    "# ax = event.inaxes\n",
    "# Find the axes\n",
    "# found_index = safe_find_index_in_list(a_controlling_pagination_controller.plots.axs, ax) # find the index on the page of the ax that was clicked\n",
    "\n",
    "current_page_idx = a_controlling_pagination_controller.current_page_idx\n",
    "# current_page_idx = 0\n",
    "print(f'current_page_idx: {current_page_idx}')\n",
    "curr_page_data_indicies, included_page_data_items = a_controlling_pagination_controller.paginator.get_page_data(page_idx=current_page_idx)\n",
    "curr_page_data_indicies\n",
    "\n",
    "# len(a_controlling_pagination_controller.plots.axs)\n",
    "ax = a_controlling_pagination_controller.plots.axs[3]\n",
    "found_index = safe_find_index_in_list(a_controlling_pagination_controller.plots.axs, ax) # find the index on the page of the ax that was clicked\n",
    "print(f'found_index: {found_index}')\n",
    "if found_index is not None:\n",
    "    found_data_index = curr_page_data_indicies[found_index]\n",
    "    print(f'{current_page_idx = }, {found_data_index =}') # array([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b08ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = a_controlling_pagination_controller.plots.axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73526ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "from neuropy.utils.matplotlib_helpers import add_selection_patch\n",
    "\n",
    "# current_page_idx = self.current_page_idx\n",
    "# curr_page_data_indicies = self.paginator.get_page_data(page_idx=current_page_idx)[0] # the [0] returns only the indicies and not the data\n",
    "\n",
    "if not a_controlling_pagination_controller.plots.has_attr('selection_rectangles_dict'):\n",
    "    ## Create the dict if needed and it doesn't exist:\n",
    "    a_controlling_pagination_controller.plots.selection_rectangles_dict = {} # empty dict to start\n",
    "\n",
    "## INPUTS: curr_page_data_indicies, axs\n",
    "assert len(axs) == len(curr_page_data_indicies), f\"len(plots.axs): {len(axs)}, len(curr_page_data_indicies): {len(curr_page_data_indicies)}\"\n",
    "for ax, found_data_idx in zip(axs, list(curr_page_data_indicies)): # TODO: might fail for the last page?\n",
    "    ## First get the ax\n",
    "    a_selection_rect = a_controlling_pagination_controller.plots.selection_rectangles_dict.get(ax, None)\n",
    "    if a_selection_rect is None:\n",
    "        # create a new one\n",
    "        print(f'need a new selection rect.')\n",
    "        a_selection_rect = add_selection_patch(ax, selection_color='green', alpha=0.6, zorder=-1, defer_draw=True)\n",
    "        a_controlling_pagination_controller.plots.selection_rectangles_dict[ax] = a_selection_rect ## add to dict\n",
    "\n",
    "    is_selected = a_controlling_pagination_controller.params.is_selected.get(found_data_idx, False)\n",
    "    a_selection_rect.set_visible(is_selected)\n",
    "\n",
    "    a_controlling_pagination_controller.perform_update_ax_selected_state(ax=ax, is_selected=is_selected)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550263ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_figure().canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04143f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_rectangles_dict = a_controlling_pagination_controller.plots.get('selection_rectangles_dict', None)\n",
    "selection_rectangles_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_pos.x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_controlling_pagination_controller.plots.fig.canvas.draw_idle()\n",
    "# a_controlling_pagination_controller.plots.fig.canvas.draw()\n",
    "# paginator_controller_widget.update()\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eeedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a_controlling_pagination_controller.params.is_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc424c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator_controller_widget.go_to_page(3)\n",
    "# paginator_controller_widget.jump_to_page(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget.jump_to_page\n",
    "\n",
    "new_obj.plots_data.paginator\n",
    "new_obj.params.active_identifying_figure_ctx\n",
    "new_obj.on_paginator_control_widget_jump_to_page(page_idx=0)\n",
    "new_obj.ui.connections['paginator_controller_widget_jump_to_page']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, extant_plots in a_plots['weighted_corr'].items():\n",
    "    extant_wcorr_text = extant_plots.get('wcorr_text', None)\n",
    "    # extant_wcorr_text = extant_plots.pop('wcorr_text', None)\n",
    "    print(f'extant_wcorr_text: {extant_wcorr_text}')\n",
    "    # plot the radon transform line on the epoch:\n",
    "    if (extant_wcorr_text is not None):\n",
    "        # already exists, clear the existing ones. \n",
    "        # Let's assume we want to remove the 'Quadratic' line (line2)\n",
    "        print(f'removing extant text object at index: {i}.')\n",
    "        # extant_wcorr_text.remove()\n",
    "        extant_wcorr_text.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    display_context = a_pagination_controller.params.get('active_identifying_figure_ctx', IdentifyingContext())\n",
    "\n",
    "    # Get context for current page of items:\n",
    "    current_page_idx: int = int(a_pagination_controller.current_page_idx)\n",
    "    a_paginator = a_pagination_controller.paginator\n",
    "    total_num_pages = int(a_paginator.num_pages)\n",
    "    page_context = display_context.overwriting_context(page=current_page_idx, num_pages=total_num_pages)\n",
    "    print(page_context)\n",
    "\n",
    "    ## Get the figure/axes:\n",
    "    a_plots = a_pagination_controller.plots # RenderPlots\n",
    "    a_plot_data = a_pagination_controller.plots_data\n",
    "\n",
    "    a_params = a_pagination_controller.params\n",
    "    a_params.skip_plotting_measured_positions\n",
    "\n",
    "    figs = a_plots.fig\n",
    "    axs = a_plots.axs\n",
    "\n",
    "    # # with mpl.rc_context({'figure.figsize': (8.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    # with mpl.rc_context({'figure.figsize': (16.8, 4.8), 'figure.dpi': '420', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    #     curr_active_pipeline.output_figure(final_context=page_context, fig=figs, write_vector_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7aca6",
   "metadata": {},
   "source": [
    "## Export Paginated Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "def export_decoder_pagination_controller_figure_page(pagination_controller_dict, curr_active_pipeline):\n",
    "    \"\"\" exports each pages single-decoder figures separately\n",
    "\n",
    "    Captures: curr_active_pipeline\n",
    "    \n",
    "    \"\"\"\n",
    "    for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "        display_context = a_pagination_controller.params.get('active_identifying_figure_ctx', IdentifyingContext())\n",
    "\n",
    "        # Get context for current page of items:\n",
    "        current_page_idx: int = int(a_pagination_controller.current_page_idx)\n",
    "        a_paginator = a_pagination_controller.paginator\n",
    "        total_num_pages = int(a_paginator.num_pages)\n",
    "        page_context = display_context.overwriting_context(page=current_page_idx, num_pages=total_num_pages)\n",
    "        print(page_context)\n",
    "\n",
    "        ## Get the figure/axes:\n",
    "        a_plots = a_pagination_controller.plots # RenderPlots\n",
    "        a_params = a_pagination_controller.params\n",
    "        \n",
    "        # with mpl.rc_context({'figure.figsize': (8.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "        with mpl.rc_context({'figure.figsize': (16.8, 4.8), 'figure.dpi': '420', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "            figs = a_plots.fig\n",
    "            axs = a_plots.axs\n",
    "            curr_active_pipeline.output_figure(final_context=page_context, fig=figs, write_vector_format=True)\n",
    "\n",
    "\n",
    "# export_decoder_pagination_controller_figure_page(pagination_controller_dict, curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_paginator = a_pagination_controller.paginator\n",
    "total_num_pages = int(a_paginator.num_pages)\n",
    "page_idx_sweep = np.arange(total_num_pages)\n",
    "page_num_sweep = page_idx_sweep + 1 # switch to 1-indexed\n",
    "# page_num_sweep\n",
    "\n",
    "for a_page_idx, a_page_num in zip(page_idx_sweep, page_num_sweep):\n",
    "    print(f'switching to page: a_page_idx: {a_page_idx}, a_page_num: {a_page_num} of total_num_pages: {total_num_pages}')\n",
    "    a_pagination_controller.on_paginator_control_widget_jump_to_page(page_idx=a_page_idx)\n",
    "    a_pagination_controller.ui.mw.draw()\n",
    "    export_decoder_pagination_controller_figure_page(pagination_controller_dict, curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190122f0",
   "metadata": {},
   "source": [
    "## Other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eed0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = _out_pagination_controller.plots['radon_transform'][7]\n",
    "extant_line = _out['line'] # matplotlib.lines.Line2D\n",
    "extant_line.linestyle = 'none'\n",
    "# extant_line.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82687641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(curr_active_pipeline.filtered_contexts.keys())) # ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "\n",
    "# long_any_name\n",
    "\n",
    "# long_LR_name\n",
    "\n",
    "# Converting between decoder names and filtered epoch names:\n",
    "{'long':'maze1', 'short':'maze2'}\n",
    "{'LR':'odd', 'RL':'even'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_name_to_session_context_name = dict(zip(track_templates.get_decoder_names(), (long_LR_name, long_RL_name, short_LR_name, short_RL_name)))\n",
    "decoder_name_to_session_context_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abcb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_num_slices: int = _out_pagination_controller.params.active_num_slices\n",
    "single_plot_fixed_height: float = _out_pagination_controller.params.single_plot_fixed_height\n",
    "all_plots_height: float = _out_pagination_controller.params.all_plots_height\n",
    "print(f'all_plots_height: {all_plots_height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93ea19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bce3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc3fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f879b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcef848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PendingNotebookCode import _add_maze_id_to_epochs\n",
    "\n",
    "\n",
    "## Add new weighted correlation results as new columns in existing filter_epochs df:\n",
    "active_filter_epochs = long_results_obj.active_filter_epochs\n",
    "# Add the maze_id to the active_filter_epochs so we can see how properties change as a function of which track the replay event occured on:\n",
    "active_filter_epochs = _add_maze_id_to_epochs(active_filter_epochs, short_session.t_start)\n",
    "active_filter_epochs._df['weighted_corr_LONG'] = epoch_long_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_SHORT'] = epoch_short_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_spearman_LONG'] = epoch_long_weighted_corr_results[:,1]\n",
    "active_filter_epochs._df['weighted_corr_spearman_SHORT'] = epoch_short_weighted_corr_results[:,1]\n",
    "\n",
    "\n",
    "active_filter_epochs\n",
    "active_filter_epochs.to_dataframe()\n",
    "## plot the `weighted_corr_LONG` over time\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=1, nrows=active_num_rows, sharex=True, sharey=sharey, figsize=figsize)\n",
    "\n",
    "## Weighted Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_LONG', title='weighted_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "## Weighted Spearman Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_LONG', title='weighted_spearman_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Spearman Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='score_LONG', title='Radon Transform Score during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='score_SHORT', xlabel='Replay Epoch Time', ylabel='Replay Radon Transform Score', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d56479",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "example_stacked_epoch_graphics = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', defer_render=False, save_figure=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a739e",
   "metadata": {},
   "source": [
    "# 2024-02-13 - Plot the correlation between the Radon score and the decoder certainty for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e028231",
   "metadata": {},
   "source": [
    "### Decoder confidence is how far away the value is from 0.5. A value of 0.5 indicates no bias towards long or short, and should recieve a value of zero. Bias in either direction (towards 1.0 or 0.0) should recieve a increasing certainty value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549853f",
   "metadata": {},
   "source": [
    "This is most easily accomplished by shifting the values towards zero and applying an absolute value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c080b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_is_most_likely_direction_LR_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eff559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laps_all_epoch_bins_marginals_df\n",
    "rescaled_P_Long = np.abs(((ripple_all_epoch_bins_marginals_df['P_Long'] - 0.5) * 2))\n",
    "rescaled_P_Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get radon scores:\n",
    "ripple_radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb1abb",
   "metadata": {
    "tags": [
     "writing"
    ]
   },
   "source": [
    "# 2024-02-15 - Do simple spike-t vs. template pf peak correlation like Kamran suggested this morning\n",
    "\n",
    "Replays can be of trajectories on either the current track configuration or on a temporally distant one (such as a trajectory on the long track after the track has been shortened). \n",
    "The goal of the decoder scoring methods are to evaluate how likely each decoder was. This means for each Epoch we obtain a score for all four decoders: Long_LR, Long_RL, Short_LR, Short_RL\n",
    "\n",
    "#### `posterior decoder likelihoods` - This scoring method produces a probability that the\n",
    "\n",
    "#### Radon Transform - TODO\n",
    "\n",
    "#### `compute_simple_spike_time_v_pf_peak_x_by_epoch` - This epoch scoring metric plots the placefield peak x position against the time in seconds of each spike relative to the start of the epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de4ac7",
   "metadata": {},
   "source": [
    "## TODO 2024-02-15 8pm - Add in to previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ba47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "laps_simple_pf_pearson_merged_df\n",
    "# laps_radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_laps_df: pd.DataFrame = laps_weighted_corr_merged_df.rename(columns={'best_decoder_index':'wcorr_best_decoder_index'}).join(laps_simple_pf_pearson_merged_df.rename(columns={'best_decoder_index':'pearsonr_best_decoder_index'})) # , lsuffix='__L'\n",
    "_test_ripple_df: pd.DataFrame = ripple_weighted_corr_merged_df.rename(columns={'best_decoder_index':'wcorr_best_decoder_index'}).join(ripple_simple_pf_pearson_merged_df.rename(columns={'best_decoder_index':'pearsonr_best_decoder_index'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c31abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_df = _test_ripple_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332caab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _test_df.pearsonr_best_decoder_index.hist()\n",
    "\n",
    "_test_df.long_LR_pf_peak_x_pearsonr.hist()\n",
    "_test_df.long_RL_pf_peak_x_pearsonr.hist()\n",
    "_test_df.short_LR_pf_peak_x_pearsonr.hist()\n",
    "_test_df.short_RL_pf_peak_x_pearsonr.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_df[corr_column_names].abs().mean(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b694aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## count up the number that the RadonTransform and the most-likely direction agree\n",
    "# laps_simple_pf_pearson_stats = _compute_matching_best_indicies(_test_laps_df, index_column_name='most_likely_decoder_index', second_index_column_name='pearsonr_best_decoder_index', enable_print=True)\n",
    "# ripple_simple_pf_pearson_stats = _compute_matching_best_indicies(_test_ripple_df, index_column_name='most_likely_decoder_index', second_index_column_name='pearsonr_best_decoder_index', enable_print=True)\n",
    "\n",
    "laps_simple_pf_pearson_stats = _compute_matching_best_indicies(laps_simple_pf_pearson_merged_df.rename(columns={'best_decoder_index':'pearsonr_best_decoder_index'}), index_column_name='most_likely_decoder_index', second_index_column_name='pearsonr_best_decoder_index', enable_print=True)\n",
    "# ripple_simple_pf_pearson_stats = _compute_matching_best_indicies(_test_ripple_df, index_column_name='most_likely_decoder_index', second_index_column_name='pearsonr_best_decoder_index', enable_print=True)\n",
    "laps_simple_pf_pearson_stats\n",
    "\n",
    "# laps_simple_pf_pearson_stats = _compute_matching_best_indicies(laps_weighted_corr_merged_df.rename(columns={'best_decoder_index':'wcorr_best_decoder_index'}).join(laps_simple_pf_pearson_merged_df.rename(columns={'best_decoder_index':'pearsonr_best_decoder_index'})), index_column_name='most_likely_decoder_index', second_index_column_name='pearsonr_best_decoder_index', enable_print=True)\n",
    "# agreeing_rows_ratio, (agreeing_rows_count, num_total_epochs) = laps_radon_stats\n",
    "# ripple_wcorr_stats = _compute_matching_best_indicies(ripple_weighted_corr_merged_df, index_column_name='most_likely_decoder_index', second_index_column_name=best_decoder_index_col_name, enable_print=True)\n",
    "\n",
    "# agreeing_rows_count/num_total_epochs: 41/84\n",
    "# \tagreeing_rows_ratio: 0.4880952380952381\n",
    "\n",
    "\n",
    "## Not well-matching for ripples surprisingly:\n",
    "# agreeing_rows_count/num_total_epochs: 77/84\n",
    "# \tagreeing_rows_ratio: 0.9166666666666666\n",
    "# agreeing_rows_count/num_total_epochs: 115/412\n",
    "# \tagreeing_rows_ratio: 0.279126213592233\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94fc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS:laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df\n",
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f45939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e41486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17625acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "decoder_aclu_peak_location_df_merged = deepcopy(track_templates.get_decoders_aclu_peak_location_df(width=None))\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "decoder_aclu_peak_location_df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_result: TrialByTrialActivity = directional_active_lap_pf_results_dicts['long_LR']\n",
    "# a_result.sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e43ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: ripple_radon_transform_merged_df, decoder_specific_Radon_transform_score_columns\n",
    "\n",
    "\n",
    "decoder_specific_probability_columns = ['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']\n",
    "decoder_specific_Radon_transform_score_columns = ['score_long_LR', 'score_long_RL', 'score_short_LR', 'score_short_RL']\n",
    "\n",
    "best_decoder_index = ripple_radon_transform_merged_df['best_decoder_index'].to_numpy()\n",
    "# best_decoder_index.shape # (611,)\n",
    "\n",
    "\n",
    "# ripple_radon_transform_merged_df['best_decoder_index']\n",
    "\n",
    "decoder_specific_probability_mat = ripple_radon_transform_merged_df[decoder_specific_probability_columns].to_numpy() # .shape (611, 4)\n",
    "decoder_specific_Radon_transform_score_mat = ripple_radon_transform_merged_df[decoder_specific_Radon_transform_score_columns].to_numpy() # .shape (611, 4)\n",
    "n_epochs = np.shape(decoder_specific_Radon_transform_score_mat)[0] # 611\n",
    "# n_epochs \n",
    "best_decoder_probability = np.array([decoder_specific_probability_mat[i, best_decoder_index[i]] for i in np.arange(n_epochs)])\n",
    "best_decoder_Radon_transform_score = np.array([decoder_specific_Radon_transform_score_mat[i, best_decoder_index[i]] for i in np.arange(n_epochs)])\n",
    "# best_decoder_Radon_transform_score.shape # (611,)\n",
    "# best_decoder_Radon_transform_score\n",
    "\n",
    "# best_decoder_probability.shape # (611,)\n",
    "\n",
    "\n",
    "# [best_decoder_index].shape\n",
    "\n",
    "\n",
    "# Assuming best_decoder_probability and best_decoder_Radon_transform_score have been computed successfully and have the same shape\n",
    "correlation_matrix = np.corrcoef(best_decoder_probability, best_decoder_Radon_transform_score)\n",
    "\n",
    "# Extract the correlation coefficient from the matrix\n",
    "point_wise_correlation = correlation_matrix[0, 1]  # This gets the correlation between the two arrays\n",
    "\n",
    "print(point_wise_correlation)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your previously calculated vectors\n",
    "# best_decoder_probability = np.array([...])\n",
    "# best_decoder_Radon_transform_score = np.array([...])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(best_decoder_probability, best_decoder_Radon_transform_score)\n",
    "\n",
    "# Optional: Specify the labels for axes\n",
    "plt.xlabel('Best Decoder Probability')\n",
    "plt.ylabel('Best Decoder Radon Transform Score')\n",
    "\n",
    "# Optional: Specify the title of the graph\n",
    "plt.title('Scatter Plot of Best Decoder Probability vs. Radon Transform Score')\n",
    "\n",
    "# Show the scatter plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f22b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0174187368f176085df53c2cec5d63d0b8400e63ae887d077931481ed3fab96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
