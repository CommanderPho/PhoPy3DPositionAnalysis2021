{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:20.608442900Z",
     "start_time": "2023-11-16T23:21:20.217442100Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "doc_output_parent_folder: C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\EXTERNAL\\DEVELOPER_NOTES\\DataStructureDocumentation\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n",
      "DAY_DATE_STR: 2024-02-09, DAY_DATE_TO_USE: 2024-02-09\n",
      "NOW_DATETIME: 2024-02-09_0447AM, NOW_DATETIME_TO_USE: 2024-02-09_0447AM\n",
      "global_data_root_parent_path changed to W:\\Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585938e37f974c6d920d549ea1680ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Data Root:', layout=Layout(width='auto'), options=(WindowsPath('W:/Data'),), style=ToggleButtonsStyle(button_width='max-content'), tooltip='global_data_root_parent_path', value=WindowsPath('W:/Data'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext viztracer\n",
    "from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "text_formatter: PlainTextFormatter = IPython.get_ipython().display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, CapturedException, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables, CapturedException\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer, RankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path(r'/media/MAX/Data'), Path(r'/media/halechr/MAX/Data'), Path(r'/home/halechr/FastData'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data')]\n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "\tglobal global_data_root_parent_path\n",
    "\tnew_global_data_root_parent_path = new_path.resolve()\n",
    "\tglobal_data_root_parent_path = new_global_data_root_parent_path\n",
    "\tprint(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "\tassert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\t\t\t\n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {},
   "source": [
    "# Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\loadedSessPickle.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:com.PhoHale.Spike3D.pipeline:NeuropyPipeline.__setstate__(state=\"{'pipeline_name': 'kdiba_pipeline', 'session_data_type': 'kdiba', '_stage': <pyphoplacecellanalysis.General.Pipeline.Stages.Display.DisplayPipelineStage object at 0x0000014B048F0BE0>}\")\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:select_filters(...) with: []\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: No global_session or position passed, using old even/odd 'lap_dir' determination.\n",
      "using provided computation_functions_name_includelist: ['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze1_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze2_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_any\"...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "INFO:com.PhoHale.Spike3D.pipeline:\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "INFO:com.PhoHale.Spike3D.pipeline:Performing global computations...\n",
      "INFO:com.PhoHale.Spike3D.pipeline:NeuropyPipeline.on_stage_changed(new_stage=\"PipelineStage.Displayed\")\n",
      "WARNING:com.PhoHale.Spike3D.pipeline:WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context: IdentifyingContext = good_contexts_list[1] # select the session from all of the good sessions here.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # DONE. Very good. Many good Pfs, many good replays.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # DONE, might be the BEST SESSION, good example session with lots of place cells, clean replays, and clear bar graphs.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # DONE, Good Pfs but no good replays ---- VERY weird effect of the replays, a sharp drop to strongly negative values more than 3/4 through the experiment.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6') # BAD, 2023-07-14, unsure why still.\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # DONE, GREAT, both good Pfs and replays! Interesting see-saw!\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25') # DONE, Added replay selections. Very \"jumpy\" between the starts and ends of the track.\n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40') # 2024-01-10 new RANKORDER APOGEE | DONE, Added replay selections. A TON of putative replays in general, most bad, but some good. LOOKIN GOOD!\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='twolong_LR_pf1Dsession_name='2006-4-12_15-25-59') # BAD, No Epochs\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-16_18-47-52')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-17_12-52-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-25_13-20-55')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-28_12-38-13')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44') # DONE, good. Many good pfs, many good replays. Noticed very strange jumping off the track in the 3D behavior/spikes viewer. Is there something wrong with this session?\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0') # DONE, good?, replays selected, few --- \"ZeroDivisionError: float division by zero\"\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # DONE, very few replays\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_12-15-3') ### KeyError: 'maze1_odd'\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-09_22-4-5') ### \n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # DONE, replays selected, quite a few replays but few are very good.\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25')\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "# \n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation',\n",
    "                                            #    'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step', \n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            # 'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=True) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RankOrder',\n",
       " 'DirectionalDecodersDecoded',\n",
       " 'DirectionalLaps',\n",
       " 'DirectionalMergedDecoders',\n",
       " 'long_short_leave_one_out_decoding_analysis',\n",
       " 'short_long_pf_overlap_analyses',\n",
       " 'long_short_fr_indicies_analysis',\n",
       " 'jonathan_firing_rate_analysis',\n",
       " 'long_short_post_decoding',\n",
       " 'long_short_endcap']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(curr_active_pipeline.global_computation_results.computed_data.keys())\n",
    "\n",
    "\n",
    "# 2024-01-22 ERROR: when the pipeline is manually saved, its global_computations seem to be saved to the pickle too. After modifying how global computations are loaded from pickle, the following global computations code block no longer appropriately overwrites the existing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=['DirectionalLaps', 'DirectionalMergedDecoders', 'RankOrder', 'DirectionalDecodersDecoded'], debug_print=True)\n",
    "# global_dropped_keys, local_dropped_keys = curr_active_pipeline.perform_drop_computed_result(computed_data_keys_to_drop=[k for k in list(curr_active_pipeline.global_computation_results.computed_data.keys())], debug_print=True) # drop all global keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba46b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.574268400Z",
     "start_time": "2023-11-16T23:21:35.966373700Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\output\\global_computation_results.pkl... done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['RankOrder',\n",
       "  'DirectionalDecodersDecoded',\n",
       "  'DirectionalLaps',\n",
       "  'DirectionalMergedDecoders',\n",
       "  'long_short_leave_one_out_decoding_analysis',\n",
       "  'short_long_pf_overlap_analyses',\n",
       "  'long_short_fr_indicies_analysis',\n",
       "  'jonathan_firing_rate_analysis',\n",
       "  'long_short_post_decoding',\n",
       "  'long_short_endcap'],\n",
       " ['RankOrder',\n",
       "  'DirectionalDecodersDecoded',\n",
       "  'DirectionalLaps',\n",
       "  'DirectionalMergedDecoders',\n",
       "  'long_short_leave_one_out_decoding_analysis',\n",
       "  'short_long_pf_overlap_analyses',\n",
       "  'long_short_fr_indicies_analysis',\n",
       "  'jonathan_firing_rate_analysis',\n",
       "  'long_short_post_decoding',\n",
       "  'long_short_endcap'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included includelist is specified: ['lap_direction_determination', 'ratemap_peaks_prominence2d', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "missing required value, so we don't need to call .validate_computation_test(...) to know it isn't valid!\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.02})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "Exception occured while computing (`perform_specific_computation(...)`):\n",
      " Inner exception: ERROR: time_bin_size_seconds (0.02) > (t_stop - t_start) (0.016158711747266352). Reduce the bin size or exclude this epoch.\n",
      "Exception occured while validating (`validate_computation_test(...)`) after recomputation:\n",
      " Validation exception: 'long_short_inst_spike_rate_groups'\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'long_short_rate_remapping': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('lap_direction_determination', 'maze_any'), ('ratemap_peaks_prominence2d', 'maze_any'), ('split_to_directional_laps', 'maze_any'), ('merged_directional_placefields', 'maze_any'), ('directional_decoders_decode_continuous', 'maze_any'), ('rank_order_shuffle_analysis', 'maze_any'), ('long_short_decoding_analyses', 'maze_any'), ('short_long_pf_overlap_analyses', 'maze_any'), ('long_short_fr_indicies_analyses', 'maze_any'), ('jonathan_firing_rate_analysis', 'maze_any'), ('long_short_post_decoding', 'maze_any'), ('long_short_endcap_analysis', 'maze_any')].\n",
      "\n",
      "\n",
      "!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"\n",
      "\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### GLOBAL COMPUTATIONS:\n",
    "extended_computations_include_includelist=['lap_direction_determination', #'pf_computation', 'firing_rate_trends',# 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "     'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', \n",
    "    'long_short_post_decoding', # #TODO 2024-01-19 05:49: - [ ] `'long_short_post_decoding' is broken for some reason `AttributeError: 'NoneType' object has no attribute 'active_filter_epochs'``\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    'directional_decoders_decode_continuous'\n",
    "] # do only specified\n",
    "\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'] # , 'directional_decoders_decode_continuous'\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_decode_continuous'] # \n",
    "\n",
    "\n",
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # curr_active_pipeline.load_pickled_global_computation_results()\n",
    "        curr_active_pipeline.load_pickled_global_computation_results(allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist) # is new\n",
    "    except Exception as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results: {e}')\n",
    "        raise\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# except Exception as e:\n",
    "#     exception_info = sys.exc_info()\n",
    "#     e = CapturedException(e, exception_info)\n",
    "#     print(f'second half threw: {e}')\n",
    "\n",
    "# 4m 5.2s for inst fr computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47820977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    'rank_order_shuffle_analysis',\n",
    "    'directional_decoders_decode_continuous'\n",
    "] # do only specified\n",
    "\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps',\n",
    "#     # 'merged_directional_placefields',\n",
    "#     # 'directional_decoders_decode_continuous',\n",
    "# ]\n",
    "force_recompute_override_computations_includelist = None\n",
    "\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "newly_computed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# force_recompute_override_computations_includelist = ['_decode_continuous_using_directional_decoders']\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(extended_computations_include_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.02}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_decode_continuous'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results() # newly_computed_values: [('pfdt_computation', 'maze_any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7af96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = curr_active_pipeline.save_split_global_computation_results(debug_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77babf98",
   "metadata": {},
   "source": [
    "## Continue Saving/Exporting stuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a869b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n",
    "curr_active_pipeline.clear_registered_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE)\n",
    "# curr_active_pipeline.save_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {},
   "source": [
    "# Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03b451593624f8982106abbbdef1b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Box(children=(Label(value='session path:', layout=Layout(width='auto')), Label(value='W:\\\\Data\\\\KDIBA\\\\gor01\\\\two\\\\2006-6-09_22-24-40\\\\output', layout=Layout(flex='1 1 auto', margin='2px', width='auto')), Button(button_style='info', description='Copy', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Copy to Clipboard'), Button(button_style='info', description='Reveal', icon='folder-tree', layout=Layout(flex='0 1 auto', margin='1px', width='auto'), style=ButtonStyle(), tooltip='Reveal in System Explorer')), layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='70%')), HBox(children=(Button(description='Output Folder', style=ButtonStyle()), Button(description='global pickle', style=ButtonStyle()), Button(description='pipeline pickle', style=ButtonStyle()), Button(description='.h5 export', style=ButtonStyle()), Button(description='TEST - Dialog', style=ButtonStyle()), Button(description='Save Pipeline', style=ButtonStyle()))), HBox(children=(Button(description='Reload display functions...', style=ButtonStyle()), Button(description='Reload computation functions...', style=ButtonStyle()))), ToggleButton(value=True, description='Figures Displaying')))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {},
   "source": [
    "# End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a533ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.700275900Z",
     "start_time": "2023-11-16T23:21:40.584273Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 911.6011600069469, 2573.457162000006)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "t_start, t_delta, t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have several python variables I want to print: t_start, t_delta, t_end\n",
    "# I want to generate a print statement that explicitly lists the variable name prior to its value like `print(f't_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')`\n",
    "# Currently I have to t_start, t_delta, t_end\n",
    "curr_active_pipeline.get_session_context()\n",
    "\n",
    "print(f'{curr_active_pipeline.session_name}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9071e94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:43.601382Z",
     "start_time": "2023-11-16T23:21:40.702275600Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "WARNING: PAPER_FIGURE_figure_1_add_replay_epoch_rasters(...): no user-assigned manually labeled replay epochs. Reeturning all epochs.\n",
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n"
     ]
    }
   ],
   "source": [
    "## long_short_decoding_analyses:\n",
    "from attrs import astuple\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import LeaveOneOutDecodingAnalysis\n",
    "\n",
    "curr_long_short_decoding_analyses: LeaveOneOutDecodingAnalysis = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba8f0cf",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyphoplacecellanalysis.Analysis.Decoder.decoder_result.LeaveOneOutDecodingAnalysisResult object at 0x0000014DBA629A00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_long_short_decoding_analyses.long_results_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83acf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_v_observed_result.observed_from_expected_diff_ptp_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c49f5d4f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7104fc37",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult, DirectionalLapsResult, DirectionalDecodersDecodedResult\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']   \n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: float = rank_order_results.included_qclu_values\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238f67cb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previously_decoded time_bin_sizes: [0.05]\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersDecodedResult\n",
    "\n",
    "directional_decoders_decode_result: DirectionalDecodersDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "spikes_df = directional_decoders_decode_result.spikes_df\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "\n",
    "most_recent_time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# most_recent_time_bin_size\n",
    "most_recent_continuously_decoded_dict = deepcopy(directional_decoders_decode_result.most_recent_continuously_decoded_dict)\n",
    "# most_recent_continuously_decoded_dict\n",
    "\n",
    "## Adds in the 'pseudo2D' decoder in:\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# time_bin_size: float = 0.01\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = continuously_decoded_result_cache_dict[time_bin_size]\n",
    "pseudo2D_decoder_continuously_decoded_result = continuously_decoded_dict.get('pseudo2D', None)\n",
    "if pseudo2D_decoder_continuously_decoded_result is None:\n",
    "\t# compute here...\n",
    "\t## Currently used for both cases to decode:\n",
    "\tt_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\tsingle_global_epoch_df: pd.DataFrame = pd.DataFrame({'start': [t_start], 'stop': [t_end], 'label': [0]}) # Build an Epoch object containing a single epoch, corresponding to the global epoch for the entire session:\n",
    "\tsingle_global_epoch: Epoch = Epoch(single_global_epoch_df)\n",
    "\tspikes_df = directional_decoders_decode_result.spikes_df\n",
    "\tpseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = pseudo2D_decoder.decode_specific_epochs(spikes_df=deepcopy(spikes_df), filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "\tcontinuously_decoded_dict['pseudo2D'] = pseudo2D_decoder_continuously_decoded_result\n",
    "\tcontinuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2dc5fe",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "# `LongShortStatsItem` form (2024-01-02):\n",
    "# LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "# RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260739a4f36c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "# active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1c6006aba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Entropy/Surprise Results:\n",
    "active_extended_stats = global_results['extended_stats']\n",
    "active_relative_entropy_results = active_extended_stats['pf_dt_sequential_surprise'] # DynamicParameters\n",
    "historical_snapshots = active_relative_entropy_results['historical_snapshots']\n",
    "post_update_times: np.ndarray = active_relative_entropy_results['post_update_times'] # (4152,) = (n_post_update_times,)\n",
    "snapshot_differences_result_dict = active_relative_entropy_results['snapshot_differences_result_dict']\n",
    "time_intervals: np.ndarray = active_relative_entropy_results['time_intervals']\n",
    "surprise_time_bin_duration = (post_update_times[2]-post_update_times[1])\n",
    "long_short_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['long_short_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "short_long_rel_entr_curves_frames: np.ndarray = active_relative_entropy_results['short_long_rel_entr_curves_frames'] # (4152, 108, 63) = (n_post_update_times, n_neurons, n_xbins)\n",
    "flat_relative_entropy_results: np.ndarray = active_relative_entropy_results['flat_relative_entropy_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_results: np.ndarray = active_relative_entropy_results['flat_jensen_shannon_distance_results'] # (149, 63) - (nSnapshots, nXbins)\n",
    "flat_jensen_shannon_distance_across_all_positions: np.ndarray = np.sum(np.abs(flat_jensen_shannon_distance_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "flat_surprise_across_all_positions: np.ndarray = np.sum(np.abs(flat_relative_entropy_results), axis=1) # sum across all position bins # (4152,) - (nSnapshots)\n",
    "\n",
    "## Get the placefield dt matrix:\n",
    "if 'snapshot_occupancy_weighted_tuning_maps' not in active_relative_entropy_results:\n",
    "\t## Compute it if missing:\n",
    "\toccupancy_weighted_tuning_maps_over_time = np.stack([placefield_snapshot.occupancy_weighted_tuning_maps_matrix for placefield_snapshot in historical_snapshots.values()])\n",
    "\tactive_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] = occupancy_weighted_tuning_maps_over_time\n",
    "else:\n",
    "\toccupancy_weighted_tuning_maps_over_time = active_relative_entropy_results['snapshot_occupancy_weighted_tuning_maps'] # (n_post_update_times, n_neurons, n_xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d3bf5955d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c62d5c18c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "# appearing_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c292d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Saving/Loading `DirectionalLaps_2Hz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6c50e",
   "metadata": {
    "tags": [
     "save",
     "persistance"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import save_rank_order_results\n",
    "\n",
    "# DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "# DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "# print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "# NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "# NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "# print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "formatted_time = get_now_rounded_time_str()\n",
    "print(formatted_time)\n",
    "save_rank_order_results(curr_active_pipeline, day_date=f\"{formatted_time}\") # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm # \"2024-01-02_301pm\" \"2024-01-02_322pm\" 322pm\n",
    "# '2024-01-09_0125PM-minimum_inclusion_fr-5-included_qclu_values-[1, 2]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_path = Path('/media/MAX/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/').resolve()\n",
    "sorted(search_path.glob(f\"{DAY_DATE_TO_USE}*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da8a7c",
   "metadata": {
    "tags": [
     "load"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import SaveStringGenerator\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import loadData\n",
    "\n",
    "# Load the data from a file into the pipeline:\n",
    "# out_filename_str: str = '2023-12-11-minimum_inclusion_fr_Hz_2_included_qclu_values_1-2_' # specific\n",
    "\n",
    "minimum_inclusion_fr_Hz: float = 5.0\n",
    "included_qclu_values: List[int] = [1,2]\n",
    "out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date=f'{DAY_DATE_TO_USE}_11am') # '2023-12-21_349am'\n",
    "# out_filename_str = SaveStringGenerator.generate_save_suffix(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values, day_date='2023-12-22_312pm') # '2023-12-21_349am'\n",
    "print(f'out_filename_str: \"{out_filename_str}\"')\n",
    "# day_date_str: str = '2023-12-11_with_tuple_newer_'\n",
    "# day_date_str: str = ''\n",
    "directional_laps_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}DirectionalLaps.pkl').resolve()\n",
    "assert directional_laps_output_path.exists()\n",
    "# loaded_directional_laps, loaded_rank_order = loadData(directional_laps_output_path)\n",
    "loaded_directional_laps = loadData(directional_laps_output_path)\n",
    "assert (loaded_directional_laps is not None)\n",
    "# assert (loaded_rank_order is not None)\n",
    "\n",
    "rank_order_output_path = curr_active_pipeline.get_output_path().joinpath(f'{out_filename_str}RankOrder.pkl').resolve()\n",
    "loaded_rank_order = loadData(rank_order_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the loaded data to the pipeline:\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'], curr_active_pipeline.global_computation_results.computed_data['RankOrder'] = loaded_directional_laps, loaded_rank_order\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd521ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec24467335a760",
   "metadata": {},
   "source": [
    "# POST-Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "728c46e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": [
     "unwrap"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_inclusion_fr_Hz: 5.0\n",
      "included_qclu_values: [1, 2]\n",
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025\n"
     ]
    }
   ],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multi_sort_raster_browser\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons, paired_incremental_sort_neurons # _display_directional_template_debugger\n",
    "from neuropy.utils.indexing_helpers import paired_incremental_sorting, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ScrollBarWithSpinBox.ScrollBarWithSpinBox import ScrollBarWithSpinBox\n",
    "\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "\n",
    "## Display Testing\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph import QtGui\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import pyqtplot_build_image_bounds_extent, pyqtplot_plot_image\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data['RankOrder']\n",
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "# ripple_result_tuple\n",
    "\n",
    "## Unpacks `rank_order_results`: \n",
    "# global_replays = Epoch(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# active_replay_epochs, active_epochs_df, active_selected_spikes_df = combine_rank_order_results(rank_order_results, global_replays, track_templates=track_templates)\n",
    "# active_epochs_df\n",
    "\n",
    "# ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices\n",
    "dir_index_to_direction_name_map: Dict[int, str] = {0:'LR', 1:\"RL\"}\n",
    "\n",
    "\n",
    "## All three DataFrames are the same number of rows, each with one row corresponding to an Epoch:\n",
    "active_replay_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "# active_replay_epochs_df\n",
    "\n",
    "# Change column type to int8 for columns: 'long_best_direction_indices', 'short_best_direction_indices'\n",
    "# directional_likelihoods_df = pd.DataFrame.from_dict(ripple_result_tuple.directional_likelihoods_tuple._asdict()).astype({'long_best_direction_indices': 'int8', 'short_best_direction_indices': 'int8'})\n",
    "directional_likelihoods_df = ripple_result_tuple.directional_likelihoods_df\n",
    "# directional_likelihoods_df\n",
    "\n",
    "# 2023-12-15 - Newest method:\n",
    "# laps_combined_epoch_stats_df = rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "# ripple_combined_epoch_stats_df: pd.DataFrame  = rank_order_results.ripple_combined_epoch_stats_df\n",
    "# ripple_combined_epoch_stats_df\n",
    "\n",
    "\n",
    "# # Concatenate the three DataFrames along the columns axis:\n",
    "# # Assert that all DataFrames have the same number of rows:\n",
    "# assert len(active_replay_epochs_df) == len(directional_likelihoods_df) == len(ripple_combined_epoch_stats_df), \"DataFrames have different numbers of rows.\"\n",
    "# # Assert that all DataFrames have at least one row:\n",
    "# assert len(active_replay_epochs_df) > 0, \"active_replay_epochs_df is empty.\"\n",
    "# assert len(directional_likelihoods_df) > 0, \"directional_likelihoods_df is empty.\"\n",
    "# assert len(ripple_combined_epoch_stats_df) > 0, \"ripple_combined_epoch_stats_df is empty.\"\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = pd.concat([active_replay_epochs_df.reset_index(drop=True, inplace=False), directional_likelihoods_df.reset_index(drop=True, inplace=False), ripple_combined_epoch_stats_df.reset_index(drop=True, inplace=False)], axis=1)\n",
    "# merged_complete_epoch_stats_df = merged_complete_epoch_stats_df.set_index(active_replay_epochs_df.index, inplace=False)\n",
    "\n",
    "# merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "# merged_complete_epoch_stats_df.to_csv('output/2023-12-21_merged_complete_epoch_stats_df.csv')\n",
    "# merged_complete_epoch_stats_df\n",
    "\n",
    "laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripple_merged_complete_epoch_stats_df\n",
    "laps_merged_complete_epoch_stats_df\n",
    "['long_best_direction_indices', 'short_best_direction_indices', 'combined_best_direction_indicies', 'long_relative_direction_likelihoods', 'short_relative_direction_likelihoods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time series of Long-likely events\n",
    "# type(long_RL_results) # DynamicParameters\n",
    "long_LR_pf1D_Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920bfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_directional_decoder_dict_value)\n",
    "list(all_directional_decoder_dict_value.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df\n",
    "laps_most_likely_direction_from_decoder\n",
    "long_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdabd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ripple_result_tuple) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.DirectionalRankOrderResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "\n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps, partial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def register_type_display(func_to_register, type_to_register):\n",
    "\t\"\"\" adds the display function (`func_to_register`) it decorates to the class (`type_to_register) as a method\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\t@wraps(func_to_register)\n",
    "\tdef wrapper(*args, **kwargs):\n",
    "\t\treturn func_to_register(*args, **kwargs)\n",
    "\n",
    "\tfunction_name: str = func_to_register.__name__ # get the name of the function to be added as the property\n",
    "\tsetattr(type_to_register, function_name, wrapper) # set the function as a method with the same name as the decorated function on objects of the class.\t\n",
    "\treturn wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots \n",
    "\n",
    "# @register_type_display(DirectionalRankOrderResult)\n",
    "def plot_histograms(self: DirectionalRankOrderResult, **kwargs) -> \"MatplotlibRenderPlots\":\n",
    "\t\"\"\" \n",
    "\tnum='RipplesRankOrderZscore'\n",
    "\t\"\"\"\n",
    "\tprint(f'.plot_histograms(..., kwargs: {kwargs})')\n",
    "\tfig = plt.figure(layout=\"constrained\", **kwargs)\n",
    "\tax_dict = fig.subplot_mosaic(\n",
    "\t\t[\n",
    "\t\t\t[\"long_short_best_z_score_diff\", \"long_short_best_z_score_diff\"],\n",
    "\t\t\t[\"long_best_z_scores\", \"short_best_z_scores\"],\n",
    "\t\t],\n",
    "\t)\n",
    "\tplots = (pd.DataFrame({'long_best_z_scores': self.long_best_dir_z_score_values}).hist(ax=ax_dict['long_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'short_best_z_scores': self.short_best_dir_z_score_values}).hist(ax=ax_dict['short_best_z_scores'], bins=21, alpha=0.8),\n",
    "\t\tpd.DataFrame({'long_short_best_z_score_diff': self.long_short_best_dir_z_score_diff_values}).hist(ax=ax_dict['long_short_best_z_score_diff'], bins=21, alpha=0.8),\n",
    "\t)\n",
    "\treturn MatplotlibRenderPlots(name='plot_histogram_figure', figures=[fig], axes=ax_dict)\n",
    "\n",
    "\n",
    "register_type_display(plot_histograms, DirectionalRankOrderResult)\n",
    "## Call the newly added `plot_histograms` function on the `ripple_result_tuple` object which is of type `DirectionalRankOrderResult`:\n",
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\t try saving to CSV...')\n",
    "merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "merged_complete_epoch_stats_df\n",
    "merged_complete_ripple_epoch_stats_df_output_path = curr_active_pipeline.get_output_path().joinpath(f'{DAY_DATE_TO_USE}_1247pm_merged_complete_epoch_stats_df.csv').resolve()\n",
    "merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
    "print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732743e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import reorder_columns\n",
    "\n",
    "dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4))\n",
    "reorder_columns(merged_complete_epoch_stats_df, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dceda30",
   "metadata": {},
   "source": [
    "## 2023-12-21 - Computing Spearman Percentiles as an alternative to the Z-score from shuffling, which does not seem to work for small numbers of active cells in an event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45aa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values, shuffled_results_output_dict, combined_variable_names, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles = rank_order_results.ripple_new_output_tuple\n",
    "# shuffled_results_output_dict['short_LR_pearson_Z']\n",
    "print(list(shuffled_results_output_dict.keys())) # ['short_LR_pearson_Z', 'short_LR_spearman_Z', 'short_RL_pearson_Z', 'short_RL_spearman_Z', 'long_LR_pearson_Z', 'long_RL_pearson_Z', 'long_RL_spearman_Z', 'long_LR_spearman_Z']\n",
    "\n",
    "['long_LR_pearson_Z', 'long_RL_pearson_Z', 'short_LR_pearson_Z', 'short_RL_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b40bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-12-22 - Add the LR-LR, RL-RL differences\n",
    "merged_complete_epoch_stats_df['LongShort_LR_quantile_diff'] = merged_complete_epoch_stats_df['LR_Long_rank_percentile'] - merged_complete_epoch_stats_df['LR_Short_rank_percentile']\n",
    "merged_complete_epoch_stats_df['LongShort_RL_quantile_diff'] = merged_complete_epoch_stats_df['RL_Long_rank_percentile'] - merged_complete_epoch_stats_df['RL_Short_rank_percentile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df = deepcopy(merged_complete_epoch_stats_df)\n",
    "\n",
    "# Filter rows based on columns: 'Long_BestDir_quantile', 'Short_BestDir_quantile'\n",
    "quantile_significance_threshold: float = 0.95\n",
    "significant_BestDir_quantile_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['Long_BestDir_quantile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['Short_BestDir_quantile'] > quantile_significance_threshold)]\n",
    "LR_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==0) & ((ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "RL_likely_active_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['combined_best_direction_indicies']==1) & ((ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold))]\n",
    "\n",
    "# significant_ripple_combined_epoch_stats_df = ripple_combined_epoch_stats_df[(ripple_combined_epoch_stats_df['LR_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['LR_Short_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Long_rank_percentile'] > quantile_significance_threshold) | (ripple_combined_epoch_stats_df['RL_Short_rank_percentile'] > quantile_significance_threshold)]\n",
    "# significant_ripple_combined_epoch_stats_df\n",
    "is_epoch_significant = np.isin(ripple_combined_epoch_stats_df.index, significant_BestDir_quantile_stats_df.index)\n",
    "active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "significant_ripple_epochs: Epoch = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df()).boolean_indicies_slice(is_epoch_significant)\n",
    "epoch_identifiers = significant_ripple_epochs._df.label.astype({'label': RankOrderAnalyses._label_column_type}).values #.labels\n",
    "x_values = significant_ripple_epochs.midtimes\n",
    "x_axis_name_suffix = 'Mid-time (Sec)'\n",
    "\n",
    "# significant_ripple_epochs_df = significant_ripple_epochs.to_dataframe()\n",
    "# significant_ripple_epochs_df\n",
    "\n",
    "significant_BestDir_quantile_stats_df['midtimes'] = significant_ripple_epochs.midtimes\n",
    "significant_BestDir_quantile_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import _plot_significant_event_quantile_fig\n",
    "\n",
    "# active_replay_epochs_df = rank_order_results.LR_ripple.epochs_df\n",
    "# if isinstance(global_events, pd.DataFrame):\n",
    "#     active_replay_epochs = Epoch(deepcopy(active_replay_epochs_df).epochs.get_valid_df())\n",
    "\n",
    "\n",
    "# _out = _plot_significant_event_quantile_fig(curr_active_pipeline, significant_ripple_combined_epoch_stats_df=significant_ripple_combined_epoch_stats_df)\n",
    "# _out\n",
    "\n",
    "marker_style = dict(linestyle='None', color='#ff7f0eff', markersize=6, markerfacecolor='#ff7f0eb4', markeredgecolor='#ff7f0eff')\n",
    "\n",
    "    # dict(facecolor='#ff7f0eb4', size=8.0)\n",
    "    # fignum='best_quantiles'\n",
    "\n",
    "# ripple_combined_epoch_stats_df['combined_best_direction_indicies']\n",
    "\n",
    "_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y='LongShort_BestDir_quantile_diff', title='Sig. (>0.95) Best Quantile Diff', **marker_style, marker='o')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import plot_quantile_diffs\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "global_epoch = curr_active_pipeline.filtered_epochs[global_epoch_name]\n",
    "short_epoch = curr_active_pipeline.filtered_epochs[short_epoch_name]\n",
    "split_time_t: float = short_epoch.t_start\n",
    "active_context = curr_active_pipeline.sess.get_context()\n",
    "\n",
    "collector = plot_quantile_diffs(ripple_merged_complete_epoch_stats_df, t_split=split_time_t, active_context=active_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd89199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "perform_update_title_subtitle(fig=fig_long_pf_1D, ax=ax_long_pf_1D, title_string=title_string, subtitle_string=subtitle_string, active_context=active_context, use_flexitext_titles=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neuropy.utils.matplotlib_helpers import draw_epoch_regions\n",
    "epochs_collection, epoch_labels = draw_epoch_regions(curr_active_pipeline.sess.epochs, ax, defer_render=False, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecb4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(significant_BestDir_quantile_stats_df.columns))\n",
    "['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']\n",
    "\n",
    "for a_name in ['LR_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Long_rank_percentile', 'RL_Short_rank_percentile', 'Long_BestDir_quantile', 'Short_BestDir_quantile', 'LongShort_BestDir_quantile_diff']:\n",
    "\t_out = significant_BestDir_quantile_stats_df[['midtimes', 'LongShort_BestDir_quantile_diff']].plot(x='midtimes', y=a_name, title=f'Sig. (>0.95) {a_name}', **marker_style, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "# quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].plot.hist(bins=21)\n",
    "\n",
    "df = quantile_results_df[['LR_Long_rank_percentile', 'RL_Long_rank_percentile', 'LR_Short_rank_percentile', 'RL_Short_rank_percentile']].copy()\n",
    "# Create the subplots and loop through columns\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 10))\n",
    "for i, col in enumerate(df.columns):\n",
    "    df[col].plot.hist(ax=axes[i], bins=21)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Adjust layout and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd61a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsLayoutWidget(show=True)\n",
    "win.resize(800,350)\n",
    "win.setWindowTitle('Z-Scorer: Histogram')\n",
    "plt1 = win.addPlot()\n",
    "vals = quantile_results_df.LR_Long_rank_percentile\n",
    "fisher_z_transformed_vals = np.arctanh(vals)\n",
    "\n",
    "## compute standard histogram\n",
    "y, x = np.histogram(vals) # , bins=np.linspace(-3, 8, 40)\n",
    "# fisher_z_transformed_y, x = np.histogram(fisher_z_transformed_vals, bins=x)\n",
    "\n",
    "## Using stepMode=\"center\" causes the plot to draw two lines for each sample.\n",
    "## notice that len(x) == len(y)+1\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "plt1.plot(x, y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,0,255,50), name='original_values')\n",
    "# plt1.plot(x, fisher_z_transformed_y, stepMode=\"center\", fillLevel=0, fillOutline=True, brush=(0,255,100,50), name='fisher_z_values')\n",
    "\n",
    "# ## Now draw all points as a nicely-spaced scatter plot\n",
    "y = pg.pseudoScatter(vals, spacing=0.15)\n",
    "# #plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5)\n",
    "plt2.plot(vals, y, pen=None, symbol='o', symbolSize=5, symbolPen=(255,255,255,200), symbolBrush=(0,0,255,150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30cb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.concat((ripple_combined_epoch_stats_df, ripple_p_values_epoch_stats_df), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.directional_likelihoods_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43327521",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_not(np.isnan(rank_order_results.ripple_combined_epoch_stats_df.index).any())\n",
    "# ripple_combined_epoch_stats_df.label.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.label).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(ripple_combined_epoch_stats_df.index).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60749347",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdone. building global result.\n"
     ]
    }
   ],
   "source": [
    "print(f'\\tdone. building global result.')\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_output_tuple (output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles) = ripple_new_output_tuple\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "unique_Probe_Epoch_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "\t# probe_epoch_df = active_selected_spikes_df[a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id']]\n",
    "\t# epoch_unique_aclus = probe_epoch_df.aclu.unique()\n",
    "\tmask = (a_probe_epoch_ID == active_selected_spikes_df['Probe_Epoch_id'])\n",
    "\t# epoch_unique_aclus = active_selected_spikes_df.loc[mask, 'aclu'].unique()\n",
    "\tfor a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "\t\t# Shuffle aclus here:\n",
    "\t\tactive_selected_spikes_df.loc[mask, 'aclu'] = active_selected_spikes_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\t\tactive_selected_spikes_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# ## Shuffle aclus here:\n",
    "\t\t# # probe_epoch_df.aclu.sample(1000)\n",
    "\t\t# # a_aclu_peak_map\n",
    "\t\t# # Assuming 'df' is your DataFrame and 'column_name' is the column you want to shuffle\n",
    "\t\t# probe_epoch_df['aclu'] = probe_epoch_df['aclu'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\t# probe_epoch_df[f'{a_decoder_name}_pf_peak_x'] = probe_epoch_df.aclu.map(a_aclu_peak_map)\n",
    "\n",
    "\t\t# active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df.aclu.map(a_aclu_peak_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c306ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2024-01-09 - More Efficient\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df: pd.DataFrame):\n",
    "    \"\"\" 2024-01-09 - Candidate for moving into RankOrderComputations \n",
    "    captures: decoder_names\n",
    "    \n",
    "    Usage:\n",
    "    \n",
    "    shuffled_dfs = _perform_efficient_shuffle(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "    \n",
    "    \"\"\"\n",
    "    decoder_names = track_templates.get_decoder_names()\n",
    "    \n",
    "    ## Compute real values here:\n",
    "    epoch_id_grouped_selected_spikes_df = active_selected_spikes_df.groupby('Probe_Epoch_id') # I can even compute this outside the loop?\n",
    "\n",
    "    # spearman_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='spearman', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "    # pearson_correlations = epoch_id_grouped_selected_spikes_df.apply(lambda group: RankOrderAnalyses._subfn_calculate_correlations(group, method='pearson', decoder_names=decoder_names)).reset_index() # Reset index to make 'Probe_Epoch_id' a column\n",
    "\n",
    "    # real_stats_df = pd.concat((spearman_correlations, pearson_correlations), axis='columns')\n",
    "    # real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()] # drop duplicated 'Probe_Epoch_id' column\n",
    "    # # Change column type to uint64 for column: 'Probe_Epoch_id'\n",
    "    # real_stats_df = real_stats_df.astype({'Probe_Epoch_id': 'uint64'})\n",
    "    # # Rename column 'Probe_Epoch_id' to 'label'\n",
    "    # real_stats_df = real_stats_df.rename(columns={'Probe_Epoch_id': 'label'})\n",
    "    \n",
    "    # Parallelize correlation computations if required\n",
    "    correlations = []\n",
    "    for method in ['spearman', 'pearson']:\n",
    "        correlations.append(\n",
    "            epoch_id_grouped_selected_spikes_df.apply(\n",
    "                lambda group: RankOrderAnalyses._subfn_calculate_correlations(\n",
    "                    group, method=method, decoder_names=decoder_names)\n",
    "            )\n",
    "        )\n",
    "  \n",
    "    # Adjust and join all calculated correlations\n",
    "    real_stats_df = pd.concat(correlations, axis='columns').reset_index()\n",
    "    real_stats_df = real_stats_df.loc[:, ~real_stats_df.columns.duplicated()]\n",
    "\n",
    "    real_stats_df.rename(columns={'Probe_Epoch_id': 'label'}, inplace=True)\n",
    "    real_stats_df['label'] = real_stats_df['label'].astype('uint64')  # in-place type casting\n",
    "    \n",
    "    return real_stats_df\n",
    "\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "def _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    \"\"\" 2024-01-09 - Performs the shuffles in a simple way\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "\n",
    "    # Create a list to hold the shuffled dataframes\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        # Working on a copy of the DataFrame\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "            \n",
    "            # Shuffle 'aclu' values\n",
    "            shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "            \n",
    "            # # Apply aclu peak map dictionary to 'aclu' column\n",
    "            # for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            #     shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "            \n",
    "\n",
    "        # end `for a_probe_epoch_ID`\n",
    "        # Once done, apply the aclu peak maps to shuffled_df's 'aclu' column:\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df[f'{a_decoder_name}_pf_peak_x'] = shuffled_df.aclu.map(a_aclu_peak_map)\n",
    "            \n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "        \n",
    "        # Adding the shuffled DataFrame to the list\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "        \n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "def _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles: int = 5):\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffled_dfs = []\n",
    "    shuffled_stats_dfs = []\n",
    "\n",
    "    def map_dict_to_group(group, a_dict, column):\n",
    "        group[column] = group[column].map(a_dict)\n",
    "        return group\n",
    "\n",
    "    for i in range(num_shuffles):\n",
    "        shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'] = shuffled_df.loc[shuffled_df['Probe_Epoch_id'] == a_probe_epoch_ID, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df = shuffled_df.groupby('Probe_Epoch_id').apply(map_dict_to_group, a_dict=a_aclu_peak_map, column=f'{a_decoder_name}_pf_peak_x')\n",
    "\n",
    "        a_shuffle_stats_df = _new_compute_single_rank_order_shuffle(track_templates, active_selected_spikes_df=shuffled_df)\n",
    "\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "        shuffled_stats_dfs.append(a_shuffle_stats_df)\n",
    "\n",
    "    return shuffled_dfs, shuffled_stats_dfs\n",
    "\n",
    "\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-suggested_perform_efficient_shuffle.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "shuffled_dfs, shuffled_stats_dfs = _suggested_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 50, 1m 21.2s, 10, 16.1s\n",
    "# shuffled_dfs, shuffled_stats_dfs = _new_perform_efficient_shuffle(track_templates, active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=10) # 10, 12.8s\n",
    "\n",
    "\n",
    "shuffled_dfs\n",
    "shuffled_stats_dfs\n",
    "# 5, 4.1 sec\n",
    "# 0.5s!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_active_epoch_computed_values = shuffled_stats_dfs\n",
    "# Build the output `stacked_arrays`: _________________________________________________________________________________ #\n",
    "\n",
    "stacked_arrays = np.stack([a_shuffle_real_stats_df[combined_variable_names].to_numpy() for a_shuffle_real_stats_df in output_active_epoch_computed_values], axis=0) # for compatibility: .shape (n_shuffles, n_epochs, n_columns)\n",
    "# stacked_df = pd.concat(output_active_epoch_computed_values, axis='index')\n",
    "\n",
    "## Drop any shuffle indicies where NaNs are returned for any of the stats values.\n",
    "is_valid_row = np.logical_not(np.isnan(stacked_arrays)).all(axis=(1,2)) # row [0, 66, :] is bad, ... so is [1, 66, :], ... [20, 66, :], ... they are repeated!!\n",
    "n_valid_shuffles = np.sum(is_valid_row)\n",
    "if debug_print:\n",
    "\tprint(f'n_valid_shuffles: {n_valid_shuffles}')\n",
    "valid_stacked_arrays = stacked_arrays[is_valid_row] ## Get only the rows where all elements along both axis (1, 2) are True\n",
    "\n",
    "# Need: valid_stacked_arrays, real_stacked_arrays, combined_variable_names\n",
    "combined_epoch_stats_df: pd.DataFrame = pd.DataFrame(real_stacked_arrays, columns=combined_variable_names)\n",
    "combined_variable_z_score_column_names = [f\"{a_name}_Z\" for a_name in combined_variable_names] # combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']\n",
    "\n",
    "## Extract the stats values for each shuffle from `valid_stacked_arrays`:\n",
    "n_epochs = np.shape(real_stacked_arrays)[0]\n",
    "n_variables = np.shape(real_stacked_arrays)[1]\n",
    "\n",
    "# valid_stacked_arrays.shape: (n_shuffles, n_epochs, n_variables)\n",
    "assert n_epochs == np.shape(valid_stacked_arrays)[-2]\n",
    "assert n_variables == np.shape(valid_stacked_arrays)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Determine the number of shuffles you want to do\n",
    "num_shuffles = 5\n",
    "\n",
    "# Define the operation to be run in parallel for a shuffle iteration\n",
    "def shuffle_iteration(i):\n",
    "    # Working on a copy of the DataFrame\n",
    "    shuffled_df = active_selected_spikes_df.copy()\n",
    "\n",
    "    for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "        mask = (a_probe_epoch_ID == shuffled_df['Probe_Epoch_id'])\n",
    "\n",
    "        # Shuffle 'aclu' values\n",
    "        shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "        # Apply aclu peak map dictionary to 'aclu' column\n",
    "        for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "            shuffled_df.loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffled_df.loc[mask, 'aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    # Return the shuffled DataFrame\n",
    "    return shuffled_df\n",
    "\n",
    "# Create a list to hold the shuffled dataframes\n",
    "shuffled_dfs = Parallel(n_jobs=-1)(delayed(shuffle_iteration)(i) for i in range(num_shuffles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()]\n",
    "print(peak_column_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles:int=5):\n",
    "    # Apply aclu peak map dictionary to each decoder name\n",
    "    for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items():\n",
    "        active_selected_spikes_df[f'{a_decoder_name}_pf_peak_x'] = active_selected_spikes_df['aclu'].map(a_aclu_peak_map)\n",
    "\n",
    "    unique_Probe_Epoch_IDs = active_selected_spikes_df['Probe_Epoch_id'].unique()\n",
    "    shuffles = {}\n",
    "    for i in range(num_shuffles):\n",
    "        shuffles[i] = active_selected_spikes_df.copy()\n",
    "        for a_probe_epoch_ID in unique_Probe_Epoch_IDs:\n",
    "            mask = (a_probe_epoch_ID == shuffles[i]['Probe_Epoch_id'])\n",
    "            # Shuffle multiple columns here:\n",
    "            for a_decoder_name in decoder_aclu_peak_map_dict.keys():\n",
    "                shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'] = shuffles[i].loc[mask, f'{a_decoder_name}_pf_peak_x'].sample(frac=1).values\n",
    "    return shuffles\n",
    "\n",
    "## Compute:\n",
    "decoder_aclu_peak_map_dict = track_templates.get_decoder_aclu_peak_map_dict()\n",
    "## Restrict to only the relevant columns, and Initialize the dataframe columns to np.nan:\n",
    "active_selected_spikes_df: pd.DataFrame = deepcopy(selected_spikes_df[['t_rel_seconds', 'aclu', 'Probe_Epoch_id']]).sort_values(['Probe_Epoch_id', 't_rel_seconds', 'aclu']).astype({'Probe_Epoch_id': RankOrderAnalyses._label_column_type}) # Sort by columns: 'Probe_Epoch_id' (ascending), 't_rel_seconds' (ascending), 'aclu' (ascending)\n",
    "# _pf_peak_x_column_names = ['LR_Long_pf_peak_x', 'RL_Long_pf_peak_x', 'LR_Short_pf_peak_x', 'RL_Short_pf_peak_x']\n",
    "_pf_peak_x_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "active_selected_spikes_df[_pf_peak_x_column_names] = pd.DataFrame([[RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type, RankOrderAnalyses._NaN_Type]], index=active_selected_spikes_df.index)\n",
    "shuffled_dfs = _perform_efficient_shuffle_pre_mapping(active_selected_spikes_df, decoder_aclu_peak_map_dict, num_shuffles=5)\n",
    "# shuffled_dfs\n",
    "# 5, 1.5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle 'aclu' values\n",
    "shuffled_df.loc[mask, 'aclu'] = shuffled_df.loc[mask, 'aclu'].sample(frac=1).values\n",
    "\n",
    "\n",
    "# Shuffle aclu and their corresponding peaks: ['aclu', 'long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "peak_column_names = [f'{a_decoder_name}_pf_peak_x' for a_decoder_name, a_aclu_peak_map in decoder_aclu_peak_map_dict.items()] # ['long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']\n",
    "shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']] = shuffled_df.loc[mask, ['aclu','long_LR_pf_peak_x', 'long_RL_pf_peak_x', 'short_LR_pf_peak_x', 'short_RL_pf_peak_x']].sample(frac=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d641689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_object_memory_usage(output_active_epoch_computed_values) # 0.946189 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #TODO 2023-12-13 02:07: - [ ] Figure out how 'Probe_Epoch_id' maps to `ripple_result_tuple.active_epochs`\n",
    "ripple_result_tuple.active_epochs\n",
    "rank_order_results.LR_ripple.ranked_aclus_stats_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the pf_x information for each aclu:\n",
    "## 2023-10-11 - Get the long/short peak locations\n",
    "# decoder_peak_coms_list = [a_decoder.pf.ratemap.peak_tuning_curve_center_of_masses[is_good_aclus] for a_decoder in decoder_args]\n",
    "decoder_aclu_peak_location_dict_list = [dict(zip(neuron_IDs, peak_locations)) for neuron_IDs, peak_locations in zip(track_templates.decoder_neuron_IDs_list, track_templates.decoder_peak_location_list)]\n",
    "decoder_aclu_peak_location_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de234ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.peak_tuning_curve_center_of_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1305ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.decoder_LR_pf_peak_ranks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b179f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replays:\n",
    "global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "if isinstance(global_replays, pd.DataFrame):\n",
    "\tglobal_replays = Epoch(global_replays.epochs.get_valid_df())\n",
    "\n",
    "# get the aligned epochs and the z-scores aligned to them:\n",
    "active_replay_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_replays.to_dataframe().copy(), is_laps=False)\n",
    "active_replay_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6384a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laps:\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_laps = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].laps).trimmed_to_non_overlapping()\n",
    "active_laps_epochs, (active_LR_ripple_long_z_score, active_RL_ripple_long_z_score, active_LR_ripple_short_z_score, active_RL_ripple_short_z_score) = rank_order_results.get_aligned_events(global_laps.to_dataframe(), is_laps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find only the significant events (|z| > 1.96):\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "filtered_z_score_df, (n_events, n_significant_events, percent_significant_events) = RankOrderAnalyses.find_only_significant_events(rank_order_results, high_z_criteria=1.96)\n",
    "filtered_z_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_z_score_df.index.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86532662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-11-20 - Finding high-significance periods for Kamran:\n",
    "z_threshold = 1.96\n",
    "is_greater_than_z_threshold_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) > z_threshold)\n",
    "is_greater_than_z_threshold_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) > z_threshold)\n",
    "is_significant_either = np.logical_or(is_greater_than_z_threshold_long, is_greater_than_z_threshold_short)\n",
    "is_significant_either\n",
    "\n",
    "# is_greater_than_3std_long = (np.abs(ripple_result_tuple.long_best_dir_z_score_values) >= 3.0)\n",
    "# is_greater_than_3std_short = (np.abs(ripple_result_tuple.short_best_dir_z_score_values) >= 3.0)\n",
    "# is_significant_either = np.logical_or(is_greater_than_3std_long, is_greater_than_3std_short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f925cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_ripple_epochs = deepcopy(Epoch(ripple_result_tuple.active_epochs)).boolean_indicies_slice(is_significant_either)\n",
    "# significant_ripple_epochs = deepcopy(global_replays).boolean_indicies_slice(is_significant_either)\n",
    "significant_ripple_epochs.to_dataframe()\n",
    "\n",
    "# significant_ripple_epochs.filename = Path(f'output/2023-11-27_SignificantReplayRipples').resolve()\n",
    "# significant_ripple_epochs.to_neuroscope()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8beece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_epochs = ripple_result_tuple.active_epochs\n",
    "active_epochs: Epoch = rank_order_results.RL_ripple.epochs_df # Epoch(rank_order_results.RL_ripple.epochs_df)\n",
    "# type(active_epochs)\n",
    "active_epochs.n_epochs\n",
    "# rank_order_results.RL_ripple.spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ffe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.epochs_df\n",
    "rank_order_results.LR_ripple.spikes_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39525572",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_variable_names: ['LR_Long_spearman', 'RL_Long_spearman', 'LR_Short_spearman', 'RL_Short_spearman', 'LR_Long_pearson', 'RL_Long_pearson', 'LR_Short_pearson', 'RL_Short_pearson']\n",
    "combined_variable_z_score_column_names: ['LR_Long_spearman_Z', 'RL_Long_spearman_Z', 'LR_Short_spearman_Z', 'RL_Short_spearman_Z', 'LR_Long_pearson_Z', 'RL_Long_pearson_Z', 'LR_Short_pearson_Z', 'RL_Short_pearson_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f47973",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.build_display_context_for_filtered_session(filtered_session_name='maze_any', display_fn_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b319650",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.LR_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_order_results.RL_ripple.selected_spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ebf52",
   "metadata": {},
   "source": [
    "#### Iterates through the epochs (via the slider) and saves out the images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f73ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = Path(r'C:\\Users\\pho\\Desktop\\2023-12-19 Exports').resolve()\n",
    "all_save_paths = _out_rank_order_event_raster_debugger.export_figure_all_slider_values(export_path=export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.active_epoch_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_y_values_dict = {_active_plot_identifier:{int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs} for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()}\n",
    "aclu_max_y_values_dict = {_active_plot_identifier:np.max(list({int(aclu):new_sorted_raster.neuron_y_pos[aclu] for aclu in new_sorted_raster.neuron_IDs}.values())) for _active_plot_identifier, new_sorted_raster in _out_rank_order_event_raster_debugger.plots_data.seperate_new_sorted_rasters_dict.items()} # {'long_LR': 51.48039215686274, 'long_RL': 53.5, 'short_LR': 51.48039215686274, 'short_RL': 53.5}\n",
    "global_max_y_value = np.max(list(aclu_max_y_values_dict.values()))\n",
    "global_max_y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01518ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_neurons = np.max([len(v) for v in _out_rank_order_event_raster_debugger.plots_data.unsorted_original_neuron_IDs_lists])\n",
    "max_n_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_rank_order_event_raster_debugger.plots.all_separate_plots['long_LR']['root_plot']\n",
    "\n",
    "\n",
    "root_plots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_alt_directional_merged_decoders_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d02ad",
   "metadata": {},
   "source": [
    "## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab95d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dabb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive-mode parameters:\n",
    "_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=True, scrollable_figure=True, defer_render=False)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_curr_interaction_mode_kwargs = _interactive_mode_kwargs # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interactive:\n",
    "_non_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=False, scrollable_figure=False, defer_render=True)\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=False, backend='AGG')\n",
    "_curr_interaction_mode_kwargs = _non_interactive_mode_kwargs # non-interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727307e",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Marginal Scatter Plots from `alt_directional_merged_decoders_result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "use_single_time_bin_per_epoch = False\n",
    "active_display_context = curr_active_pipeline.build_display_context_for_session('plot_all_epoch_bins_marginal_predictions', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size) # \n",
    "if use_single_time_bin_per_epoch:\n",
    "\tactive_display_context = active_display_context.adding_context_if_missing(use_single_time_bin_per_epoch=use_single_time_bin_per_epoch)\n",
    "\n",
    "# 'directional_decoded_epochs_marginals'\n",
    "collector_decoded_epoch_marginals = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs_marginals', curr_active_pipeline.get_session_context(), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=active_display_context,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsave_figure=True, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdirectional_merged_decoders_result=alt_directional_merged_decoders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae85fcb",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Marginal Yellow-Blue Plots from `alt_directional_merged_decoders_result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5876c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_context = owning_pipeline_reference.sess.get_context()\n",
    "# Build the active context directly:\n",
    "active_display_context: IdentifyingContext = curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size)\n",
    "if use_single_time_bin_per_epoch:\n",
    "\tactive_display_context = active_display_context.adding_context_if_missing(use_single_time_bin_per_epoch=use_single_time_bin_per_epoch)\n",
    "active_display_context\n",
    "\n",
    "## Plot the decoded epoch bins of the custom result:\n",
    "_out_decoded_epochs = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', curr_active_pipeline.get_session_context(), #active_display_context,\n",
    "\tmax_num_lap_epochs = 80, max_num_ripple_epochs = 100,\n",
    "\t# render_directional_marginal_laps=True, render_directional_marginal_ripples=True, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=True,\n",
    "\trender_directional_marginal_laps=False, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=False, render_track_identity_marginal_ripples=True,\n",
    "\t# constrained_layout=True, # layout='none',\n",
    "\t# build_fn='basic_view', constrained_layout=True, # 25.5s\n",
    "\tbuild_fn='insets_view', constrained_layout=True, #constrained_layout=None, layout='none', # , constrained_layout=False constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast, 4.2s\n",
    "\t**_curr_interaction_mode_kwargs, # interactive mode\n",
    "\tskip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=True, \n",
    "\tdirectional_merged_decoders_result=alt_directional_merged_decoders_result, # Custom `directional_merged_decoders_result` to use instead of the computed one.\n",
    "\t)\n",
    "collector_decoded_epochs = _out_decoded_epochs['collector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_only_keys = [item for item in active_display_context.keys() if 'lap' in item] # items exclusive to laps: ['laps_t_bin']\n",
    "ripple_only_keys = [item for item in active_display_context.keys() if 'ripple' in item]\n",
    "laps_context = active_display_context.get_subset(subset_excludelist=ripple_only_keys) # laps specific context filtering out the ripple keys\n",
    "ripple_context = active_display_context.get_subset(subset_excludelist=laps_only_keys) # ripple specific context filtering out the laps keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4169a",
   "metadata": {},
   "source": [
    "### 2024-01-19 - Build General Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## `alt_directional_merged_decoders_result`\n",
    "from PendingNotebookCode import test_build_new_marginals_df\n",
    "\n",
    "# `alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result`\n",
    "\n",
    "# laps_time_bin_marginals_df = test_build_new_marginals_df(alt_directional_merged_decoders_result)\n",
    "laps_time_bin_marginals_df: pd.DataFrame = test_build_new_marginals_df(a_decoder_result=deepcopy(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t a_track_identity_marginals=alt_directional_merged_decoders_result.laps_directional_marginals_tuple[0]\n",
    "\t\t\t\t\t\t\t )\n",
    "laps_time_bin_marginals_df\n",
    "\n",
    "ripple_time_bin_marginals_df: pd.DataFrame = test_build_new_marginals_df(a_decoder_result=deepcopy(alt_directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result),\n",
    "\t\t\t\t\t\t\t\t\t\t\t a_track_identity_marginals=alt_directional_merged_decoders_result.ripple_directional_marginals_tuple[0]\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "ripple_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from flexitext import flexitext ## flexitext for formatted matplotlib text\n",
    "\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText\n",
    "\n",
    "\n",
    "perform_write_to_file_callback = None\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = deepcopy(laps_time_bin_marginals_df)\n",
    "ripple_all_epoch_bins_marginals_df = deepcopy(ripple_time_bin_marginals_df)\n",
    "\n",
    "if active_context is not None:\n",
    "\tdisplay_context = active_context.adding_context('display_fn', display_fn_name='plot_all_epoch_bins_marginal_predictions')\n",
    "\t\n",
    "# These subset contexts are used to filter out lap/ripple only keys.\n",
    "# e.g. active_context=curr_active_pipeline.build_display_context_for_session('directional_merged_pf_decoded_epochs', laps_t_bin=laps_decoding_time_bin_size, ripple_t_bin=ripple_decoding_time_bin_size)\n",
    "\t# only want laps_t_bin on the laps plot and ripple_t_bin on the ripples plot\n",
    "laps_only_keys = [item for item in display_context.keys() if 'lap' in item] # items exclusive to laps: ['laps_t_bin']\n",
    "ripple_only_keys = [item for item in display_context.keys() if 'ripple' in item]\n",
    "laps_display_context = display_context.get_subset(subset_excludelist=ripple_only_keys) # laps specific context filtering out the ripple keys\n",
    "ripple_display_context = display_context.get_subset(subset_excludelist=laps_only_keys) # ripple specific context filtering out the laps keys\n",
    "\n",
    "\n",
    "with mpl.rc_context({'figure.figsize': (12.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42,\n",
    "\t\t\t\t\t\t\"axes.spines.left\": False, \"axes.spines.right\": False, \"axes.spines.bottom\": False, \"axes.spines.top\": False,\n",
    "\t\t\t\t\t\t\"axes.edgecolor\": \"none\", \"xtick.bottom\": False, \"xtick.top\": False, \"ytick.left\": False, \"ytick.right\": False}):\n",
    "\t# Create a FigureCollector instance\n",
    "\twith FigureCollector(name='plot_all_epoch_bins_marginal_predictions', base_context=display_context) as collector:\n",
    "\n",
    "\t\t## Define common operations to do after making the figure:\n",
    "\t\tdef setup_common_after_creation(a_collector, fig, axes, sub_context, title=f'<size:22> Sig. (>0.95) <weight:bold>Best</> <weight:bold>Quantile Diff</></>'):\n",
    "\t\t\t\"\"\" Captures:\n",
    "\n",
    "\t\t\tt_split, t_start, t_end)\n",
    "\t\t\t\"\"\"\n",
    "\t\t\ta_collector.contexts.append(sub_context)\n",
    "\t\t\t\n",
    "\t\t\tfor ax in (axes if isinstance(axes, Iterable) else [axes]):\n",
    "\t\t\t\t# Update the xlimits with the new bounds\n",
    "\t\t\t\tax.set_ylim(0.0, 1.0)\n",
    "\t\t\t\t# Add epoch indicators\n",
    "\t\t\t\t_tmp_output_dict = PlottingHelpers.helper_matplotlib_add_long_short_epoch_indicator_regions(ax=ax, t_split=t_delta, t_start=t_start, t_end=t_end)\n",
    "\t\t\t\t# Update the xlimits with the new bounds\n",
    "\t\t\t\tax.set_xlim(t_start, t_end)\n",
    "\t\t\t\t# Draw a horizontal line at y=0.5\n",
    "\t\t\t\tax.axhline(y=0.5, color=(0,0,0,1)) # , linestyle='--'\n",
    "\t\t\t\t## This is figure level stuff and only needs to be done once:\n",
    "\t\t\t\t# `flexitext` version:\n",
    "\t\t\t\ttext_formatter = FormattedFigureText()\n",
    "\t\t\t\tax.set_title('')\n",
    "\t\t\t\tfig.suptitle('')\n",
    "\t\t\t\t# top=0.84, bottom=0.125, left=0.07, right=0.97,\n",
    "\t\t\t\t# text_formatter.setup_margins(fig, top_margin=1.0, left_margin=0.0, right_margin=1.0, bottom_margin=0.05)\n",
    "\t\t\t\ttext_formatter.setup_margins(fig, top_margin=0.84, left_margin=0.07, right_margin=0.97, bottom_margin=0.125)\n",
    "\t\t\t\t# fig.subplots_adjust(top=top_margin, left=left_margin, right=right_margin, bottom=bottom_margin)\n",
    "\t\t\t\t# title_text_obj = flexitext(text_formatter.left_margin, text_formatter.top_margin, title, va=\"bottom\", xycoords=\"figure fraction\")\n",
    "\t\t\t\ttitle_text_obj = flexitext(text_formatter.left_margin, 0.98, title, va=\"top\", xycoords=\"figure fraction\") # 0.98, va=\"top\" means the top edge of the title will be aligned to the fig_y=0.98 mark of the figure.\n",
    "\t\t\t\t# footer_text_obj = flexitext((text_formatter.left_margin * 0.1), (text_formatter.bottom_margin * 0.25),\n",
    "\t\t\t\t#                             text_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t#                             va=\"top\", xycoords=\"figure fraction\")\n",
    "\n",
    "\t\t\t\tfooter_text_obj = flexitext((text_formatter.left_margin * 0.1), (0.0025), ## (va=\"bottom\", (0.0025)) - this means that the bottom edge of the footer text is aligned with the fig_y=0.0025 in figure space\n",
    "\t\t\t\t\t\t\t\t\t\t\ttext_formatter._build_footer_string(active_context=sub_context),\n",
    "\t\t\t\t\t\t\t\t\t\t\tva=\"bottom\", xycoords=\"figure fraction\")\n",
    "\t\t\n",
    "\t\t\tif ((perform_write_to_file_callback is not None) and (sub_context is not None)):\n",
    "\t\t\t\tperform_write_to_file_callback(sub_context, fig)\n",
    "\t\t\t\n",
    "\t\t# Plot for BestDir\n",
    "\t\tfig, ax = collector.subplots(num='Laps_Marginal', clear=True)\n",
    "\t\t_out_Laps = sns.scatterplot(\n",
    "\t\t\tax=ax,\n",
    "\t\t\tdata=laps_all_epoch_bins_marginals_df,\n",
    "\t\t\tx='t_bin_center',\n",
    "\t\t\ty='P_Long',\n",
    "\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t)\n",
    "\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=laps_display_context.adding_context('subplot', subplot_name='Laps all_epoch_binned Marginals'), \n",
    "\t\t\t\t\t\t\t\t\ttitle=f'<size:22> Laps <weight:bold>all_epoch_binned</> Marginals</>')\n",
    "\t\t\n",
    "\t\tfig, ax = collector.subplots(num='Ripple_Marginal', clear=True)\n",
    "\t\t_out_Ripple = sns.scatterplot(\n",
    "\t\t\tax=ax,\n",
    "\t\t\tdata=ripple_all_epoch_bins_marginals_df,\n",
    "\t\t\tx='t_bin_center',\n",
    "\t\t\ty='P_Long',\n",
    "\t\t\t# size='LR_Long_rel_num_cells',  # Use the 'size' parameter for variable marker sizes\n",
    "\t\t)\n",
    "\t\tsetup_common_after_creation(collector, fig=fig, axes=ax, sub_context=ripple_display_context.adding_context('subplot', subplot_name='Ripple all_epoch_binned Marginals'), \n",
    "\t\t\t\t\t\ttitle=f'<size:22> Ripple <weight:bold>all_epoch_binned</> Marginals</>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d655560",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_time_bin_marginals_df['lap_idx'] = laps_time_bin_marginals_df.index.to_numpy()\n",
    "laps_time_bin_marginals_df['lap_start_t'] = laps_epochs_df['start'].to_numpy()\n",
    "laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3800d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-01-19 - Can decode position from the pseudo2D posterior directly, or by using the pseudo2D decoder to determine the best direction and track_id and use the corresponding 1D decoder's predicted position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f76dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-01-19 - Export All Epoch Time bin marginals to CSV also\n",
    "## Laps:\n",
    "laps_epochs_df: pd.DataFrame = deepcopy(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs).to_dataframe()\n",
    "laps_directional_marginals_tuple = DirectionalMergedDecodersResult.determine_directional_likelihoods(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals = DirectionalMergedDecodersResult.determine_long_short_likelihoods(alt_directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "track_identity_marginals, track_identity_all_epoch_bins_marginal, most_likely_track_identity_from_decoder, is_most_likely_track_identity_Long = laps_track_identity_marginals\n",
    "\n",
    "laps_marginals_df: pd.DataFrame = pd.DataFrame(np.hstack((laps_directional_all_epoch_bins_marginal, track_identity_all_epoch_bins_marginal)), columns=['P_LR', 'P_RL', 'P_Long', 'P_Short'])\n",
    "laps_marginals_df['lap_idx'] = laps_marginals_df.index.to_numpy()\n",
    "laps_marginals_df['lap_start_t'] = laps_epochs_df['start'].to_numpy()\n",
    "laps_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78fbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(laps_marginals_df)\n",
    "laps_marginals_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local computation: check laps\n",
    "laps = curr_active_pipeline.sess.laps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(start=0.030, step=0.01, stop=0.10) # [0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc018065",
   "metadata": {},
   "source": [
    "# Call perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c6522e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "\n",
    "BATCH_DATE_TO_USE: str = '2024-02-02_Lab' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "# collected_outputs_path = Path('/nfs/turbo/umms-kdiba/Data/Output/collected_outputs').resolve() # Linux\n",
    "# collected_outputs_path: Path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs').resolve() # GreatLakes\n",
    "collected_outputs_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs').resolve() # Apogee\n",
    "\n",
    "\n",
    "def perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict, save_hdf=True, save_csvs=True) -> dict:\n",
    "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    print(f'perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...,across_session_results_extended_dict: {across_session_results_extended_dict})')\n",
    "    from copy import deepcopy\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from neuropy.utils.debug_helpers import parameter_sweeps\n",
    "    from neuropy.core.laps import Laps\n",
    "    from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _check_result_laps_epochs_df_performance\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalMergedDecodersResult\n",
    "    from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "    # Export CSVs:\n",
    "    def export_marginals_df_csv(marginals_df: pd.DataFrame, data_identifier_str: str, parent_output_path: Path, active_context):\n",
    "        \"\"\" captures nothing\n",
    "        \"\"\"\n",
    "        # output_date_str: str = get_now_rounded_time_str()\n",
    "        output_date_str: str = get_now_day_str()\n",
    "        # parent_output_path: Path = Path('output').resolve()\n",
    "        # active_context = curr_active_pipeline.get_session_context()\n",
    "        session_identifier_str: str = active_context.get_description()\n",
    "        assert output_date_str is not None\n",
    "        out_basename = '-'.join([output_date_str, session_identifier_str, data_identifier_str]) # '2024-01-04|kdiba_gor01_one_2006-6-09_1-22-43|(laps_marginals_df).csv'\n",
    "        out_filename = f\"{out_basename}.csv\"\n",
    "        out_path = parent_output_path.joinpath(out_filename).resolve()\n",
    "        marginals_df.to_csv(out_path)\n",
    "        return out_path \n",
    "\n",
    "\n",
    "    def _subfn_process_time_bin_swept_results(curr_active_pipeline, output_extracted_result_tuples):\n",
    "        \"\"\" After the sweeps are complete and multiple (one for each time_bin_size swept) indepdnent dfs are had with the four results types this function concatenates each of the four into a single dataframe for all time_bin_size values with a column 'time_bin_size'. \n",
    "        It also saves them out to CSVs in a manner similar to what `compute_and_export_marginals_dfs_completion_function` did to be compatible with `2024-01-23 - Across Session Point and YellowBlue Marginal CSV Exports.ipynb`\n",
    "        Captures: save_csvs\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        several_time_bin_sizes_laps_time_bin_marginals_df_list = []\n",
    "        several_time_bin_sizes_laps_per_epoch_marginals_df_list = []\n",
    "\n",
    "        several_time_bin_sizes_ripple_time_bin_marginals_df_list = []\n",
    "        several_time_bin_sizes_ripple_per_epoch_marginals_df_list = []\n",
    "\n",
    "\n",
    "        # for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
    "        for a_sweep_tuple, (a_laps_time_bin_marginals_df, a_laps_all_epoch_bins_marginals_df, a_ripple_time_bin_marginals_df, a_ripple_all_epoch_bins_marginals_df) in output_extracted_result_tuples.items():\n",
    "            a_sweep_dict = dict(a_sweep_tuple)\n",
    "            \n",
    "            # Shared\n",
    "            desired_laps_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
    "            desired_ripple_decoding_time_bin_size = float(a_sweep_dict['desired_shared_decoding_time_bin_size'])\n",
    "            \n",
    "            # a_laps_time_bin_marginals_df.\n",
    "            df = a_laps_time_bin_marginals_df\n",
    "            df['time_bin_size'] = desired_laps_decoding_time_bin_size # desired_laps_decoding_time_bin_size\n",
    "            # df['session_name'] = session_name\n",
    "            df = a_laps_all_epoch_bins_marginals_df\n",
    "            df['time_bin_size'] = desired_laps_decoding_time_bin_size\n",
    "\n",
    "            df = a_ripple_time_bin_marginals_df\n",
    "            df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
    "            df = a_ripple_all_epoch_bins_marginals_df\n",
    "            df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
    "\n",
    "            several_time_bin_sizes_laps_time_bin_marginals_df_list.append(a_laps_time_bin_marginals_df)\n",
    "            several_time_bin_sizes_laps_per_epoch_marginals_df_list.append(a_laps_all_epoch_bins_marginals_df)\n",
    "            \n",
    "            several_time_bin_sizes_ripple_time_bin_marginals_df_list.append(a_ripple_time_bin_marginals_df)\n",
    "            several_time_bin_sizes_ripple_per_epoch_marginals_df_list.append(a_ripple_all_epoch_bins_marginals_df)\n",
    "\n",
    "\n",
    "        ## Build across_sessions join dataframes:\n",
    "        several_time_bin_sizes_time_bin_laps_df: pd.DataFrame = pd.concat(several_time_bin_sizes_laps_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
    "        several_time_bin_sizes_laps_df: pd.DataFrame = pd.concat(several_time_bin_sizes_laps_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
    "\n",
    "        several_time_bin_sizes_time_bin_ripple_df: pd.DataFrame = pd.concat(several_time_bin_sizes_ripple_time_bin_marginals_df_list, axis='index', ignore_index=True)\n",
    "        several_time_bin_sizes_ripple_df: pd.DataFrame = pd.concat(several_time_bin_sizes_ripple_per_epoch_marginals_df_list, axis='index', ignore_index=True) # per epoch\n",
    "\n",
    "        # Export time_bin_swept results to CSVs:\n",
    "        if save_csvs:\n",
    "            assert collected_outputs_path.exists()\n",
    "            active_context = curr_active_pipeline.get_session_context()\n",
    "            laps_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_laps_df, data_identifier_str=f'(laps_time_bin_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            laps_out_path = export_marginals_df_csv(several_time_bin_sizes_laps_df, data_identifier_str=f'(laps_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            ripple_time_bin_marginals_out_path = export_marginals_df_csv(several_time_bin_sizes_time_bin_ripple_df, data_identifier_str=f'(ripple_time_bin_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "            ripple_out_path = export_marginals_df_csv(several_time_bin_sizes_ripple_df, data_identifier_str=f'(ripple_marginals_df)', parent_output_path=collected_outputs_path, active_context=active_context)\n",
    "        else:\n",
    "            laps_time_bin_marginals_out_path, laps_out_path, ripple_time_bin_marginals_out_path, ripple_out_path = None, None, None, None\n",
    "            \n",
    "        return (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
    "        # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path)\n",
    "        \n",
    "\n",
    "\n",
    "    def add_session_df_columns(df: pd.DataFrame, session_name: str, curr_session_t_delta: Optional[float], time_col: str) -> pd.DataFrame:\n",
    "        \"\"\" adds session-specific information to the marginal dataframes \"\"\"\n",
    "        df['session_name'] = session_name \n",
    "        if curr_session_t_delta is not None:\n",
    "            df['delta_aligned_start_t'] = df[time_col] - curr_session_t_delta\n",
    "        return df\n",
    "\n",
    "\n",
    "    ## Single decode:\n",
    "    def _try_single_decode(owning_pipeline_reference, directional_merged_decoders_result, use_single_time_bin_per_epoch: bool, desired_laps_decoding_time_bin_size: Optional[float]=None, desired_ripple_decoding_time_bin_size: Optional[float]=None, desired_shared_decoding_time_bin_size: Optional[float]=None, minimum_event_duration: Optional[float]=None):\n",
    "        \"\"\" decodes laps and ripples for a single bin size. \n",
    "        \n",
    "        minimum_event_duration: if provided, excludes all events shorter than minimum_event_duration\n",
    "        \"\"\"\n",
    "        if desired_shared_decoding_time_bin_size is not None:\n",
    "            assert desired_laps_decoding_time_bin_size is None\n",
    "            assert desired_ripple_decoding_time_bin_size is None\n",
    "            desired_laps_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
    "            desired_ripple_decoding_time_bin_size = desired_shared_decoding_time_bin_size\n",
    "            \n",
    "\n",
    "        ## Decode Laps:\n",
    "        laps_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result.filter_epochs)\n",
    "        if not isinstance(laps_epochs_df, pd.DataFrame):\n",
    "            laps_epochs_df = laps_epochs_df.to_dataframe()\n",
    "        # global_any_laps_epochs_obj = deepcopy(owning_pipeline_reference.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any' (? same as global_epoch_name?)\n",
    "        min_possible_laps_time_bin_size: float = find_minimum_time_bin_duration(laps_epochs_df['duration'].to_numpy())\n",
    "        min_bounded_laps_decoding_time_bin_size: float = min(desired_laps_decoding_time_bin_size, min_possible_laps_time_bin_size) # 10ms # 0.002\n",
    "        if desired_laps_decoding_time_bin_size < min_bounded_laps_decoding_time_bin_size:\n",
    "            print(f'WARN: desired_laps_decoding_time_bin_size: {desired_laps_decoding_time_bin_size} < min_bounded_laps_decoding_time_bin_size: {min_bounded_laps_decoding_time_bin_size}... hopefully it works.')\n",
    "        laps_decoding_time_bin_size: float = desired_laps_decoding_time_bin_size # allow direct use\n",
    "        if use_single_time_bin_per_epoch:\n",
    "            laps_decoding_time_bin_size = None\n",
    "        directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=laps_epochs_df,\n",
    "                                                                                                                                                        decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "        \n",
    "\n",
    "        ## Decode Ripples:\n",
    "        if desired_ripple_decoding_time_bin_size is not None:\n",
    "            # global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(owning_pipeline_reference.filtered_sessions[global_epoch_name].replay))\n",
    "            replay_epochs_df = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.filter_epochs)\n",
    "            if not isinstance(replay_epochs_df, pd.DataFrame):\n",
    "                replay_epochs_df = replay_epochs_df.to_dataframe()\n",
    "            # min_possible_ripple_time_bin_size: float = find_minimum_time_bin_duration(replay_epochs_df['duration'].to_numpy())\n",
    "            # min_bounded_ripple_decoding_time_bin_size: float = min(desired_ripple_decoding_time_bin_size, min_possible_ripple_time_bin_size) # 10ms # 0.002\n",
    "            # if desired_ripple_decoding_time_bin_size < min_bounded_ripple_decoding_time_bin_size:\n",
    "            #     print(f'WARN: desired_ripple_decoding_time_bin_size: {desired_ripple_decoding_time_bin_size} < min_bounded_ripple_decoding_time_bin_size: {min_bounded_ripple_decoding_time_bin_size}... hopefully it works.')\n",
    "            ripple_decoding_time_bin_size: float = desired_ripple_decoding_time_bin_size # allow direct use            \n",
    "            ## Drop those less than the time bin duration\n",
    "            print(f'DropShorterMode:')\n",
    "            pre_drop_n_epochs = len(replay_epochs_df)\n",
    "            if minimum_event_duration is not None:                \n",
    "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > minimum_event_duration]\n",
    "                post_drop_n_epochs = len(replay_epochs_df)\n",
    "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
    "                print(f'\\tminimum_event_duration present (minimum_event_duration={minimum_event_duration}).\\n\\tdropping {n_dropped_epochs} that are shorter than our minimum_event_duration of {minimum_event_duration}.', end='\\t')\n",
    "            else:\n",
    "                replay_epochs_df = replay_epochs_df[replay_epochs_df['duration'] > desired_ripple_decoding_time_bin_size]\n",
    "                post_drop_n_epochs = len(replay_epochs_df)\n",
    "                n_dropped_epochs = post_drop_n_epochs - pre_drop_n_epochs\n",
    "                print(f'\\tdropping {n_dropped_epochs} that are shorter than our ripple decoding time bin size of {desired_ripple_decoding_time_bin_size}', end='\\t') \n",
    "\n",
    "            print(f'{post_drop_n_epochs} remain.')\n",
    "            directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result = directional_merged_decoders_result.all_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(owning_pipeline_reference.sess.spikes_df), filter_epochs=replay_epochs_df,\n",
    "                                                                                                                                                                                            decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "\n",
    "        directional_merged_decoders_result.perform_compute_marginals()\n",
    "        return directional_merged_decoders_result\n",
    "        \n",
    "\n",
    "    def _update_result_laps(a_result: DecodedFilterEpochsResult, laps_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" captures nothing. Can reusing the same laps_df as it makes no modifications to it. \n",
    "        \n",
    "        e.g. a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple]\n",
    "        \"\"\"\n",
    "        result_laps_epochs_df: pd.DataFrame = a_result.laps_epochs_df\n",
    "        ## 2024-01-17 - Updates the `a_directional_merged_decoders_result.laps_epochs_df` with both the ground-truth values and the decoded predictions\n",
    "        result_laps_epochs_df['maze_id'] = laps_df['maze_id'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
    "        ## add the 'is_LR_dir' groud-truth column in:\n",
    "        result_laps_epochs_df['is_LR_dir'] = laps_df['is_LR_dir'].to_numpy()[np.isin(laps_df['lap_id'], result_laps_epochs_df['lap_id'])] # this works despite the different size because of the index matching\n",
    "        \n",
    "        laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir = a_result.laps_directional_marginals_tuple\n",
    "        laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = a_result.laps_track_identity_marginals_tuple\n",
    "        ## Add the decoded results to the laps df:\n",
    "        result_laps_epochs_df['is_most_likely_track_identity_Long'] = laps_is_most_likely_track_identity_Long\n",
    "        result_laps_epochs_df['is_most_likely_direction_LR'] = laps_is_most_likely_direction_LR_dir\n",
    "        return result_laps_epochs_df\n",
    "\n",
    "    # BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "    assert collected_outputs_path.exists()\n",
    "    curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "    CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "    print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "    active_context = curr_active_pipeline.get_session_context()\n",
    "    session_ctxt_key:str = active_context.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "    \n",
    "    ## INPUT PARAMETER: time_bin_size sweep paraemters\n",
    "    desired_shared_decoding_time_bin_size = np.linspace(start=0.030, stop=0.10, num=6)\n",
    "    \n",
    "    # Shared time bin sizes\n",
    "    # all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_sizes, use_single_time_bin_per_epoch=[False], desired_ripple_decoding_time_bin_size=[None])\n",
    "    all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_shared_decoding_time_bin_size=desired_shared_decoding_time_bin_size, use_single_time_bin_per_epoch=[False], minimum_event_duration=[desired_shared_decoding_time_bin_size[-1]]) # with Ripples\n",
    "    # len(all_param_sweep_options)\n",
    "    \n",
    "    ## Perfrom the computations:\n",
    "\n",
    "    # DirectionalMergedDecoders: Get the result after computation:\n",
    "    ## Copy the default result:\n",
    "    directional_merged_decoders_result: DirectionalMergedDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "    alt_directional_merged_decoders_result: DirectionalMergedDecodersResult = deepcopy(directional_merged_decoders_result)\n",
    "\n",
    "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size-{laps_decoding_time_bin_size}_{data_identifier_str}\"\n",
    "    # out_path_basename_str: str = f\"{now_day_str}_{active_context}_time_bin_size_sweep_results\"\n",
    "    out_path_basename_str: str = f\"{CURR_BATCH_OUTPUT_PREFIX}_time_bin_size_sweep_results\"\n",
    "    # out_path_filenname_str: str = f\"{out_path_basename_str}.csv\"\n",
    "\n",
    "    out_path_filenname_str: str = f\"{out_path_basename_str}.h5\"\n",
    "    out_path: Path = collected_outputs_path.resolve().joinpath(out_path_filenname_str).resolve()\n",
    "    print(f'\\out_path_str: \"{out_path_filenname_str}\"')\n",
    "    print(f'\\tout_path: \"{out_path}\"')\n",
    "    \n",
    "    # Ensure it has the 'lap_track' column\n",
    "    ## Compute the ground-truth information using the position information:\n",
    "    # adds columns: ['maze_id', 'is_LR_dir']\n",
    "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "    laps_obj: Laps = curr_active_pipeline.sess.laps\n",
    "    laps_obj.update_lap_dir_from_smoothed_velocity(pos_input=curr_active_pipeline.sess.position)\n",
    "    laps_obj.update_maze_id_if_needed(t_start=t_start, t_delta=t_delta, t_end=t_end)\n",
    "    laps_df = laps_obj.to_dataframe()\n",
    "    \n",
    "    # Uses: session_ctxt_key, all_param_sweep_options\n",
    "    output_alt_directional_merged_decoders_result = {} # empty dict\n",
    "    output_laps_decoding_accuracy_results_dict = {} # empty dict\n",
    "    output_extracted_result_tuples = {}\n",
    "\n",
    "    for a_sweep_dict in all_param_sweep_options:\n",
    "        a_sweep_tuple = frozenset(a_sweep_dict.items())\n",
    "        print(f'a_sweep_dict: {a_sweep_dict}')\n",
    "        # Convert parameters to string because Parquet supports metadata as string\n",
    "        a_sweep_str_params = {key: str(value) for key, value in a_sweep_dict.items() if value is not None}\n",
    "        \n",
    "        output_alt_directional_merged_decoders_result[a_sweep_tuple] = _try_single_decode(curr_active_pipeline, alt_directional_merged_decoders_result, **a_sweep_dict)\n",
    "\n",
    "        laps_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_time_bin_marginals_df.copy()\n",
    "        laps_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_all_epoch_bins_marginals_df.copy()\n",
    "        \n",
    "        ## Ripples:\n",
    "        ripple_time_bin_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_time_bin_marginals_df.copy()\n",
    "        ripple_all_epoch_bins_marginals_df: pd.DataFrame = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_all_epoch_bins_marginals_df.copy()\n",
    "\n",
    "        session_name = curr_session_name\n",
    "        curr_session_t_delta = t_delta\n",
    "        \n",
    "        for a_df, a_time_bin_column_name in zip((laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df), ('t_bin_center', 'lap_start_t', 't_bin_center', 'ripple_start_t')):\n",
    "            ## Add the session-specific columns:\n",
    "            a_df = add_session_df_columns(a_df, session_name, curr_session_t_delta, a_time_bin_column_name)\n",
    "\n",
    "        ## Build the output tuple:\n",
    "        output_extracted_result_tuples[a_sweep_tuple] = (laps_time_bin_marginals_df, laps_all_epoch_bins_marginals_df, ripple_time_bin_marginals_df, ripple_all_epoch_bins_marginals_df)\n",
    "        \n",
    "        # desired_laps_decoding_time_bin_size_str: str = a_sweep_str_params.get('desired_laps_decoding_time_bin_size', None)\n",
    "        laps_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].laps_decoding_time_bin_size\n",
    "        # ripple_decoding_time_bin_size: float = output_alt_directional_merged_decoders_result[a_sweep_tuple].ripple_decoding_time_bin_size\n",
    "        actual_laps_decoding_time_bin_size_str: str = str(laps_decoding_time_bin_size)\n",
    "        if save_hdf and (actual_laps_decoding_time_bin_size_str is not None):\n",
    "            laps_time_bin_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_time_bin_marginals_df', format='table', data_columns=True)\n",
    "            laps_all_epoch_bins_marginals_df.to_hdf(out_path, key=f'{session_ctxt_key}/{actual_laps_decoding_time_bin_size_str}/laps_all_epoch_bins_marginals_df', format='table', data_columns=True)\n",
    "\n",
    "        ## TODO: output ripple .h5 here if desired.\n",
    "            \n",
    "\n",
    "        # get the current lap object and determine the percentage correct:\n",
    "        result_laps_epochs_df: pd.DataFrame = _update_result_laps(a_result=output_alt_directional_merged_decoders_result[a_sweep_tuple], laps_df=laps_df)\n",
    "        (is_decoded_track_correct, is_decoded_dir_correct, are_both_decoded_properties_correct), (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly) = _check_result_laps_epochs_df_performance(result_laps_epochs_df)\n",
    "        output_laps_decoding_accuracy_results_dict[laps_decoding_time_bin_size] = (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly)\n",
    "        \n",
    "\n",
    "    ## Output the performance:\n",
    "    output_laps_decoding_accuracy_results_df: pd.DataFrame = pd.DataFrame(output_laps_decoding_accuracy_results_dict.values(), index=output_laps_decoding_accuracy_results_dict.keys(), \n",
    "                    columns=['percent_laps_track_identity_estimated_correctly',\n",
    "                            'percent_laps_direction_estimated_correctly',\n",
    "                            'percent_laps_estimated_correctly'])\n",
    "    output_laps_decoding_accuracy_results_df.index.name = 'laps_decoding_time_bin_size'\n",
    "    ## Save out the laps peformance result\n",
    "    if save_hdf:\n",
    "        output_laps_decoding_accuracy_results_df.to_hdf(out_path, key=f'{session_ctxt_key}/laps_decoding_accuracy_results', format='table', data_columns=True)\n",
    "\n",
    "    ## Call the subfunction to process the time_bin_size swept result and produce combined output dataframes:\n",
    "    combined_multi_timebin_outputs_tuple = _subfn_process_time_bin_swept_results(curr_active_pipeline, output_extracted_result_tuples)\n",
    "    # Unpacking:    \n",
    "    # (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
    "\n",
    "    # add to output dict\n",
    "    # across_session_results_extended_dict['compute_and_export_marginals_dfs_completion_function'] = _out\n",
    "    across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function'] = (out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple)\n",
    "    # can unpack like:\n",
    "    (several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
    "\n",
    "    print(f'>>\\t done with {curr_session_context}')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "    return across_session_results_extended_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "773e9f51",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f6b31",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(None, None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, save_hdf=False)\n",
    "out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a71abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file_pat\n",
    "collected_outputs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_laps_decoding_accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_histograms( data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str ) -> None:\n",
    "#     # get the pre-delta epochs\n",
    "#     pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "#     post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "#     descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "#     # plot pre-delta histogram\n",
    "#     pre_delta_df.hist(column='P_Long')\n",
    "#     plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "#     plt.show()\n",
    "\n",
    "#     # plot post-delta histogram\n",
    "#     post_delta_df.hist(column='P_Long')\n",
    "#     plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "def plot_histograms(data_type: str, session_spec: str, data_results_df: pd.DataFrame, time_bin_duration_str: str) -> None:\n",
    "    \"\"\" plots a stacked histogram of the many time-bin sizes \"\"\"\n",
    "    # get the pre-delta epochs\n",
    "    pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "    post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "    descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "    \n",
    "    # plot pre-delta histogram\n",
    "    time_bin_sizes = pre_delta_df['time_bin_size'].unique()\n",
    "    \n",
    "    figure_identifier: str = f\"{descriptor_str}_preDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = pre_delta_df[pre_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot post-delta histogram\n",
    "    time_bin_sizes = post_delta_df['time_bin_size'].unique()\n",
    "    figure_identifier: str = f\"{descriptor_str}_postDelta\"\n",
    "    plt.figure(num=figure_identifier, clear=True, figsize=(6, 2))\n",
    "    for time_bin_size in time_bin_sizes:\n",
    "        df_tbs = post_delta_df[post_delta_df['time_bin_size']==time_bin_size]\n",
    "        df_tbs['P_Long'].hist(alpha=0.5, label=str(time_bin_size)) \n",
    "    \n",
    "    plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# # You can use it like this:\n",
    "# plot_histograms('Laps', 'All Sessions', all_sessions_laps_time_bin_df, \"75 ms\")\n",
    "# plot_histograms('Ripples', 'All Sessions', all_sessions_ripple_time_bin_df, \"75 ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from neuropy.utils.matplotlib_helpers import pho_jointplot\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# def pho_jointplot(*args, **kwargs):\n",
    "# \t\"\"\" wraps sns.jointplot to allow adding titles/axis labels/etc.\"\"\"\n",
    "# \ttitle = kwargs.pop('title', None)\n",
    "# \t_out = sns.jointplot(*args, **kwargs)\n",
    "# \tif title is not None:\n",
    "# \t\tplt.suptitle(title)\n",
    "# \treturn _out\n",
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "plot_histograms('Ripples', 'One Session', several_time_bin_sizes_time_bin_ripple_df, \"several\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "several_time_bin_sizes_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(\n",
    "#     several_time_bin_sizes_laps_df, x=\"P_Long\", col=\"species\", row=\"time_bin_size\",\n",
    "#     binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    "# )\n",
    "\n",
    "sns.displot(\n",
    "    several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', row=\"time_bin_size\",\n",
    "    binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be351c18",
   "metadata": {},
   "source": [
    "# 2024-01-31 - Reinvestigation regarding remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911d7495",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncationCheckingResults(_VersionedResultMixin_version='2024.01.10_0', disappearing_endcap_aclus=Int64Index([15, 23, 31, 40, 49, 83, 95], dtype='int64'), non_disappearing_endcap_aclus=Int64Index([4, 5, 8, 9, 13, 19, 20, 24, 32, 33, 38, 41, 43, 50, 52, 53, 54, 56, 60, 62, 64, 68, 69, 71, 73, 74, 75, 76, 79, 84, 85, 92, 97], dtype='int64'), significant_distant_remapping_endcap_aclus=Int64Index([8, 9, 19, 20, 24, 32, 38, 41, 43, 50, 56, 60, 62, 64, 69, 71, 73, 74, 76, 79, 84, 85, 92, 97], dtype='int64'), minor_remapping_endcap_aclus=Int64Index([4, 5, 13, 33, 52, 53, 54, 68, 75], dtype='int64'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7907651",
   "metadata": {},
   "source": [
    "### _perform_long_short_pf_overlap_analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.dynamic_parameters import DynamicParameters\n",
    "# '_perform_long_short_pf_overlap_analyses'\n",
    "\n",
    "## Unwrapping `short_long_pf_overlap_analyses`:\n",
    "short_long_pf_overlap_analyses: DynamicParameters = curr_active_pipeline.global_computation_results.computed_data['short_long_pf_overlap_analyses']\n",
    "short_long_neurons_diff = short_long_pf_overlap_analyses['short_long_neurons_diff']\n",
    "poly_overlap_df = short_long_pf_overlap_analyses['poly_overlap_df']\n",
    "conv_overlap_dict = short_long_pf_overlap_analyses['conv_overlap_dict']\n",
    "conv_overlap_scalars_df = short_long_pf_overlap_analyses['conv_overlap_scalars_df']\n",
    "product_overlap_dict = short_long_pf_overlap_analyses['product_overlap_dict']\n",
    "product_overlap_scalars_df = short_long_pf_overlap_analyses['product_overlap_scalars_df']\n",
    "relative_entropy_overlap_dict = short_long_pf_overlap_analyses['relative_entropy_overlap_dict']\n",
    "relative_entropy_overlap_scalars_df = short_long_pf_overlap_analyses['relative_entropy_overlap_scalars_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848dc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_conv_overlap_dict = {aclu:an_aclu_conv_overlap_output for aclu, an_aclu_conv_overlap_output in conv_overlap_dict.items() if an_aclu_conv_overlap_output is not None}\n",
    "valid_convolved_results = [an_aclu_conv_overlap_output['full'] for aclu, an_aclu_conv_overlap_output in valid_conv_overlap_dict.items()]\n",
    "valid_x_convolved_results = [an_aclu_result['x'] for an_aclu_result in valid_convolved_results]\n",
    "valid_aclu_normalized_convolved_results = [an_aclu_result['normalized_convolved_result'] for an_aclu_result in valid_convolved_results]\n",
    "# valid_aclu_normalized_convolved_results = [an_aclu_conv_overlap_output['valid_subset']['normalized_convolved_result'] for aclu, an_aclu_conv_overlap_output in valid_conv_overlap_dict.items()]\n",
    "valid_convolved_results_df = pd.DataFrame(valid_convolved_results)\n",
    "# valid_convolved_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import pho_plothelper\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "# Define the palette as a list to specify exact values\n",
    "palette = sns.color_palette(\"rocket_r\")\n",
    "\n",
    "extracted_value_kwargs = pho_plothelper(data=an_aclu_conv_overlap_output['valid_subset'], x='x', y='normalized_convolved_result')\n",
    "extracted_value_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d346a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure('Full normalized_convolved_results', clear=True)\n",
    "out_axs = []\n",
    "for idx, row in valid_convolved_results_df.iterrows():\n",
    "    out_axs.append(plt.plot(row['x'], row['normalized_convolved_result']))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d9b54",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## From Jonathan Long/Short Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3641aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df = deepcopy(jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "\n",
    "## try to add the 2D peak information to the cells in `neuron_replay_stats_df`:\n",
    "neuron_replay_stats_df['long_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_x'] = pd.NA\n",
    "neuron_replay_stats_df['long_pf2D_peak_y'] = pd.NA\n",
    "neuron_replay_stats_df['short_pf2D_peak_y'] = pd.NA\n",
    "\n",
    "# flat_peaks_df: pd.DataFrame = deepcopy(active_peak_prominence_2d_results['flat_peaks_df']).reset_index(drop=True)\n",
    "long_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[long_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "short_filtered_flat_peaks_df: pd.DataFrame = deepcopy(curr_active_pipeline.computation_results[short_any_name].computed_data['RatemapPeaksAnalysis']['PeakProminence2D']['filtered_flat_peaks_df']).reset_index(drop=True)\n",
    "\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), long_filtered_flat_peaks_df.neuron_id.to_numpy()), ['long_pf2D_peak_x', 'long_pf2D_peak_y']] = long_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "neuron_replay_stats_df.loc[np.isin(neuron_replay_stats_df['aclu'].to_numpy(), short_filtered_flat_peaks_df.neuron_id.to_numpy()), ['short_pf2D_peak_x', 'short_pf2D_peak_y']] = short_filtered_flat_peaks_df[['peak_center_x', 'peak_center_y']].to_numpy()\n",
    "\n",
    "both_included_neuron_stats_df = deepcopy(neuron_replay_stats_df[neuron_replay_stats_df['LS_pf_peak_x_diff'].notnull()]).drop(columns=['track_membership', 'neuron_type'])\n",
    "both_included_neuron_stats_df\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# both_included_neuron_stats_df['LS_pf_peak_x_diff'].plot()\n",
    "\n",
    "# _out_scatter = sns.scatterplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', y='aclu') # , hue='aclu'\n",
    "# _out_scatter.show()\n",
    "# _out_hist = sns.histplot(both_included_neuron_stats_df, x='LS_pf_peak_x_diff', bins=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_long_pf].to_numpy()\n",
    "short_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_short_pf].to_numpy()\n",
    "\n",
    "long_pf_aclus, short_pf_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18cfba",
   "metadata": {},
   "source": [
    "## 2024-02-06 - `directional_compute_trial_by_trial_correlation_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c383e8ea",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5,  8,  9, 11, 13, 18, 19, 20, 21, 23, 24, 25, 28, 31, 33, 35, 38, 39, 40, 41, 43, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 89, 92, 94, 97])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import compute_spatial_binned_activity_via_pfdt, compute_trial_by_trial_correlation_matrix\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import TrialByTrialActivity\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs\n",
    "\n",
    "# track_templates.shared_LR_aclus_only_neuron_IDs\n",
    "# track_templates.shared_RL_aclus_only_neuron_IDs\n",
    "\n",
    "\n",
    "## Directional Trial-by-Trial Activity:\n",
    "if 'pf1D_dt' not in curr_active_pipeline.computation_results[global_epoch_name].computed_data:\n",
    "\t# if `KeyError: 'pf1D_dt'` recompute\n",
    "\tcurr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "\n",
    "active_pf_1D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D_dt'])\n",
    "active_pf_2D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D_dt'])\n",
    "\n",
    "active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_1D_dt)\n",
    "# active_pf_dt.res\n",
    "# Limit only to the placefield aclus:\n",
    "active_pf_dt = active_pf_dt.get_by_id(ids=any_decoder_neuron_IDs)\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_2D_dt) # 2D\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3c34fd9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aclu</th>\n",
       "      <th>series_idx</th>\n",
       "      <th>subpeak_idx</th>\n",
       "      <th>long_LR_peak</th>\n",
       "      <th>long_RL_peak</th>\n",
       "      <th>short_LR_peak</th>\n",
       "      <th>short_RL_peak</th>\n",
       "      <th>long_LR_peak_height</th>\n",
       "      <th>long_RL_peak_height</th>\n",
       "      <th>short_LR_peak_height</th>\n",
       "      <th>short_RL_peak_height</th>\n",
       "      <th>LR_peak_diff</th>\n",
       "      <th>RL_peak_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233.842664</td>\n",
       "      <td>229.979380</td>\n",
       "      <td>206.799675</td>\n",
       "      <td>202.936391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.042989</td>\n",
       "      <td>27.042989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.268310</td>\n",
       "      <td>36.815173</td>\n",
       "      <td>48.405026</td>\n",
       "      <td>48.405026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.257402</td>\n",
       "      <td>3.863284</td>\n",
       "      <td>-11.589852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>233.842664</td>\n",
       "      <td>229.979380</td>\n",
       "      <td>102.491004</td>\n",
       "      <td>117.944140</td>\n",
       "      <td>0.307182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>131.351661</td>\n",
       "      <td>112.035240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.533992</td>\n",
       "      <td>141.123845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581123</td>\n",
       "      <td>0.540409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.799675</td>\n",
       "      <td>202.936391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261603</td>\n",
       "      <td>0.822171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>48.405026</td>\n",
       "      <td>48.405026</td>\n",
       "      <td>48.405026</td>\n",
       "      <td>125.670708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>0.900169</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-77.265683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>92</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.405026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.257204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>92</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.807424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>94</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.987129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.080856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.262948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.906273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>94</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.979380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202.936391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.406733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.042989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>97</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>40.678457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.491004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.622354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-61.812546</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>97</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>233.842664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.533992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.308672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aclu  series_idx  subpeak_idx  long_LR_peak  long_RL_peak  short_LR_peak  short_RL_peak  long_LR_peak_height  long_RL_peak_height  short_LR_peak_height  short_RL_peak_height  LR_peak_diff  RL_peak_diff\n",
       "0       4           0            0    233.842664    229.979380     206.799675     202.936391             1.000000             1.000000              1.000000              1.000000     27.042989     27.042989\n",
       "1       5           1            0     52.268310     36.815173      48.405026      48.405026             1.000000             0.877329              1.000000              0.257402      3.863284    -11.589852\n",
       "2       5           1            1    233.842664    229.979380     102.491004     117.944140             0.307182             1.000000              0.968753              1.000000    131.351661    112.035240\n",
       "3       5           1            2           NaN           NaN     129.533992     141.123845                  NaN                  NaN              0.581123              0.540409           NaN           NaN\n",
       "4       5           1            3           NaN           NaN     206.799675     202.936391                  NaN                  NaN              0.261603              0.822171           NaN           NaN\n",
       "5       8           2            0     48.405026     48.405026      48.405026     125.670708             1.000000             0.679641              0.900169              0.827778      0.000000    -77.265683\n",
       "..    ...         ...          ...           ...           ...            ...            ...                  ...                  ...                   ...                   ...           ...           ...\n",
       "124    92          38            0           NaN           NaN      48.405026            NaN                  NaN                  NaN              0.257204                   NaN           NaN           NaN\n",
       "125    92          38            1           NaN           NaN     121.807424            NaN                  NaN                  NaN              1.000000                   NaN           NaN           NaN\n",
       "126    94          37            0           NaN    144.987129            NaN     114.080856                  NaN             0.262948                   NaN              1.000000           NaN     30.906273\n",
       "127    94          37            1           NaN    229.979380            NaN     202.936391                  NaN             1.000000                   NaN              0.406733           NaN     27.042989\n",
       "128    97          39            0     40.678457           NaN     102.491004            NaN             1.000000                  NaN              0.622354                   NaN    -61.812546           NaN\n",
       "129    97          39            1    233.842664           NaN     129.533992            NaN             0.202391                  NaN              1.000000                   NaN    104.308672           NaN\n",
       "\n",
       "[130 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decoder_aclu_peak_location_df_merged = deepcopy(track_templates.get_decoders_aclu_peak_location_df(width=None))\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "decoder_aclu_peak_location_df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e102bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(LR_shift_x, LR_shift, LR_neuron_ids), (RL_shift_x, RL_shift, RL_neuron_ids) = track_templates.get_long_short_decoder_shifts()\n",
    "\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "included_aclus_only = deepcopy(both_included_neuron_stats_df.aclu.to_numpy()) # do we need these to be shared? I don't think so.\n",
    "\n",
    "LR_shift_df: pd.DataFrame = pd.DataFrame({'aclu': LR_neuron_ids, 'shift': LR_shift, 'shift_x': LR_shift_x})\n",
    "RL_shift_df: pd.DataFrame = pd.DataFrame({'aclu': RL_neuron_ids, 'shift': RL_shift, 'shift_x': RL_shift_x})\n",
    "\n",
    "# LR_shift_df = LR_shift_df[np.isin(LR_shift_df['aclu'], included_aclus_only)]\n",
    "# RL_shift_df = RL_shift_df[np.isin(RL_shift_df['aclu'], included_aclus_only)]\n",
    "\n",
    "## Get only the non-zero values:\n",
    "# Filter rows based on column: 'shift'\n",
    "LR_shift_df = LR_shift_df[LR_shift_df['shift'].abs() > 0.01]\n",
    "RL_shift_df = RL_shift_df[RL_shift_df['shift'].abs() > 0.01]\n",
    "\n",
    "LR_shift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "504580c3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## Single Global Laps version:\n",
    "laps_df = deepcopy(global_any_laps_epochs_obj.to_dataframe())\n",
    "n_laps = len(laps_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b01804af",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aclu</th>\n",
       "      <th>long_LR_num_peaks</th>\n",
       "      <th>long_RL_num_peaks</th>\n",
       "      <th>short_LR_num_peaks</th>\n",
       "      <th>short_RL_num_peaks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aclu  long_LR_num_peaks  long_RL_num_peaks  short_LR_num_peaks  short_RL_num_peaks\n",
       "0      4                  1                  1                   1                   1\n",
       "1      5                  2                  2                   4                   4\n",
       "2      8                  1                  3                   3                   1\n",
       "3      9                  3                  4                   1                   2\n",
       "4     11                  1                  0                   1                   0\n",
       "5     13                  1                  2                   1                   1\n",
       "..   ...                ...                ...                 ...                 ...\n",
       "50    84                  1                  2                   2                   2\n",
       "51    85                  0                  0                   2                   1\n",
       "52    89                  1                  0                   2                   0\n",
       "53    92                  0                  0                   2                   0\n",
       "54    94                  0                  2                   0                   2\n",
       "55    97                  2                  0                   2                   0\n",
       "\n",
       "[56 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_aclu_num_peaks_df: pd.DataFrame = track_templates.get_decoders_aclu_num_peaks_df()\n",
    "decoder_aclu_num_peaks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b90dc",
   "metadata": {},
   "source": [
    "## 2024-02-08 - Filter to find only the clear remap examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import TrialByTrialActivity\n",
    "from pyphocorehelpers.indexing_helpers import dict_to_full_array\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0cf56",
   "metadata": {},
   "source": [
    "### Get num peaks exclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_ids_dict = {k:v.neuron_ids for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# neuron_ids_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42a8d",
   "metadata": {},
   "source": [
    "### Get stability for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ec2d7",
   "metadata": {},
   "source": [
    "#### 2024-02-08 - 3pm - new stability dataframe to look at stability of each cell across decoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57869e27",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aclu</th>\n",
       "      <th>long_LR</th>\n",
       "      <th>long_RL</th>\n",
       "      <th>short_LR</th>\n",
       "      <th>short_RL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.945550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.611303</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.785017</td>\n",
       "      <td>0.688787</td>\n",
       "      <td>0.418533</td>\n",
       "      <td>0.224667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.951821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265417</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.552514</td>\n",
       "      <td>0.889661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963689</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0.752682</td>\n",
       "      <td>0.291230</td>\n",
       "      <td>0.734482</td>\n",
       "      <td>0.873376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>84</td>\n",
       "      <td>0.594364</td>\n",
       "      <td>0.925593</td>\n",
       "      <td>0.319242</td>\n",
       "      <td>0.866007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>85</td>\n",
       "      <td>0.826677</td>\n",
       "      <td>0.910703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>89</td>\n",
       "      <td>0.797770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>97</td>\n",
       "      <td>0.172473</td>\n",
       "      <td>0.971182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aclu   long_LR   long_RL  short_LR  short_RL\n",
       "0      4  0.945550  0.000000  0.611303  0.000000\n",
       "1      5  0.785017  0.688787  0.418533  0.224667\n",
       "2      8  0.951821  0.000000  0.265417  0.000000\n",
       "3      9  0.552514  0.889661  0.000000  0.761754\n",
       "4     11  0.000000  0.000000  0.963689  0.000000\n",
       "5     13  0.752682  0.291230  0.734482  0.873376\n",
       "..   ...       ...       ...       ...       ...\n",
       "50    84  0.594364  0.925593  0.319242  0.866007\n",
       "51    85  0.826677  0.910703  0.000000  0.000000\n",
       "52    89  0.797770  0.000000  0.273964  0.000000\n",
       "53    92  0.000000  0.205707  0.000000  0.000000\n",
       "54    94  0.000000  0.000000  0.971126  0.000000\n",
       "55    97  0.172473  0.971182  0.000000  0.000000\n",
       "\n",
       "[56 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k,v in directional_active_lap_pf_results_dicts.items():\n",
    "# stability_dict = {k:v.aclu_to_stability_score_dict for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict = {k:dict_to_full_array(v.aclu_to_stability_score_dict, full_indicies=any_decoder_neuron_IDs, fill_value=0.0) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "\n",
    "\n",
    "# list(stability_dict.values())\n",
    "\n",
    "stability_dict = {k:list(v.aclu_to_stability_score_dict.values()) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "# stability_dict\n",
    "## all the same size hopefully!\n",
    "# [len(v) for v in list(stability_dict.values())]\n",
    "\n",
    "stability_df: pd.DataFrame = pd.DataFrame({'aclu': any_decoder_neuron_IDs, **stability_dict})\n",
    "# stability_df.rename(dict(zip([], [])))\n",
    "stability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_df.plot.scatter(x='aclu', y=['long_LR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3308ff",
   "metadata": {},
   "source": [
    "### Ensure that we're only getting the location of the maximum peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.decoder_neuron_IDs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1698d3a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aclu</th>\n",
       "      <th>long_LR_num_peaks</th>\n",
       "      <th>long_RL_num_peaks</th>\n",
       "      <th>short_LR_num_peaks</th>\n",
       "      <th>short_RL_num_peaks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    aclu  long_LR_num_peaks  long_RL_num_peaks  short_LR_num_peaks  short_RL_num_peaks\n",
       "0      4                  1                  1                   1                   1\n",
       "37    68                  1                  1                   1                   1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# target_df[['long_LR_num_peaks', 'short_LR_num_peaks']]\n",
    "\n",
    "unimodal_max_num_peaks = 1\n",
    "\n",
    "target_df: pd.DataFrame = deepcopy(decoder_aclu_num_peaks_df)\n",
    "\n",
    "# Filter rows based on columns: 'long_LR_num_peaks', 'short_LR_peak'\n",
    "# unimodal_LR_target_df = target_df[(target_df['long_LR_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_LR_num_peaks'] <= unimodal_max_num_peaks)]\n",
    "unimodal_LR_target_df = target_df[((target_df['long_LR_num_peaks'] <= unimodal_max_num_peaks) & (target_df['long_LR_num_peaks'] > 0)) & ((target_df['short_LR_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_LR_num_peaks'] > 0))]\n",
    "\n",
    "# unimodal_RL_target_df = target_df[(target_df['long_RL_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_RL_num_peaks'] <= unimodal_max_num_peaks)]\n",
    "unimodal_RL_target_df = target_df[((target_df['long_RL_num_peaks'] <= unimodal_max_num_peaks) & (target_df['long_RL_num_peaks'] > 0)) & ((target_df['short_RL_num_peaks'] <= unimodal_max_num_peaks) & (target_df['short_RL_num_peaks'] > 0))]\n",
    "\n",
    "# np.union1d(unimodal_LR_target_df.aclu.to_numpy(), unimodal_RL_target_df.aclu.to_numpy())\n",
    "# unimodal_all_target_df = target_df[target_df.aclu.isin(np.union1d(unimodal_LR_target_df.aclu.to_numpy(), unimodal_RL_target_df.aclu.to_numpy()))]\n",
    "unimodal_all_target_df = target_df[target_df.aclu.isin(np.intersect1d(unimodal_LR_target_df.aclu.to_numpy(), unimodal_RL_target_df.aclu.to_numpy()))]\n",
    "\n",
    "# unimodal_LR_target_df\n",
    "# unimodal_RL_target_df\n",
    "unimodal_all_target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539eb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_aclu_peak_location_df_merged[['LR_peak_diff', 'RL_peak_diff']].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024-02-08 - Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5de5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_peak_heatmap_test\n",
    "from neuropy.utils.matplotlib_helpers import perform_update_title_subtitle\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent\n",
    "\n",
    "# INPUTS: directional_active_lap_pf_results_dicts, test_aclu: int = 26, xbin_centers, decoder_aclu_peak_location_df_merged\n",
    "\n",
    "def plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, decoder_aclu_peak_location_df_merged: pd.DataFrame, aclu: int = 26, **kwargs):\n",
    "    \"\"\" 2024-02-06 - Plot all four decoders for a single aclu, with overlayed red lines for the detected peaks. \n",
    "    \n",
    "    plot_single_heatmap_set_with_points\n",
    "\n",
    "    plot_cell_position_binned_activity_over_time\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## TEst: Look at a single aclu value\n",
    "    # test_aclu: int = 26\n",
    "    # test_aclu: int = 28\n",
    "    \n",
    "    active_context: IdentifyingContext = kwargs.get('active_context', IdentifyingContext())\n",
    "    active_context = active_context.overwriting_context(aclu=aclu)\n",
    "\n",
    "    decoders_tuning_curves_dict = kwargs.get('decoders_tuning_curves_dict', None)\n",
    "    \n",
    "    matching_aclu_df = decoder_aclu_peak_location_df_merged[decoder_aclu_peak_location_df_merged.aclu == aclu].copy()\n",
    "    assert len(matching_aclu_df) > 0, f\"matching_aclu_df: {matching_aclu_df} for aclu == {aclu}\"\n",
    "    new_peaks_dict: Dict = list(matching_aclu_df.itertuples(index=False))[0]._asdict() # {'aclu': 28, 'long_LR_peak': 185.29063638457257, 'long_RL_peak': nan, 'short_LR_peak': 176.75276643746625, 'short_RL_peak': nan, 'LR_peak_diff': 8.537869947106316, 'RL_peak_diff': nan}\n",
    "        \n",
    "    # long_LR_name, long_RL_name, short_LR_name, short_RL_name\n",
    "    curr_aclu_z_scored_tuning_map_matrix_dict = {}\n",
    "    curr_aclu_mean_epoch_peak_location_dict = {}\n",
    "    curr_aclu_median_peak_location_dict = {}\n",
    "    curr_aclu_extracted_decoder_peak_locations_dict = {}\n",
    "\n",
    "    ## Find the peak location for each epoch:\n",
    "    for a_name, a_decoder_directional_active_lap_pf_result in directional_active_lap_pf_results_dicts.items():\n",
    "        # print(f'a_name: {a_name}')\n",
    "        matrix_idx = a_decoder_directional_active_lap_pf_result.aclu_to_matrix_IDX_map[aclu]\n",
    "        curr_aclu_z_scored_tuning_map_matrix = a_decoder_directional_active_lap_pf_result.z_scored_tuning_map_matrix[:,matrix_idx,:] # .shape (22, 80, 56)\n",
    "        curr_aclu_z_scored_tuning_map_matrix_dict[a_name] = curr_aclu_z_scored_tuning_map_matrix\n",
    "\n",
    "        # curr_aclu_mean_epoch_peak_location_dict[a_name] = np.nanmax(curr_aclu_z_scored_tuning_map_matrix, axis=-1)\n",
    "        assert np.shape(curr_aclu_z_scored_tuning_map_matrix)[-1] == len(xbin_centers), f\"np.shape(curr_aclu_z_scored_tuning_map_matrix)[-1]: {np.shape(curr_aclu_z_scored_tuning_map_matrix)} != len(xbin_centers): {len(xbin_centers)}\"\n",
    "        curr_peak_value = new_peaks_dict[f'{a_name}_peak']\n",
    "        # print(f'curr_peak_value: {curr_peak_value}')\n",
    "        curr_aclu_extracted_decoder_peak_locations_dict[a_name] = curr_peak_value\n",
    "\n",
    "        curr_aclu_mean_epoch_peak_location_dict[a_name] = np.nanargmax(curr_aclu_z_scored_tuning_map_matrix, axis=-1)\n",
    "        curr_aclu_mean_epoch_peak_location_dict[a_name] = xbin_centers[curr_aclu_mean_epoch_peak_location_dict[a_name]] # convert to actual positions instead of indicies\n",
    "        curr_aclu_median_peak_location_dict[a_name] = np.nanmedian(curr_aclu_mean_epoch_peak_location_dict[a_name])\n",
    "\n",
    "    # curr_aclu_mean_epoch_peak_location_dict # {'maze1_odd': array([ 0, 55, 54, 55, 55, 53, 50, 55, 52, 52, 55, 53, 53, 52, 51, 52, 55, 55, 53, 55, 55, 54], dtype=int64), 'maze2_odd': array([46, 45, 43, 46, 45, 46, 46, 46, 45, 45, 44, 46, 44, 45, 46, 45, 44, 44, 45, 45], dtype=int64)}\n",
    "\n",
    "\n",
    "    if decoders_tuning_curves_dict is not None:\n",
    "        curr_aclu_tuning_curves_dict = {name:v.get(aclu, None) for name, v in decoders_tuning_curves_dict.items()}\n",
    "    else:\n",
    "        curr_aclu_tuning_curves_dict = None\n",
    "                \n",
    "    # point_value = curr_aclu_median_peak_location_dict\n",
    "    point_value = curr_aclu_extracted_decoder_peak_locations_dict\n",
    "    fig, ax_dict = plot_peak_heatmap_test(curr_aclu_z_scored_tuning_map_matrix_dict, xbin=xbin, point_dict=point_value, tuning_curves_dict=curr_aclu_tuning_curves_dict, include_tuning_curves=True)\n",
    "    # Set window title and plot title\n",
    "    perform_update_title_subtitle(fig=fig, ax=None, title_string=f\"Position-Binned Activity per Lap - aclu {aclu}\", subtitle_string=None, active_context=active_context, use_flexitext_titles=True)\n",
    "\n",
    "    # fig, ax_dict = plot_peak_heatmap_test(curr_aclu_z_scored_tuning_map_matrix_dict, xbin=xbin, point_dict=curr_aclu_extracted_decoder_peak_locations_dict) # , defer_show=True\n",
    "    \n",
    "    # fig.show()\n",
    "    return fig, ax_dict\n",
    "\n",
    "\n",
    "\n",
    "decoders_tuning_curves_dict = track_templates.decoder_normalized_tuning_curves_dict_dict.copy()\n",
    "\n",
    "extra_decoder_values_dict = {'tuning_curves': decoders_tuning_curves_dict, 'points': decoder_aclu_peak_location_df_merged}\n",
    "\n",
    "# decoders_tuning_curves_dict\n",
    "xbin_centers = deepcopy(active_pf_dt.xbin_centers)\n",
    "xbin = deepcopy(active_pf_dt.xbin)\n",
    "fig, ax_dict = plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, extra_decoder_values_dict=extra_decoder_values_dict, aclu=4, \n",
    "                                                   decoders_tuning_curves_dict=decoders_tuning_curves_dict, decoder_aclu_peak_location_df_merged=decoder_aclu_peak_location_df_merged,\n",
    "                                                    active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a399108",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot a couple\n",
    "# long_pf_aclus, short_pf_aclus\n",
    "\n",
    "# plot_aclus = [4,  5,  8,  9]\n",
    "plot_aclus = [11, 18, 68]\n",
    "# plot_aclus = [68]\n",
    "\n",
    "for test_aclu in plot_aclus:\n",
    "\tfig, ax_dict = plot_single_heatmap_set_with_points(directional_active_lap_pf_results_dicts, xbin_centers, xbin, extra_decoder_values_dict=extra_decoder_values_dict, aclu=test_aclu, \n",
    "                                                   decoders_tuning_curves_dict=decoders_tuning_curves_dict, decoder_aclu_peak_location_df_merged=decoder_aclu_peak_location_df_merged,\n",
    "                                                    active_context=curr_active_pipeline.build_display_context_for_session('single_heatmap_set_with_points'))\n",
    "\t\n",
    "\tfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f217aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = fig.get_layout_engine()\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these layout changes don't seem to take effect until the window containing the figure is resized.\n",
    "# fig.set_layout_engine('compressed') # TAKEWAY: Use 'compressed' instead of 'constrained'\n",
    "fig.set_layout_engine('none') # disabling layout engine. Strangely still allows window to resize and the plots scale, so I'm not sure what the layout engine is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(directional_active_lap_pf_results_dicts.keys()) # ['maze1_odd', 'maze1_even', 'maze2_odd', 'maze2_even']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.pf.plot_ratemaps_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef13541",
   "metadata": {},
   "source": [
    "# 2024-02-02 - napari_plot_directional_trial_by_trial_activity_viz Trial-by-trial Correlation Matrix C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd735e",
   "metadata": {},
   "source": [
    "### 🎨 Show Trial-by-trial Correlation Matrix C in `napari`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72df59",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import napari\n",
    "# import afinder\n",
    "from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_from_layers_dict\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_trial_by_trial_activity_viz\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_plot_directional_trial_by_trial_activity_viz\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_export_image_sequence\n",
    "\n",
    "## Directional\n",
    "directional_viewer, directional_image_layer_dict, custom_direction_split_layers_dict = napari_plot_directional_trial_by_trial_activity_viz(directional_active_lap_pf_results_dicts, include_trial_by_trial_correlation_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9934d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Global:\n",
    "viewer, image_layer_dict = napari_trial_by_trial_activity_viz(z_scored_tuning_map_matrix, C_trial_by_trial_correlation_matrix, title='Trial-by-trial Correlation Matrix C', axis_labels=('aclu', 'lap', 'xbin')) # GLOBAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f61fa",
   "metadata": {},
   "source": [
    "# Napari Plotting Long/Short Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80146cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import test_LinearTrackDimensions_2D_pyqtgraph, LinearTrackDimensions, LinearTrackInstance\n",
    "\n",
    "long_track_dims = LinearTrackDimensions(track_length=170.0)\n",
    "short_track_dims = LinearTrackDimensions(track_length=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get grid_bin_bounds:\n",
    "long_grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "short_grid_bin_bounds = deepcopy(short_pf2D.config.grid_bin_bounds)\n",
    "\n",
    "long_grid_bin_bounds\n",
    "short_grid_bin_bounds\n",
    "linear_track_instance = LinearTrackInstance.init_from_grid_bin_bounds(grid_bin_bounds=long_grid_bin_bounds)\n",
    "linear_track_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app, w, cw, (long_track_dims, long_rect_items, long_rects), (short_track_dims, short_rect_items, short_rects) = test_LinearTrackDimensions_2D_pyqtgraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aeb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Napari Shapes Layer Test:\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_napari_track_shapes_layer\n",
    "\n",
    "# add the image\n",
    "# viewer = napari.view_image(data.camera(), name='photographer')\n",
    "\n",
    "test_shapes_viewer = napari.Viewer() # name='Test Shapes Viewer'\n",
    "# add the tracks\n",
    "long_rectangles_poly_shapes_layer, short_rectangles_poly_shapes_layer = add_napari_track_shapes_layer(test_shapes_viewer, long_rect_items, short_rect_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_layer_info(long_rectangles_poly_shapes_layer)\n",
    "extract_layer_info(short_rectangles_poly_shapes_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_rectangles_poly_shapes_layer.bounding_box\n",
    "# long_rectangles_poly_shapes_layer.interaction_box\n",
    "long_rectangles_poly_shapes_layer.corner_pixels # np.array([[ 44, 124], [376, 161]])\n",
    "# long_rectangles_poly_shapes_layer.frame\n",
    "\n",
    "\n",
    "# long_rectangles_poly_shapes_layer.rotate = 90\n",
    "# data_to_world, world_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_rectangles_poly_shapes_layer.rotate = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af347488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #TODO 2024-02-02 22:31: - [ ] These need to be update for global support\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import napari_add_aclu_slider\n",
    "\n",
    "def build_filename_from_viewer(viewer, desired_save_parent_path: Path, slider_axis_IDX: int = 0) -> Path:\n",
    "    \"\"\"\n",
    "    Captures: curr_active_pipeline, neuron_ids, global_any_name\n",
    "    \n",
    "     Usage:\n",
    "        file_out_path = build_filename_from_viewer(viewer)\n",
    "        viewer.screenshot(path=file_out_path, canvas_only=True, flash=False)\n",
    "\n",
    "    \"\"\"\n",
    "    # desired_save_parent_path = Path('/home/halechr/Desktop/test_napari_out').resolve()\n",
    "\n",
    "    matrix_aclu_IDX: int = int(viewer.dims.current_step[slider_axis_IDX])\n",
    "    # find the aclu value for this index:\n",
    "    aclu: int = int(neuron_ids[matrix_aclu_IDX])\n",
    "    curr_context = curr_active_pipeline.build_display_context_for_filtered_session(global_any_name, 'napari_trial_by_trial_activity_viz', aclu=str(aclu))\n",
    "    curr_context_string: str = curr_context.get_description() #.get_description(suffix_items=[f'aclu-{aclu}'])\n",
    "    filename_string: str = f\"{curr_context_string}.png\"\n",
    "\n",
    "    file_out_path = desired_save_parent_path.joinpath(filename_string).resolve()\n",
    "    return file_out_path\n",
    "\n",
    "\n",
    "# desired_save_parent_path = Path('/home/halechr/Desktop/test_napari_out').resolve()\n",
    "desired_save_parent_path = Path(r'C:\\Users\\pho\\Desktop\\test_napari_out').resolve()\n",
    "_connected_on_update_slider_event = napari_add_aclu_slider(viewer=directional_viewer, neuron_ids=neuron_ids)\n",
    "imageseries_output_directory = napari_export_image_sequence(viewer=viewer, imageseries_output_directory=desired_save_parent_path, slider_axis_IDX=0, build_filename_from_viewer_callback_fn=build_filename_from_viewer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional_viewer.Config\n",
    "directional_viewer.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {},
   "source": [
    "# 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_shift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8eceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_neuron_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "# graphics_output_dict = curr_active_pipeline.display('_display_long_short_pf1D_comparison', active_identifying_session_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "\n",
    "graphics_output_dict = curr_active_pipeline.display('_display_long_short_pf1D_comparison', active_identifying_session_ctx,\n",
    "                                                     include_includelist=[long_LR_name, short_LR_name, global_LR_name], active_context=active_identifying_session_ctx.overwriting_context(dir='LR'), included_any_context_neuron_ids=LR_shift_df.aclu.unique())\n",
    "\n",
    "\n",
    "# fig, axs, plot_data = graphics_output_dict['fig'], graphics_output_dict['axs'], graphics_output_dict['plot_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51201e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "\n",
    "_out = TemplateDebugger.init_templates_debugger(track_templates) # , included_any_context_neuron_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feab852",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot.display_function_items\n",
    "\n",
    "# '_display_directional_template_debugger'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd064d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_directional_template_debugger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba36573",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_template_debugger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_track_template_pf1Ds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_display_directional_merged_pfs'\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pfs', plot_all_directions=False, plot_long_directional=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting on 2024-02-06 to display the LR/RL directions instead of the All/Long/Short pfs:\n",
    "def _display_directional_merged_pfs(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist=None, save_figure=True, included_any_context_neuron_ids=None,\n",
    "\t\t\t\t\t\t\t\t\tplot_all_directions=True, plot_long_directional=False, plot_short_directional=False, **kwargs):\n",
    "\t\"\"\" Plots the merged pseduo-2D pfs/ratemaps. Plots: All-Directions, Long-Directional, Short-Directional in seperate windows. \n",
    "\t\n",
    "\tHistory: this is the Post 2022-10-22 display_all_pf_2D_pyqtgraph_binned_image_rendering-based method:\n",
    "\t\"\"\"\n",
    "\tfrom pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "\tfrom pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow \n",
    "\t\n",
    "\n",
    "\tdefer_render = kwargs.pop('defer_render', False)\n",
    "\tdirectional_merged_decoders_result: DirectionalMergedDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\tactive_merged_pf_plots_data_dict = {} #empty dict\n",
    "\t\n",
    "\tif plot_all_directions:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='All-Directions', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.all_directional_pf1D_Decoder.pf # all-directions\n",
    "\tif plot_long_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Long-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.long_directional_pf1D_Decoder.pf # Long-only\n",
    "\tif plot_short_directional:\n",
    "\t\tactive_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Short-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.short_directional_pf1D_Decoder.pf # Short-only\n",
    "\n",
    "\tout_plots_dict = {}\n",
    "\t\n",
    "\tfor active_context, active_pf_2D in active_merged_pf_plots_data_dict.items():\n",
    "\t\t# figure_format_config = {} # empty dict for config\n",
    "\t\tfigure_format_config = {'scrollability_mode': LayoutScrollability.NON_SCROLLABLE} # kwargs # kwargs as default figure_format_config\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig: BasicBinnedImageRenderingWindow  = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow\n",
    "\t\n",
    "\t\t# Set the window title from the context\n",
    "\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.setWindowTitle(f'{active_context.get_description()}')\n",
    "\t\tout_plots_dict[active_context] = out_all_pf_2D_pyqtgraph_binned_image_fig\n",
    "\n",
    "\t\t# Tries to update the display of the item:\n",
    "\t\tnames_list = [v for v in list(out_all_pf_2D_pyqtgraph_binned_image_fig.plots.keys()) if v not in ('name', 'context')]\n",
    "\t\tfor a_name in names_list:\n",
    "\t\t\t# Adjust the size of the text for the item by passing formatted text\n",
    "\t\t\ta_plot: pg.PlotItem = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[a_name].mainPlotItem # PlotItem \n",
    "\t\t\t# no clue why 2 is a good value for this...\n",
    "\t\t\ta_plot.titleLabel.setMaximumHeight(2)\n",
    "\t\t\ta_plot.layout.setRowFixedHeight(0, 2)\n",
    "\t\t\t\n",
    "\n",
    "\t\tif not defer_render:\n",
    "\t\t\tout_all_pf_2D_pyqtgraph_binned_image_fig.show()\n",
    "\n",
    "\treturn out_plots_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcc19a",
   "metadata": {},
   "source": [
    "# 2023-12-18 - Simpily detect bimodal cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc55f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import ContinuousPeakLocationRepresentingMixin\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from scipy.signal import find_peaks\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', sortby=None)\n",
    "\n",
    "# active_ratemap = deepcopy(long_pf1D.ratemap)\n",
    "active_ratemap: Ratemap = deepcopy(long_LR_pf1D.ratemap)\n",
    "peaks_dict, aclu_n_peaks_dict, peaks_results_df = active_ratemap.compute_tuning_curve_modes(height=0.2, width=None)\n",
    "\n",
    "\n",
    "included_columns = ['pos', 'peak_heights'] # the columns of interest that you want in the final dataframe.\n",
    "included_columns_renamed = dict(zip(included_columns, ['peak', 'peak_height']))\n",
    "decoder_peaks_results_dfs = [a_decoder.pf.ratemap.get_tuning_curve_peak_df(height=0.2, width=None) for a_decoder in (track_templates.long_LR_decoder, track_templates.long_RL_decoder, track_templates.short_LR_decoder, track_templates.short_RL_decoder)]\n",
    "prefix_names = [f'{a_decoder_name}_' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "all_included_columns = ['aclu', 'series_idx', 'subpeak_idx'] + included_columns # Used to filter out the unwanted columns from the output\n",
    "\n",
    "# [['aclu', 'series_idx', 'subpeak_idx', 'pos']]\n",
    "\n",
    "# rename_list_fn = lambda a_prefix: {'pos': f\"{a_prefix}pos\"}\n",
    "rename_list_fn = lambda a_prefix: {a_col_name:f\"{a_prefix}{included_columns_renamed[a_col_name]}\" for a_col_name in included_columns}\n",
    "\n",
    "# column_names = [f'{a_decoder_name}_peak' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "\n",
    "# dataFrames = decoder_peaks_results_dfs\n",
    "# names = self.get_decoder_names()\n",
    "\n",
    "# rename 'pos' column in each dataframe and then reduce to perform cumulative outer merge\n",
    "result_df = decoder_peaks_results_dfs[0][all_included_columns].rename(columns=rename_list_fn(prefix_names[0]))\n",
    "for df, a_prefix in zip(decoder_peaks_results_dfs[1:], prefix_names[1:]):\n",
    "    result_df = pd.merge(result_df, df[all_included_columns].rename(columns=rename_list_fn(a_prefix)), on=['aclu', 'series_idx', 'subpeak_idx'], how='outer')\n",
    "\n",
    "# result = reorder_columns(result, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "# list(filter(lambda column: column.endswith('_peak_heights'), result.columns))\n",
    "# result_df = reorder_columns(result_df, column_name_desired_index_dict=dict(zip(list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), np.arange(len(result_df.columns)-4, len(result_df.columns)))))\n",
    "# result_df\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "result_df = reorder_columns_relative(result_df, column_names=list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), relative_mode='end')\n",
    "result_df\n",
    "# print(list(result.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ccd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_results_df = track_templates.get_decoders_aclu_peak_location_df().sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True)\n",
    "peaks_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d377cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_n_peaks_dict: Dict = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index().set_index('aclu').to_dict()['subpeak_idx_count'] # number of peaks (\"models\" for each aclu)\n",
    "# aclu_n_peaks_dict\n",
    "\n",
    "# peaks_results_df = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index()\n",
    "\n",
    "# peaks_results_df[peaks_results_df.aclu == 5]\n",
    "# peaks_results_df.aclu.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeccfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ratemap.n_neurons\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=active_ratemap.neuron_ids, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aclu_n_peaks_dict\n",
    "unimodal_only_aclus = np.array(list(unimodal_peaks_dict.keys()))\n",
    "unimodal_only_aclus\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=unimodal_only_aclus, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aebbf1",
   "metadata": {},
   "source": [
    "# 2024-02-08 Directional Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_ripple_filter_epochs_decoder_result_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da36235",
   "metadata": {},
   "source": [
    "### 2024-02-09 - Recover Radon Transform info to confirm the Pseudo2D decoder-based detection of long/short replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc2420d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laps_decoding_time_bin_size: 0.025, ripple_decoding_time_bin_size: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\Analysis\\Decoder\\reconstruction.py:329: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior /= np.sum(posterior, axis=0) # C(tau, n) = np.sum(posterior, axis=0): normalization condition mentioned in eqn 36 to convert to P_x_given_n\n",
      "C:\\Users\\pho\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\analyses\\decoders.py:86: RuntimeWarning: Mean of empty slice\n",
      "  posterior_mean = np.nanmean(posterior, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'long_LR':        score   velocity   intercept      speed\n",
       " 0   0.216853   1.794237  207.020435   1.794237\n",
       " 1   0.397501  10.728862   11.201531  10.728862\n",
       " 2   0.356712  -3.066169   23.489624   3.066169\n",
       " 3   0.343649  18.060682    7.911551  18.060682\n",
       " 4   0.378424  -4.305613   23.487177   4.305613\n",
       " 5   0.419303   9.952092    8.280020   9.952092\n",
       " ..       ...        ...         ...        ...\n",
       " 82  0.340120  29.392001  123.340118  29.392001\n",
       " 83  0.345915 -13.283987   51.495477  13.283987\n",
       " 84  0.347885  31.152961  115.908910  31.152961\n",
       " 85  0.350080  -6.328976   23.242243   6.328976\n",
       " 86  0.203893  -9.535356   68.274868   9.535356\n",
       " 87  0.265127   0.059825  207.921908   0.059825\n",
       " \n",
       " [88 rows x 4 columns],\n",
       " 'long_RL':        score   velocity   intercept      speed\n",
       " 0   0.255124  -0.035435  205.273340   0.035435\n",
       " 1   0.285456   6.442902   15.678620   6.442902\n",
       " 2   0.336948 -26.976445  223.799332  26.976445\n",
       " 3   0.264231  13.542497   11.569150  13.542497\n",
       " 4   0.318183 -17.360126  180.945501  17.360126\n",
       " 5   0.312155  17.744186   -4.653166  17.744186\n",
       " ..       ...        ...         ...        ...\n",
       " 82  0.319266  -3.661560  218.648128   3.661560\n",
       " 83  0.345774 -34.582732  200.641652  34.582732\n",
       " 84  0.282799   5.235091  196.105525   5.235091\n",
       " 85  0.367260   5.689022    3.031042   5.689022\n",
       " 86  0.232139   0.600580  205.046242   0.600580\n",
       " 87  0.231013  14.479512   18.637549  14.479512\n",
       " \n",
       " [88 rows x 4 columns],\n",
       " 'short_LR':        score   velocity   intercept      speed\n",
       " 0   0.322392  -1.278878  190.127023   1.278878\n",
       " 1   0.386622  31.744131   29.207704  31.744131\n",
       " 2   0.290276 -20.556077  182.039185  20.556077\n",
       " 3   0.430961  22.931103   97.190833  22.931103\n",
       " 4   0.330432   1.996247   88.803858   1.996247\n",
       " 5   0.454467  22.054339   79.448334  22.054339\n",
       " ..       ...        ...         ...        ...\n",
       " 82  0.579612  15.580472  134.541752  15.580472\n",
       " 83  0.256491  17.577258   54.095928  17.577258\n",
       " 84  0.569143  26.871042  108.004520  26.871042\n",
       " 85  0.272254   5.874348   87.573174   5.874348\n",
       " 86  0.232662   0.192344  105.520393   0.192344\n",
       " 87  0.303888  64.280123   67.876982  64.280123\n",
       " \n",
       " [88 rows x 4 columns],\n",
       " 'short_RL':        score   velocity   intercept      speed\n",
       " 0   0.331264  -6.718332  183.738734   6.718332\n",
       " 1   0.267649  20.376096   92.203965  20.376096\n",
       " 2   0.358977 -12.490743  182.315288  12.490743\n",
       " 3   0.294296  21.524754  110.570772  21.524754\n",
       " 4   0.397916 -19.109383  168.829597  19.109383\n",
       " 5   0.300446  24.558686   83.892270  24.558686\n",
       " ..       ...        ...         ...        ...\n",
       " 82  0.427308  13.305375  142.263751  13.305375\n",
       " 83  0.462687 -33.025571  183.772022  33.025571\n",
       " 84  0.357938  15.437927  136.025496  15.437927\n",
       " 85  0.342806 -36.680772  167.092337  36.680772\n",
       " 86  0.240702 -25.052252  221.516942  25.052252\n",
       " 87  0.275272 -17.273981  202.622938  17.273981\n",
       " \n",
       " [88 rows x 4 columns]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## From scratch:\n",
    "from neuropy.core.session.dataSession import Laps\n",
    "from neuropy.utils.mixins.binning_helpers import find_minimum_time_bin_duration\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.DefaultComputationFunctions import compute_radon_transforms\n",
    "\n",
    "\n",
    "# Inputs: all_directional_pf1D_Decoder, alt_directional_merged_decoders_result\n",
    "def _compute_epoch_decoding_for_decoder(a_directional_pf1D_Decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size: float = 0.5, desired_ripple_decoding_time_bin_size: float = 0.1, use_single_time_bin_per_epoch: bool=False):\n",
    "    \"\"\" Decodes the laps and the ripples and their RadonTransforms using the provided decoder.\n",
    "    ~12.2s per decoder.\n",
    "\n",
    "    \"\"\"\n",
    "    # Modifies alt_directional_merged_decoders_result, a copy of the original result, with new timebins\n",
    "    long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "    a_directional_pf1D_Decoder = deepcopy(a_directional_pf1D_Decoder)\n",
    "\n",
    "    if use_single_time_bin_per_epoch:\n",
    "        print(f'WARNING: use_single_time_bin_per_epoch=True so time bin sizes will be ignored.')\n",
    "        \n",
    "    ## Decode Laps:\n",
    "    global_any_laps_epochs_obj = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computation_config.pf_params.computation_epochs) # global_epoch_name='maze_any' (? same as global_epoch_name?)\n",
    "    min_possible_laps_time_bin_size: float = find_minimum_time_bin_duration(global_any_laps_epochs_obj.to_dataframe()['duration'].to_numpy())\n",
    "    laps_decoding_time_bin_size: float = min(desired_laps_decoding_time_bin_size, min_possible_laps_time_bin_size) # 10ms # 0.002\n",
    "    if use_single_time_bin_per_epoch:\n",
    "        laps_decoding_time_bin_size = None\n",
    "\n",
    "    a_directional_laps_filter_epochs_decoder_result = a_directional_pf1D_Decoder.decode_specific_epochs(spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df), filter_epochs=global_any_laps_epochs_obj, decoding_time_bin_size=laps_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "    laps_radon_transform_df = compute_radon_transforms(a_directional_pf1D_Decoder, a_directional_laps_filter_epochs_decoder_result)\n",
    "\n",
    "    ## Decode Ripples:\n",
    "    if desired_ripple_decoding_time_bin_size is not None:\n",
    "        global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "        min_possible_time_bin_size: float = find_minimum_time_bin_duration(global_replays['duration'].to_numpy())\n",
    "        ripple_decoding_time_bin_size: float = min(desired_ripple_decoding_time_bin_size, min_possible_time_bin_size) # 10ms # 0.002\n",
    "        if use_single_time_bin_per_epoch:\n",
    "            ripple_decoding_time_bin_size = None\n",
    "        a_directional_ripple_filter_epochs_decoder_result = a_directional_pf1D_Decoder.decode_specific_epochs(deepcopy(curr_active_pipeline.sess.spikes_df), filter_epochs=global_replays, decoding_time_bin_size=ripple_decoding_time_bin_size, use_single_time_bin_per_epoch=use_single_time_bin_per_epoch, debug_print=False)\n",
    "        ripple_radon_transform_df = compute_radon_transforms(a_directional_pf1D_Decoder, a_directional_ripple_filter_epochs_decoder_result)\n",
    "\n",
    "    else:\n",
    "        a_directional_ripple_filter_epochs_decoder_result = None\n",
    "        ripple_radon_transform_df = None\n",
    "\n",
    "    ## Post Compute Validations:\n",
    "    # alt_directional_merged_decoders_result.perform_compute_marginals()\n",
    "    return a_directional_laps_filter_epochs_decoder_result, a_directional_ripple_filter_epochs_decoder_result, (laps_radon_transform_df, ripple_radon_transform_df)\n",
    "\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "# all_directional_pf1D_Decoder_dict['long_LR'].dec\n",
    "# a_directional_laps_filter_epochs_decoder_result, a_directional_ripple_filter_epochs_decoder_result, (a_laps_radon_transform_df, a_ripple_radon_transform_df) = _compute_epoch_decoding_for_decoder(long_LR_pf1D_Decoder, curr_active_pipeline,\n",
    "#                                                                                                                                                                                                    desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "\n",
    "## Decode epochs for all four decoders:\n",
    "decoder_laps_radon_transform_df_dict = {}\n",
    "decoder_ripple_radon_transform_df_dict = {}\n",
    "\n",
    "for a_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    a_directional_laps_filter_epochs_decoder_result, a_directional_ripple_filter_epochs_decoder_result, (decoder_laps_radon_transform_df_dict[a_name], decoder_ripple_radon_transform_df_dict[a_name]) = _compute_epoch_decoding_for_decoder(a_decoder, curr_active_pipeline,\n",
    "                                                                                                                                                                                                                desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "    \n",
    "\n",
    "decoder_laps_radon_transform_df_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05850029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_long_LR</th>\n",
       "      <th>score_long_RL</th>\n",
       "      <th>score_short_LR</th>\n",
       "      <th>score_short_RL</th>\n",
       "      <th>best_decoder_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.216853</td>\n",
       "      <td>0.255124</td>\n",
       "      <td>0.322392</td>\n",
       "      <td>0.331264</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397501</td>\n",
       "      <td>0.285456</td>\n",
       "      <td>0.386622</td>\n",
       "      <td>0.267649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.356712</td>\n",
       "      <td>0.336948</td>\n",
       "      <td>0.290276</td>\n",
       "      <td>0.358977</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.343649</td>\n",
       "      <td>0.264231</td>\n",
       "      <td>0.430961</td>\n",
       "      <td>0.294296</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.378424</td>\n",
       "      <td>0.318183</td>\n",
       "      <td>0.330432</td>\n",
       "      <td>0.397916</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.419303</td>\n",
       "      <td>0.312155</td>\n",
       "      <td>0.454467</td>\n",
       "      <td>0.300446</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.340120</td>\n",
       "      <td>0.319266</td>\n",
       "      <td>0.579612</td>\n",
       "      <td>0.427308</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.345915</td>\n",
       "      <td>0.345774</td>\n",
       "      <td>0.256491</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.347885</td>\n",
       "      <td>0.282799</td>\n",
       "      <td>0.569143</td>\n",
       "      <td>0.357938</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.350080</td>\n",
       "      <td>0.367260</td>\n",
       "      <td>0.272254</td>\n",
       "      <td>0.342806</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.203893</td>\n",
       "      <td>0.232139</td>\n",
       "      <td>0.232662</td>\n",
       "      <td>0.240702</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.265127</td>\n",
       "      <td>0.231013</td>\n",
       "      <td>0.303888</td>\n",
       "      <td>0.275272</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    score_long_LR  score_long_RL  score_short_LR  score_short_RL  best_decoder_index\n",
       "0        0.216853       0.255124        0.322392        0.331264                   3\n",
       "1        0.397501       0.285456        0.386622        0.267649                   0\n",
       "2        0.356712       0.336948        0.290276        0.358977                   3\n",
       "3        0.343649       0.264231        0.430961        0.294296                   2\n",
       "4        0.378424       0.318183        0.330432        0.397916                   3\n",
       "5        0.419303       0.312155        0.454467        0.300446                   2\n",
       "..            ...            ...             ...             ...                 ...\n",
       "82       0.340120       0.319266        0.579612        0.427308                   2\n",
       "83       0.345915       0.345774        0.256491        0.462687                   3\n",
       "84       0.347885       0.282799        0.569143        0.357938                   2\n",
       "85       0.350080       0.367260        0.272254        0.342806                   1\n",
       "86       0.203893       0.232139        0.232662        0.240702                   3\n",
       "87       0.265127       0.231013        0.303888        0.275272                   2\n",
       "\n",
       "[88 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## INPUTS: decoder_laps_radon_transform_df_dict\n",
    "\n",
    "## Build a dataframe from the radon transform results for each decoder:\n",
    "radon_transform_merged_df: pd.DataFrame = None\n",
    "filter_columns_fn = lambda df: df[['score']]\n",
    "for a_name, a_df in decoder_laps_radon_transform_df_dict.items():\n",
    "    # a_name: str = a_name.capitalize()\n",
    "    if radon_transform_merged_df is None:\n",
    "        radon_transform_merged_df = filter_columns_fn(deepcopy(a_df))\n",
    "        radon_transform_merged_df = radon_transform_merged_df.add_suffix(f\"_{a_name}\") # suffix the columns so they're unique\n",
    "    else:\n",
    "        ## append to the initial_df\n",
    "        # initial_df = initial_df.join(deepcopy(a_df), lsuffix=None, rsuffix=f'_{a_name}')\n",
    "        radon_transform_merged_df = radon_transform_merged_df.join(filter_columns_fn(deepcopy(a_df)).add_suffix(f\"_{a_name}\"), lsuffix=None, rsuffix=None)\n",
    "\n",
    "# Get the column name with the maximum value for each row\n",
    "# initial_df['best_decoder_index'] = initial_df.idxmax(axis=1)\n",
    "radon_transform_merged_df['best_decoder_index'] = radon_transform_merged_df.apply(lambda row: np.argmax(row.values), axis=1)\n",
    "\n",
    "radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare the radon-transform ['score'] column for each decoder\n",
    "laps_all_epoch_bins_marginals_df = deepcopy(directional_merged_decoders_result.laps_all_epoch_bins_marginals_df)\n",
    "ripple_all_epoch_bins_marginals_df = deepcopy(directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df)\n",
    "laps_all_epoch_bins_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb10d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: laps_all_epoch_bins_marginals_df\n",
    "\n",
    "## Get the probability of each decoder:\n",
    "a_marginals_df = deepcopy(laps_all_epoch_bins_marginals_df)\n",
    "a_marginals_df['P_Long_LR'] = a_marginals_df['P_LR'] * a_marginals_df['P_Long']\n",
    "a_marginals_df['P_Long_RL'] = a_marginals_df['P_RL'] * a_marginals_df['P_Long']\n",
    "a_marginals_df['P_Short_LR'] = a_marginals_df['P_LR'] * a_marginals_df['P_Short']\n",
    "a_marginals_df['P_Short_RL'] = a_marginals_df['P_RL'] * a_marginals_df['P_Short']\n",
    "assert np.allclose(a_marginals_df[['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']].sum(axis=1), 1.0)\n",
    "# Get the column name with the maximum value for each row\n",
    "# a_marginals_df['most_likely_decoder_index'] = a_marginals_df[['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']].idxmax(axis=1)\n",
    "a_marginals_df['most_likely_decoder_index'] = a_marginals_df[['P_Long_LR', 'P_Long_RL', 'P_Short_LR', 'P_Short_RL']].apply(lambda row: np.argmax(row.values), axis=1)\n",
    "\n",
    "## Merge in the RadonTransform df:\n",
    "a_marginals_df = a_marginals_df.join(radon_transform_merged_df)\n",
    "a_marginals_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783198e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_epochs: int = len(a_marginals_df)\n",
    "equal_rows_count: int = (a_marginals_df['most_likely_decoder_index'] == a_marginals_df['best_decoder_index']).sum()\n",
    "print(equal_rows_count)\n",
    "\n",
    "equal_rows_count/num_total_epochs\n",
    "\n",
    "# (a_marginals_df['most_likely_decoder_index'] == a_marginals_df['best_decoder_index']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd337f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional_lap_epochs_dict\n",
    "# laps_most_likely_direction_from_decoder\n",
    "# laps_merged_complete_epoch_stats_df # no marginals\n",
    "# laps_track_identity_marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# type(a_directional_laps_filter_epochs_decoder_result) # pyphoplacecellanalysis.Analysis.Decoder.reconstruction.DecodedFilterEpochsResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a518883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@property\n",
    "def radon_transform(self):\n",
    "    \"\"\"The radon_transform property.\"\"\"\n",
    "    return compute_radon_transforms(a_decoder, a_decoder_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# long_LR_pf1D_Decoder\n",
    "# long_LR_decoder.decod\n",
    "# long_LR_epochs_obj\n",
    "# long_LR_results.pf1D_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_decoder = deepcopy(directional_merged_decoders_result.all_directional_pf1D_Decoder)\n",
    "# a_decoder_result = deepcopy(directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "\n",
    "a_decoder = deepcopy(long_LR_pf1D_Decoder)\n",
    "a_decoder_result = deepcopy(a_directional_laps_filter_epochs_decoder_result)\n",
    "\n",
    "epochs_linear_fit_df_ACTIVE = compute_radon_transforms(a_decoder, a_decoder_result)\n",
    "# assert long_results_obj.active_filter_epochs.n_epochs == np.shape(epochs_linear_fit_df_LONG)[0]\n",
    "# long_results_obj.active_filter_epochs._df.drop(columns=['score', 'velocity', 'intercept', 'speed'], inplace=True, errors='ignore') # 'ignore' doesn't raise an exception if the columns don't already exist.\n",
    "# long_results_obj.active_filter_epochs._df = long_results_obj.active_filter_epochs.to_dataframe().join(epochs_linear_fit_df_LONG) # add the newly computed columns to the Epochs object\n",
    "# epochs_linear_fit_df_LONG\n",
    "epochs_linear_fit_df_ACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b15824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2023-05-25 - Get the 1D Posteriors for each replay epoch so they can be analyzed via score_posterior(...) with a Radon Transform approch to find the line of best fit (which gives the velocity).\n",
    "epochs_linear_fit_df_LONG = compute_radon_transforms(long_results_obj.original_1D_decoder, long_results_obj.all_included_filter_epochs_decoder_result)\n",
    "assert long_results_obj.active_filter_epochs.n_epochs == np.shape(epochs_linear_fit_df_LONG)[0]\n",
    "long_results_obj.active_filter_epochs._df.drop(columns=['score', 'velocity', 'intercept', 'speed'], inplace=True, errors='ignore') # 'ignore' doesn't raise an exception if the columns don't already exist.\n",
    "long_results_obj.active_filter_epochs._df = long_results_obj.active_filter_epochs.to_dataframe().join(epochs_linear_fit_df_LONG) # add the newly computed columns to the Epochs object\n",
    "# epochs_linear_fit_df_LONG\n",
    "\n",
    "epochs_linear_fit_df_SHORT = compute_radon_transforms(short_results_obj.original_1D_decoder, short_results_obj.all_included_filter_epochs_decoder_result)\n",
    "assert short_results_obj.active_filter_epochs.n_epochs == np.shape(epochs_linear_fit_df_SHORT)[0]\n",
    "short_results_obj.active_filter_epochs._df.drop(columns=['score', 'velocity', 'intercept', 'speed'], inplace=True, errors='ignore') # 'ignore' doesn't raise an exception if the columns don't already exist.\n",
    "short_results_obj.active_filter_epochs._df = short_results_obj.active_filter_epochs.to_dataframe().join(epochs_linear_fit_df_SHORT) # add the newly computed columns to the Epochs object\n",
    "# epochs_linear_fit_df_SHORT\n",
    "\n",
    "# Join the columns of LONG and SHORT dataframes with suffixes\n",
    "assert np.shape(epochs_linear_fit_df_LONG)[0] == np.shape(epochs_linear_fit_df_SHORT)[0]\n",
    "combined_epochs_linear_fit_df = epochs_linear_fit_df_LONG.join(epochs_linear_fit_df_SHORT, lsuffix='_LONG', rsuffix='_SHORT')\n",
    "# combined_epochs_linear_fit_df\n",
    "\n",
    "assert active_filter_epochs.n_epochs == np.shape(combined_epochs_linear_fit_df)[0]\n",
    "active_filter_epochs._df.drop(columns=['score_LONG', 'velocity_LONG', 'intercept_LONG', 'speed_LONG', 'score_SHORT', 'velocity_SHORT', 'intercept_SHORT', 'speed_SHORT'], inplace=True, errors='ignore') # 'ignore' doesn't raise an exception if the columns don't already exist.\n",
    "active_filter_epochs._df = active_filter_epochs.to_dataframe().join(combined_epochs_linear_fit_df) # add the newly computed columns to the Epochs object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8161a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0174187368f176085df53c2cec5d63d0b8400e63ae887d077931481ed3fab96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
