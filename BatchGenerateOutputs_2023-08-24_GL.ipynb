{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "BATCH_DATE_TO_USE = '2023-08-24' # used for filenames throught the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/global_batch_result_2023-08-24.pkl... done.\n",
      "Failure loading /nfs/turbo/umms-kdiba/Data/global_batch_result_2023-08-24.pkl.\n",
      "creating new batch_run\n",
      "local_session_names_list: ['2006-6-07_11-26-53', '2006-6-08_14-26-15', '2006-6-09_1-22-43', '2006-6-09_3-23-37', '2006-6-12_15-55-31', '2006-6-13_14-42-6']\n",
      "local_session_names_list: ['2006-6-07_16-40-19', '2006-6-08_15-46-47', '2006-6-08_21-16-25', '2006-6-09_22-24-40', '2006-6-12_16-53-46', '2006-6-13_15-22-3']\n",
      "local_session_names_list: ['2006-4-09_17-29-30', '2006-4-10_12-25-50', '2006-4-10_21-2-40', '2006-4-11_15-16-59', '2006-4-12_14-39-31', '2006-4-12_17-53-55', '2006-4-16_15-12-23', '2006-4-17_12-33-47', '2006-4-18_13-6-1', '2006-4-18_15-23-32', '2006-4-19_13-34-40', '2006-4-19_16-48-9', '2006-4-21_10-24-35', '2006-4-25_14-28-51', '2006-4-25_17-17-6', '2006-4-26_13-22-13', '2006-4-27_14-43-12', '2006-4-28_12-17-27', '2006-4-28_16-48-29']\n",
      "local_session_names_list: ['2006-4-09_16-40-54', '2006-4-10_12-58-3', '2006-4-10_19-11-57', '2006-4-11_12-48-38', '2006-4-11_16-2-46', '2006-4-12_14-59-23', '2006-4-12_15-25-59', '2006-4-16_14-49-24', '2006-4-16_18-47-52', '2006-4-17_12-52-15', '2006-4-18_13-28-57', '2006-4-18_15-38-2', '2006-4-19_13-50-7', '2006-4-19_16-37-40', '2006-4-21_11-19-2', '2006-4-25_13-20-55', '2006-4-25_17-33-28', '2006-4-26_13-51-50', '2006-4-27_18-21-57', '2006-4-28_12-38-13', '2006-4-28_17-6-14']\n",
      "local_session_names_list: ['11-02_17-46-44', '11-02_19-28-0', '11-03_12-3-25', '11-03_21-26-8', '11-05_19-26-43', '11-09_11-43-50', '11-09_12-15-3', '11-09_21-17-16', '11-09_22-4-5', '11-19_12-35-59', '11-19_13-2-0', '11-19_13-55-7', 'fet11-01_12-58-54', 'fet11-03_11-0-53', 'fet11-03_20-28-3', 'fet11-04_21-20-3', 'redundant', 'showclus', 'sleep', 'tmaze']\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/global_batch_result_2023-08-24.pkl... done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-07_11-26-53</td>\n",
       "      <td>kdiba_gor01_one_2006-6-07_11-26-53</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_3-23-37</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_3-23-37</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-04_21-20-3</td>\n",
       "      <td>kdiba_pin01_one_fet11-04_21-20-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>redundant</td>\n",
       "      <td>kdiba_pin01_one_redundant</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>showclus</td>\n",
       "      <td>kdiba_pin01_one_showclus</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>sleep</td>\n",
       "      <td>kdiba_pin01_one_sleep</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>tmaze</td>\n",
       "      <td>kdiba_pin01_one_tmaze</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "0        kdiba  gor01        one  2006-6-07_11-26-53   \n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "3        kdiba  gor01        one   2006-6-09_3-23-37   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "..         ...    ...        ...                 ...   \n",
       "67       kdiba  pin01        one    fet11-04_21-20-3   \n",
       "68       kdiba  pin01        one           redundant   \n",
       "69       kdiba  pin01        one            showclus   \n",
       "70       kdiba  pin01        one               sleep   \n",
       "71       kdiba  pin01        one               tmaze   \n",
       "\n",
       "                               context  \\\n",
       "0   kdiba_gor01_one_2006-6-07_11-26-53   \n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "3    kdiba_gor01_one_2006-6-09_3-23-37   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "..                                 ...   \n",
       "67    kdiba_pin01_one_fet11-04_21-20-3   \n",
       "68           kdiba_pin01_one_redundant   \n",
       "69            kdiba_pin01_one_showclus   \n",
       "70               kdiba_pin01_one_sleep   \n",
       "71               kdiba_pin01_one_tmaze   \n",
       "\n",
       "                                             basedirs  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...   \n",
       "69  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...   \n",
       "70   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep   \n",
       "71   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze   \n",
       "\n",
       "                              status errors  n_long_laps  n_long_replays  \\\n",
       "0   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "1   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "2   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "3   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "4   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "..                               ...    ...          ...             ...   \n",
       "67  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "68  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "69  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "70  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "71  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "\n",
       "    n_short_laps  n_short_replays  is_ready  \\\n",
       "0              0                0     False   \n",
       "1              0                0     False   \n",
       "2              0                0     False   \n",
       "3              0                0     False   \n",
       "4              0                0     False   \n",
       "..           ...              ...       ...   \n",
       "67             0                0     False   \n",
       "68             0                0     False   \n",
       "69             0                0     False   \n",
       "70             0                0     False   \n",
       "71             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "0                         False                                  True  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "3                         False                                  True  \n",
       "4                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "67                        False                                  True  \n",
       "68                        False                                 False  \n",
       "69                        False                                 False  \n",
       "70                        False                                 False  \n",
       "71                        False                                 False  \n",
       "\n",
       "[72 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# active_global_batch_result_filename='global_batch_result_2023-07-12.pkl'\n",
    "# active_global_batch_result_filename='global_batch_result_2023-07-07.pkl'\n",
    "# active_global_batch_result_filename='global_batch_result_2023-07-07_extra.pkl'\n",
    "# active_global_batch_result_filename='global_batch_result_2023-07-20.pkl'\n",
    "# active_global_batch_result_filename='global_batch_result_2023-07-21_Greatlakes.pkl'\n",
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/nfs/turbo/umms-kdiba/Data')]\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "# try to load an existing batch result:\n",
    "# with set_posix_windows():\n",
    "global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     # display(batch_progress_df)\n",
    "#     # display(good_only_batch_progress_df)\n",
    "#     display(batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019afbbd-70d2-4e75-9548-b6f22d2e31ca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>kdiba_gor01_two_2006-6-07_16-40-19</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>kdiba_gor01_two_2006-6-08_21-16-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-09_22-24-40</td>\n",
       "      <td>kdiba_gor01_two_2006-6-09_22-24-40</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-12_16-53-46</td>\n",
       "      <td>kdiba_gor01_two_2006-6-12_16-53-46</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-4-09_17-29-30</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-09_17-29-30</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-4-10_12-25-50</td>\n",
       "      <td>kdiba_vvp01_one_2006-4-10_12-25-50</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-09_16-40-54</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-09_16-40-54</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-10_12-58-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>kdiba_pin01_one_11-02_17-46-44</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>kdiba_pin01_one_11-02_19-28-0</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>kdiba_pin01_one_11-03_12-3-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>kdiba_pin01_one_fet11-01_12-58-54</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "6        kdiba  gor01        two  2006-6-07_16-40-19   \n",
       "8        kdiba  gor01        two  2006-6-08_21-16-25   \n",
       "9        kdiba  gor01        two  2006-6-09_22-24-40   \n",
       "10       kdiba  gor01        two  2006-6-12_16-53-46   \n",
       "12       kdiba  vvp01        one  2006-4-09_17-29-30   \n",
       "13       kdiba  vvp01        one  2006-4-10_12-25-50   \n",
       "31       kdiba  vvp01        two  2006-4-09_16-40-54   \n",
       "32       kdiba  vvp01        two   2006-4-10_12-58-3   \n",
       "52       kdiba  pin01        one      11-02_17-46-44   \n",
       "53       kdiba  pin01        one       11-02_19-28-0   \n",
       "54       kdiba  pin01        one       11-03_12-3-25   \n",
       "64       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "\n",
       "                               context  \\\n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "6   kdiba_gor01_two_2006-6-07_16-40-19   \n",
       "8   kdiba_gor01_two_2006-6-08_21-16-25   \n",
       "9   kdiba_gor01_two_2006-6-09_22-24-40   \n",
       "10  kdiba_gor01_two_2006-6-12_16-53-46   \n",
       "12  kdiba_vvp01_one_2006-4-09_17-29-30   \n",
       "13  kdiba_vvp01_one_2006-4-10_12-25-50   \n",
       "31  kdiba_vvp01_two_2006-4-09_16-40-54   \n",
       "32   kdiba_vvp01_two_2006-4-10_12-58-3   \n",
       "52      kdiba_pin01_one_11-02_17-46-44   \n",
       "53       kdiba_pin01_one_11-02_19-28-0   \n",
       "54       kdiba_pin01_one_11-03_12-3-25   \n",
       "64   kdiba_pin01_one_fet11-01_12-58-54   \n",
       "\n",
       "                                             basedirs  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "9   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "10  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "12  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "13  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "31  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                              status errors  n_long_laps  n_long_replays  \\\n",
       "1   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "2   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "4   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "6   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "8   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "9   SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "10  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "12  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "13  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "31  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "32  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "52  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "53  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "54  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "64  SessionBatchProgress.NOT_STARTED   None            0               0   \n",
       "\n",
       "    n_short_laps  n_short_replays  is_ready  \\\n",
       "1              0                0     False   \n",
       "2              0                0     False   \n",
       "4              0                0     False   \n",
       "6              0                0     False   \n",
       "8              0                0     False   \n",
       "9              0                0     False   \n",
       "10             0                0     False   \n",
       "12             0                0     False   \n",
       "13             0                0     False   \n",
       "31             0                0     False   \n",
       "32             0                0     False   \n",
       "52             0                0     False   \n",
       "53             0                0     False   \n",
       "54             0                0     False   \n",
       "64             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "9   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "10  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "12  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "13  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "31  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "9   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "10  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "12  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "13  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "31  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "9   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "10  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "12  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "13  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "31  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "4                          True                                  True  \n",
       "6                          True                                  True  \n",
       "8                          True                                  True  \n",
       "9                          True                                  True  \n",
       "10                         True                                  True  \n",
       "12                         True                                  True  \n",
       "13                         True                                  True  \n",
       "31                         True                                  True  \n",
       "32                         True                                  True  \n",
       "52                         True                                  True  \n",
       "53                         True                                  True  \n",
       "54                         True                                  True  \n",
       "64                         True                                  True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # Bad?\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54')]\n",
    "\n",
    "\n",
    "included_session_batch_progress_df = batch_progress_df[np.isin(batch_progress_df['context'].values, included_session_contexts)]\n",
    "included_session_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_h5_paths = [a_dir.joinpath('output','pipeline_results.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']]\n",
    "included_global_computation_h5_paths = [a_dir.joinpath('output','global_computations.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']] \n",
    "included_h5_paths\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata\n",
    "\n",
    "[FilesystemMetadata.get_last_modified_time(a_file) for a_file in included_h5_paths]\n",
    "[FilesystemMetadata.get_last_modified_time(a_file) for a_file in included_h5_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1bb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import H5FileReference, H5ExternalLinkBuilder, ExternallyReferencedItem\n",
    "# restore native column types:\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_Converter\n",
    "\n",
    "session_short_names: List[str] = [a_ctxt.get_description(separator='_') for a_ctxt in included_session_contexts] # 'kdiba.gor01.one.2006-6-08_14-26-15'\n",
    "session_group_keys: List[str] = [(\"/\" + a_ctxt.get_description(separator=\"/\", include_property_names=False)) for a_ctxt in included_session_contexts] # 'kdiba/gor01/one/2006-6-08_14-26-15'\n",
    "# Particular Table Keys:\n",
    "neuron_replay_stats_df_table_keys = [f\"{session_group_key}/global_computations/jonathan_fr_analysis/neuron_replay_stats_df/table\" for session_group_key in session_group_keys]\n",
    "neuron_identities_table_keys = [f\"{session_group_key}/neuron_identities/table\" for session_group_key in session_group_keys]\n",
    "long_short_fr_indicies_analysis_table_keys = [f\"{session_group_key}/global_computations/long_short_fr_indicies_analysis/table\" for session_group_key in session_group_keys]\n",
    "\n",
    "a_loader = H5ExternalLinkBuilder.init_from_file_lists(file_list=included_h5_paths, table_key_list=[f\"{session_group_key}/neuron_identities/table\" for session_group_key in session_group_keys], short_name_list=session_short_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# neuron_identities_table_keys = [ExternallyReferencedItem(f\"{session_group_key}/neuron_identities/table\", '/neuron_identities') for session_group_key in session_group_keys]\n",
    "destination_file_path, external_file_links = a_loader.build_linking_results('output/test_linking_file.h5', referential_group_key='neuron_identities', table_key_list=neuron_identities_table_keys, fail_on_exception=False)\n",
    "external_file_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3856e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "destination_file_path, external_file_links = a_loader.build_linking_results('output/test_linking_file_new.h5', referential_group_key='long_short_fr_indicies_analysis', table_key_list=long_short_fr_indicies_analysis_table_keys, fail_on_exception=False)\n",
    "external_file_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination_file_path, external_file_links = a_loader.build_linking_results('output/test_linking_file_new.h5', referential_group_key='neuron_replay_stats_df', table_key_list=neuron_replay_stats_df_table_keys, fail_on_exception=False)\n",
    "# external_file_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_table = a_loader.load_and_consolidate(table_key_list=neuron_replay_stats_df_table_keys, fail_on_exception=False)\n",
    "_out_table = HDF_Converter.general_post_load_restore_table_as_needed(_out_table)\n",
    "# Drop columns: 'neuron_IDX', 'has_short_pf' and 3 other columns\n",
    "_out_table = _out_table.drop(columns=['neuron_IDX', 'has_short_pf', 'has_na', 'has_long_pf', 'index'])\n",
    "_out_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_table.groupby(['session_uid', 'track_membership']).agg(index_count=('index', 'count')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_table_long_short_fr_indicies_analysis = a_loader.load_and_consolidate(table_key_list=long_short_fr_indicies_analysis_table_keys, fail_on_exception=False)\n",
    "_out_table_long_short_fr_indicies_analysis = HDF_Converter.general_post_load_restore_table_as_needed(_out_table_long_short_fr_indicies_analysis)\n",
    "_out_table_long_short_fr_indicies_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8486431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=_out_table_long_short_fr_indicies_analysis, num_sessions=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f293a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: More robust and automatic attempt to restore native column types:\n",
    "# _out_table = HDF_Converter.HDFConvertableEnum.restore_hdf_dataframe_column_original_type(_out_table, column_name='track_membership') # AttributeError: 'int' object has no attribute 'lower'\n",
    "# SplitPartitionMembership.restore_hdf_dataframe_column_original_type(_out_table, column_name='track_membership')\n",
    "# _out_table = HDF_Converter.HDFConvertableEnum.restore_hdf_dataframe_column_original_type(_out_table, column_name='neuron_type')\n",
    "# _out_table[\"neuron_type\"] = _out_table[\"neuron_type\"].apply(lambda x: NeuronType.from_hdf_coding_string(x)).astype(object) #.astype(str) # interestingly this leaves the dtype of this column as 'category' still, but _spikes_df[\"cell_type\"].to_numpy() returns the correct array of objects... this is better than it started before saving, but not the same. \n",
    "_out_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cf7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_table.groupby(['session_uid', 'track_membership', 'neuron_type']).agg(neuron_uid_count=('neuron_uid', 'count')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a49b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970190b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa42831d-0708-4625-bc76-8e46d85344ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from neuropy.core import Epoch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e7cda-d9b5-4735-bfc7-049bd5b9a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "@define(slots=False, repr=False)\n",
    "class HDFSpecificBatchSessionCompletionHandler(BatchSessionCompletionHandler):\n",
    "    \"\"\" handles completion of a single session's batch processing. \n",
    "\n",
    "    Allows accumulating results across sessions and runs.\n",
    "\n",
    "    \n",
    "    Usage:\n",
    "        from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Completion Result object returned from callback ____________________________________________________________________ #\n",
    " \n",
    "    # Cross-session Results:\n",
    "    across_sessions_instantaneous_fr_dict: dict = Factory(dict) # Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "    ## Main function that's called with the complete pipeline:\n",
    "    def on_complete_success_execution_session(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline) -> PipelineCompletionResult:\n",
    "        \"\"\" called when the execute_session completes like:\n",
    "            `post_run_callback_fn_output = post_run_callback_fn(curr_session_context, curr_session_basedir, curr_active_pipeline)`\n",
    "            \n",
    "            Meant to be assigned like:\n",
    "            , post_run_callback_fn=_on_complete_success_execution_session\n",
    "            \n",
    "            Captures nothing.\n",
    "            \n",
    "            from Spike3D.scripts.run_BatchAnalysis import _on_complete_success_execution_session\n",
    "            \n",
    "            \n",
    "            LOGIC: really we want to recompute global whenever local is recomputed.\n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "        print(f'HDFProcessing.on_complete_success_execution_session(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...)')\n",
    "        # print(f'curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}')\n",
    "        long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "        # long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "        # long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "\n",
    "        # Get existing laps from session:\n",
    "        long_laps, short_laps, global_laps = [curr_active_pipeline.filtered_sessions[an_epoch_name].laps.as_epoch_obj() for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "        long_replays, short_replays, global_replays = [Epoch(curr_active_pipeline.filtered_sessions[an_epoch_name].replay.epochs.get_valid_df()) for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "        # short_laps.n_epochs: 40, n_long_laps.n_epochs: 40\n",
    "        # short_replays.n_epochs: 6, long_replays.n_epochs: 8\n",
    "        if self.debug_print:\n",
    "            print(f'short_laps.n_epochs: {short_laps.n_epochs}, n_long_laps.n_epochs: {long_laps.n_epochs}')\n",
    "            print(f'short_replays.n_epochs: {short_replays.n_epochs}, long_replays.n_epochs: {long_replays.n_epochs}')\n",
    "\n",
    "        # ## Post Compute Validate 2023-05-16:\n",
    "        try:\n",
    "            was_updated = self.post_compute_validate(curr_active_pipeline)\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            an_err = CapturedException(e, exception_info)\n",
    "            print(f'self.post_compute_validate(...) failed with exception: {an_err}')\n",
    "            raise \n",
    "\n",
    "        delta_since_last_compute: timedelta = curr_active_pipeline.get_time_since_last_computation()\n",
    "        print(f'\\t time since last computation: {delta_since_last_compute}')\n",
    "        \n",
    "        ## Save the pipeline since that's disabled by default now:\n",
    "        try:\n",
    "            curr_active_pipeline.save_pipeline(saving_mode=self.saving_mode, active_pickle_filename=self.override_session_computation_results_pickle_filename) # AttributeError: 'PfND_TimeDependent' object has no attribute '_included_thresh_neurons_indx'\n",
    "        except Exception as e:\n",
    "            ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'ERROR SAVING PIPELINE for curr_session_context: {curr_session_context}. error: {e}')\n",
    "\n",
    "\n",
    "        ## GLOBAL FUNCTION:\n",
    "        if self.force_reload_all and (not self.force_global_recompute):\n",
    "            print(f'WARNING: self.force_global_recompute was False but self.force_reload_all was true. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.')\n",
    "            self.force_global_recompute = True\n",
    "            \n",
    "        if was_updated and (not self.force_global_recompute):\n",
    "            print(f'WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.')\n",
    "            self.force_global_recompute = True\n",
    "\n",
    "\n",
    "        ## GLOBAL FUNCTION:\n",
    "        if self.force_reload_all and (not self.force_global_recompute):\n",
    "            print(f'WARNING: self.force_global_recompute was False but self.force_reload_all was true. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.')\n",
    "            self.force_global_recompute = True\n",
    "            \n",
    "        if was_updated and (not self.force_global_recompute):\n",
    "            print(f'WARNING: self.force_global_recompute was False but pipeline was_updated. The global properties must be recomputed when the local functions change, so self.force_global_recompute will be set to True and computation will continue.')\n",
    "            self.force_global_recompute = True\n",
    "\n",
    "        self.try_compute_global_computations_if_needed(curr_active_pipeline, curr_session_context=curr_session_context)\n",
    "\n",
    "        ### Aggregate Outputs specific computations:\n",
    "\n",
    "        ## Get some more interesting session properties:\n",
    "        \n",
    "        delta_since_last_compute: timedelta = curr_active_pipeline.get_time_since_last_computation()\n",
    "        \n",
    "        print(f'\\t time since last computation: {delta_since_last_compute}')\n",
    "\n",
    "        # Export the pipeline's HDF5:\n",
    "        hdf5_output_path, hdf5_output_err = self.try_export_pipeline_hdf5_if_needed(curr_active_pipeline=curr_active_pipeline, curr_session_context=curr_session_context)\n",
    "        \n",
    "        # ## Specify the output file:\n",
    "        # common_file_path = Path('output/active_across_session_scatter_plot_results.h5')\n",
    "        # print(f'common_file_path: {common_file_path}')\n",
    "        # InstantaneousFiringRatesDataframeAccessor.add_results_to_inst_fr_results_table(curr_active_pipeline, common_file_path, file_mode='a')\n",
    "\n",
    "        across_session_results_extended_dict = {}\n",
    "            \n",
    "        return PipelineCompletionResult(long_epoch_name=long_epoch_name, long_laps=long_laps, long_replays=long_replays,\n",
    "                                           short_epoch_name=short_epoch_name, short_laps=short_laps, short_replays=short_replays,\n",
    "                                           delta_since_last_compute=delta_since_last_compute,\n",
    "                                           outputs_local={'pkl': curr_active_pipeline.pickle_path},\n",
    "                                            outputs_global={'pkl': curr_active_pipeline.global_computation_results_pickle_path, 'hdf5': hdf5_output_path},\n",
    "                                            across_session_results=across_session_results_extended_dict)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629c5dda",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(included_session_contexts): 15\n",
      "Beginning processing with len(included_session_contexts): 15\n",
      "build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15.log\n",
      "build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31\"):========================== runBatch STARTING ==========================\n",
      "build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43.log\n",
      "\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31.log========================== runBatch STARTING ==========================\n",
      "\tsession_context: kdiba_gor01_one_2006-6-08_14-26-15\n",
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\n",
      "\tsession_context: kdiba_gor01_one_2006-6-09_1-22-43\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43\n",
      "__________________________________________________________________\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43========================== runBatch STARTING ==========================\n",
      "\n",
      "__________________________________________________________________\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15active_data_mode_name: kdiba\tsession_context: kdiba_gor01_one_2006-6-12_15-55-31\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "\n",
      "active_data_mode_name: kdiba\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31Skipping loading from pickled file because force_reload == True.\n",
      "\n",
      "\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "active_data_mode_name: kdiba"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.epochs_info.mat...Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.epochs_info.mat...  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.epochs_info.mat... done.\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.position_info.mat...done.\n",
      " Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.position_info.mat...Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.position_info.mat...  done.\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.spikes.mat...Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.spikes.mat...  done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.spikes.mat... done.\n",
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 3 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.position.npy... 2006-6-12_15-55-31.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.interpolated_spike_positions.npy... 2006-6-12_15-55-31.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.mua.npy... 2006-6-12_15-55-31.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "done.\n",
      "Saving pbe results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/2006-6-12_15-55-31.pbe.npy... 2006-6-12_15-55-31.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "Saving updated position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.position.npy... 2006-6-08_14-26-15.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.interpolated_spike_positions.npy... Saving updated position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.position.npy... 2006-6-09_1-22-43.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.interpolated_spike_positions.npy... 2006-6-08_14-26-15.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "2006-6-09_1-22-43.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/ripple_df.pkl.\n",
      "\t curr_replays: 92\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']Loading success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/ripple_df.pkl.\n",
      "\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 656.0648088779999)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 656.0648088779999, end: 1122.1864874939201)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.mua.npy... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "2006-6-08_14-26-15.mua.npy saved\n",
      "Constraining to epoch with times (start: 0.0, end: 1122.1864874939201)done.\n",
      "\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving pbe results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/2006-6-08_14-26-15.pbe.npy... 2006-6-08_14-26-15.pbe.npy saved\n",
      "done.due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Computing spikes_df PBEs column results : spikes_df... \n",
      "Recomputing active_epoch_placefields... Saving mua results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.mua.npy... 2006-6-09_1-22-43.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/2006-6-09_1-22-43.pbe.npy... 2006-6-09_1-22-43.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (18286,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (18286,)!\n",
      "done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\t curr_replays: 321\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1029.316608761903)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1029.316608761903, end: 1737.1968310000375)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1737.1968310000375)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... \t curr_replays: 503\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1211.5580800310709)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1211.5580800310709, end: 2093.8978568242164)\n",
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2093.8978568242164)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (30442,)!\n",
      "using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (30442,)!\n",
      "using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (27562,) should be less than time_window_edges: (35889,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (18467,)\n",
      "\n",
      "\t spikes_df[time_variable_name]: (27562,) should be less than time_window_edges: (35889,)!\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (12943,)\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20211,) should be less than time_window_edges: (31969,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (20211,) should be less than time_window_edges: (31969,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (30745,)\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36246,)\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (18711,)\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19981,) should be less than time_window_edges: (24848,)!\n",
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (32287,)\n",
      "using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (49509,) should be less than time_window_edges: (49637,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (19981,) should be less than time_window_edges: (24848,)!\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (49509,) should be less than time_window_edges: (49637,)!\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl... done.\n",
      "included includelist is specified: ['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'long_short_decoding_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (18286,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (18286,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (25095,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 49, 16, 466974)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47543,) should be less than time_window_edges: (61862,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 49, 43, 197234)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (18286,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11696,) should be less than time_window_edges: (18286,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47543,) should be less than time_window_edges: (61862,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 50, 24, 240193)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (50132,)\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl... _execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 50, 49, 531383)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8515,) should be less than time_window_edges: (12817,)!\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "done.\n",
      "included includelist is specified: ['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'long_short_decoding_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "(n_neurons = 36, n_all_epoch_timebins = 183)\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "(n_neurons = 36, n_all_epoch_timebins = 183)\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (30442,)!\n",
      "\t done.\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: ['jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'long_short_decoding_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups']. Saving global results...\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (30442,)!\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:02:03.713774\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.InstantaneousSpikeRateGroupsComputation'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps:\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Fig2_Replay_FR\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2_Replay_FR\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "a_field: Fig2_Laps_FR\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2_Laps_FR\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "a_field: LxC_ReplayDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: LxC_ReplayDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ReplayDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ReplayDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: LxC_ThetaDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: LxC_ThetaDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ThetaDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ThetaDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5, with key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/one/2006-6-12_15-55-31/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "an_attribute_field: instantaneous_time_bin_size_seconds\n",
      "an_attribute_field: active_identifying_session_ctx\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: output/test_ExpectedVsObservedResult.h5, with key: /expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/Flat_epoch_time_bins_mean! Will be replaced by new value.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_ptp_LONG! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_mean_LONG! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_std_LONG! Will be replaced by new value.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_ptp_SHORT! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_mean_SHORT! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_std_SHORT! Will be replaced by new value.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "common_file_path: output/active_across_session_scatter_plot_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "\t\t done (success).\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-07_16-40-19\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.epochs_info.mat... done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.position_info.mat... done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.spikes.mat... _execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 54, 2, 504404)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Saving updated position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.position.npy... 2006-6-07_16-40-19.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.interpolated_spike_positions.npy... 2006-6-07_16-40-19.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.mua.npy... 2006-6-07_16-40-19.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/2006-6-07_16-40-19.pbe.npy... 2006-6-07_16-40-19.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... done.\n",
      "Computing added spike scISI column results : spikes_df... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (62479,)\n",
      "\t curr_replays: 545\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1236.2662453636294)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 1236.2662453636294, end: 2587.801681999932)\n",
      "computing neurons mua for session...\n",
      "\n",
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2587.801681999932)\n",
      "computing neurons mua for session...\n",
      "\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (18768,) should be less than time_window_edges: (35780,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 54, 52, 382914)}\t spikes_df[time_variable_name]: (18768,) should be less than time_window_edges: (35780,)!\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (30442,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (28101,) should be less than time_window_edges: (30442,)!\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-08_14-26-15, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl... done.\n",
      "included includelist is specified: ['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'long_short_decoding_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 56, 18, 229882)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36136,)\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15458,) should be less than time_window_edges: (38260,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (15458,) should be less than time_window_edges: (38260,)!\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 9, 57, 8, 694771)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "using self.config.grid_bin_bounds_1D: (29.16, 261.7)\n",
      "using self.config.grid_bin_bounds: ((29.16, 261.7), (130.23, 150.99))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (27562,) should be less than time_window_edges: (35889,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (27562,) should be less than time_window_edges: (35889,)!\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-1:\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py\", line 1468, in run_specific_batch\n",
      "    curr_active_pipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NonInteractiveProcessing.py\", line 203, in batch_load_session\n",
      "    curr_active_pipeline.perform_computations(a_computation_config, computation_functions_name_includelist=computation_functions_name_includelist, computation_functions_name_excludelist=computation_functions_name_excludelist, fail_on_exception=fail_on_exception, debug_print=debug_print) #, overwrite_extant_results=False  ], fail_on_exception=True, debug_print=False)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Model/SpecificComputationValidation.py\", line 50, in _perform_try_computation_if_needed\n",
      "    comp_specifier.validate_computation_test(curr_active_pipeline)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 846, in perform_computations\n",
      "    self.stage.perform_action_for_all_contexts(EvaluationActions.EVALUATE_COMPUTATIONS, enabled_filter_names=enabled_filter_names, active_computation_params=active_computation_params, overwrite_extant_results=overwrite_extant_results,\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NonInteractiveProcessing.py\", line 278, in <lambda>\n",
      "    SpecificComputationValidator(short_name='long_short_decoding_analyses', computation_fn_name='_perform_long_short_decoding_analyses', validate_computation_test=lambda curr_active_pipeline: (curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis'].long_results_obj, curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis'].short_results_obj), is_global=True),\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 433, in perform_action_for_all_contexts\n",
      "    active_computation_results[a_select_config_name] = self.perform_registered_computations_single_context(active_computation_results[a_select_config_name],\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 243, in perform_registered_computations_single_context\n",
      "    return ComputedPipelineStage._execute_computation_functions(active_computation_functions, previous_computation_result=previous_computation_result, fail_on_exception=fail_on_exception, progress_logger_callback=progress_logger_callback, are_global=are_global, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py\", line 33, in __getitem__\n",
      "    return self._mapping[key] #@IgnoreException\n",
      "KeyError: 'long_short_leave_one_out_decoding_analysis'\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 592, in _execute_computation_functions\n",
      "    previous_computation_result = f(previous_computation_result, **computation_kwargs_list[i])\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/DefaultComputationFunctions.py\", line 196, in _perform_two_step_position_decoding_computation\n",
      "    computation_result.computed_data[two_step_decoder_key] = _subfn_compute_two_step_decoder(active_xbins, active_ybins, prev_one_step_bayesian_decoder, computation_result.sess.position.df, computation_config=computation_result.computation_config, debug_print=debug_print)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/DefaultComputationFunctions.py\", line 130, in _subfn_compute_two_step_decoder\n",
      "    two_step_decoder_result['flat_p_x_given_n_and_x_prev'][:,time_window_bin_idx] = Zhang_Two_Step.compute_bayesian_two_step_prob_single_timestep(flat_p_x_given_n, prev_x_position, two_step_decoder_result['flat_all_x'], two_step_decoder_result['flat_sigma_t_all'], two_step_decoder_result['C'], active_k) # output shape (1856, )\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 380, in compute_bayesian_two_step_prob_single_timestep\n",
      "    return k * one_step_p_x_given_n * cls.compute_conditional_probability_x_prev_given_x_t(x_prev, all_x, sigma_t, C)\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 363, in compute_conditional_probability_x_prev_given_x_t\n",
      "    numerator = -np.square(np.linalg.norm(x - x_prev, axis=1)) # (1950,)\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py\", line 1487, in run_specific_batch\n",
      "    post_run_callback_fn_output = post_run_callback_fn(global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py\", line 1264, in on_complete_success_execution_session\n",
      "    self.try_compute_global_computations_if_needed(curr_active_pipeline, curr_session_context=curr_session_context)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py\", line 1155, in try_compute_global_computations_if_needed\n",
      "    newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=self.extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True, force_recompute=self.force_global_recompute, debug_print=False)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NonInteractiveProcessing.py\", line 286, in batch_extended_computations\n",
      "    newly_computed_values += _comp_specifier.try_computation_if_needed(curr_active_pipeline, on_already_computed_fn=_subfn_on_already_computed, fail_on_exception=fail_on_exception, progress_print=progress_print, debug_print=debug_print, force_recompute=force_recompute)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Model/SpecificComputationValidation.py\", line 37, in try_computation_if_needed\n",
      "    return self._perform_try_computation_if_needed(self, curr_active_pipeline, **kwargs)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Model/SpecificComputationValidation.py\", line 64, in _perform_try_computation_if_needed\n",
      "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=[comp_specifier.computation_fn_name], computation_kwargs_list=[comp_specifier.computation_fn_kwargs], fail_on_exception=True, debug_print=False) # fail_on_exception MUST be True or error handling is all messed up\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 873, in perform_specific_computation\n",
      "    return self.stage.perform_specific_computation(active_computation_params=active_computation_params, enabled_filter_names=enabled_filter_names, computation_functions_name_includelist=computation_functions_name_includelist, computation_kwargs_list=computation_kwargs_list, fail_on_exception=fail_on_exception, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 523, in perform_specific_computation\n",
      "    self.global_computation_results = self.run_specific_computations_single_context(global_kwargs, computation_functions_name_includelist=computation_functions_name_includelist, are_global=True, fail_on_exception=fail_on_exception, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 262, in run_specific_computations_single_context\n",
      "    return ComputedPipelineStage._execute_computation_functions(active_computation_functions, previous_computation_result=previous_computation_result, computation_kwargs_list=computation_kwargs_list, fail_on_exception=fail_on_exception, progress_logger_callback=progress_logger_callback, are_global=are_global, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 592, in _execute_computation_functions\n",
      "    previous_computation_result = f(previous_computation_result, **computation_kwargs_list[i])\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/MultiContextComputationFunctions.py\", line 18, in _\n",
      "    x[1] = global_comp_fcn(*x) # update global_computation_results\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py\", line 383, in _perform_long_short_decoding_analyses\n",
      "    (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_long_short_constrained_decoders(owning_pipeline_reference, recalculate_anyway=True)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py\", line 1166, in compute_long_short_constrained_decoders\n",
      "    long_one_step_decoder_1D, short_one_step_decoder_1D, long_two_step_decoder_1D, short_two_step_decoder_1D = compute_short_long_constrained_decoders_1D(curr_active_pipeline, enable_two_step_decoders=enable_two_step_decoders)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py\", line 1116, in compute_short_long_constrained_decoders_1D\n",
      "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_perform_position_decoding_computation'], computation_kwargs_list=[dict(ndim=1)], enabled_filter_names=[long_epoch_name, short_epoch_name], fail_on_exception=True, debug_print=True)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 538, in perform_specific_computation\n",
      "    self.computation_results[a_select_config_name] = self.run_specific_computations_single_context(previous_computation_result, computation_functions_name_includelist=computation_functions_name_includelist, computation_kwargs_list=computation_kwargs_list, are_global=False, fail_on_exception=fail_on_exception, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 262, in run_specific_computations_single_context\n",
      "    return ComputedPipelineStage._execute_computation_functions(active_computation_functions, previous_computation_result=previous_computation_result, computation_kwargs_list=computation_kwargs_list, fail_on_exception=fail_on_exception, progress_logger_callback=progress_logger_callback, are_global=are_global, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 592, in _execute_computation_functions\n",
      "    previous_computation_result = f(previous_computation_result, **computation_kwargs_list[i])\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/DefaultComputationFunctions.py\", line 53, in _perform_position_decoding_computation\n",
      "    return position_decoding_computation(computation_result.sess, placefield_computation_config, computation_result)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/DefaultComputationFunctions.py\", line 49, in position_decoding_computation\n",
      "    prev_output_result.computed_data['pf2D_Decoder'].compute_all() #\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 1322, in compute_all\n",
      "    self.time_binning_container, self.p_x_given_n, self.most_likely_positions, curr_unit_marginal_x, curr_unit_marginal_y, flat_outputs_container = self.hyper_perform_decode(self.spikes_df, decoding_time_bin_size=self.time_bin_size, output_flat_versions=True, debug_print=(debug_print or self.debug_print))\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 738, in hyper_perform_decode\n",
      "    most_likely_positions, p_x_given_n, most_likely_position_indicies, flat_outputs_container = self.decode(spkcount, time_bin_size=decoding_time_bin_size, output_flat_versions=output_flat_versions, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 682, in decode\n",
      "    curr_flat_p_x_given_n = ZhangReconstructionImplementation.neuropy_bayesian_prob(time_bin_size, self.P_x, self.F, unit_specific_time_binned_spike_counts, debug_intermediates_mode=debug_intermediates_mode, use_flat_computation_mode=use_flat_computation_mode, debug_print=(debug_print or self.debug_print))\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 313, in neuropy_bayesian_prob\n",
      "    cell_prob *= (((tau * cell_ratemap) ** cell_spkcnt) * coeff) * (np.exp(-tau * cell_ratemap)) # product equal using *=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25\"):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25.log\n",
      "========================== runBatch STARTING =========================="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-08_21-16-25\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Model/SpecificComputationValidation.py\", line 50, in _perform_try_computation_if_needed\n",
      "    comp_specifier.validate_computation_test(curr_active_pipeline)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_data_mode_name: kdiba"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NonInteractiveProcessing.py\", line 278, in <lambda>\n",
      "    SpecificComputationValidator(short_name='long_short_decoding_analyses', computation_fn_name='_perform_long_short_decoding_analyses', validate_computation_test=lambda curr_active_pipeline: (curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis'].long_results_obj, curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis'].short_results_obj), is_global=True),\n",
      "  File \"/home/halechr/repos/pyPhoCoreHelpers/src/pyphocorehelpers/DataStructure/dynamic_parameters.py\", line 33, in __getitem__\n",
      "    return self._mapping[key] #@IgnoreException\n",
      "KeyError: 'long_short_leave_one_out_decoding_analysis'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py\", line 1487, in run_specific_batch\n",
      "    post_run_callback_fn_output = post_run_callback_fn(global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py\", line 1264, in on_complete_success_execution_session\n",
      "    self.try_compute_global_computations_if_needed(curr_active_pipeline, curr_session_context=curr_session_context)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py\", line 1155, in try_compute_global_computations_if_needed\n",
      "    newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=self.extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True, force_recompute=self.force_global_recompute, debug_print=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping loading from pickled file because force_reload == True."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/NonInteractiveProcessing.py\", line 286, in batch_extended_computations\n",
      "    newly_computed_values += _comp_specifier.try_computation_if_needed(curr_active_pipeline, on_already_computed_fn=_subfn_on_already_computed, fail_on_exception=fail_on_exception, progress_print=progress_print, debug_print=debug_print, force_recompute=force_recompute)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Model/SpecificComputationValidation.py\", line 37, in try_computation_if_needed\n",
      "    return self._perform_try_computation_if_needed(self, curr_active_pipeline, **kwargs)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Model/SpecificComputationValidation.py\", line 64, in _perform_try_computation_if_needed\n",
      "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=[comp_specifier.computation_fn_name], computation_kwargs_list=[comp_specifier.computation_fn_kwargs], fail_on_exception=True, debug_print=False) # fail_on_exception MUST be True or error handling is all messed up\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 873, in perform_specific_computation\n",
      "    return self.stage.perform_specific_computation(active_computation_params=active_computation_params, enabled_filter_names=enabled_filter_names, computation_functions_name_includelist=computation_functions_name_includelist, computation_kwargs_list=computation_kwargs_list, fail_on_exception=fail_on_exception, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 523, in perform_specific_computation\n",
      "    self.global_computation_results = self.run_specific_computations_single_context(global_kwargs, computation_functions_name_includelist=computation_functions_name_includelist, are_global=True, fail_on_exception=fail_on_exception, debug_print=debug_print)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 262, in run_specific_computations_single_context\n",
      "    return ComputedPipelineStage._execute_computation_functions(active_computation_functions, previous_computation_result=previous_computation_result, computation_kwargs_list=computation_kwargs_list, fail_on_exception=fail_on_exception, progress_logger_callback=progress_logger_callback, are_global=are_global, debug_print=debug_print)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/Computation.py\", line 592, in _execute_computation_functions\n",
      "    previous_computation_result = f(previous_computation_result, **computation_kwargs_list[i])\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/MultiContextComputationFunctions.py\", line 18, in _\n",
      "    x[1] = global_comp_fcn(*x) # update global_computation_results\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py\", line 424, in _perform_long_short_decoding_analyses\n",
      "    leave_one_out_decoding_analysis_obj = _long_short_decoding_analysis_from_decoders(long_one_step_decoder_1D, short_one_step_decoder_1D, long_session, short_session, global_session,\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py\", line 1204, in _long_short_decoding_analysis_from_decoders\n",
      "    long_results_obj = perform_full_session_leave_one_out_decoding_analysis(global_session, original_1D_decoder=long_shared_aclus_only_decoder, decoding_time_bin_size=decoding_time_bin_size, cache_suffix = '_long', perform_cache_load=perform_cache_load) # , perform_cache_load=False\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/decoder_result.py\", line 954, in perform_full_session_leave_one_out_decoding_analysis\n",
      "    original_1D_decoder, all_included_filter_epochs_decoder_result, one_left_out_decoder_dict, one_left_out_filter_epochs_decoder_result_dict = perform_leave_one_aclu_out_decoding_analysis(pyramidal_only_spikes_df, active_pos_df, active_filter_epochs, original_all_included_decoder=original_1D_decoder, decoding_time_bin_size=decoding_time_bin_size)\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/decoder_result.py\", line 716, in perform_leave_one_aclu_out_decoding_analysis\n",
      "    filter_epochs_decoder_result = curr_aclu_omitted_decoder.decode_specific_epochs(spikes_df[spikes_df['aclu'] != left_out_aclu], filter_epochs=active_filter_epochs, decoding_time_bin_size=decoding_time_bin_size, debug_print=False) # get the spikes_df except spikes for the left-out cell\n",
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 630, in decode_specific_epochs\n",
      "    return self.perform_decode_specific_epochs(self, spikes_df=spikes_df, filter_epochs=filter_epochs, decoding_time_bin_size=decoding_time_bin_size, debug_print=debug_print)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.epochs_info.mat..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py\", line 824, in perform_decode_specific_epochs\n",
      "    filter_epoch_spikes_df = add_epochs_id_identity(filter_epoch_spikes_df, filter_epochs_df, epoch_id_key_name='temp_epoch_id', epoch_label_column_name=None, no_interval_fill_value=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py\", line 154, in add_epochs_id_identity\n",
      "    spike_epoch_identity_arr = _compute_spike_arbitrary_provided_epoch_ids(spk_df, epochs_df, epoch_label_column_name=epoch_label_column_name, override_time_variable_name=override_time_variable_name, no_interval_fill_value=no_interval_fill_value, overlap_behavior=overlap_behavior)\n",
      "  File \"/home/halechr/repos/NeuroPy/neuropy/utils/mixins/time_slicing.py\", line 143, in _compute_spike_arbitrary_provided_epoch_ids\n",
      "    spike_epoch_identity_arr = determine_event_interval_identity(spk_times_arr, curr_epochs_start_stop_arr, curr_epoch_identity_labels, no_interval_fill_value=no_interval_fill_value, overlap_behavior=overlap_behavior)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.position_info.mat..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py\", line 258, in determine_event_interval_identity\n",
      "    return _compiled_searchsorted_event_interval_identity(times_arr, start_stop_times_arr, period_identity_labels, no_interval_fill_value=no_interval_fill_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numba/core/serialize.py\", line 29, in _numba_unpickle\n",
      "    def _numba_unpickle(address, bytedata, hashed):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.spikes.mat... build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-09_22-24-40\n",
      "\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40 \n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.epochs_info.mat...done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.position_info.mat... done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.spikes.mat... build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46.log\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 68\u001b[0m\n\u001b[1;32m     57\u001b[0m active_computation_functions_name_includelist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_perform_baseline_placefield_computation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     58\u001b[0m                                         \u001b[38;5;66;03m# '_perform_time_dependent_placefield_computation',\u001b[39;00m\n\u001b[1;32m     59\u001b[0m                                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_perform_extended_statistics_computation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                         \u001b[38;5;66;03m# '_perform_recursive_latent_placefield_decoding'\u001b[39;00m\n\u001b[1;32m     66\u001b[0m                                     ]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# active_computation_functions_name_includelist=['_perform_baseline_placefield_computation']\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[43mglobal_batch_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_reload_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaving_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_extended_batch_computations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_run_callback_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_post_run_callback_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfail_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincluded_session_contexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincluded_session_contexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                                                                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomputation_functions_name_includelist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mactive_computation_functions_name_includelist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                                                                                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactive_session_computation_configs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                                                                                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallow_processing_previously_completed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmultiprocessing_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 4m 39.8s\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Batch/runBatch.py:329\u001b[0m, in \u001b[0;36mBatchRun.execute_all\u001b[0;34m(self, use_multiprocessing, num_processes, included_session_contexts, session_inclusion_filter, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     results[curr_session_context] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    328\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 329\u001b[0m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m session_context, result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    332\u001b[0m     status, error, output \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/sw/pkgs/arc/python/3.9.12/lib/python3.9/multiprocessing/pool.py:662\u001b[0m, in \u001b[0;36mPool.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (CLOSE, TERMINATE):\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn unknown state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 662\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_handler\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_handler\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/sw/pkgs/arc/python/3.9.12/lib/python3.9/threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/sw/pkgs/arc/python/3.9.12/lib/python3.9/threading.py:1073\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1074\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1075\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-12_16-53-46\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:122: UserWarning: WARNING: Optional File: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.epochs_info.mat... done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.position_info.mat... done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.spikes.mat... done.\n",
      "done.\n",
      "done.\n",
      "Failure loading .position.npy. Must recompute.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/sklearn/manifold/_isomap.py:359: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.position.npy... 2006-6-12_16-53-46.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.interpolated_spike_positions.npy... 2006-6-12_16-53-46.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "Loading success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.mua.npy... 2006-6-12_16-53-46.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/2006-6-12_16-53-46.pbe.npy... 2006-6-12_16-53-46.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... Failure loading .position.npy. Must recompute.\n",
      "\n",
      "Saving updated position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.position.npy... 2006-6-08_21-16-25.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.interpolated_spike_positions.npy... done.\n",
      "Computing added spike scISI column results : spikes_df... 2006-6-08_21-16-25.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "Loading success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving mua results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.mua.npy... 2006-6-08_21-16-25.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/2006-6-08_21-16-25.pbe.npy... 2006-6-08_21-16-25.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "done.\n",
      "Computing added spike scISI column results : spikes_df... Failure loading .position.npy. Must recompute.\n",
      "\n",
      "\t curr_replays: 109\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 471.0674003356835)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 471.0674003356835, end: 785.4513262689579)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 785.4513262689579)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "Saving updated position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.position.npy... 2006-6-09_22-24-40.position.npy saved\n",
      "done.\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n",
      "\t Saving updated interpolated spike position results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.interpolated_spike_positions.npy... computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\t using KnownFilterEpochs.PBE as surrogate replays...\n",
      "\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... 2006-6-09_22-24-40.interpolated_spike_positions.npy saved\n",
      "done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.laps_info.mat... done.\n",
      "setting laps object.\n",
      "session.laps loaded successfully!\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.replay_info.mat... done.\n",
      "session.replays loaded successfully!\n",
      "using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Loading success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/ripple_df.pkl.\n",
      "force_recompute is True, recomputing...\n",
      "computing neurons mua for session...\n",
      "\n",
      "Saving mua results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.mua.npy... 2006-6-09_22-24-40.mua.npy saved\n",
      "done.\n",
      "force_recompute is True, recomputing...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "Saving pbe results results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/2006-6-09_22-24-40.pbe.npy... 2006-6-09_22-24-40.pbe.npy saved\n",
      "done.\n",
      "Computing spikes_df PBEs column results : spikes_df... \t curr_replays: 108\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 722.653951405664)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 722.653951405664, end: 1201.0839364149142)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 1201.0839364149142)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 6 out of 16 registered computation functions.done.\n",
      "Computing added spike scISI column results : spikes_df... \n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "done.\n",
      "POSTLOAD_estimate_laps_and_replays()...\n",
      "computing PBE epochs for session...\n",
      "\n",
      "computing estimated replay epochs for session...\n",
      "\n",
      "\t using KnownFilterEpochs.PBE as surrogate replays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t curr_replays: 512\n",
      "skip_save_on_initial_load is True so resultant pipeline will not be saved to the pickle file.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "Applying session filter named \"maze1\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 911.6011600069469)\n",
      "computing neurons mua for session...\n",
      "\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (13127,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze2\"...\n",
      "Constraining to epoch with times (start: 911.6011600069469, end: 2573.457162000006)\n",
      "computing neurons mua for session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['lap_id']] = laps_df[['lap_id']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df[['start_spike_index', 'end_spike_index']] = laps_df[['start_spike_index', 'end_spike_index']].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['num_spikes'] = laps_df['end_spike_index'] - laps_df['start_spike_index']\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['lap_dir'] = laps_df['lap_dir'].astype('int')\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/laps.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laps_df['label'] = laps_df['lap_id'].astype('str') # add the string \"label\" column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying session filter named \"maze\"...\n",
      "Constraining to epoch with times (start: 0.0, end: 2573.457162000006)\n",
      "computing neurons mua for session...\n",
      "\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\n",
      "\t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (9198,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (20270,)\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (27155,)\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14699,) should be less than time_window_edges: (49412,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (12894,)\n",
      "due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (22940,)\n",
      "using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl\n",
      "using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl will be overwritten even though exists.\t done.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl... \n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14699,) should be less than time_window_edges: (49412,)!\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl... done.\n",
      "included includelist is specified: ['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'long_short_decoding_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 10, 2, 29, 400189)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 10, 2, 47, 110090)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (34680,)\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (49904,)\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 10, 3, 14, 526131)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl... due to includelist, including only 6 out of 16 registered computation functions.\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47040,) should be less than time_window_edges: (76857,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 10, 3, 33, 264744)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "done.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: filtered_contexts[long_epoch_name]'s actual context name is incorrect. \n",
      "\tlong_epoch_context.filter_name: maze2 != long_epoch_name: maze1\n",
      "\tUpdating it. (THIS IS A HACK)\n",
      "finalized_loaded_sess_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl\n",
      "WARNING: saving_mode is OVERWRITE_IN_PLACE so /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl will be overwritten even though exists.\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl... done.\n",
      "included includelist is specified: ['long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'long_short_decoding_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze\"\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (47040,) should be less than time_window_edges: (76857,)!\n",
      "(n_neurons = 38, n_all_epoch_timebins = 187)\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\n",
      "\t Recomputing long_short_decoding_analyses...DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "setting new computation epochs because laps changed.\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "(n_neurons = 38, n_all_epoch_timebins = 187)\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "_perform_long_short_instantaneous_spike_rate_groups_analysis is lacking a required computation config parameter! creating a new curr_active_pipeline.global_computation_results.computation_config\n",
      "\t done.\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: ['jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'long_short_decoding_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups']. Saving global results...\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl\n",
      "Saving (file mode 'w+b') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:02:06.053826\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 10, 5, 41, 156218)}\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.InstantaneousSpikeRateGroupsComputation'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps:\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: Fig2_Replay_FR\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2_Replay_FR\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/LxC_ReplayDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Replay/inst_FR_Bars/SxC_ReplayDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "a_field: Fig2_Laps_FR\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2_Laps_FR\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/LxC_ThetaDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaMinus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.SingleBarResult'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus:\n",
      "a_field: values\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/values\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/values is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: LxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/LxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/LxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: SxC_aclus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/SxC_aclus\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/Fig2/Laps/inst_FR_Bars/SxC_ThetaDeltaPlus/SxC_aclus is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: mean\n",
      "an_attribute_field: std\n",
      "a_field: LxC_ReplayDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: LxC_ReplayDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ReplayDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ReplayDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ReplayDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ReplayDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: LxC_ThetaDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: LxC_ThetaDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/LxC_ThetaDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ThetaDeltaMinus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaMinus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "a_field: SxC_ThetaDeltaPlus\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus\n",
      "\t field is serializable! Calling a_value.to_hdf(...)...\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis.SpikeRateTrends'> to file_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5, with key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus:\n",
      "a_field: epoch_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/epoch_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/epoch_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "a_field: cell_agg_inst_fr_list\n",
      "\ta_field_key: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/cell_agg_inst_fr_list\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /kdiba/gor01/two/2006-6-12_16-53-46/global_computations/inst_fr_comps/SxC_ThetaDeltaPlus/cell_agg_inst_fr_list is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "an_attribute_field: all_agg_inst_fr\n",
      "an_attribute_field: instantaneous_time_bin_size_seconds\n",
      "an_attribute_field: active_identifying_session_ctx\n",
      "WARNING: experimental automatic `to_hdf` implementation for object of type <class 'pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.ExpectedVsObservedResult'> to file_path: output/test_ExpectedVsObservedResult.h5, with key: /expected_v_observed_result:\n",
      "a_field: Flat_epoch_time_bins_mean\n",
      "\ta_field_key: /expected_v_observed_result/Flat_epoch_time_bins_mean\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/Flat_epoch_time_bins_mean is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/Flat_epoch_time_bins_mean! Will be replaced by new value.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\ta_field_key: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_LONG! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_ptp_LONG\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_ptp_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_ptp_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_ptp_LONG! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_mean_LONG\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_mean_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_mean_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_mean_LONG! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_std_LONG\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_std_LONG\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_std_LONG is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_std_LONG! Will be replaced by new value.\n",
      "a_field: Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/Flat_all_epochs_computed_expected_cell_num_spikes_SHORT! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_ptp_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_ptp_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ma.core.MaskedArray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_ptp_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_ptp_SHORT! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_mean_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_mean_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_mean_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_mean_SHORT! Will be replaced by new value.\n",
      "a_field: observed_from_expected_diff_std_SHORT\n",
      "\ta_field_key: /expected_v_observed_result/observed_from_expected_diff_std_SHORT\n",
      "\t field not custom serializable! a_field_attr.type: <class 'numpy.ndarray'>.\n",
      "WARNING: /expected_v_observed_result/observed_from_expected_diff_std_SHORT is not custom serializable, but we will try HDF_Converter._try_default_to_hdf_conversion_fn(file_path=file_path, key=a_field_key, value=a_value) with the value. Will raise a NotImplementedException if this fails.\n",
      "WARNING: clobbering existing dataset in output/test_ExpectedVsObservedResult.h5 at /expected_v_observed_result/observed_from_expected_diff_std_SHORT! Will be replaced by new value.\n",
      "an_attribute_field: num_neurons\n",
      "an_attribute_field: num_total_flat_timebins\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "common_file_path: output/active_across_session_scatter_plot_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function DefaultComputationFunctions._perform_position_decoding_computation at 0x152bef04d3a0>: datetime.datetime(2023, 8, 24, 10, 6, 16, 340342)}\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "self will be re-binned to match target_one_step_decoder...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "Performing _execute_computation_functions(...) with 1 registered_computation_functions...\n",
      "\t\t done (success).\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "\t\t done (success).\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl3040.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl3040.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/turbo/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-09_17-29-30\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "active_data_mode_name: kdiba\n",
      "Skipping loading from pickled file because force_reload == True.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.epochs_info.mat... done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.position_info.mat... done.\n",
      "Loading matlab import file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/2006-4-09_17-29-30.spikes.mat..."
     ]
    }
   ],
   "source": [
    "# multiprocessing_kwargs = dict(use_multiprocessing=False, num_processes=2)\n",
    "multiprocessing_kwargs = dict(use_multiprocessing=True, num_processes=3)\n",
    "\n",
    "enable_full_pipeline_in_ram = False\n",
    "# enable_full_pipeline_in_ram = True\n",
    "\n",
    "# Whether to output figures:\n",
    "should_perform_figure_generation_to_file=False\n",
    "# should_perform_figure_generation_to_file=True\n",
    "\n",
    "## Included Session Contexts:\n",
    "# included_session_contexts = batch_progress_df[np.logical_and(batch_progress_df['has_user_replay_annotations'], batch_progress_df['is_ready'])]['context'].to_numpy().tolist()\n",
    "\n",
    "# Only require sessions to have replay annotations:\n",
    "# included_session_contexts = batch_progress_df[batch_progress_df['has_user_replay_annotations']]['context'].to_numpy().tolist()\n",
    "\n",
    "# included_session_contexts = batch_progress_df['context'].to_numpy().tolist()[:4] # Only get the first 6\n",
    "## Limit the contexts to run to the last N:\n",
    "# included_session_contexts = included_session_contexts[3:6]\n",
    "\n",
    "# ALL\n",
    "included_session_contexts = included_session_contexts\n",
    "\n",
    "# ## No filtering the sessions:\n",
    "# included_session_contexts = None\n",
    "\n",
    "if included_session_contexts is not None:\n",
    "    print(f'len(included_session_contexts): {len(included_session_contexts)}')\n",
    "else:\n",
    "    print(f'included_session_contexts is None so all session contexts will be included.')\n",
    "\n",
    "# included_session_contexts\n",
    "\n",
    "\n",
    "# # No Reloading\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=False),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=False),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "#                                                 enable_full_pipeline_in_ram=enable_full_pipeline_in_ram,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "# Forced Reloading:\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=True,\n",
    "                                                session_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=True),\n",
    "                                                global_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=True),\n",
    "                                                should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE, force_global_recompute=True,\n",
    "                                                **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "active_post_run_callback_fn = result_handler.on_complete_success_execution_session\n",
    "# active_post_run_callback_fn = _temp_on_complete_success_execution_session\n",
    "\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "active_computation_functions_name_includelist=['_perform_baseline_placefield_computation',\n",
    "                                        # '_perform_time_dependent_placefield_computation',\n",
    "                                        '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "# active_computation_functions_name_includelist=['_perform_baseline_placefield_computation']\n",
    "global_batch_run.execute_all(force_reload=result_handler.force_reload_all, saving_mode=result_handler.saving_mode, skip_extended_batch_computations=True, post_run_callback_fn=active_post_run_callback_fn,\n",
    "                             fail_on_exception=True, included_session_contexts=included_session_contexts,\n",
    "                                                                                        **{'computation_functions_name_includelist': active_computation_functions_name_includelist,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}, **multiprocessing_kwargs) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de182953",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_session_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0085be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import LoadedObjectPersistanceState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb4c19-1006-4e32-88c7-40f3b732b8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_progress_df[np.isin(batch_progress_df['context'].values, included_session_contexts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36194ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f6ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19afdb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db966387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc83c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-08-10\n",
    "# Add output modes for .pkl and .h5 files similar to figures. For GreatLakes Batch runs, allow output to the run folder.\n",
    "\n",
    "# like the figures, file naming conventions should change for the different types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb13a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyphocorehelpers.indexing_helpers import partition, safe_pandas_get_group\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor, AcrossSessionsVisualizations\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import SingleBarResult\n",
    "\n",
    "common_file_path = Path('output/active_across_session_scatter_plot_results.h5')\n",
    "print(f'common_file_path: {common_file_path}')\n",
    "\n",
    "# InstantaneousSpikeRateGroupsComputation, : pd.DataFrame\n",
    "_shell_obj, loaded_result_df = InstantaneousFiringRatesDataframeAccessor.load_and_prepare_for_plot(common_file_path)\n",
    "# Perform the actual plotting:\n",
    "AcrossSessionsVisualizations.across_sessions_bar_graphs(_shell_obj, num_sessions=13, save_figure=False, enable_tiny_point_labels=False, enable_hover_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c02c97-a566-4e24-9219-6ee09a4075b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "sessions_with_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e19412-2b21-4841-8207-952a04e1080b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_identifiers = included_session_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2e13-bbdf-4c82-9fd6-8d7ac702e114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to pickle:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21482b-23ac-44e6-bfb2-8e3d480d74bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine across .h5 files:\n",
    "import tables as tb\n",
    "import h5py\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import H5ExternalLinkBuilder\n",
    "\n",
    "# f'/kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table'\n",
    "\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths\n",
    "\n",
    "file_path = f'output/test_external_links.h5'\n",
    "a_global_computations_group_key: str = f\"{session_group_key}/global_computations\"\n",
    "with tb.open_file(file_path, mode='w') as f: # this mode='w' is correct because it should overwrite the previous file and not append to it.\n",
    "    a_global_computations_group = f.create_group(session_group_key, 'global_computations', title='the result of computations that operate over many or all of the filters in the session.', createparents=True)\n",
    "    an_external_link = f.create_external_link(f'', n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb43f9-214a-401b-8ae9-7dbe9af40180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import List\n",
    "from attrs import define, field, Factory\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "\n",
    "    \n",
    "session_group_keys: List[str] = [(\"/\" + a_ctxt.get_description(separator=\"/\", include_property_names=False)) for a_ctxt in session_identifiers] # 'kdiba/gor01/one/2006-6-08_14-26-15'\n",
    "# session_uid: str = session_context.get_description(separator=\"|\", include_property_names=False)\n",
    "neuron_identities_table_keys = [f\"{session_group_key}/neuron_identities/table\" for session_group_key in session_group_keys]\n",
    "\n",
    "\n",
    "a_loader = H5ExternalLinkBuilder(file_list=hdf5_output_paths, table_key_list=neuron_identities_table_keys)\n",
    "a_loader\n",
    "\n",
    "_out_table = a_loader.load_and_consolidate()\n",
    "_out_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749849c-9f57-4d2c-aab8-5ee9353f6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'\n",
    "\n",
    "# kdiba_vvp01_two_2006-4-10_12-58-3\n",
    "# \toutputs_local ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl')}\n",
    "# \toutputs_global ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'), 'hdf5': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5')}\n",
    "\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths = global_batch_run.build_output_files_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361385b-385d-4e12-997b-e2a68404858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8288dee-a41a-4640-a139-cb0a64f01814",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdc211-56e3-4e38-b02f-ea71451766eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0cd9-3e60-4425-9351-dfc903f3f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output filelist:\n",
    "\n",
    "\n",
    "def save_filelist_to_text_file(hdf5_output_paths, filelist_path: Path):\n",
    "    _out_string = '\\n'.join([str(a_file) for a_file in hdf5_output_paths])\n",
    "    print(f'{_out_string}')\n",
    "    print(f'saving out to \"{filelist_path}\"...')\n",
    "    with open(filelist_path, 'w') as f:\n",
    "        f.write(_out_string)\n",
    "    return _out_string, filelist_path\n",
    "\n",
    "\n",
    "h5_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_HDF5_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_HDF5_savepath = save_filelist_to_text_file(hdf5_output_paths, h5_filelist_path)\n",
    "\n",
    "pkls_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_pkls_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_pkls_savepath = save_filelist_to_text_file(pkl_output_paths, pkls_filelist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afc323-364c-4716-a30f-97a5e4fb2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import convert_filelist_to_new_parent\n",
    "# source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "source_parent_path = Path(r'/nfs/turbo/umms-kdiba/Data')\n",
    "dest_parent_path = Path(r'/~/W/Data/')\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_source = hdf5_output_paths\n",
    "filelist_dest_paths = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest_paths\n",
    "\n",
    "dest_Apogee_h5_filelist_path = global_data_root_parent_path.joinpath(f'dest_fileList_Apogee_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, dest_filelist_savepath = save_filelist_to_text_file(filelist_dest_paths, dest_Apogee_h5_filelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e69a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0d19b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ce774-98fb-4e69-a7e2-e94cbff1b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "\n",
    "# list(global_batch_run.session_batch_outputs.keys())\n",
    "\n",
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "# good_session_batch_outputs = global_batch_run.session_batch_outputs\n",
    "\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d19a7-5a89-43f1-aa33-3a7450d1f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178426c-54df-47ac-8103-a66f114c77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[a_ctxt.get_initialization_code_string() for a_ctxt in sessions_with_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-07-14 - Load Saved across-sessions-data and testing Batch-computed inst_firing_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PaperFigureTwo, InstantaneousSpikeRateGroupsComputation\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import list_of_dicts_to_dict_of_lists\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "## Load the saved across-session results:\n",
    "# inst_fr_output_filename = 'long_short_inst_firing_rate_result_handlers_2023-07-12.pkl'\n",
    "# inst_fr_output_filename = 'across_session_result_long_short_inst_firing_rate.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-07-21.pkl'\n",
    "# inst_fr_output_filename=f'across_session_result_handler_{BATCH_DATE_TO_USE}.pkl'\n",
    "inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-08-09_Test.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a11886",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_frs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \n",
    "global_multi_session_context, _out_aggregate_fig_2 = AcrossSessionsVisualizations.across_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels=False, enable_hover_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381fcf5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "across_session_inst_fr_computation.LxC_scatter_props\n",
    "across_session_inst_fr_computation.SxC_scatter_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, InstantaneousFiringRatesDataframeAccessor, InstantaneousSpikeRateGroupsComputation, trackMembershipTypesEnum, trackExclusiveToMembershipTypeDict, trackExclusiveToMembershipTypeReverseDict\n",
    "\n",
    "## Specify the output file:\n",
    "common_file_path = Path('output/test_across_session_scatter_plot_new.h5')\n",
    "print(f'common_file_path: {common_file_path}')\n",
    "InstantaneousFiringRatesDataframeAccessor.add_results_to_inst_fr_results_table(curr_active_pipeline, common_file_path, file_mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45d728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the unique scatter plot dictionaries:\n",
    "across_session_contexts = list(across_sessions_instantaneous_fr_dict.keys())\n",
    "unique_animals = IdentifyingContext.find_unique_values(across_session_contexts)['animal'] # {'gor01', 'pin01', 'vvp01'}\n",
    "# Get number of animals to plot\n",
    "marker_list = [(5, i) for i in np.arange(len(unique_animals))] # [(5, 0), (5, 1), (5, 2)]\n",
    "scatter_props = [{'marker': mkr} for mkr in marker_list]  # Example, you should provide your own scatter properties\n",
    "scatter_props_dict = dict(zip(unique_animals, scatter_props))\n",
    "# {'pin01': {'marker': (5, 0)},\n",
    "#  'gor01': {'marker': (5, 1)},\n",
    "#  'vvp01': {'marker': (5, 2)}}\n",
    "scatter_props_dict\n",
    "\n",
    "# Pass a function that will return a set of kwargs for a given context\n",
    "def _return_scatter_props_fn(ctxt: IdentifyingContext):\n",
    "\t\"\"\" captures `scatter_props_dict` \"\"\"\n",
    "\tanimal_id = str(ctxt.animal)\n",
    "\treturn scatter_props_dict[animal_id]\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate across all of the sessions to build a new combined `InstantaneousSpikeRateGroupsComputation`, which can be used to plot the \"PaperFigureTwo\", bar plots for many sessions.\n",
    "global_multi_session_context = IdentifyingContext(format_name='kdiba', num_sessions=num_sessions) # some global context across all of the sessions, not sure what to put here.\n",
    "\n",
    "# To correctly aggregate results across sessions, it only makes sense to combine entries at the `.cell_agg_inst_fr_list` variable and lower (as the number of cells can be added across sessions, treated as unique for each session).\n",
    "\n",
    "## Display the aggregate across sessions:\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # WARNING: we didn't save this info\n",
    "_out_fig_2.computation_result = across_session_inst_fr_computation # the result loaded from the file\n",
    "_out_fig_2.active_identifying_session_ctx = across_session_inst_fr_computation.active_identifying_session_ctx\n",
    "# Set callback, the only self-specific property\n",
    "# _out_fig_2._pipeline_file_callback_fn = curr_active_pipeline.output_figure # lambda args, kwargs: self.write_to_file(args, kwargs, curr_active_pipeline)\n",
    "_out_fig_2.scatter_props_fn = _return_scatter_props_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LxC_aclus = _out_fig_2.computation_result.LxC_aclus\n",
    "SxC_aclus = _out_fig_2.computation_result.SxC_aclus\n",
    "\n",
    "LxC_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureOutputManager, FigureOutputLocation, ContextToPathMode\n",
    "\n",
    "registered_output_files = {}\n",
    "\n",
    "def output_figure(final_context: IdentifyingContext, fig, write_vector_format:bool=False, write_png:bool=True, debug_print=True):\n",
    "    \"\"\" outputs the figure using the provided context. \"\"\"\n",
    "    from pyphoplacecellanalysis.General.Mixins.ExportHelpers import build_and_write_to_file\n",
    "    def register_output_file(output_path, output_metadata=None):\n",
    "        \"\"\" registers a new output file for the pipeline \"\"\"\n",
    "        print(f'register_output_file(output_path: {output_path}, ...)')\n",
    "        registered_output_files[output_path] = output_metadata or {}\n",
    "\n",
    "    fig_out_man = FigureOutputManager(figure_output_location=FigureOutputLocation.DAILY_PROGRAMMATIC_OUTPUT_FOLDER, context_to_path_mode=ContextToPathMode.HIERARCHY_UNIQUE)\n",
    "    active_out_figure_paths = build_and_write_to_file(fig, final_context, fig_out_man, write_vector_format=write_vector_format, write_png=write_png, register_output_file_fn=register_output_file)\n",
    "    return active_out_figure_paths, final_context\n",
    "\n",
    "\n",
    "# Set callback, the only self-specific property\n",
    "_out_fig_2._pipeline_file_callback_fn = output_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17b920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing\n",
    "restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# Perform interactive Matplotlib operations with 'Qt5Agg' backend\n",
    "_fig_2_theta_out, _fig_2_replay_out = _out_fig_2.display(active_context=global_multi_session_context, title_modifier_fn=lambda original_title: f\"{original_title} ({num_sessions} sessions)\", save_figure=True)\n",
    "\t\n",
    "_out_fig_2.perform_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32781c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Single Session testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_out = global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, computation_functions_name_includelist =['_perform_baseline_placefield_computation'], active_session_computation_configs=None) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "_test_out\n",
    "\n",
    "# global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, **{'computation_functions_name_includelist': ['_perform_baseline_placefield_computation'], 'active_session_computation_configs': None}) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 23.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2bb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "full_good_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is None]\n",
    "bad_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is not None]\n",
    "full_good_dirs\n",
    "bad_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f73f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status\n",
    "global_batch_run.session_batch_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faf2bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed516134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf02efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the list of sessions that are completely ready to process:\n",
    "full_good_ready_to_process_sessions = list(good_only_batch_progress_df['context'].to_numpy())\n",
    "full_good_ready_to_process_sessions\n",
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ad809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run[\"good_sessions_list\"].extend(full_good_ready_to_process_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636e3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.stop()\n",
    "project.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\",\\n\".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # List definitions\n",
    "\n",
    "# [IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a4bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\ncurr_context = \".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # Line definitions\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5689d1d-6400-4f87-be0e-a184a1d5bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82aedf-b024-4f07-b1a9-350730b4db8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "save_time = datetime.now()\n",
    " \n",
    "print(\"save_time =\", save_time)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = save_time.strftime(\"%Y-%m-%d_%I-%M%p\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc6bd-5b34-41d0-b906-b0ad9927b08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get output file paths:\n",
    "completed_pipeline_filename = 'loadedSessPickle.pkl'\n",
    "completed_global_computations_filename = 'outputs/global_computation_results.pkl'\n",
    "\n",
    "full_good_ready_to_process_session_paths = list(good_only_batch_progress_df['basedirs'].to_numpy())\n",
    "session_paths_output_folders = [sess_path.joinpath('outputs').resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "\n",
    "completed_pipeline_file_paths = [sess_path.joinpath(completed_pipeline_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths = [sess_path.joinpath(completed_global_computations_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab75122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countable Additivity \n",
    "# Any countable collections of points is size 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
