{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionTables, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult, BatchSessionCompletionHandler\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_file_metadata\n",
    "from pyphocorehelpers.Filesystem.path_helpers import discover_data_files, generate_copydict, copy_movedict, copy_file, save_copydict_to_text_file, read_copydict_from_text_file, invert_filedict\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import get_file_str_if_file_exists\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import check_output_h5_files, copy_files_in_filelist_to_dest\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import ConcreteSessionFolder, BackupMethods\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots, BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import build_merged_neuron_firing_rate_indicies\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsHelpers\n",
    "\n",
    "# BATCH_DATE_TO_USE = '2023-10-20' # used for filenames throught the notebook\n",
    "# BATCH_DATE_TO_USE = '2023-10-18_Apogee' # used for filenames throught the notebook\n",
    "BATCH_DATE_TO_USE = '2023-12-07_GL' # used for filenames throught the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/global_batch_result_2023-12-07_GL.pkl... done.\n",
      "no difference between provided and internal paths.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-07_11-26-53</td>\n",
       "      <td>kdiba_gor01_one_2006-6-07_11-26-53</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 11:26:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_3-23-37</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_3-23-37</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 03:23:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-04_21-20-3</td>\n",
       "      <td>kdiba_pin01_one_fet11-04_21-20-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-04 21:20:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>redundant</td>\n",
       "      <td>kdiba_pin01_one_redundant</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>showclus</td>\n",
       "      <td>kdiba_pin01_one_showclus</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>sleep</td>\n",
       "      <td>kdiba_pin01_one_sleep</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>tmaze</td>\n",
       "      <td>kdiba_pin01_one_tmaze</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "0        kdiba  gor01        one  2006-6-07_11-26-53   \n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "3        kdiba  gor01        one   2006-6-09_3-23-37   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "..         ...    ...        ...                 ...   \n",
       "67       kdiba  pin01        one    fet11-04_21-20-3   \n",
       "68       kdiba  pin01        one           redundant   \n",
       "69       kdiba  pin01        one            showclus   \n",
       "70       kdiba  pin01        one               sleep   \n",
       "71       kdiba  pin01        one               tmaze   \n",
       "\n",
       "                               context  \\\n",
       "0   kdiba_gor01_one_2006-6-07_11-26-53   \n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "3    kdiba_gor01_one_2006-6-09_3-23-37   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "..                                 ...   \n",
       "67    kdiba_pin01_one_fet11-04_21-20-3   \n",
       "68           kdiba_pin01_one_redundant   \n",
       "69            kdiba_pin01_one_showclus   \n",
       "70               kdiba_pin01_one_sleep   \n",
       "71               kdiba_pin01_one_tmaze   \n",
       "\n",
       "                                             basedirs  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...   \n",
       "69  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...   \n",
       "70   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep   \n",
       "71   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze   \n",
       "\n",
       "                              status errors    session_datetime  n_long_laps  \\\n",
       "0   SessionBatchProgress.NOT_STARTED   None 2006-06-07 11:26:53            0   \n",
       "1     SessionBatchProgress.COMPLETED   None 2006-06-08 14:26:15            0   \n",
       "2     SessionBatchProgress.COMPLETED   None 2006-06-09 01:22:43            0   \n",
       "3   SessionBatchProgress.NOT_STARTED   None 2006-06-09 03:23:37            0   \n",
       "4     SessionBatchProgress.COMPLETED   None 2006-06-12 15:55:31            0   \n",
       "..                               ...    ...                 ...          ...   \n",
       "67  SessionBatchProgress.NOT_STARTED   None 2009-11-04 21:20:03            0   \n",
       "68  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "69  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "70  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "71  SessionBatchProgress.NOT_STARTED   None                 NaT            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "0                0             0                0     False   \n",
       "1                0             0                0     False   \n",
       "2                0             0                0     False   \n",
       "3                0             0                0     False   \n",
       "4                0             0                0     False   \n",
       "..             ...           ...              ...       ...   \n",
       "67               0             0                0     False   \n",
       "68               0             0                0     False   \n",
       "69               0             0                0     False   \n",
       "70               0             0                0     False   \n",
       "71               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "0                         False                                  True  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "3                         False                                  True  \n",
       "4                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "67                        False                                  True  \n",
       "68                        False                                 False  \n",
       "69                        False                                 False  \n",
       "70                        False                                 False  \n",
       "71                        False                                 False  \n",
       "\n",
       "[72 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'/media/MAX/Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data')] # , Path(r'/home/halechr/FastData'), Path(r'/nfs/turbo/umms-kdiba/Data'), Path(r'/home/halechr/turbo/Data'), Path(r'W:\\Data')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "# try to load an existing batch result:\n",
    "global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    # display(batch_progress_df)\n",
    "    # display(good_only_batch_progress_df)\n",
    "    display(batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019afbbd-70d2-4e75-9548-b6f22d2e31ca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>kdiba_gor01_two_2006-6-07_16-40-19</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>kdiba_gor01_two_2006-6-08_21-16-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-10_12-58-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>kdiba_pin01_one_11-02_17-46-44</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>kdiba_pin01_one_11-02_19-28-0</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>kdiba_pin01_one_11-03_12-3-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>kdiba_pin01_one_fet11-01_12-58-54</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "6        kdiba  gor01        two  2006-6-07_16-40-19   \n",
       "8        kdiba  gor01        two  2006-6-08_21-16-25   \n",
       "..         ...    ...        ...                 ...   \n",
       "32       kdiba  vvp01        two   2006-4-10_12-58-3   \n",
       "52       kdiba  pin01        one      11-02_17-46-44   \n",
       "53       kdiba  pin01        one       11-02_19-28-0   \n",
       "54       kdiba  pin01        one       11-03_12-3-25   \n",
       "64       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "\n",
       "                               context  \\\n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "6   kdiba_gor01_two_2006-6-07_16-40-19   \n",
       "8   kdiba_gor01_two_2006-6-08_21-16-25   \n",
       "..                                 ...   \n",
       "32   kdiba_vvp01_two_2006-4-10_12-58-3   \n",
       "52      kdiba_pin01_one_11-02_17-46-44   \n",
       "53       kdiba_pin01_one_11-02_19-28-0   \n",
       "54       kdiba_pin01_one_11-03_12-3-25   \n",
       "64   kdiba_pin01_one_fet11-01_12-58-54   \n",
       "\n",
       "                                             basedirs  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                            status errors    session_datetime  n_long_laps  \\\n",
       "1   SessionBatchProgress.COMPLETED   None 2006-06-08 14:26:15            0   \n",
       "2   SessionBatchProgress.COMPLETED   None 2006-06-09 01:22:43            0   \n",
       "4   SessionBatchProgress.COMPLETED   None 2006-06-12 15:55:31            0   \n",
       "6   SessionBatchProgress.COMPLETED   None 2006-06-07 16:40:19            0   \n",
       "8   SessionBatchProgress.COMPLETED   None 2006-06-08 21:16:25            0   \n",
       "..                             ...    ...                 ...          ...   \n",
       "32  SessionBatchProgress.COMPLETED   None 2006-04-10 12:58:03            0   \n",
       "52  SessionBatchProgress.COMPLETED   None 2009-11-02 17:46:44            0   \n",
       "53  SessionBatchProgress.COMPLETED   None 2009-11-02 19:28:00            0   \n",
       "54  SessionBatchProgress.COMPLETED   None 2009-11-03 12:03:25            0   \n",
       "64  SessionBatchProgress.COMPLETED   None 2009-11-01 12:58:54            0   \n",
       "\n",
       "    n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "1                0             0                0     False   \n",
       "2                0             0                0     False   \n",
       "4                0             0                0     False   \n",
       "6                0             0                0     False   \n",
       "8                0             0                0     False   \n",
       "..             ...           ...              ...       ...   \n",
       "32               0             0                0     False   \n",
       "52               0             0                0     False   \n",
       "53               0             0                0     False   \n",
       "54               0             0                0     False   \n",
       "64               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "4                          True                                  True  \n",
       "6                          True                                  True  \n",
       "8                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "32                         True                                  True  \n",
       "52                         True                                  True  \n",
       "53                         True                                  True  \n",
       "54                         True                                  True  \n",
       "64                         True                                  True  \n",
       "\n",
       "[15 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "good_session_concrete_folders\n",
    "\n",
    "# from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import generate_batch_single_session_scripts\n",
    "\n",
    "# ## Build Slurm Scripts:\n",
    "# session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "# included_session_contexts, output_python_scripts, output_slurm_scripts = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts, output_directory=Path('output/generated_slurm_scripts/').resolve(), use_separate_run_directories=True, should_perform_figure_generation_to_file=True)\n",
    "# display(output_python_scripts)\n",
    "\n",
    "included_session_batch_progress_df = batch_progress_df[np.isin(batch_progress_df['context'].values, included_session_contexts)]\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(included_session_batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71584edc",
   "metadata": {},
   "source": [
    "# Execute Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6ae279",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(included_session_contexts): 15\n",
      "Beginning processing with len(included_session_contexts): 15\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40\"):build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46\"):build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43\"):build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30\"):build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15\"):build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31\"):build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50\"):build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54\"):\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25\"):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46.log\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43.log\n",
      "\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15.log\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19.log\n",
      "\n",
      "\n",
      "\n",
      "========================== runBatch STARTING ==================================================== runBatch STARTING ==================================================== runBatch STARTING ==========================\n",
      "========================== runBatch STARTING ==================================================== runBatch STARTING ==========================\n",
      "========================== runBatch STARTING ==========================\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40.log========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\n",
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data========================== runBatch STARTING ==========================\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-10_12-25-50\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-09_16-40-54\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\tsession_context: kdiba_vvp01_one_2006-4-09_17-29-30\tsession_context: kdiba_gor01_two_2006-6-08_21-16-25========================== runBatch STARTING ==================================================== runBatch STARTING ==========================\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\n",
      "\n",
      "\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "\tsession_context: kdiba_gor01_two_2006-6-12_16-53-46\n",
      "\n",
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data____________________________________________________________________________________________________________________________________\n",
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46\tsession_context: kdiba_gor01_one_2006-6-08_14-26-15__________________________________________________________________basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "\n",
      "\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "__________________________________________________________________basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\tsession_context: kdiba_gor01_two_2006-6-07_16-40-19\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15\n",
      "\n",
      "\n",
      "\n",
      "active_data_mode_name: kdibabasedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46__________________________________________________________________\n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19\n",
      "\n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl...active_data_mode_name: kdiba\n",
      "\n",
      "__________________________________________________________________ \n",
      "\n",
      "active_data_mode_name: kdibaactive_data_mode_name: kdiba\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "active_data_mode_name: kdibaactive_data_mode_name: kdiba\tsession_context: kdiba_gor01_one_2006-6-09_1-22-43\n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl...\n",
      "\n",
      "\n",
      "\n",
      " Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl...\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43\tsession_context: kdiba_gor01_one_2006-6-12_15-55-31\n",
      " \n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl...\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl... __________________________________________________________________Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl...\n",
      " \n",
      "active_data_mode_name: kdibabasedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43__________________________________________________________________ \n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31\n",
      "\n",
      "\n",
      "active_data_mode_name: kdiba\tsession_context: kdiba_gor01_two_2006-6-09_22-24-40\n",
      "active_data_mode_name: kdiba\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "\n",
      "__________________________________________________________________Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl...\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40 \n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl... Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl...\n",
      " Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "done.\n",
      "were pipeline preprocessing parameters missing and updated?: FalseLoading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl.\n",
      "\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl... properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-NoneLoading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.done.\n",
      "\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "properties already present in pickled version. No need to save.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "pipeline load success!\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "properties already present in pickled version. No need to save.done.\n",
      "\n",
      "pipeline load success!\n",
      "\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl.\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl... saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...)\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl...\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      " \t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl.\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-Noneproperties already present in pickled version. No need to save.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_sessionWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "pipeline load success!\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...)\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "properties already present in pickled version. No need to save.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_datapipeline load success!\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-Noneusing provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "properties already present in pickled version. No need to save.\n",
      "were pipeline preprocessing parameters missing and updated?: Falsepipeline load success!\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl...\n",
      " WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_sessionWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...)WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "done.\n",
      "\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl...\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      " \n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-Nonehave an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneRunning batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "pf_computation, maze_any already computed.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "pfdt_computation, maze_any already computed.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.position_decoding, maze_any already computed.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-Noneextended_stats missing.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t Recomputing extended_stats...\n",
      "\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.done.\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...)\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...)\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-Noneincluded includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "\n",
      "\n",
      "\n",
      "pfdt_computation, maze_any already computed.Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataposition_decoding, maze_any already computed.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "extended_stats missing.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "\t Recomputing extended_stats...WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-NoneDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl...Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl...\n",
      "  \t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.saving_mode.shouldSave == False, so not saving at the end of batch_load_sessionWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...)\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_sessionPerforming run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...)Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-NoneDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...were pipeline preprocessing parameters missing and updated?: False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-08_14-26-15, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15, ...)\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataWARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-NoneDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataWARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl...DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl...  \n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl... \n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).done.\n",
      "\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl... \t done.\n",
      "firing_rate_trends, maze_any already computed.Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "\n",
      "done.Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...build_global_directional_result_from_natural_epochs(...): was_modified: False\n",
      "\n",
      "\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "done.Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "n_neurons: 40, shared_aclus: [  2   3   6   7   9  12  17  18  22  26  27  29  30  31  32  37  42  44\n",
      "  46  47  51  55  56  57  58  63  65  67  71  74  75  79  81  83  87  94\n",
      "  96 100 102 103]included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "pf_computation, maze_any already computed.\n",
      "\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\t done.\n",
      "\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "firing_rate_trends, maze_any already computed.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "\n",
      "\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataPerforming run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...long_short_decoding_analyses, maze_any already computed.\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...short_long_pf_overlap_analyses, maze_any already computed.\n",
      "\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...jonathan_firing_rate_analysis, maze_any already computed.\n",
      "\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl\n",
      "\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl... reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/20231207181720-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl'\n",
      "done.\t done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pklDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl... \n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "pfdt_computation, maze_any already computed.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesposition_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\t Recomputing extended_stats...\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:05.677279\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...position_decoding, maze_any already computed.\n",
      "\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl... Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl... \tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/20231207181717-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl'\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/20231207181721-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesdone.\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesskipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\n",
      "\t time since last computation: 0:00:11.456283DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\n",
      "\t time since last computation: 0:00:07.869942\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5!\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5!pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n",
      "\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/20231207181721-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:09.058579\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/20231207181720-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:11.974985\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_16-40-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14f075eb5bc0>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14f075f1f9c0>) while trying to build the session HDF output for kdiba_vvp01_two_2006-4-09_16-40-54\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-09_16-40-54...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-25-50'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_16-53-46'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_17-29-30'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14efcda79380>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14f075d9a800>) while trying to build the session HDF output for kdiba_vvp01_one_2006-4-09_17-29-30\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-09_17-29-30...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_15-55-31'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14efcd964200>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14efcd9531c0>) while trying to build the session HDF output for kdiba_gor01_one_2006-6-12_15-55-31\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14edc96a7cc0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-10_12-58-3\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...)DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl... done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl... \t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14ee033ec7c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_17-46-44\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl... \t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14eda75f4fc0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_19-28-0\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "done.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-NoneLoading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.properties already present in pickled version. No need to save.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "pipeline load success!\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-NoneWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0, ...)WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-NoneDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataWARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-Nonewere pipeline preprocessing parameters missing and updated?: False\n",
      "\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl...\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter. \n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl... done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_dataPerforming run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"long_short_fr_indicies_analyses, maze_any already computed.\n",
      "\n",
      "jonathan_firing_rate_analysis, maze_any already computed.pf_computation, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pklextended_stats missing.\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl... \n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/20231207181756-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:09.786058\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-10_12-25-50...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-58-3'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/20231207181804-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:13.452428\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5!ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14edc8c929c0>) while trying to build the session HDF output.\n",
      "\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14edc84c8780>) while trying to build the session HDF output for kdiba_vvp01_two_2006-4-10_12-58-3\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-10_12-58-3...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/20231207181805-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:15.499658\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14f04aaf7300>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_pin01_one_11-03_12-3-25\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl... \tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/20231207181725-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_19-28-0'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:01:03.418664\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14eda6454b00>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14eda4de0fc0>) while trying to build the session HDF output for kdiba_pin01_one_11-02_19-28-0\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_19-28-0...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\t\t done (success).WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t>> calling external computation function: a_test_completion_function\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...,across_session_results_extended_dict: {})\n",
      "\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14edc8aaf800>). Suppressing.\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "build_batch_task_logger(module_name=\"gl0003.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54\"):\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0003.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54.log\n",
      "========================== runBatch STARTING ==========================\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\tsession_context: kdiba_pin01_one_fet11-01_12-58-54\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "__________________________________________________________________\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-Noneactive_data_mode_name: kdiba\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl...\n",
      " WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_17-46-44'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps, maze_any already computed.\n",
      "long_short_decoding_analyses, maze_any already computed.\n",
      "short_long_pf_overlap_analyses, maze_any already computed.\n",
      "long_short_fr_indicies_analyses, maze_any already computed.\n",
      "jonathan_firing_rate_analysis, maze_any already computed.\n",
      "long_short_post_decoding, maze_any already computed.\n",
      "long_short_inst_spike_rate_groups, maze_any already computed.\n",
      "long_short_endcap_analysis, maze_any already computed.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl... \tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/20231207181725-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:01:11.505277\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\t\t done (success).\n",
      "\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14f075ecd080>). Suppressing.WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_odd] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_even] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze_any] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/20231207181835-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl'\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesDEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timeswere pipeline preprocessing parameters missing and updated?: False\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_timesWARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:05.898733\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5done.\n",
      "\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'extended_stats', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps', 'rank_order_shuffle_analysis'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_any\"\n",
      "pf_computation, maze_any already computed.\n",
      "pfdt_computation, maze_any already computed.\n",
      "position_decoding, maze_any already computed.\n",
      "extended_stats missing.\n",
      "\t Recomputing extended_stats...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_odd\"...\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14eecea467c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2_any\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_any\"...\n",
      "\t done.\n",
      "firing_rate_trends, maze_any already computed.\n",
      "split_to_directional_laps missing.\n",
      "\t Recomputing split_to_directional_laps...\n",
      "WARN: _split_to_directional_laps(...): include_includelist: ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any'] is specified but include_includelist is currently ignored! Continuing with defaults.\n",
      "build_global_directional_result_from_natural_epochs(...): was_modified: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-03_12-3-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "n_neurons: 24, shared_aclus: [ 2  3  4  5  7  8  9 11 12 13 14 15 17 18 20 21 22 26 27 28 29 30 31 32]\n",
      "\t done.\n",
      "long_short_decoding_analyses missing.\n",
      "\t Recomputing long_short_decoding_analyses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "`is_certain_properly_constrained`: True - Correctly initialized pipelines (pfs limited to laps, decoders already long/short constrainted by default, replays already the estimated versions\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14ed7ada5200>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14f04a9d8900>) while trying to build the session HDF output for kdiba_pin01_one_11-03_12-3-25\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-03_12-3-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14f04aaf7940>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/20231207181727-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:01:24.603186\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/pipeline_results.h5\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-07_16-40-19'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14f074c13d40>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14f04daac4c0>) while trying to build the session HDF output for kdiba_gor01_two_2006-6-07_16-40-19\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-07_16-40-19...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_1-22-43'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/20231207181727-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:01:39.549569\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_17-46-44...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_14-26-15'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14ebc404fd00>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_22-24-40'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14efcdaa3ac0>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! Object dtype dtype('O') has no native HDF5 equivalent ::::: (<class 'TypeError'>, TypeError(\"Object dtype dtype('O') has no native HDF5 equivalent\"), <traceback object at 0x14f075dfd600>) while trying to build the session HDF output for kdiba_gor01_two_2006-6-09_22-24-40\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-09_22-24-40...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14efcc33bb80>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 67, n_all_epoch_timebins = 616)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14ebac549f40>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-09_1-22-43...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-08_14-26-15...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14ecee092800>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_one_2006-6-08_14-26-15, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14ec24679880>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_neurons = 67, n_all_epoch_timebins = 616)\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "(n_neurons = 24, n_all_epoch_timebins = 5668)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n",
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reusing extant decoder.\n",
      "USING EXISTING original_1D_decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1645: RuntimeWarning: invalid value encountered in divide\n",
      "  return ((long_fr - short_fr) / (long_fr + short_fr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"non_replays\"\n",
      "\t done.\n",
      "jonathan_firing_rate_analysis missing.\n",
      "\t Recomputing jonathan_firing_rate_analysis...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: 2023-09-28 16:15: - [ ] fix the combination properties. Would work if we directly used the computed _is_L_only and _is_S_only above\n",
      "\t done.\n",
      "long_short_post_decoding missing.\n",
      "\t Recomputing long_short_post_decoding...\n",
      "\t done.\n",
      "long_short_inst_spike_rate_groups missing.\n",
      "\t Recomputing long_short_inst_spike_rate_groups...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "long_short_endcap_analysis missing.\n",
      "\t Recomputing long_short_endcap_analysis...\n",
      "num_disappearing_endcap_cells/num_total_endcap_cells: 2/41\n",
      "num_non_disappearing_endcap_cells/num_total_endcap_cells: 39/41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:1302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_disappearing_endcap_cells_df['has_significant_distance_remapping'] = (np.abs(non_disappearing_endcap_cells_df['LS_pf_peak_x_diff']) >= 40.0) # The most a placefield could translate intwards would be (35 + (pf_width/2.0)) I think.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_significant_position_remappping_endcap_cells/num_non_disappearing_endcap_cells: 3/39\n",
      "\t done.\n",
      "WARNING: after execution of all _comp_specifiers found the functions: {'rank_order_shuffle_analysis': False} still remain! Are they correct and do they have proper validator decorators?\n",
      "done with all batch_extended_computations(...).\n",
      "newly_computed_values: [('extended_stats', 'maze_any'), ('split_to_directional_laps', 'maze_any'), ('long_short_decoding_analyses', 'maze_any'), ('short_long_pf_overlap_analyses', 'maze_any'), ('long_short_fr_indicies_analyses', 'maze_any'), ('jonathan_firing_rate_analysis', 'maze_any'), ('long_short_post_decoding', 'maze_any'), ('long_short_inst_spike_rate_groups', 'maze_any'), ('long_short_endcap_analysis', 'maze_any')]. Saving global results...\n",
      "WARNING: supposed to skip_saving because of self.saving_mode: PipelineSavingScheme.SKIP_SAVING but supposedly has new global results! Figure out if these are actually new.\n",
      "global_computation_results_pickle_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl\n",
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/scipy/spatial/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(js / 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmoving new output at '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/20231207182427-global_computation_results.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl'\n",
      "done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:34.329106\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "(n_neurons = 24, n_all_epoch_timebins = 5668)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_21-16-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-08_21-16-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t done.\n",
      "short_long_pf_overlap_analyses missing.\n",
      "\t Recomputing short_long_pf_overlap_analyses...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:2558: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_convolved_result_subset = convolved_result_subset / convolved_result_subset_area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "long_short_fr_indicies_analyses missing.\n",
      "\t Recomputing long_short_fr_indicies_analyses...\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t done (success).\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14efb6b04a80>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/utils/efficient_interval_search.py:650: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  epoch_split_spike_dfs_aclu_firingrates_Hz = [{an_aclu:(float(a_count)/trimmed_epoch_duration) for an_aclu, a_count in a_spike_count_dict.items()} for trimmed_epoch_duration, a_spike_count_dict in zip(spike_trimmed_active_epochs.durations, epoch_split_spike_dfs_aclu_spikecounts)] # just the non-zero aclus values: e.g. {108: 16.582832394938322, 36: 16.582832394938322, 34: 16.582832394938322, 66: 16.582832394938322, 58: 12.437124296203741, 74: 12.437124296203741, 51: 12.437124296203741, 23: 8.291416197469161, 57: 8.291416197469161, 32: 8.291416197469161, 63: 8.291416197469161, 11: 8.291416197469161, 73: 8.291416197469161, 88: 8.291416197469161, 16: 8.291416197469161, 31: 8.291416197469161, 13: 4.1457080987345805, 27: 4.1457080987345805, 10: 4.1457080987345805, 19: 4.1457080987345805, 25: 4.1457080987345805, 62: 4.1457080987345805, 59: 4.1457080987345805, 21: 4.1457080987345805, 98: 4.1457080987345805, 14: 4.1457080987345805}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"laps\"\n",
      "_generalized_compute_long_short_firing_rate_indicies(...): processing key: \"replays\"\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: ERROR: time_bin_size_seconds (0.01) > (t_stop - t_start) (0.006266876793233678). Reduce the bin size or exclude this epoch.\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_pin01_one_fet11-01_12-58-54. error: !! ERROR: time_bin_size_seconds (0.01) > (t_stop - t_start) (0.006266876793233678). Reduce the bin size or exclude this epoch. ::::: (<class 'AssertionError'>, AssertionError('ERROR: time_bin_size_seconds (0.01) > (t_stop - t_start) (0.006266876793233678). Reduce the bin size or exclude this epoch.'), <traceback object at 0x14f04a9ac880>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:27.462174\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'fet11-01_12-58-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x14f04a9c8b00>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x14edd2fd8c80>) while trying to build the session HDF output for kdiba_pin01_one_fet11-01_12-58-54\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_fet11-01_12-58-54...\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'jonathan_firing_rate_analysis' ::::: (<class 'KeyError'>, KeyError('jonathan_firing_rate_analysis'), <traceback object at 0x14efca48e880>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_fet11-01_12-58-54]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...,across_session_results_extended_dict: {})\n",
      "error occured in post_run_callback_fn: !! 'RankOrder' ::::: (<class 'KeyError'>, KeyError('RankOrder'), <traceback object at 0x14efcd8df6c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "\n",
    "# multiprocessing_kwargs = dict(use_multiprocessing=False, num_processes=1)\n",
    "multiprocessing_kwargs = dict(use_multiprocessing=True, num_processes=10)\n",
    "\n",
    "# Whether to output figures:\n",
    "should_perform_figure_generation_to_file=False\n",
    "# should_perform_figure_generation_to_file=True\n",
    "\n",
    "## Included Session Contexts:\n",
    "# included_session_contexts = batch_progress_df[np.logical_and(batch_progress_df['has_user_replay_annotations'], batch_progress_df['is_ready'])]['context'].to_numpy().tolist()\n",
    "\n",
    "# Only require sessions to have replay annotations:\n",
    "# included_session_contexts = batch_progress_df[batch_progress_df['has_user_replay_annotations']]['context'].to_numpy().tolist()\n",
    "\n",
    "# included_session_contexts = batch_progress_df['context'].to_numpy().tolist()[:4] # Only get the first 6\n",
    "# Limit the contexts to run to the last N:\n",
    "# included_session_contexts = included_session_contexts[3:5]\n",
    "\n",
    "# included_session_contexts = [included_session_contexts[3]]\n",
    "\n",
    "# ALL\n",
    "included_session_contexts = included_session_contexts\n",
    "\n",
    "# ## No filtering the sessions:\n",
    "# included_session_contexts = None\n",
    "\n",
    "if included_session_contexts is not None:\n",
    "    print(f'len(included_session_contexts): {len(included_session_contexts)}')\n",
    "else:\n",
    "    print(f'included_session_contexts is None so all session contexts will be included.')\n",
    "\n",
    "# included_session_contexts\n",
    "\n",
    "# # No recomputing at all:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER), # , override_file=\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "# No Reloading\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "                                                session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "                                                **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "# # Forced Reloading:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=True,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE, force_global_recompute=True,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "active_post_run_callback_fn = result_handler.on_complete_success_execution_session\n",
    "# active_post_run_callback_fn = _temp_on_complete_success_execution_session\n",
    "\n",
    "\n",
    "def a_test_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
    "    # print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    print(f'a_test_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...,across_session_results_extended_dict: {across_session_results_extended_dict})')\n",
    "    long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "\n",
    "    # 2023-11-27 - I'd like to be able to save/load single results a time, (meaning specific to their parameters):\n",
    "\n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData\n",
    "\n",
    "    # list = ['2Hz', '12Hz']\n",
    "    directional_laps_output_path = curr_active_pipeline.get_output_path().joinpath(f'DirectionalLaps_2Hz_new_{BATCH_DATE_TO_USE}.pkl').resolve()\n",
    "    saveData(directional_laps_output_path, (curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'], curr_active_pipeline.global_computation_results.computed_data['RankOrder']))\n",
    "\n",
    "    print(f'>>\\t done with {curr_session_context}')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "    return across_session_results_extended_dict\n",
    "\n",
    "\n",
    "result_handler.completion_functions.append(a_test_completion_function)\n",
    "\n",
    "## Specific Setup for 2023-09-28 Changes to LxC/SxC \"refinements\"\n",
    "result_handler.extended_computations_include_includelist = ['pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise',\n",
    "                                                # 'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                # 'position_decoding_two_step',\n",
    "                                                'extended_stats',\n",
    "                                                'long_short_decoding_analyses',\n",
    "                                                'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', #, 'long_short_rate_remapping',\n",
    "                                                'long_short_inst_spike_rate_groups',\n",
    "                                                'long_short_endcap_analysis',\n",
    "                                                # 'spike_burst_detection',\n",
    "                                                'split_to_directional_laps',\n",
    "                                                'rank_order_shuffle_analysis'\n",
    "                                                ]\n",
    "\n",
    "\n",
    "basic_local_computations = ['pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "#                                                 'pf_dt_sequential_surprise',\n",
    "                                                # 'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                #'position_decoding_two_step', \n",
    "                                                ]\n",
    " \n",
    "\n",
    "# basic_local_computations = [] # set to empty so we don't overwrite these computations\n",
    "# result_handler.extended_computations_include_includelist = ['split_to_directional_laps', 'rank_order_shuffle_analysis'] # OVERRIDE with specific computations to do\n",
    "\n",
    "\n",
    "result_handler.enable_hdf5_output = True # output the HDF5 when done.\n",
    "# result_handler.override_existing_frs_index_values = True\n",
    "# result_handler.frs_index_inclusion_magnitude = 0.1\n",
    "\n",
    "# result_handler.enable_hdf5_output = False\n",
    "result_handler.override_existing_frs_index_values = False\n",
    "\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "global_batch_run.execute_all(force_reload=result_handler.force_reload_all, saving_mode=result_handler.saving_mode, skip_extended_batch_computations=True, post_run_callback_fn=active_post_run_callback_fn,\n",
    "                             fail_on_exception=False, included_session_contexts=included_session_contexts,\n",
    "                                                                                        **{'computation_functions_name_includelist': basic_local_computations,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}, **multiprocessing_kwargs) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f130e5c9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving (file mode '/nfs/turbo/umms-kdiba/Data/global_batch_result_2023-12-07_GL.pkl') saved session pickle file results : /nfs/turbo/umms-kdiba/Data/global_batch_result_2023-12-07_GL.pkl... \tmoving new output at '/nfs/turbo/umms-kdiba/Data/20231207182628-global_batch_result_2023-12-07_GL.pkltmp' -> to desired location: '/nfs/turbo/umms-kdiba/Data/global_batch_result_2023-12-07_GL.pkl'\n",
      "done.\n",
      "done outputting HDF file.\n"
     ]
    }
   ],
   "source": [
    "# Save to pickle:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0981cde1",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [format_name, animal, exper_name, session_name, context, basedirs, status, errors, session_datetime, n_long_laps, n_long_replays, n_short_laps, n_short_replays, is_ready, global_computation_result_file, loaded_session_pickle_file, ripple_result_file, has_user_replay_annotations, has_user_grid_bin_bounds_annotations]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Across Sessions After Batching Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dce885b-5b99-4a7a-9f72-2eed2e45ae18",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_batch_progress_df = included_session_batch_progress_df.copy()\n",
    "\n",
    "good_session_concrete_folders = [ConcreteSessionFolder(a_context, a_basedir) for a_context, a_basedir in zip(list(a_batch_progress_df.context.values), list(a_batch_progress_df.basedirs.values))]\n",
    "\n",
    "# good_only_batch_progress_df.batch_results\n",
    "# included_h5_paths = [get_file_str_if_file_exists(v.joinpath('output','pipeline_results.h5').resolve()) for v in list(good_only_batch_progress_df.basedirs.values)]\n",
    "# included_h5_paths = [a_dir.joinpath('output','pipeline_results.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']]\n",
    "included_h5_paths = [get_file_str_if_file_exists(v.pipeline_results_h5) for v in good_session_concrete_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39738fa-f8f3-46d2-a937-0fb08c0ccb43",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle_2023-12-07_GL.pkl'),\n",
       " PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl'): PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results_2023-12-07_GL.pkl')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_dir = Path('output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-10-03').resolve()\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, target_dir=target_dir)\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-05', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix=BATCH_DATE_TO_USE, only_include_file_types=['local_pkl', 'global_pkl'])\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-07', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "copy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cb2d8-65b3-405b-a41b-22b2fa7cb28e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_dict_h5_files = copy_movedict(copy_dict)\n",
    "moved_files_dict_h5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e3954-15a4-449c-b927-56d5d79153c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_copydict_output_filename=f'backed_up_files_copydict_{BATCH_DATE_TO_USE}.csv'\n",
    "moved_files_copydict_file_path = Path(global_data_root_parent_path).joinpath(moved_files_copydict_output_filename).resolve() # Use Default\n",
    "print(f'moved_files_copydict_file_path: {moved_files_copydict_file_path}')\n",
    "\n",
    "_out_string, filedict_out_path = save_copydict_to_text_file(moved_files_dict_h5_files, moved_files_copydict_file_path, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab869730",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_moved_files_dict_files = read_copydict_from_text_file(moved_files_copydict_file_path, debug_print=False)\n",
    "read_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_moved_files_dict_files\n",
    "restore_moved_files_dict_files = invert_filedict(read_moved_files_dict_files)\n",
    "restore_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d067d8f2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5!\n",
      "WARN: file_size < 0.01 for /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>modification_time</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>file_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>2023-12-07 18:20:44.531143</td>\n",
       "      <td>2023-12-07 18:20:44.531143</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>2023-12-07 18:20:28.093476</td>\n",
       "      <td>2023-12-07 18:20:28.093476</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>2023-12-07 18:25:52.576427</td>\n",
       "      <td>2023-12-07 18:25:52.576427</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>2023-12-07 18:18:14.311862</td>\n",
       "      <td>2023-12-07 18:18:14.311862</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...</td>\n",
       "      <td>2023-12-07 18:18:09.408493</td>\n",
       "      <td>2023-12-07 18:18:09.408493</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>2023-12-07 18:19:10.840485</td>\n",
       "      <td>2023-12-07 18:19:15.604994</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "3  /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "4  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/200...   \n",
       "5  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "\n",
       "           modification_time              creation_time  file_size  \n",
       "0 2023-12-07 18:20:44.531143 2023-12-07 18:20:44.531143       0.91  \n",
       "1 2023-12-07 18:20:28.093476 2023-12-07 18:20:28.093476       0.81  \n",
       "2 2023-12-07 18:25:52.576427 2023-12-07 18:25:52.576427       0.60  \n",
       "3 2023-12-07 18:18:14.311862 2023-12-07 18:18:14.311862       0.29  \n",
       "4 2023-12-07 18:18:09.408493 2023-12-07 18:18:09.408493       0.26  \n",
       "5 2023-12-07 18:19:10.840485 2023-12-07 18:19:15.604994       0.37  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_output_h5_files(included_h5_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0953",
   "metadata": {},
   "source": [
    "## Extract `across_sessions_instantaneous_fr_dict` from the computation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39691fe2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sessions: 0\n"
     ]
    }
   ],
   "source": [
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "across_sessions_recomputed_instantaneous_fr_dict = {}\n",
    "\n",
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "        a_good_recomp_result = a_result.across_session_results.get('recomputed_inst_fr_comps', None)\n",
    "        if a_good_recomp_result is not None:\n",
    "            across_sessions_recomputed_instantaneous_fr_dict[a_ctxt] = a_good_recomp_result\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "\n",
    "## Outputs: across_sessions_instantaneous_fr_dict, across_sessions_recomputed_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_sessions_recomputed_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09136799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "# AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "#                                                  inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "across_session_result_long_short_recomputed_inst_firing_rate_filename: str = f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_recomputed_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "                                                 inst_fr_output_filename=across_session_result_long_short_recomputed_inst_firing_rate_filename)\n",
    "\n",
    "\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "# neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True, )\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_and_save_all_combined_tables(included_session_contexts, included_h5_paths, override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'{BATCH_DATE_TO_USE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a859bff-8cdb-4281-a64f-251d24db7cb9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True)\n",
    "# neuron_replay_stats_table['is_refined_LxC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b2430-c9ec-4adb-81a8-1ffbf8a0cb3c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95013ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_identities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(neuron_replay_stats_table['is_refined_LxC'])\n",
    "# np.isnan(neuron_replay_stats_table['is_refined_LxC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "# _out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "# _out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership','is_refined_LxC', 'is_refined_SxC'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'is_refined_LxC', 'is_refined_SxC', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0cd9-3e60-4425-9351-dfc903f3f067",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import convert_filelist_to_new_parent\n",
    "\n",
    "def save_filelist_to_text_file(hdf5_output_paths, filelist_path: Path):\n",
    "    _out_string = '\\n'.join([str(a_file) for a_file in hdf5_output_paths])\n",
    "    print(f'{_out_string}')\n",
    "    print(f'saving out to \"{filelist_path}\"...')\n",
    "    with open(filelist_path, 'w') as f:\n",
    "        f.write(_out_string)\n",
    "    return _out_string, filelist_path\n",
    "\n",
    "# Save output filelist:\n",
    "\n",
    "# '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'\n",
    "\n",
    "# kdiba_vvp01_two_2006-4-10_12-58-3\n",
    "# \toutputs_local ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl')}\n",
    "# \toutputs_global ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'), 'hdf5': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5')}\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths = global_batch_run.build_output_files_lists()\n",
    "\n",
    "h5_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_HDF5_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_HDF5_savepath = save_filelist_to_text_file(hdf5_output_paths, h5_filelist_path)\n",
    "\n",
    "pkls_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_pkls_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_pkls_savepath = save_filelist_to_text_file(pkl_output_paths, pkls_filelist_path)\n",
    "\n",
    "# source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "source_parent_path = Path(r'/nfs/turbo/umms-kdiba/Data')\n",
    "dest_parent_path = Path(r'/~/W/Data/')\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_source = hdf5_output_paths\n",
    "filelist_dest_paths = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest_paths\n",
    "\n",
    "dest_Apogee_h5_filelist_path = global_data_root_parent_path.joinpath(f'dest_fileList_Apogee_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, dest_filelist_savepath = save_filelist_to_text_file(filelist_dest_paths, dest_Apogee_h5_filelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e69a3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "file_path\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ce774-98fb-4e69-a7e2-e94cbff1b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "\n",
    "# list(global_batch_run.session_batch_outputs.keys())\n",
    "\n",
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "# good_session_batch_outputs = global_batch_run.session_batch_outputs\n",
    "\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d19a7-5a89-43f1-aa33-3a7450d1f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178426c-54df-47ac-8103-a66f114c77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[a_ctxt.get_initialization_code_string() for a_ctxt in sessions_with_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28828512",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d9a7",
   "metadata": {},
   "source": [
    "# 2023-10-06 - `joined_neruon_fri_df` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE = '2023-10-05_NewParameters'\n",
    "BATCH_DATE_TO_USE = '2023-10-07'\n",
    "all_sessions_joined_neruon_fri_df, out_path = build_and_merge_all_sessions_joined_neruon_fri_df(global_data_root_parent_path, BATCH_DATE_TO_USE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_neruon_fri_df_basename = f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df'\n",
    "AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=joined_neruon_fri_df_basename, include_csv=False)\n",
    "print(f'>>\\t done with {output_file_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c4c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-10-04 - Load Saved across-sessions-data and testing Batch-computed inst_firing_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "# from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PaperFigureTwo, InstantaneousSpikeRateGroupsComputation\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "# from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import list_of_dicts_to_dict_of_lists\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34549c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved across-session results:\n",
    "# inst_fr_output_filename = 'long_short_inst_firing_rate_result_handlers_2023-07-12.pkl'\n",
    "# inst_fr_output_filename = 'across_session_result_long_short_inst_firing_rate.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-07-21.pkl'\n",
    "# inst_fr_output_filename=f'across_session_result_handler_{BATCH_DATE_TO_USE}.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-08-09_Test.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-10-04-GL.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-04-GL-Recomp.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-07.pkl'\n",
    "inst_fr_output_filename: str = f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abea5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionTables\n",
    " \n",
    "## Load all across-session tables from the pickles:\n",
    "# output_path_suffix: str = f'2023-10-07'\n",
    "output_path_suffix: str = f'{BATCH_DATE_TO_USE}'\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.load_all_combined_tables(override_output_parent_path=global_data_root_parent_path, output_path_suffix=output_path_suffix) # output_path_suffix=f'2023-10-04-GL-Recomp'\n",
    "num_sessions = len(neuron_replay_stats_table.session_uid.unique().to_numpy())\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager, SessionCellExclusivityRecord\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "annotation_man = UserAnnotationsManager()\n",
    "\n",
    "LxC_uids = []\n",
    "SxC_uids = []\n",
    "\n",
    "for a_ctxt in included_session_contexts:\n",
    "\tsession_uid = a_ctxt.get_description(separator=\"|\", include_property_names=False)\n",
    "\tsession_uid\n",
    "\tsession_cell_exclusivity: SessionCellExclusivityRecord = annotation_man.annotations[a_ctxt].get('session_cell_exclusivity', None)\n",
    "\tLxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.LxC])\n",
    "\tSxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.SxC])\n",
    "\t\n",
    "# [a_ctxt.get_description(separator=\"|\", include_property_names=False) for a_ctxt in included_session_contexts]\n",
    "\n",
    "long_short_fr_indicies_analysis_table['XxC_status'] = 'Shared'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, LxC_uids), 'XxC_status'] = 'LxC'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, SxC_uids), 'XxC_status'] = 'SxC'\n",
    "\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Get the long peak location\n",
    "\n",
    "long_short_fr_indicies_analysis_table['long_pf_peak_x'] = neuron_replay_stats_table['long_pf_peak_x']\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1641aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='x_frs_index', title='Pf Peak position vs. LapsFRI', ylabel='Lap FRI')\n",
    "\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='y_frs_index', title='Pf Peak position vs. ReplayFRI', ylabel='Replay FRI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9987dc",
   "metadata": {},
   "source": [
    " #TODO 2023-10-05 11:40: - [ ] Extract the \"contrarian cells\", the ones that have a strong exclusivity on the laps but the opposite tendency on the replays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_short_fr_indicies_analysis_table_filename = 'output/2023-10-07_long_short_fr_indicies_analysis_table.csv'\n",
    "long_short_fr_indicies_analysis_table_filename: str = 'output/{BATCH_DATE_TO_USE}_long_short_fr_indicies_analysis_table.csv'\n",
    "long_short_fr_indicies_analysis_table.to_csv(long_short_fr_indicies_analysis_table_filename)\n",
    "print(f'saved: {long_short_fr_indicies_analysis_table_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427597f",
   "metadata": {},
   "source": [
    "# 2023-10-10 - Statistics for `across_sessions_bar_graphs`, analysing `across_session_inst_fr_computation` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33422d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import pho_stats_perform_diagonal_line_binomial_test, pho_stats_bar_graph_t_tests\n",
    "\n",
    "binom_test_chance_result = pho_stats_perform_diagonal_line_binomial_test(long_short_fr_indicies_analysis_table)\n",
    "print(f'binom_test_chance_result: {binom_test_chance_result}')\n",
    "\n",
    "LxC_Laps_T_result, SxC_Laps_T_result, LxC_Replay_T_result, SxC_Replay_T_result = pho_stats_bar_graph_t_tests(across_session_inst_fr_computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea5a5",
   "metadata": {},
   "source": [
    "## 2023-10-04 - Run `AcrossSessionsVisualizations` corresponding to the PhoDibaPaper2023 figures for all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1152c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \n",
    "global_multi_session_context, _out_aggregate_fig_2 = AcrossSessionsVisualizations.across_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels=False, enable_hover_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_long_and_short_firing_rate_replays_v_laps_figure(neuron_replay_stats_table=neuron_replay_stats_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_man = UserAnnotationsManager()\n",
    "included_annotations = {ctxt:ann_man.annotations[ctxt].get('session_cell_exclusivity', None) for ctxt in included_session_contexts}\n",
    "\n",
    "all_LxCs = []\n",
    "all_SxCs = []\n",
    "\n",
    "for ctxt, an_ann in included_annotations.items():\n",
    "\tsession_ctxt_key:str = ctxt.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "\tall_LxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.LxC])\n",
    "\tall_SxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.SxC])\n",
    "\t\n",
    "all_LxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1222a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_SxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381fcf5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "across_session_inst_fr_computation.LxC_scatter_props\n",
    "across_session_inst_fr_computation.SxC_scatter_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate across all of the sessions to build a new combined `InstantaneousSpikeRateGroupsComputation`, which can be used to plot the \"PaperFigureTwo\", bar plots for many sessions.\n",
    "global_multi_session_context = IdentifyingContext(format_name='kdiba', num_sessions=num_sessions) # some global context across all of the sessions, not sure what to put here.\n",
    "\n",
    "# To correctly aggregate results across sessions, it only makes sense to combine entries at the `.cell_agg_inst_fr_list` variable and lower (as the number of cells can be added across sessions, treated as unique for each session).\n",
    "\n",
    "## Display the aggregate across sessions:\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # WARNING: we didn't save this info\n",
    "_out_fig_2.computation_result = across_session_inst_fr_computation # the result loaded from the file\n",
    "_out_fig_2.active_identifying_session_ctx = across_session_inst_fr_computation.active_identifying_session_ctx\n",
    "# Set callback, the only self-specific property\n",
    "# _out_fig_2._pipeline_file_callback_fn = curr_active_pipeline.output_figure # lambda args, kwargs: self.write_to_file(args, kwargs, curr_active_pipeline)\n",
    "_out_fig_2.scatter_props_fn = _return_scatter_props_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LxC_aclus = _out_fig_2.computation_result.LxC_aclus\n",
    "SxC_aclus = _out_fig_2.computation_result.SxC_aclus\n",
    "\n",
    "LxC_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureOutputManager, FigureOutputLocation, ContextToPathMode\n",
    "\n",
    "registered_output_files = {}\n",
    "\n",
    "def output_figure(final_context: IdentifyingContext, fig, write_vector_format:bool=False, write_png:bool=True, debug_print=True):\n",
    "    \"\"\" outputs the figure using the provided context. \"\"\"\n",
    "    from pyphoplacecellanalysis.General.Mixins.ExportHelpers import build_and_write_to_file\n",
    "    def register_output_file(output_path, output_metadata=None):\n",
    "        \"\"\" registers a new output file for the pipeline \"\"\"\n",
    "        print(f'register_output_file(output_path: {output_path}, ...)')\n",
    "        registered_output_files[output_path] = output_metadata or {}\n",
    "\n",
    "    fig_out_man = FigureOutputManager(figure_output_location=FigureOutputLocation.DAILY_PROGRAMMATIC_OUTPUT_FOLDER, context_to_path_mode=ContextToPathMode.HIERARCHY_UNIQUE)\n",
    "    active_out_figure_paths = build_and_write_to_file(fig, final_context, fig_out_man, write_vector_format=write_vector_format, write_png=write_png, register_output_file_fn=register_output_file)\n",
    "    return active_out_figure_paths, final_context\n",
    "\n",
    "\n",
    "# Set callback, the only self-specific property\n",
    "_out_fig_2._pipeline_file_callback_fn = output_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing\n",
    "restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# Perform interactive Matplotlib operations with 'Qt5Agg' backend\n",
    "_fig_2_theta_out, _fig_2_replay_out = _out_fig_2.display(active_context=global_multi_session_context, title_modifier_fn=lambda original_title: f\"{original_title} ({num_sessions} sessions)\", save_figure=True)\n",
    "\t\n",
    "_out_fig_2.perform_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Surprise Shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d829b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a87449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
