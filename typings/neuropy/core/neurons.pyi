"""
This type stub file was generated by pyright.
"""

import numpy as np
from typing import Sequence, Union
from .datawriter import DataWriter
from neuropy.utils.mixins.time_slicing import StartStopTimesMixin, TimeSlicableObjectProtocol
from neuropy.utils.mixins.unit_slicing import NeuronUnitSlicableObjectProtocol
from neuropy.utils.mixins.concatenatable import ConcatenationInitializable
from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin

class Neurons(HDF_SerializationMixin, NeuronUnitSlicableObjectProtocol, StartStopTimesMixin, TimeSlicableObjectProtocol, ConcatenationInitializable, DataWriter):
    """Class to hold a group of spiketrains and their labels, ids etc."""
    def __init__(self, spiketrains: np.ndarray, t_stop, t_start=..., sampling_rate=..., neuron_ids=..., neuron_type=..., waveforms=..., peak_channels=..., shank_ids=..., extended_neuron_properties_df=..., metadata=...) -> None:
        ...
    
    @property
    def neuron_ids(self): # -> NDArray[signedinteger[Any]] | NDArray[Any] | None:
        """The neuron_ids property."""
        ...
    
    @neuron_ids.setter
    def neuron_ids(self, value): # -> None:
        """ ensures the indicies are integers and builds the reverse index map upon setting this value """
        ...
    
    @property
    def reverse_cellID_index_map(self): # -> dict[Any, Any] | None:
        """The reverse_cellID_index_map property: Allows reverse indexing into the linear imported array using the original cell ID indicies."""
        ...
    
    @property
    def neuron_type(self): # -> list[Any] | NDArray[Any]:
        """The neuron_type property."""
        ...
    
    @neuron_type.setter
    def neuron_type(self, value): # -> None:
        ...
    
    @property
    def aclu_to_neuron_type_map(self): # -> dict[_KT, _VT]:
        """ builds a map from the neuron_id to the neuron_type """
        ...
    
    def __getitem__(self, i): # -> Neurons:
        ...
    
    @property
    def sampling_rate(self): # -> int:
        ...
    
    @property
    def n_neurons(self): # -> int:
        ...
    
    def time_slice(self, t_start=..., t_stop=...): # -> Neurons:
        ...
    
    def get_neuron_type(self, query_neuron_type): # -> list[Any] | Neurons:
        """ filters self by the specified query_neuron_type, only returning neurons that match. """
        ...
    
    def __repr__(self) -> str:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def add_metadata(self): # -> None:
        ...
    
    def get_all_spikes(self): # -> NDArray[Any]:
        ...
    
    @property
    def n_total_spikes(self): # -> Any:
        ...
    
    @property
    def n_spikes(self): # -> NDArray[Any]:
        "number of spikes within each spiketrain"
        ...
    
    @property
    def firing_rate(self):
        """Return average firing rate for each neuron over the entire duration of the recording """
        ...
    
    def get_above_firing_rate(self, thresh: float): # -> Neurons:
        """Return neurons which have firing rate above thresh (on average, for the entire duration of the recording) """
        ...
    
    def get_by_id(self, ids): # -> Neurons:
        """Returns neurons object with neuron_ids equal to ids"""
        ...
    
    def get_isi(self, bin_size=..., n_bins=...): # -> NDArray[Any]:
        """Interspike interval

        Parameters
        ----------
        bin_size : float, optional
            [description], by default 0.001
        n_bins : int, optional
            [description], by default 200

        Returns
        -------
        [type]
            [description]
        """
        ...
    
    def get_waveform_similarity(self):
        ...
    
    def get_binned_spiketrains(self, bin_size=...): # -> BinnedSpiketrain:
        """Get binned spike counts

        Parameters
        ----------
        bin_size : float, optional
            bin size in seconds, by default 0.25

        Returns
        -------
        neuropy.core.BinnedSpiketrains

        """
        ...
    
    def get_mua(self, bin_size=...): # -> Mua:
        """Get mua between two time points

        Parameters
        ----------
        bin_size : float, optional
            [description], by default 0.001

        Returns
        -------
        MUA object
            [description]
        """
        ...
    
    def add_jitter(self): # -> None:
        ...
    
    def to_dict(self, recurrsively=...): # -> dict[str, Any]:
        ...
    
    @staticmethod
    def from_dict(d): # -> Neurons:
        ...
    
    def to_dataframe(self):
        ...
    
    @classmethod
    def initialize_missing_spikes_df_columns(cls, spikes_df, debug_print=...): # -> None:
        """ make sure the needed columns exist on spikes_df """
        ...
    
    @classmethod
    def from_dataframe(cls, spikes_df, dat_sampling_rate, time_variable_name=...): # -> Neurons:
        """ Builds a Neurons object from a spikes_df, such as the one belonging to its complementary FlattenedSpiketrains:
            Usage:
                neurons_obj = Neurons.from_dataframe(sess.flattened_spiketrains.spikes_df, sess.recinfo.dat_sampling_rate, time_variable_name='t_rel_seconds') 
        """
        ...
    
    @classmethod
    def concat(cls, objList: Union[Sequence, np.array]): # -> Neurons:
        """ Concatenates the object list along the time axis """
        ...
    
    def to_hdf(self, file_path, key: str, session_uid: str = ..., **kwargs):
        """ Saves the object to key in the hdf5 file specified by file_path
        Usage:
            hdf5_output_path: Path = curr_active_pipeline.get_output_path().joinpath('test_data.h5')
            _pfnd_obj: PfND = long_one_step_decoder_1D.pf
            _pfnd_obj.to_hdf(hdf5_output_path, key='test_pfnd')
        """
        ...
    
    @classmethod
    def read_hdf(cls, file_path, key: str, **kwargs) -> Neurons:
        """ Reads the data from the key in the hdf5 file at file_path
        Usage:
            _reread_pfnd_obj = PfND.read_hdf(hdf5_output_path, key='test_pfnd')
            _reread_pfnd_obj


        # Neurons.__init__( self, spiketrains: np.ndarray, t_stop, t_start=0.0, sampling_rate=1, neuron_ids=None, neuron_type=None, waveforms=None, peak_channels=None, shank_ids=None, extended_neuron_properties_df=None, metadata=None, )
        """
        ...
    


class BinnedSpiketrain(NeuronUnitSlicableObjectProtocol, DataWriter):
    """Class to hold binned spiketrains"""
    def __init__(self, spike_counts: np.ndarray, bin_size: float, t_start=..., neuron_ids=..., peak_channels=..., shank_ids=..., metadata=...) -> None:
        ...
    
    @staticmethod
    def from_neurons(neurons: Neurons, t_start=..., t_stop=..., bin_size=...): # -> None:
        ...
    
    @property
    def spike_counts(self):
        ...
    
    @spike_counts.setter
    def spike_counts(self, arr): # -> None:
        ...
    
    @property
    def n_neurons(self):
        ...
    
    @property
    def n_bins(self):
        ...
    
    @property
    def duration(self):
        ...
    
    @property
    def t_stop(self):
        ...
    
    def add_metadata(self): # -> None:
        ...
    
    @property
    def time(self):
        ...
    
    def to_dict(self, recurrsively=...): # -> dict[str, Any]:
        ...
    
    @staticmethod
    def from_dict(d): # -> BinnedSpiketrain:
        ...
    
    def get_pairwise_corr(self, pairs_bool=..., return_pair_id=...):
        """Pairwise correlation between pairs of binned of spiketrains

        Parameters
        ----------
        pairs_bool : 2D bool/logical array, optional
            Only these pairs are returned, by default None which means all pairs
        return_pair_id : bool, optional
            If true pair_ids are returned, by default False

        Returns
        -------
        corr
            1d vector of pairwise correlations
        """
        ...
    
    def __getitem__(self, i): # -> BinnedSpiketrain:
        ...
    
    def get_by_id(self, ids): # -> BinnedSpiketrain:
        """Returns BinnedSpiketrain object with neuron_ids equal to ids"""
        ...
    


class Mua(DataWriter):
    """ Mua stands for Multi-unit activity """
    def __init__(self, spike_counts: np.ndarray, bin_size: float, t_start: float = ..., metadata=...) -> None:
        ...
    
    @property
    def spike_counts(self): # -> ndarray[Any, Any]:
        ...
    
    @spike_counts.setter
    def spike_counts(self, arr: np.ndarray): # -> None:
        ...
    
    @property
    def bin_size(self):
        ...
    
    @bin_size.setter
    def bin_size(self, val): # -> None:
        ...
    
    @property
    def n_bins(self): # -> int:
        ...
    
    @property
    def duration(self):
        ...
    
    @property
    def t_stop(self):
        ...
    
    @property
    def time(self):
        ...
    
    @property
    def firing_rate(self):
        ...
    
    def get_smoothed(self, sigma=..., truncate=...): # -> Mua:
        ...
    
    def to_dict(self, recurrsively=...): # -> dict[str, Any]:
        ...
    
    @staticmethod
    def from_dict(d): # -> Mua:
        ...
    
    def to_dataframe(self): # -> DataFrame[Any]:
        ...
    


