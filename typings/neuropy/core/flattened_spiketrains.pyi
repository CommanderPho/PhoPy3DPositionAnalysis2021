"""
This type stub file was generated by pyright.
"""

import numpy as np
import pandas as pd
from typing import Sequence, Union
from neuropy.utils.mixins.binning_helpers import BinningInfo
from .datawriter import DataWriter
from neuropy.utils.mixins.time_slicing import TimeSlicableObjectProtocol, TimeSlicedMixin
from neuropy.utils.mixins.unit_slicing import NeuronUnitSlicableObjectProtocol
from neuropy.utils.mixins.concatenatable import ConcatenationInitializable
from neuropy.utils.mixins.HDF5_representable import HDFMixin

module_logger = ...
_REQUIRE_NEURON_TYPE_COLUMN: bool = ...
@pd.api.extensions.register_dataframe_accessor("spikes")
class SpikesAccessor(TimeSlicedMixin):
    """ Part of the December 2021 Rewrite of the neuropy.core classes to be Pandas DataFrame based and easily manipulatable """
    __time_variable_name = ...
    def __init__(self, pandas_obj) -> None:
        ...
    
    @property
    def time_variable_name(self): # -> str:
        ...
    
    def set_time_variable_name(self, new_time_variable_name): # -> None:
        ...
    
    @property
    def times(self):
        """ convenience property to access the times of the spikes in the dataframe 
            ## TODO: why doesn't this have a `times` property to access `self._obj[self.time_variable_name].values`?
        """
        ...
    
    @property
    def neuron_ids(self):
        """ return the unique cell identifiers (given by the unique values of the 'aclu' column) for this DataFrame """
        ...
    
    @property
    def neuron_probe_tuple_ids(self): # -> list[NeuronExtendedIdentityTuple]:
        """ returns a list of NeuronExtendedIdentityTuple tuples where the first element is the shank_id and the second is the cluster_id. Returned in the same order as self.neuron_ids """
        ...
    
    @property
    def n_total_spikes(self): # -> int:
        ...
    
    @property
    def n_neurons(self): # -> int:
        ...
    
    def get_split_by_unit(self, included_neuron_ids=...): # -> list[Any]:
        """ returns a list containing the spikes dataframe split by the 'aclu' column. """
        ...
    
    def sliced_by_neuron_id(self, included_neuron_ids) -> pd.DataFrame:
        """ gets the slice of spikes with the specified `included_neuron_ids` """
        ...
    
    def get_unit_spiketrains(self, included_neuron_ids=...): # -> NDArray[Any]:
        """ returns an array of the spiketrains (an array of the times that each spike occured) for each unit """
        ...
    
    def sliced_by_neuron_type(self, query_neuron_type) -> pd.DataFrame:
        """ returns a copy of self._obj filtered by the specified query_neuron_type, only returning neurons that match.
            e.g. query_neuron_type = NeuronType.PYRAMIDAL 
                or query_neuron_type = 'PYRAMIDAL' 
                or query_neuron_type = 'Pyr'
         """
        ...
    
    def sliced_by_neuron_qclu(self, included_qclu_values=...) -> pd.DataFrame:
        """ returns a copy of self._obj filtered by the specified included_qclu_values, only returning neurons that match.

        """
        ...
    
    def extract_unique_neuron_identities(self):
        """ Tries to build information about the unique neuron identitiies from the (highly reundant) information in the spikes_df. """
        ...
    
    def interpolate_spike_positions(self, position_sampled_times, position_x, position_y, position_linear_pos=..., position_speeds=...): # -> Any:
        ...
    
    def add_same_cell_ISI_column(self): # -> None:
        """ Compute the inter-spike-intervals (ISIs) for each cell/unit separately. Meaning the list should be the difference from the current spike to the last spike of the previous unit.
            spikes: curr_active_pipeline.sess.spikes_df
            adds column 'scISI' to spikes df.
            
            TODO: PERFORMANCE: This takes over a minute to compute for Bapun's data.
            
            # Created Columns:
                'scISI'
                
            # Called only from _default_add_spike_scISIs_if_needed(...)
        """
        ...
    
    def rebuild_fragile_linear_neuron_IDXs(self, debug_print=...): # -> tuple[Any, OrderedDict[Any, Any]]:
        """ Rebuilds the 'fragile_linear_neuron_IDX' and 'neuron_IDX' columns from the complete list of 'aclu' values in the current spike dataframe so that they're monotonic and without gaps. Ensures that all the fragile_linear_neuron_IDXs are valid after removing neurons or filtering cells.
        
        History:
            Refactored from a static function in SpikeRenderingBaseMixin.
    
                
        Called by helper_setup_neuron_colors_and_order(...)
        
        # Created/Updated Columns:
            'old_fragile_linear_neuron_IDX'
            'fragile_linear_neuron_IDX'
            'neuron_IDX'
            
        """
        ...
    
    def add_binned_time_column(self, time_window_edges, time_window_edges_binning_info: BinningInfo, debug_print: bool = ...): # -> Any:
        """ adds a 'binned_time' column to spikes_df given the time_window_edges and time_window_edges_binning_info provided 
        
        """
        ...
    
    def adding_lap_identity_column(self, laps_epoch_df, epoch_id_key_name: str = ...): # -> Any:
        """ Adds the lap IDX column to the spikes df from a set of lap epochs.

            spikes: curr_active_pipeline.sess.spikes_df
            adds column 'new_lap_IDX' to spikes df.
            
            # Created Columns:
                'new_lap_IDX'

        """
        ...
    
    def adding_epochs_identity_column(self, epochs_df: pd.DataFrame, epoch_id_key_name: str = ..., epoch_label_column_name=..., override_time_variable_name=..., no_interval_fill_value=...): # -> Any:
        """ Adds the arbitrary column with name epoch_id_key_name to the dataframe.

            spikes: curr_active_pipeline.sess.spikes_df
            adds column epoch_id_key_name to spikes df.
            
            # Created Columns:
                epoch_id_key_name

        """
        ...
    
    def to_hdf(self, file_path, key: str, **kwargs): # -> None:
        """ Saves the object to key in the hdf5 file specified by file_path 
        Usage:

        .spikes.to_hdf(
        """
        ...
    
    @classmethod
    def read_hdf(cls, file_path, key: str, **kwargs) -> pd.DataFrame:
        """  Reads the data from the key in the hdf5 file at file_path         
        # TODO 2023-07-30 13:05: - [ ] interestingly this leaves the dtype of this column as 'category' still, but _spikes_df["neuron_type"].to_numpy() returns the correct array of objects... this is better than it started before saving, but not the same. 
            - UPDATE: I think adding `.astype(str)` to the end of the conversion resolves it and makes the type the same as it started. Still not sure if it would be better to leave it a categorical because I think it's more space efficient and better than it started anyway.
        """
        ...
    


class FlattenedSpiketrains(HDFMixin, ConcatenationInitializable, NeuronUnitSlicableObjectProtocol, TimeSlicableObjectProtocol, DataWriter):
    """Class to hold flattened spikes for all cells"""
    def __init__(self, spikes_df: pd.DataFrame, time_variable_name=..., t_start=..., metadata=...) -> None:
        ...
    
    @property
    def spikes_df(self): # -> DataFrame[Any]:
        """The spikes_df property."""
        ...
    
    @property
    def flattened_sort_indicies(self): # -> ArrayLike:
        ...
    
    @property
    def flattened_spike_identities(self): # -> ArrayLike:
        ...
    
    @property
    def flattened_spike_times(self): # -> ArrayLike:
        ...
    
    def to_dict(self, recurrsively=...): # -> dict[str, Any]:
        ...
    
    @staticmethod
    def from_dict(d: dict): # -> FlattenedSpiketrains:
        ...
    
    def to_dataframe(self): # -> DataFrame[Any]:
        ...
    
    def time_slice(self, t_start=..., t_stop=...): # -> FlattenedSpiketrains:
        ...
    
    def get_by_id(self, ids): # -> FlattenedSpiketrains:
        """Returns neurons object with neuron_ids equal to ids"""
        ...
    
    def get_neuron_type(self, query_neuron_type): # -> list[Any] | FlattenedSpiketrains:
        """ filters self by the specified query_neuron_type, only returning neurons that match. """
        ...
    
    @classmethod
    def concat(cls, objList: Union[Sequence, np.array]): # -> FlattenedSpiketrains:
        """ Concatenates the object list """
        ...
    
    @staticmethod
    def interpolate_spike_positions(spikes_df, position_sampled_times, position_x, position_y, position_linear_pos=..., position_speeds=..., spike_timestamp_column_name=...):
        ...
    
    @staticmethod
    def build_spike_dataframe(active_session, timestamp_scale_factor=..., spike_timestamp_column_name=..., progress_tracing=...): # -> DataFrame[Any]:
        """ Builds the spike_df from the active_session's .neurons object.

        Args:
            active_session (_type_): _description_
            timestamp_scale_factor (tuple, optional): _description_. Defaults to (1/1E4).

        Returns:
            _type_: _description_
        
        # TODO: only use ProgressMessagePrinter if progress_tracing is True
        
        """
        ...
    
    def to_hdf(self, file_path, key: str, **kwargs): # -> None:
        """ Saves the object to key in the hdf5 file specified by file_path """
        ...
    
    @classmethod
    def read_hdf(cls, file_path, key: str, **kwargs) -> FlattenedSpiketrains:
        """  Reads the data from the key in the hdf5 file at file_path """
        ...
    


