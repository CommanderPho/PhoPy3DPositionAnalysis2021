{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a45f3f6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ ReviewOfWork (Main Notebook) - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:20.608442900Z",
     "start_time": "2023-11-16T23:21:20.217442100Z"
    },
    "collapsed": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all",
     "pho-run-2024"
    ]
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "# !pip install viztracer\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "import importlib\n",
    "from copy import deepcopy\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory, make_class\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "ip = get_ipython()\n",
    "\n",
    "\n",
    "# from pho_jupyter_preview_widget.display_helpers import array_repr_with_graphical_preview\n",
    "# from pho_jupyter_preview_widget.ipython_helpers import PreviewWidgetMagics\n",
    "\n",
    "# # Register the magic\n",
    "# ip.register_magics(PreviewWidgetMagics)\n",
    "\n",
    "# # %config_ndarray_preview width=500\n",
    "\n",
    "# # Register the custom display function for NumPy arrays\n",
    "# # ip.display_formatter.formatters['text/html'].for_type(np.ndarray, lambda arr: array_preview_with_graphical_shape_repr_html(arr))\n",
    "# # ip = array_repr_with_graphical_shape(ip=ip)\n",
    "# ip = array_repr_with_graphical_preview(ip=ip)\n",
    "# # ip = dataframe_show_more_button(ip=ip)\n",
    "\n",
    "text_formatter: PlainTextFormatter = ip.display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "from pyphocorehelpers.indexing_helpers import get_dict_subset\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "import pyphocorehelpers.programming_helpers as programming_helpers\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns, parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.notebook_helpers import NotebookCellExecutionLogger\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "\n",
    "doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "assert doc_output_parent_folder.exists()\n",
    "\n",
    "_notebook_path:Path = Path(IPythonHelpers.try_find_notebook_filepath(IPython.extract_module_locals())).resolve() # Finds the path of THIS notebook\n",
    "# _notebook_execution_logger: NotebookCellExecutionLogger = NotebookCellExecutionLogger(notebook_path=_notebook_path, enable_logging_to_file=False) # Builds a logger that records info about this notebook\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_evaluate_required_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphocorehelpers.exception_helpers import ExceptionPrintingContext, CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates, DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions,  RankOrderComputationsContainer, RankOrderResult, RankOrderAnalyses\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.ComputationFunctionRegistryHolder import ComputationFunctionRegistryHolder, computation_precidence_specifying_function, global_function\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "from neuropy.utils.mixins.binning_helpers import transition_matrix\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.transition_matrix import TransitionMatrixComputations\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates, get_proper_global_spikes_df\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap, visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters\n",
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "# Jupyter Widget Interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import fullwidth_path_widget, render_colors\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "DAY_DATE_STR: str = date.today().strftime(\"%Y-%m-%d\")\n",
    "DAY_DATE_TO_USE = f'{DAY_DATE_STR}' # used for filenames throught the notebook\n",
    "print(f'DAY_DATE_STR: {DAY_DATE_STR}, DAY_DATE_TO_USE: {DAY_DATE_TO_USE}')\n",
    "\n",
    "NOW_DATETIME: str = get_now_rounded_time_str()\n",
    "NOW_DATETIME_TO_USE = f'{NOW_DATETIME}' # used for filenames throught the notebook\n",
    "print(f'NOW_DATETIME: {NOW_DATETIME}, NOW_DATETIME_TO_USE: {NOW_DATETIME_TO_USE}')\n",
    "\n",
    "from pyphocorehelpers.gui.Jupyter.simple_widgets import build_global_data_root_parent_path_selection_widget\n",
    "all_paths = [Path('/Volumes/SwapSSD/Data'), Path('/Users/pho/data'), Path(r'/media/halechr/MAX/Data'), Path(r'/home/halechr/FastData'), Path(r'W:\\Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/Users/pho/cloud/turbo/Data')] # Path('/Volumes/FedoraSSD/FastData'), \n",
    "global_data_root_parent_path = None\n",
    "def on_user_update_path_selection(new_path: Path):\n",
    "    global global_data_root_parent_path\n",
    "    new_global_data_root_parent_path = new_path.resolve()\n",
    "    global_data_root_parent_path = new_global_data_root_parent_path\n",
    "    print(f'global_data_root_parent_path changed to {global_data_root_parent_path}')\n",
    "    assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "            \n",
    "global_data_root_parent_path_widget = build_global_data_root_parent_path_selection_widget(all_paths, on_user_update_path_selection)\n",
    "global_data_root_parent_path_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db844b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07773d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Data                                                                                                            #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "active_data_mode_name = 'kdiba'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('KDIBA')\n",
    "\n",
    "# [*] - indicates bad or session with a problem\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, [8], [9], 10, 11, [12], 13, 14, [15], [16], 17, \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15') # Recomputed 2024-12-02 13:11 \n",
    "curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43') # Recomputed 2024-12-02 16:34 \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31') # Recomputed 2024-12-02 13:34 \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19') # Recomputed 2024-12-02 13:50 --- Sadly completely frigged, tragic\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46') # Recomputed 2024-12-02 13:56 \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30') ## BLOCKING ERROR with pf2D computation (empty) for 5Hz 2024-12-02 15:24 \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50') # Recomputed 2024-12-02 15:43, had one error, unsure what it was \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54') # Recomputed 2024-12-02 15:50 \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3') # Recomputed 2024-12-02 16:00 \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25') # Recomputed 2024-12-02 18:29 \n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54') # Recomputed 2024-12-03 01:45 \n",
    "\n",
    "\n",
    "local_session_parent_path: Path = local_session_root_parent_path.joinpath(curr_context.animal, curr_context.exper_name) # 'gor01', 'one' - probably not needed anymore\n",
    "basedir: Path = local_session_parent_path.joinpath(curr_context.session_name).resolve()\n",
    "print(f'basedir: {str(basedir)}')\n",
    "\n",
    "# Read if possible:\n",
    "saving_mode = PipelineSavingScheme.SKIP_SAVING\n",
    "force_reload = False\n",
    "\n",
    "# \n",
    "# # Force write:\n",
    "# saving_mode = PipelineSavingScheme.TEMP_THEN_OVERWRITE\n",
    "# saving_mode = PipelineSavingScheme.OVERWRITE_IN_PLACE\n",
    "# force_reload = True\n",
    "\n",
    "## TODO: if loading is not possible, we need to change the `saving_mode` so that the new results are properly saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21478b32",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    # 'ratemap_peaks_prominence2d',\n",
    "    'extended_stats',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    # 'rank_order_shuffle_analysis', # skip\n",
    "    # 'directional_train_test_split',\n",
    "    'directional_decoders_decode_continuous',\n",
    "    'directional_decoders_evaluate_epochs',\n",
    "    # 'directional_decoders_epoch_heuristic_scoring',\n",
    "    # 'perform_wcorr_shuffle_analysis', # skip\n",
    "    'trial_by_trial_metrics',\n",
    "    'extended_pf_peak_information',\n",
    "] # do only specified\n",
    "force_recompute_override_computations_includelist = None\n",
    "# force_recompute_override_computations_includelist = ['merged_directional_placefields']\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis'] # , 'directional_decoders_decode_continuous'\n",
    "# force_recompute_override_computations_includelist = ['directional_decoders_epoch_heuristic_scoring'] # \n",
    "# force_recompute_override_computations_includelist = ['long_short_inst_spike_rate_groups','firing_rate_trends','extended_stats','long_short_decoding_analyses','jonathan_firing_rate_analysis','long_short_fr_indicies_analyses','long_short_post_decoding',]\n",
    "# force_recompute_override_computations_includelist = ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous'] # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98170b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 0️⃣ Normal Pipeline Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167df1c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# Load Pipeline                                                                                                        #\n",
    "# ==================================================================================================================== #\n",
    "# with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-full_session_LOO_decoding_analysis.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "# epoch_name_includelist = ['maze']\n",
    "epoch_name_includelist = None\n",
    "active_computation_functions_name_includelist=['lap_direction_determination', 'pf_computation',\n",
    "                                               'pfdt_computation',\n",
    "                                                'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise', \n",
    "                                            #    'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                'position_decoding_two_step', #'directional_decoders_epoch_heuristic_scoring',\n",
    "                                            #    'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                            #     'long_short_inst_spike_rate_groups',\n",
    "                                            #     'long_short_endcap_analysis',\n",
    "                                            'split_to_directional_laps',\n",
    "]\n",
    "\n",
    "curr_active_pipeline: NeuropyPipeline = batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist=epoch_name_includelist,\n",
    "                                        computation_functions_name_includelist=active_computation_functions_name_includelist,\n",
    "                                        saving_mode=saving_mode, force_reload=force_reload,\n",
    "                                        skip_extended_batch_computations=True, debug_print=False, fail_on_exception=False) # , active_pickle_filename = 'loadedSessPickle_withParameters.pkl'\n",
    "\n",
    "## Post Compute Validate 2023-05-16:\n",
    "# was_updated = BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline) ## TODO: need to potentially re-save if was_updated. This will fail because constained versions not ran yet.\n",
    "was_updated = False\n",
    "if was_updated:\n",
    "    print(f'was_updated: {was_updated}')\n",
    "    try:\n",
    "        curr_active_pipeline.save_pipeline(saving_mode=saving_mode)\n",
    "    except Exception as e:\n",
    "        ## TODO: catch/log saving error and indicate that it isn't saved.\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'ERROR RE-SAVING PIPELINE after update. error: {e}')\n",
    "\n",
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Pre-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# valid_computed_results_output_list\n",
    "if not force_reload: # not just force_reload, needs to recompute whenever the computation fails.\n",
    "    try:\n",
    "        # curr_active_pipeline.load_pickled_global_computation_results()\n",
    "        with set_posix_windows():\n",
    "            sucessfully_updated_keys, successfully_loaded_keys = curr_active_pipeline.load_pickled_global_computation_results(allow_overwrite_existing=True, allow_overwrite_existing_allow_keys=extended_computations_include_includelist) # is new\n",
    "            \n",
    "        print(f'sucessfully_updated_keys: {sucessfully_updated_keys}\\nsuccessfully_loaded_keys: {successfully_loaded_keys}')\n",
    "    except FileNotFoundError as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'cannot load global results because pickle file does not exist! Maybe it has never been created? {e}')\n",
    "    except BaseException as e:\n",
    "        exception_info = sys.exc_info()\n",
    "        e = CapturedException(e, exception_info)\n",
    "        print(f'Unhandled exception: cannot load global results: {e}')\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac55a8",
   "metadata": {
    "tags": [
     "run-group-0",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "force_recompute_global = force_reload\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "force_recompute_global = force_reload\n",
    "# force_recompute_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738b166",
   "metadata": {
    "tags": [
     "run-group-0",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "if (len(newly_computed_values) > 0):\n",
    "    print(f'newly_computed_values: {newly_computed_values}.')\n",
    "    if (saving_mode.value != 'skip_saving'):\n",
    "        print(f'Saving global results...')\n",
    "        try:\n",
    "            # curr_active_pipeline.global_computation_results.persist_time = datetime.now()\n",
    "            # Try to write out the global computation function results:\n",
    "            curr_active_pipeline.save_global_computation_results()\n",
    "        except Exception as e:\n",
    "            exception_info = sys.exc_info()\n",
    "            e = CapturedException(e, exception_info)\n",
    "            print(f'\\n\\n!!WARNING!!: saving the global results threw the exception: {e}')\n",
    "            print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "    else:\n",
    "        print(f'\\n\\n!!WARNING!!: changes to global results have been made but they will not be saved since saving_mode.value == \"skip_saving\"')\n",
    "        print(f'\\tthe global results are currently unsaved! proceed with caution and save as soon as you can!\\n\\n\\n')\n",
    "else:\n",
    "    print(f'no changes in global results.')\n",
    "\n",
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=False, force_recompute_override_computations_includelist=[], debug_print=True)\n",
    "print(f'Post-compute validation: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f755b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 0️⃣ Shared Post-Pipeline load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ed6fa",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-0",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_GL'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_rMBP' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Apogee'\n",
    "# BATCH_DATE_TO_USE: str = f'{DAY_DATE_TO_USE}_Lab'\n",
    " \n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        BATCH_DATE_TO_USE = f'{BATCH_DATE_TO_USE}{custom_suffix}'\n",
    "        print(f'Adding custom suffix: \"{custom_suffix}\" - BATCH_DATE_TO_USE: \"{BATCH_DATE_TO_USE}\"')\n",
    "except NameError as err:\n",
    "    custom_suffix = None\n",
    "    print(f'NO CUSTOM SUFFIX.')\n",
    "\n",
    "known_collected_output_paths = [Path(v).resolve() for v in ['/nfs/turbo/umms-kdiba/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/',\n",
    "                                                           '/home/halechr/cloud/turbo/Data/Output/collected_outputs',\n",
    "                                                           r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs',\n",
    "                                                           r\"K:\\scratch\\collected_outputs\",\n",
    "                                                           '/Users/pho/data/collected_outputs',\n",
    "                                                          'output/gen_scripts/']]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_output_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: {collected_outputs_path} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_path: {collected_outputs_path}')\n",
    "# collected_outputs_path.mkdir(exist_ok=True)\n",
    "# assert collected_outputs_path.exists()\n",
    "\n",
    "## Build the output prefix from the session context:\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: \"{CURR_BATCH_OUTPUT_PREFIX}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebedccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop everything except qclu = [1, 2]\n",
    "curr_active_pipeline.global_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e0148",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## Specific Recomputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010d1a9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "any_most_recent_computation_time, each_epoch_latest_computation_time, each_epoch_each_result_computation_completion_times, (global_computations_latest_computation_time, global_computation_completion_times) = curr_active_pipeline.get_computation_times(debug_print=False)\n",
    "# each_epoch_latest_computation_time\n",
    "each_epoch_each_result_computation_completion_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26bc99",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# reload_exported_kdiba_session_position_info_mat_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import reload_exported_kdiba_session_position_info_mat_completion_function\n",
    "\n",
    "# Results can be extracted from batch output by \n",
    "\n",
    "# Extracts the callback results 'determine_session_t_delta_completion_function':\n",
    "# extracted_callback_fn_results = {a_sess_ctxt:a_result.across_session_results.get('determine_session_t_delta_completion_function', {}) for a_sess_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import compute_diba_quiescent_style_replay_events, overwrite_replay_epochs_and_recompute, try_load_neuroscope_EVT_file_epochs, replace_replay_epochs, _get_custom_suffix_for_replay_filename, finalize_output_shuffled_wcorr\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import get_proper_global_spikes_df\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "a_dummy.should_suppress_errors = False\n",
    "\n",
    "## Settings:\n",
    "\n",
    "# SimpleBatchComputationDummy = make_class('SimpleBatchComputationDummy', attrs=['BATCH_DATE_TO_USE', 'collected_outputs_path'])\n",
    "# a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path)\n",
    "\n",
    "_temp_batch_results_extended_dict = {}\n",
    "## Combine the output of `reload_exported_kdiba_session_position_info_mat_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_temp_batch_results_extended_dict = _temp_batch_results_extended_dict | reload_exported_kdiba_session_position_info_mat_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_temp_batch_results_extended_dict,\n",
    "                                                # save_hdf=save_hdf, return_full_decoding_results=return_full_decoding_results,\n",
    "                                                # desired_shared_decoding_time_bin_sizes=desired_shared_decoding_time_bin_sizes,\n",
    "                                                )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4f61",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd11d0f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.filtered_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff30b7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config.instantaneous_time_bin_size_seconds = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58db94e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "force_recompute_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285cd23",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "force_recompute_global = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47820977",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "extended_computations_include_includelist=['lap_direction_determination', 'pf_computation', 'firing_rate_trends', 'pfdt_computation',\n",
    "    # 'pf_dt_sequential_surprise',\n",
    "    #  'ratemap_peaks_prominence2d',\n",
    "    'extended_stats',\n",
    "    'long_short_decoding_analyses',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    # 'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    # 'spike_burst_detection',\n",
    "    'split_to_directional_laps',\n",
    "    'merged_directional_placefields',\n",
    "    # 'rank_order_shuffle_analysis',\n",
    "    # 'directional_decoders_decode_continuous',\n",
    "    # 'directional_decoders_evaluate_epochs',\n",
    "    # 'directional_decoders_epoch_heuristic_scoring',\n",
    "] # do only specified\n",
    "\n",
    "# ['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous']\n",
    "\n",
    "# force_recompute_override_computations_includelist = [\n",
    "#     'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring',\n",
    "#     'split_to_directional_laps', 'lap_direction_determination', 'DirectionalLaps',\n",
    "#     'merged_directional_placefields',\n",
    "#     'directional_decoders_decode_continuous',\n",
    "# ]\n",
    "force_recompute_override_computations_includelist = None\n",
    "\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "newly_computed_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0a702",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# extended_computations_include_includelist=['ratemap_peaks_prominence2d', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring',] # do only specified\n",
    "extended_computations_include_includelist=['rank_order_shuffle_analysis', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'ratemap_peaks_prominence2d', ] # do only specified\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62d145",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d357f9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# mmm ## lots of m's to break computations\n",
    "\n",
    "## Next wave of computations\n",
    "extended_computations_include_includelist=['directional_decoders_epoch_heuristic_scoring',] # do only specified\n",
    "force_recompute_override_computations_includelist = deepcopy(extended_computations_include_includelist)\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n",
    "# Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e20f8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# 'rank_order_shuffle_analysis',\n",
    "## Next wave of computations\n",
    "extended_computations_include_includelist=['rank_order_shuffle_analysis'] # do only specified\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d510c66",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# # Post-hoc verification that the computations worked and that the validators reflect that. The list should be empty now.\n",
    "# newly_computed_values = batch_extended_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=True, progress_print=True,\n",
    "#                                                     force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "\n",
    "\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['rank_order_shuffle_analysis','_add_extended_pf_peak_information',\n",
    " '_build_trial_by_trial_activity_metrics',\n",
    " '_decode_and_evaluate_epochs_using_directional_decoders',\n",
    " '_decode_continuous_using_directional_decoders',\n",
    " '_decoded_epochs_heuristic_scoring',\n",
    " '_split_train_test_laps_data',\n",
    " 'perform_wcorr_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 100, 'skip_laps': False, 'minimum_inclusion_fr_Hz':2.0, 'included_qclu_values':[1,2,4,5,6,7]}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "\n",
    "force_recompute_override_computation_kwargs_dict = {'rank_order_shuffle_analysis': {'num_shuffles': 100, 'skip_laps': False, 'minimum_inclusion_fr_Hz':2.0, 'included_qclu_values':[1,2,4,5,6,7]},\n",
    " \n",
    "}\n",
    "\n",
    "force_recompute_override_computations_includelist = list(force_recompute_override_computation_kwargs_dict.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bb808",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['rank_order_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 5, 'skip_laps': False, 'minimum_inclusion_fr_Hz':2.0, 'included_qclu_values':[1,2,4,5,6,7]}], \n",
    "                                                  enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_decode_continuous'], computation_kwargs_list=[{'time_bin_size': 0.058}], #computation_kwargs_list=[{'time_bin_size': 0.025}], \n",
    "                                                  enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.global_computation_results.accumulated_errors\n",
    "curr_active_pipeline.global_computation_results.computation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=extended_computations_include_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "print(f'Post-load global computations: needs_computation_output_dict: {[k for k,v in needs_computation_output_dict.items() if (v is not None)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac9f88",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# force_recompute_override_computations_includelist = ['_decode_continuous_using_directional_decoders']\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], force_recompute_override_computations_includelist=force_recompute_override_computations_includelist,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(extended_computations_include_includelist=['_decode_continuous_using_directional_decoders'], computation_kwargs_list=[{'time_bin_size': 0.02}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.20}, {'time_bin_size': 0.20}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.025}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "# 2024-04-20 - HACK -- FIXME: Invert the 'is_LR_dir' column since it is clearly reversed. No clue why.\n",
    "# fails due to some types thing?\n",
    "# \terr: Length of values (82) does not match length of index (80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3dca2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748deeb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# minimum ~10ms\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields'], computation_kwargs_list=[{'ripple_decoding_time_bin_size': 0.002, 'laps_decoding_time_bin_size': 0.002}], enabled_filter_names=None, fail_on_exception=True, debug_print=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ffa88",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# minimum ~10ms\n",
    "\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_evaluate_epochs'], computation_kwargs_list=[{'should_skip_radon_transform': True}], enabled_filter_names=None, fail_on_exception=True, debug_print=True)\n",
    "# ## produces: 'DirectionalDecodersEpochsEvaluations'\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_epoch_heuristic_scoring'], enabled_filter_names=None, fail_on_exception=True, debug_print=False) # OK FOR PICKLE\n",
    "\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'],\n",
    "                                                   computation_kwargs_list=[{'ripple_decoding_time_bin_size': 0.025, 'laps_decoding_time_bin_size': 0.025}, {'time_bin_size': 0.025}, {'should_skip_radon_transform': False}, {}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_global_computation_function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479935f",
   "metadata": {
    "tags": [
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_decoders_epoch_heuristic_scoring'],\n",
    "                                                   computation_kwargs_list=[{'same_thresh_fraction_of_track': 0.05, 'max_ignore_bins': 2, 'use_bin_units_instead_of_realworld': False, 'max_jump_distance_cm': 60.0}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92db81b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# MemoryError: Unable to allocate 9.74 GiB for an array with shape (57, 22940809) and data type float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372737e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39417243",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['lap_direction_determination'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89757a66",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['_perform_long_short_firing_rate_analyses'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2745bd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['EloyAnalysis'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130eb2e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_train_test_split'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d567e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['trial_by_trial_metrics'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da92d47",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['perform_wcorr_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 350}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a8eb8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs', 'directional_decoders_epoch_heuristic_scoring'], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.025}, {'time_bin_size': 0.025}, {'should_skip_radon_transform': True}, {}], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dec23",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['merged_directional_placefields', 'directional_decoders_decode_continuous', 'directional_decoders_evaluate_epochs',], computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.002}, {'time_bin_size': 0.002}, {'should_skip_radon_transform': True},], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012effc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "['split_to_directional_laps', 'merged_directional_placefields', 'rank_order_shuffle_analysis', 'directional_decoders_decode_continuous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62493eb3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=[\n",
    "    'merged_directional_placefields', \n",
    "    'long_short_decoding_analyses', #'pipeline_complete_compute_long_short_fr_indicies',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    'long_short_rate_remapping',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    ], enabled_filter_names=None, fail_on_exception=False, debug_print=False) # , computation_kwargs_list=[{'should_skip_radon_transform': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca34a0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=[\n",
    "    # 'long_short_decoding_analyses', #'pipeline_complete_compute_long_short_fr_indicies',\n",
    "    'jonathan_firing_rate_analysis',\n",
    "    # 'long_short_fr_indicies_analyses',\n",
    "    'short_long_pf_overlap_analyses',\n",
    "    'long_short_post_decoding',\n",
    "    'long_short_inst_spike_rate_groups',\n",
    "    'long_short_endcap_analysis',\n",
    "    ], enabled_filter_names=None, fail_on_exception=False, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99749819",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "if 'TrainTestSplit' in curr_active_pipeline.global_computation_results.computed_data:\n",
    "    directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data.get('TrainTestSplit', None)\n",
    "    training_data_portion: float = directional_train_test_split_result.training_data_portion\n",
    "    test_data_portion: float = directional_train_test_split_result.test_data_portion\n",
    "    test_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.test_epochs_dict\n",
    "    train_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.train_epochs_dict\n",
    "    train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_train_test_split_result.train_lap_specific_pf1D_Decoder_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cbcb7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "'trial_by_trial_metrics'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed38171",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData\n",
    "\n",
    "# directional_decoders_epochs_decode_result\n",
    "# save_path = Path(\"/Users/pho/data/KDIBA/gor01/one/2006-6-09_1-22-43/output/2024-04-25_CustomDecodingResults.pkl\").resolve()\n",
    "# save_path = curr_active_pipeline.get_output_path().joinpath(\"2024-04-28_CustomDecodingResults.pkl\").resolve()\n",
    "save_path = curr_active_pipeline.get_output_path().joinpath(f\"{DAY_DATE_TO_USE}_CustomDecodingResults.pkl\").resolve()\n",
    "\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "xbin_centers = deepcopy(long_pf2D.xbin_centers)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "ybin_centers = deepcopy(long_pf2D.ybin_centers)\n",
    "\n",
    "print(xbin_centers)\n",
    "save_dict = {\n",
    "'directional_decoders_epochs_decode_result': directional_decoders_epochs_decode_result.__getstate__(),\n",
    "'xbin': xbin, 'xbin_centers': xbin_centers}\n",
    "\n",
    "saveData(save_path, save_dict)\n",
    "print(f'save_path: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b4531",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# 💾 Export CSVs: \n",
    "## INPUTS: directional_decoders_epochs_decode_result,\n",
    "\n",
    "extracted_merged_scores_df = directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df()\n",
    "# extracted_merged_scores_df\n",
    "\n",
    "print(f'\\tAll scores df CSV exporting...')\n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "export_df_dict = {'ripple_all_scores_merged_df': extracted_merged_scores_df}\n",
    "_csv_export_paths = directional_decoders_epochs_decode_result.perform_export_dfs_dict_to_csvs(extracted_dfs_dict=export_df_dict, parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                            #   user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                            #   valid_epochs_selections={'ripple': filtered_valid_epoch_times},\n",
    "                                                                            )\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported ripple_all_scores_merged_df to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _csv_export_paths.items()])\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac96c6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "t_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98fc6f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# extracted_merged_scores_df.to_csv('test_(ripple_all_scores_merged_df).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a860a1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "decoder_ripple_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_extras_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbf34d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "decoder_ripple_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_extras_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcbb0c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# filtered_laps_simple_pf_pearson_merged_df\n",
    "# filtered_ripple_simple_pf_pearson_merged_df\n",
    "# decoder_ripple_weighted_corr_df_dict\n",
    "ripple_weighted_corr_merged_df['ripple_start_t']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a95450",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "wcorr_column_names = ['wcorr_long_LR', 'wcorr_long_RL', 'wcorr_short_LR', 'wcorr_short_RL']\n",
    "filtered_ripple_simple_pf_pearson_merged_df.label = filtered_ripple_simple_pf_pearson_merged_df.label.astype('int64')\n",
    "ripple_weighted_corr_merged_df['label'] = ripple_weighted_corr_merged_df['ripple_idx'].astype('int64')\n",
    "\n",
    "filtered_ripple_simple_pf_pearson_merged_df.join(ripple_weighted_corr_merged_df[wcorr_column_names], on='start') # , on='label'\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bc77f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433aac",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print(list(ripple_weighted_corr_merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2381",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_decoded_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3599f8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window.save_selections()\n",
    "\n",
    "a_decoded_filter_epochs_decoder_result_dict.epochs.find_data_indicies_from_epoch_times([380.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77babf98",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 💾 Continue Saving/Exporting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89da89",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_global_computation_results() # newly_computed_values: [('pfdt_computation', 'maze_any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7af96d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "split_save_folder, split_save_paths, split_save_output_types, failed_keys = curr_active_pipeline.save_split_global_computation_results(debug_print=True,\n",
    "                                                                                                                                    #    include_includelist=['long_short_inst_spike_rate_groups'],\n",
    "                                                                                                                                       ) # encountered issue with pickling `long_short_post_decoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a869b1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.export_pipeline_to_h5() # NotImplementedError: a_field_attr: Attribute(name='LxC_aclus', default=None, validator=None, repr=True, eq=True, eq_key=None, order=True, order_key=None, hash=None, init=False, metadata=mappingproxy({'tags': ['dataset'], 'serialization': {'hdf': True}, 'custom_serialization_fn': None, 'hdf_metadata': {'track_eXclusive_cells': 'LxC'}}), type=<class 'numpy.ndarray'>, converter=None, kw_only=False, inherited=False, on_setattr=None, alias='LxC_aclus') could not be serialized and _ALLOW_GLOBAL_NESTED_EXPANSION is not allowed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06d1f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.clear_display_outputs()\n",
    "curr_active_pipeline.clear_registered_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f39f2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.TEMP_THEN_OVERWRITE) ## #TODO 2024-02-16 14:25: - [ ] PicklingError: Can't pickle <function make_set_closure_cell.<locals>.set_closure_cell at 0x7fd35e66b700>: it's not found as attr._compat.make_set_closure_cell.<locals>.set_closure_cell\n",
    "# curr_active_pipeline.save_pipeline(saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE)\n",
    "# TypeError: cannot pickle 'traceback' object\n",
    "# Exception: Can't pickle <enum 'PipelineSavingScheme'>: it's not the same object as pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline.PipelineSavingScheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10989b4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "#### Get computation times/info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cab8e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "any_most_recent_computation_time, each_epoch_latest_computation_time, each_epoch_each_result_computation_completion_times, (global_computations_latest_computation_time, global_computation_completion_times) = curr_active_pipeline.get_computation_times(debug_print=False)\n",
    "# each_epoch_latest_computation_time\n",
    "# each_epoch_each_result_computation_completion_times\n",
    "# global_computation_completion_times\n",
    "\n",
    "# curr_active_pipeline.get_merged_computation_function_validators()\n",
    "# Get the names of the global and non-global computations:\n",
    "all_validators_dict = curr_active_pipeline.get_merged_computation_function_validators()\n",
    "global_only_validators_dict = {k:v for k, v in all_validators_dict.items() if v.is_global}\n",
    "non_global_only_validators_dict = {k:v for k, v in all_validators_dict.items() if (not v.is_global)}\n",
    "non_global_comp_names: List[str] = [v.short_name for k, v in non_global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))] # ['firing_rate_trends', 'spike_burst_detection', 'pf_dt_sequential_surprise', 'extended_stats', 'placefield_overlap', 'ratemap_peaks_prominence2d', 'velocity_vs_pf_simplified_count_density', 'EloyAnalysis', '_perform_specific_epochs_decoding', 'recursive_latent_pf_decoding', 'position_decoding_two_step', 'position_decoding', 'lap_direction_determination', 'pfdt_computation', 'pf_computation']\n",
    "global_comp_names: List[str] = [v.short_name for k, v in global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))] # ['long_short_endcap_analysis', 'long_short_inst_spike_rate_groups', 'long_short_post_decoding', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_decoding_analyses', 'PBE_stats', 'rank_order_shuffle_analysis', 'directional_decoders_epoch_heuristic_scoring', 'directional_decoders_evaluate_epochs', 'directional_decoders_decode_continuous', 'merged_directional_placefields', 'split_to_directional_laps']\n",
    "\n",
    "# mappings between the long computation function names and their short names:\n",
    "non_global_comp_names_map: Dict[str, str] = {v.computation_fn_name:v.short_name for k, v in non_global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))}\n",
    "global_comp_names_map: Dict[str, str] = {v.computation_fn_name:v.short_name for k, v in global_only_validators_dict.items() if (not v.short_name.startswith('_DEP'))} # '_perform_long_short_endcap_analysis': 'long_short_endcap_analysis', '_perform_long_short_instantaneous_spike_rate_groups_analysis': 'long_short_inst_spike_rate_groups', ...}\n",
    "\n",
    "# convert long function names to short-names:\n",
    "each_epoch_each_result_computation_completion_times = {an_epoch:{non_global_comp_names_map.get(k, k):v for k,v in a_results_dict.items()} for an_epoch, a_results_dict in each_epoch_each_result_computation_completion_times.items()}\n",
    "global_computation_completion_times = {global_comp_names_map.get(k, k):v for k,v in global_computation_completion_times.items()}\n",
    "\n",
    "each_epoch_each_result_computation_completion_times\n",
    "global_computation_completion_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967369f1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_evaluate_required_computations\n",
    "\n",
    "# force_recompute_global = force_reload\n",
    "force_recompute_global = True\n",
    "active_probe_includelist = extended_computations_include_includelist\n",
    "# active_probe_includelist = ['lap_direction_determination']\n",
    "needs_computation_output_dict, valid_computed_results_output_list, remaining_include_function_names = batch_evaluate_required_computations(curr_active_pipeline, include_includelist=active_probe_includelist, include_global_functions=True, fail_on_exception=False, progress_print=True,\n",
    "                                                    force_recompute=force_recompute_global, force_recompute_override_computations_includelist=force_recompute_override_computations_includelist, debug_print=False)\n",
    "needs_computation_output_dict\n",
    "# valid_computed_results_output_list\n",
    "# remaining_include_function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aeeda",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "['merged_directional_placefields', ]\n",
    "\n",
    "['long_short_decoding_analyses', 'long_short_fr_indicies_analyses', 'jonathan_firing_rate_analysis', 'extended_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85822a88",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "replay_estimation_parameters = curr_active_pipeline.sess.config.preprocessing_parameters.epoch_estimation_parameters.replays\n",
    "assert replay_estimation_parameters is not None\n",
    "replay_estimation_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799ab99",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "recompute_earlier_than_date = datetime(2024, 4, 1, 0, 0, 0)\n",
    "recompute_earlier_than_date\n",
    "\n",
    "each_epoch_needing_recompute = [an_epoch for an_epoch, last_computed_datetime in each_epoch_latest_computation_time.items() if (last_computed_datetime < recompute_earlier_than_date)]\n",
    "each_epoch_needing_recompute\n",
    "each_epoch_each_result_needing_recompute = {an_epoch:{a_computation_name:last_computed_datetime for a_computation_name, last_computed_datetime in last_computed_datetimes_dict.items() if (last_computed_datetime < recompute_earlier_than_date)} for an_epoch, last_computed_datetimes_dict in each_epoch_each_result_computation_completion_times.items()}\n",
    "each_epoch_each_result_needing_recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ffa0a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_times\n",
    "curr_active_pipeline.global_computation_results\n",
    "# curr_active_pipeline.try_load_split_pickled_global_computation_results\n",
    "\n",
    "global_computation_times = deepcopy(curr_active_pipeline.global_computation_results.computation_times.to_dict()) # DynamicParameters({'perform_rank_order_shuffle_analysis': datetime.datetime(2024, 4, 3, 5, 41, 31, 287680), '_decode_continuous_using_directional_decoders': datetime.datetime(2024, 4, 3, 5, 12, 7, 337326), '_perform_long_short_decoding_analyses': datetime.datetime(2024, 4, 3, 5, 43, 10, 361685), '_perform_long_short_pf_overlap_analyses': datetime.datetime(2024, 4, 3, 5, 43, 10, 489296), '_perform_long_short_firing_rate_analyses': datetime.datetime(2024, 4, 3, 5, 45, 3, 73472), '_perform_jonathan_replay_firing_rate_analyses': datetime.datetime(2024, 4, 3, 5, 45, 5, 168790), '_perform_long_short_post_decoding_analysis': datetime.datetime(2024, 2, 16, 18, 13, 4, 734621), '_perform_long_short_endcap_analysis': datetime.datetime(2024, 4, 3, 5, 45, 24, 274261), '_decode_and_evaluate_epochs_using_directional_decoders': datetime.datetime(2024, 4, 3, 5, 14, 37, 935482), '_perform_long_short_instantaneous_spike_rate_groups_analysis': datetime.datetime(2024, 4, 3, 5, 45, 24, 131955), '_split_to_directional_laps': datetime.datetime(2024, 4, 3, 5, 11, 22, 627789), '_build_merged_directional_placefields': datetime.datetime(2024, 4, 3, 5, 11, 28, 376078)})\n",
    "global_computation_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db067",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 0️⃣ Pho Interactive Pipeline Jupyter Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275e3bb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import interactive_pipeline_widget, interactive_pipeline_files\n",
    "\n",
    "_pipeline_jupyter_widget = interactive_pipeline_widget(curr_active_pipeline=curr_active_pipeline)\n",
    "# display(_pipeline_jupyter_widget)\n",
    "_pipeline_jupyter_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54599",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 1️⃣ End Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a533ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:40.700275900Z",
     "start_time": "2023-11-16T23:21:40.584273Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# (long_one_step_decoder_1D, short_one_step_decoder_1D), (long_one_step_decoder_2D, short_one_step_decoder_2D) = compute_short_long_constrained_decoders(curr_active_pipeline, recalculate_anyway=True)\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_epoch_context, short_epoch_context, global_epoch_context = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_epoch_name, short_epoch_name, global_epoch_name)]\n",
    "long_epoch_obj, short_epoch_obj = [Epoch(curr_active_pipeline.sess.epochs.to_dataframe().epochs.label_slice(an_epoch_name.removesuffix('_any'))) for an_epoch_name in [long_epoch_name, short_epoch_name]] #TODO 2023-11-10 20:41: - [ ] Issue with getting actual Epochs from sess.epochs for directional laps: emerges because long_epoch_name: 'maze1_any' and the actual epoch label in curr_active_pipeline.sess.epochs is 'maze1' without the '_any' part.\n",
    "long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_computation_config, short_computation_config, global_computation_config = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "long_pf1D, short_pf1D, global_pf1D = long_results.pf1D, short_results.pf1D, global_results.pf1D\n",
    "long_pf2D, short_pf2D, global_pf2D = long_results.pf2D, short_results.pf2D, global_results.pf2D\n",
    "\n",
    "assert short_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "assert long_epoch_obj.n_epochs > 0, f'long_epoch_obj: {long_epoch_obj}, short_epoch_obj: {short_epoch_obj}'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "t_start, t_delta, t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e348e0c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# directional_merged_decoders_result = deepcopy(directional_decoders_epochs_decode_result)\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "\n",
    "spikes_df = deepcopy(curr_active_pipeline.sess.spikes_df)\n",
    "\n",
    "global_computation_results = curr_active_pipeline.global_computation_results\n",
    "\n",
    "rank_order_results = curr_active_pipeline.global_computation_results.computed_data.get('RankOrder', None) # : \"RankOrderComputationsContainer\"\n",
    "if rank_order_results is not None:\n",
    "    minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "else:        \n",
    "    ## get from parameters:\n",
    "    minimum_inclusion_fr_Hz: float = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.included_qclu_values\n",
    "\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "# print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "# print(f'included_qclu_values: {included_qclu_values}')\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "# pos_bin_size = _recover_position_bin_size(track_templates.get_decoders()[0]) # 3.793023081021702\n",
    "# print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "# pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "\n",
    "## Simple Pearson Correlation\n",
    "assert spikes_df is not None\n",
    "(laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df), corr_column_names = directional_merged_decoders_result.compute_simple_spike_time_v_pf_peak_x_by_epoch(track_templates=track_templates, spikes_df=deepcopy(spikes_df))\n",
    "## OUTPUTS: (laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df), corr_column_names\n",
    "## Computes the highest-valued decoder for this score:\n",
    "best_decoder_index_col_name: str = 'best_decoder_index'\n",
    "laps_simple_pf_pearson_merged_df[best_decoder_index_col_name] = laps_simple_pf_pearson_merged_df[corr_column_names].abs().apply(lambda row: np.argmax(row.values), axis=1)\n",
    "ripple_simple_pf_pearson_merged_df[best_decoder_index_col_name] = ripple_simple_pf_pearson_merged_df[corr_column_names].abs().apply(lambda row: np.argmax(row.values), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed3e0c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "print(f'pos_bin_size: {pos_bin_size}')\n",
    "print(f'ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}')\n",
    "\n",
    "# Radon Transforms:\n",
    "decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "# Weighted correlations:\n",
    "laps_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "ripple_weighted_corr_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "decoder_laps_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "decoder_ripple_weighted_corr_df_dict: Dict[str, pd.DataFrame] = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "# Pearson's correlations:\n",
    "laps_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "ripple_simple_pf_pearson_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "# laps_simple_pf_pearson_merged_df\n",
    "# ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## Drop rows where all are missing\n",
    "corr_column_names = ['long_LR_pf_peak_x_pearsonr', 'long_RL_pf_peak_x_pearsonr', 'short_LR_pf_peak_x_pearsonr', 'short_RL_pf_peak_x_pearsonr']\n",
    "# ripple_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='all') # 350/412 rows\n",
    "filtered_laps_simple_pf_pearson_merged_df: pd.DataFrame = laps_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='any') # 320/412 rows\n",
    "filtered_ripple_simple_pf_pearson_merged_df: pd.DataFrame = ripple_simple_pf_pearson_merged_df.dropna(subset=corr_column_names, axis='index', how='any') # 320/412 rows\n",
    "\n",
    "## Update the `decoder_ripple_filter_epochs_decoder_result_dict` with the included epochs:\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epochs(filtered_ripple_simple_pf_pearson_merged_df.index) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epochs(filtered_laps_simple_pf_pearson_merged_df.index) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_ripple_simple_pf_pearson_merged_df[['start', 'stop']].to_numpy()) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_laps_simple_pf_pearson_merged_df[['start', 'stop']].to_numpy()) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_ripple_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_ripple_simple_pf_pearson_merged_df['start'].to_numpy()) for a_name, a_df in decoder_ripple_filter_epochs_decoder_result_dict.items()}\n",
    "# decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:decoder_laps_filter_epochs_decoder_result_dict[a_name].filtered_by_epoch_times(filtered_laps_simple_pf_pearson_merged_df['start'].to_numpy()) for a_name, a_df in decoder_laps_filter_epochs_decoder_result_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a593338",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: collected_outputs_path, directional_decoders_epochs_decode_result\n",
    "\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "## add the additional contexts:\n",
    "# active_context = active_context.adding_context_if_missing(custom_replay_name='TESTNEW', time_bin_size=directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size)\n",
    "# additional_session_context = None\n",
    "# try:\n",
    "# \tif custom_suffix is not None:\n",
    "# \t\tadditional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "# \t\tprint(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "# except NameError as err:\n",
    "# \tadditional_session_context = None\n",
    "# \tprint(f'NO CUSTOM SUFFIX.')    \n",
    "    \n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_output_csv_paths = directional_decoders_epochs_decode_result.export_csvs(parent_output_path=collected_outputs_path, active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                            # user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                            # valid_epochs_selections={'ripple': filtered_valid_epoch_times},\n",
    "                                                                            )\n",
    "\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported directional_decoders_epochs_decode_result to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _output_csv_paths.items()])\n",
    "# print(f'\\t\\t\\tCSV Paths: {_output_csv_paths}\\n')\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35196",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# I have several python variables I want to print: t_start, t_delta, t_end\n",
    "# I want to generate a print statement that explicitly lists the variable name prior to its value like `print(f't_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')`\n",
    "# Currently I have to t_start, t_delta, t_end\n",
    "curr_active_pipeline.get_session_context()\n",
    "\n",
    "print(f'{curr_active_pipeline.session_name}:\\tt_start: {t_start}, t_delta: {t_delta}, t_end: {t_end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071e94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T23:21:43.601382Z",
     "start_time": "2023-11-16T23:21:40.702275600Z"
    },
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## long_short_decoding_analyses:\n",
    "from attrs import astuple\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import LeaveOneOutDecodingAnalysis\n",
    "\n",
    "curr_long_short_decoding_analyses: LeaveOneOutDecodingAnalysis = curr_active_pipeline.global_computation_results.computed_data['long_short_leave_one_out_decoding_analysis']\n",
    "long_one_step_decoder_1D, short_one_step_decoder_1D, long_replays, short_replays, global_replays, long_shared_aclus_only_decoder, short_shared_aclus_only_decoder, shared_aclus, long_short_pf_neurons_diff, n_neurons, long_results_obj, short_results_obj, is_global = curr_long_short_decoding_analyses.long_decoder, curr_long_short_decoding_analyses.short_decoder, curr_long_short_decoding_analyses.long_replays, curr_long_short_decoding_analyses.short_replays, curr_long_short_decoding_analyses.global_replays, curr_long_short_decoding_analyses.long_shared_aclus_only_decoder, curr_long_short_decoding_analyses.short_shared_aclus_only_decoder, curr_long_short_decoding_analyses.shared_aclus, curr_long_short_decoding_analyses.long_short_pf_neurons_diff, curr_long_short_decoding_analyses.n_neurons, curr_long_short_decoding_analyses.long_results_obj, curr_long_short_decoding_analyses.short_results_obj, curr_long_short_decoding_analyses.is_global \n",
    "decoding_time_bin_size = long_one_step_decoder_1D.time_bin_size # 1.0/30.0 # 0.03333333333333333\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_laps, long_replays, short_laps, short_replays, global_laps, global_replays = [long_short_fr_indicies_analysis_results[k] for k in ['long_laps', 'long_replays', 'short_laps', 'short_replays', 'global_laps', 'global_replays']]\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "\n",
    "## Get global 'long_short_post_decoding' results:\n",
    "curr_long_short_post_decoding = curr_active_pipeline.global_computation_results.computed_data['long_short_post_decoding']\n",
    "expected_v_observed_result, curr_long_short_rr = curr_long_short_post_decoding.expected_v_observed_result, curr_long_short_post_decoding.rate_remapping\n",
    "rate_remapping_df, high_remapping_cells_only = curr_long_short_rr.rr_df, curr_long_short_rr.high_only_rr_df\n",
    "Flat_epoch_time_bins_mean, Flat_decoder_time_bin_centers, num_neurons, num_timebins_in_epoch, num_total_flat_timebins, is_short_track_epoch, is_long_track_epoch, short_short_diff, long_long_diff = expected_v_observed_result.Flat_epoch_time_bins_mean, expected_v_observed_result.Flat_decoder_time_bin_centers, expected_v_observed_result.num_neurons, expected_v_observed_result.num_timebins_in_epoch, expected_v_observed_result.num_total_flat_timebins, expected_v_observed_result.is_short_track_epoch, expected_v_observed_result.is_long_track_epoch, expected_v_observed_result.short_short_diff, expected_v_observed_result.long_long_diff\n",
    "\n",
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "(epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "## Update long_exclusive/short_exclusive properties with `long_short_fr_indicies_df`\n",
    "# long_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n",
    "# short_exclusive.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49f5d4f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Most popular\n",
    "# long_LR_name, short_LR_name, long_RL_name, short_RL_name, global_any_name\n",
    "\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104fc37",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult, DirectionalLapsResult, DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']   \n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('RankOrder', None)\n",
    "if rank_order_results is not None:\n",
    "    minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "else:        \n",
    "    ## get from parameters:\n",
    "    minimum_inclusion_fr_Hz: float = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.included_qclu_values\n",
    "\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_merged_decoders_result.laps_time_bin_marginals_df\n",
    "directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00672033",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_laps_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result ## here is a single result, but not a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f90757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_build_individual_time_bin_decoded_posteriors_df\n",
    "\n",
    "## From `directional_merged_decoders_result`\n",
    "# transfer_column_names_list: List[str] = ['maze_id', 'lap_dir', 'lap_id']\n",
    "transfer_column_names_list: List[str] = []\n",
    "filtered_laps_time_bin_marginals_df = _perform_build_individual_time_bin_decoded_posteriors_df(curr_active_pipeline, track_templates=track_templates, all_directional_laps_filter_epochs_decoder_result=directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result, transfer_column_names_list=transfer_column_names_list)\n",
    "filtered_laps_time_bin_marginals_df['lap_id'] = filtered_laps_time_bin_marginals_df['parent_epoch_label'].astype(int) + 1\n",
    "filtered_laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "\n",
    "# directional_merged_decoders_result.all_directional_decoder_dict\n",
    "directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow, DecodedEpochSlicesPaginatedFigureController, EpochSelectionsObject, ClickActionCallbacks\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ThinButtonBar.ThinButtonBarWidget import ThinButtonBarWidget\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.PaginationCtrl.PaginationControlWidget import PaginationControlWidget, PaginationControlWidgetState\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.Resources import GuiResources, ActionIcons, silx_resources_rc\n",
    "## INPUTS filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "\n",
    "app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=filtered_ripple_simple_pf_pearson_merged_df, epochs_name='ripple',\n",
    "                                                                                                decoder_decoded_epochs_result_dict=long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple', title='Long-like post-Delta Ripples Only', ## RIPPLE\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', ## LAPS\n",
    "                                                                                                included_epoch_indicies=None, debug_print=False,\n",
    "                                                                                                params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, \n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 10,\n",
    "                                                                                                    'scrollable_figure': False,\n",
    "                                                                                                    # 'scrollable_figure': True,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': False,\n",
    "                                                                                                    'should_suppress_callback_exceptions': False,\n",
    "                                                                                                    # 'build_fn': 'insets_view',\n",
    "                                                                                                })\n",
    "\n",
    "### attached raster viewer widget:\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import DisplayColorsEnum\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import get_proper_global_spikes_df\n",
    "from pyphoplacecellanalysis.Pho2D.data_exporting import PosteriorExporting\n",
    "\n",
    "## INPUTS: active_spikes_df\n",
    "# active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "\n",
    "# PosteriorExporting._perform_export_current_epoch_marginal_and_raster_images\n",
    "\n",
    "# _out_ripple_rasters, update_attached_raster_viewer_epoch_callback = build_attached_raster_viewer_widget(paginated_multi_decoder_decoded_epochs_window=paginated_multi_decoder_decoded_epochs_window, track_templates=track_templates, active_spikes_df=active_spikes_df, filtered_ripple_simple_pf_pearson_merged_df=filtered_epochs_df) ## BEST\n",
    "# _out_ripple_rasters, update_attached_raster_viewer_epoch_callback = build_attached_raster_viewer_widget(paginated_multi_decoder_decoded_epochs_window=paginated_multi_decoder_decoded_epochs_window, track_templates=track_templates, active_spikes_df=active_spikes_df, filtered_ripple_simple_pf_pearson_merged_df=filtered_ripple_simple_pf_pearson_merged_df) # original\n",
    "# _out_ripple_rasters, update_attached_raster_viewer_epoch_callback = build_attached_raster_viewer_widget(paginated_multi_decoder_decoded_epochs_window=paginated_multi_decoder_decoded_epochs_window, track_templates=track_templates, active_spikes_df=active_spikes_df, filtered_ripple_simple_pf_pearson_merged_df=extracted_merged_scores_df)\n",
    "_out_ripple_rasters, update_attached_raster_viewer_epoch_callback = paginated_multi_decoder_decoded_epochs_window.build_attached_raster_viewer_widget(track_templates=track_templates, active_spikes_df=active_spikes_df, filtered_epochs_df=long_like_during_post_delta_only_filter_epochs_df) # Long-like-during-post-delta\n",
    "\n",
    "\n",
    "# all_directional_laps_filter_epochs_decoder_result_value\n",
    "# laps_filter_epochs = ensure_dataframe(deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)) \n",
    "# _out_ripple_rasters, update_attached_raster_viewer_epoch_callback = build_attached_raster_viewer_widget(paginated_multi_decoder_decoded_epochs_window=paginated_multi_decoder_decoded_epochs_window, track_templates=track_templates, active_spikes_df=laps_spikes_df, filtered_ripple_simple_pf_pearson_merged_df=filtered_laps_simple_pf_pearson_merged_df) ## LAPS\n",
    "\n",
    "# _out_ripple_rasters: RankOrderRastersDebugger\n",
    "### Add yellow-blue marginals to `paginated_multi_decoder_decoded_epochs_window`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "from pyphocorehelpers.gui.Qt.widget_positioning_helpers import WidgetPositioningHelpers, DesiredWidgetLocation, WidgetGeometryInfo\n",
    "\n",
    "yellow_blue_trackID_marginals_plot_tuple = paginated_multi_decoder_decoded_epochs_window.build_attached_yellow_blue_track_identity_marginal_window(directional_merged_decoders_result, global_session, ripple_decoding_time_bin_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and max number of active aclus per time bin for each epoch (lap)\n",
    "filtered_laps_time_bin_marginals_df.groupby(['lap_id']).agg(n_unique_aclus_mean=('n_unique_aclus', 'mean'), n_unique_aclus_max=('n_unique_aclus', 'max')).reset_index()\n",
    "filtered_laps_time_bin_marginals_df.groupby(['maze_id']).agg(n_unique_aclus_mean=('n_unique_aclus', 'mean'), n_unique_aclus_max=('n_unique_aclus', 'max')).reset_index() ## per maze\n",
    "filtered_laps_time_bin_marginals_df.groupby(['maze_id', 'lap_dir']).agg(n_unique_aclus_mean=('n_unique_aclus', 'mean'), n_unique_aclus_max=('n_unique_aclus', 'max')).reset_index() # per maze x lap_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93346114",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.BatchCompletionHandler import BatchSessionCompletionHandler\n",
    "\n",
    "BatchSessionCompletionHandler.post_compute_validate(curr_active_pipeline=curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617e7a3",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "list(directional_laps_results.directional_lap_specific_configs.keys()) # ['maze1_odd', 'maze1_even', 'maze2_odd', 'maze2_even']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912656a7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers\n",
    "\n",
    "if ('DirectionalDecodersEpochsEvaluations' in curr_active_pipeline.global_computation_results.computed_data) and (curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations'] is not None):\n",
    "    directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "    directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "    ## UNPACK HERE via direct property access:\n",
    "    pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "    ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "    laps_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "    print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }, {laps_decoding_time_bin_size = }') # pos_bin_size = 3.8054171165052444, ripple_decoding_time_bin_size = 0.025, laps_decoding_time_bin_size = 0.2\n",
    "    decoder_laps_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "    decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "    decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "    decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "\n",
    "    # New items:\n",
    "    decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "    decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "    # Weighted correlations:\n",
    "    laps_weighted_corr_merged_df = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "    ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "    decoder_laps_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "    decoder_ripple_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "    # Pearson's correlations:\n",
    "    laps_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "    ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "    \n",
    "    # for k, v in directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict.items():\n",
    "    #     print(f'{k}: v.decoding_time_bin_size: {v.decoding_time_bin_size}')\n",
    "    \n",
    "    individual_result_ripple_time_bin_sizes = [v.decoding_time_bin_size for k, v in directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict.items()]\n",
    "    if not np.allclose(ripple_decoding_time_bin_size, individual_result_ripple_time_bin_sizes):\n",
    "        individual_result_ripple_time_bin_size = individual_result_ripple_time_bin_sizes[0] # get the first\n",
    "        assert np.allclose(individual_result_ripple_time_bin_size, individual_result_ripple_time_bin_sizes), f\"`individual_result_ripple_time_bin_size ({individual_result_ripple_time_bin_size}) does not equal the individual result time bin sizes: {individual_result_ripple_time_bin_sizes}`. This can occur when there are epochs smaller than the desired size ({ripple_decoding_time_bin_size}) for the result and epochs_filtering_mode=EpochFilteringMode.ConstrainDecodingTimeBinSizeToMinimum\"\n",
    "        print(f'WARN: overriding directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size (original value: {directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size}) with individual_result_ripple_time_bin_size: {individual_result_ripple_time_bin_size}')\n",
    "        directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size = individual_result_ripple_time_bin_size # override the time_bin_size with the actually used one\n",
    "        ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "        print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }, {laps_decoding_time_bin_size = }') # pos_bin_size = 3.8054171165052444, ripple_decoding_time_bin_size = 0.025, laps_decoding_time_bin_size = 0.2\n",
    "    else:\n",
    "        # all are close, it's good\n",
    "        pass\n",
    "\n",
    "    # assert np.allclose(ripple_decoding_time_bin_size, individual_result_ripple_time_bin_sizes), f\"`directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size ({ripple_decoding_time_bin_size}) does not equal the individual result time bin sizes: {individual_result_ripple_time_bin_sizes}`. This can occur when there are epochs smaller than the desired size ({ripple_decoding_time_bin_size}) for the result and epochs_filtering_mode=EpochFilteringMode.ConstrainDecodingTimeBinSizeToMinimum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1c291",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dce38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_decoders_epochs_decode_result # DecoderDecodedEpochsResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "881402df",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# active_config_name: str = 'maze_any'\n",
    "active_config_name: str = global_epoch_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b80230",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: curr_active_pipeline, active_config_name\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "if active_peak_prominence_2d_results is None:\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=None, fail_on_exception=False, debug_print=False)\n",
    "    # curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['ratemap_peaks_prominence2d'], enabled_filter_names=[short_LR_name, short_RL_name, long_any_name, short_any_name], fail_on_exception=False, debug_print=False) # or at least\n",
    "    active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "    assert active_peak_prominence_2d_results is not None, f\"bad even after computation\"\n",
    "\n",
    "# active_peak_prominence_2d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f67cb",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalDecodersContinuouslyDecodedResult\n",
    "\n",
    "if 'DirectionalDecodersDecoded' in curr_active_pipeline.global_computation_results.computed_data:\n",
    "    directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "    all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "    pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "    spikes_df = directional_decoders_decode_result.spikes_df\n",
    "    continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "    previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "    print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directional_pf1D_Decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4e959",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.SequenceBasedComputations import WCorrShuffle, SequenceBasedComputationsContainer\n",
    "\n",
    "wcorr_shuffle_results: SequenceBasedComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('SequenceBased', None)\n",
    "if wcorr_shuffle_results is not None:    \n",
    "    wcorr_ripple_shuffle: WCorrShuffle = wcorr_shuffle_results.wcorr_ripple_shuffle\n",
    "    if wcorr_ripple_shuffle is not None:\n",
    "        print(f'wcorr_ripple_shuffle.n_completed_shuffles: {wcorr_ripple_shuffle.n_completed_shuffles}')\n",
    "    else:\n",
    "        print(f'SequenceBased is computed but `wcorr_shuffle_results.wcorr_ripple_shuffle` is None.')        \n",
    "else:\n",
    "    print(f'SequenceBased is not computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['trial_by_trial_metrics'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "directional_trial_by_trial_activity_result = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None) ## try again to get the result\n",
    "assert directional_trial_by_trial_activity_result is not None, f\"directional_trial_by_trial_activity_result is None even after forcing recomputation!!\"\n",
    "print(f'\\t done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10d7394a",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "\n",
    "directional_trial_by_trial_activity_result: TrialByTrialActivityResult = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None)\n",
    "if directional_trial_by_trial_activity_result is None:\n",
    "    # if `KeyError: 'TrialByTrialActivity'` recompute\n",
    "    print(f'TrialByTrialActivity is not computed, computing it...')\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['trial_by_trial_metrics'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "    directional_trial_by_trial_activity_result = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None) ## try again to get the result\n",
    "    assert directional_trial_by_trial_activity_result is not None, f\"directional_trial_by_trial_activity_result is None even after forcing recomputation!!\"\n",
    "    print(f'\\t done.')\n",
    "\n",
    "## unpack either way:\n",
    "any_decoder_neuron_IDs = directional_trial_by_trial_activity_result.any_decoder_neuron_IDs\n",
    "active_pf_dt: PfND_TimeDependent = directional_trial_by_trial_activity_result.active_pf_dt\n",
    "directional_lap_epochs_dict: Dict[str, Epoch] = directional_trial_by_trial_activity_result.directional_lap_epochs_dict\n",
    "directional_active_lap_pf_results_dicts: Dict[str, TrialByTrialActivity] = directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts\n",
    "stability_dict = {k:list(v.aclu_to_stability_score_dict.values()) for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "stability_df: pd.DataFrame = pd.DataFrame({'aclu': any_decoder_neuron_IDs, **stability_dict})\n",
    "## OUTPUTS: stability_df, stability_dict\n",
    "\n",
    "## OUTPUTS: directional_trial_by_trial_activity_result, directional_active_lap_pf_results_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad74c",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "wcorr_shuffle_results: SequenceBasedComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('SequenceBased', None)\n",
    "if wcorr_shuffle_results is not None:    \n",
    "    wcorr_ripple_shuffle: WCorrShuffle = wcorr_shuffle_results.wcorr_ripple_shuffle\n",
    "    if wcorr_ripple_shuffle is not None:  \n",
    "        print(f'wcorr_ripple_shuffle.n_completed_shuffles: {wcorr_ripple_shuffle.n_completed_shuffles}')\n",
    "    else:\n",
    "        print(f'SequenceBased is computed but wcorr_ripple_shuffle is None.')\n",
    "else:\n",
    "    print(f'SequenceBased is not computed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0870",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "\n",
    "most_recent_time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# most_recent_time_bin_size\n",
    "most_recent_continuously_decoded_dict = deepcopy(directional_decoders_decode_result.most_recent_continuously_decoded_dict)\n",
    "# most_recent_continuously_decoded_dict\n",
    "\n",
    "## Adds in the 'pseudo2D' decoder in:\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "# time_bin_size: float = 0.01\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "continuously_decoded_dict = continuously_decoded_result_cache_dict[time_bin_size]\n",
    "pseudo2D_decoder_continuously_decoded_result = continuously_decoded_dict.get('pseudo2D', None)\n",
    "if pseudo2D_decoder_continuously_decoded_result is None:\n",
    "    # compute here...\n",
    "    ## Currently used for both cases to decode:\n",
    "    t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "    single_global_epoch_df: pd.DataFrame = pd.DataFrame({'start': [t_start], 'stop': [t_end], 'label': [0]}) # Build an Epoch object containing a single epoch, corresponding to the global epoch for the entire session:\n",
    "    single_global_epoch: Epoch = Epoch(single_global_epoch_df)\n",
    "    spikes_df = directional_decoders_decode_result.spikes_df\n",
    "    pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = pseudo2D_decoder.decode_specific_epochs(spikes_df=deepcopy(spikes_df), filter_epochs=single_global_epoch, decoding_time_bin_size=time_bin_size, debug_print=False)\n",
    "    continuously_decoded_dict['pseudo2D'] = pseudo2D_decoder_continuously_decoded_result\n",
    "    continuously_decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa2dc5fe",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# NEW 2023-11-22 method: Get the templates (which can be filtered by frate first) and the from those get the decoders):        \n",
    "# track_templates: TrackTemplates = directional_laps_results.get_shared_aclus_only_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # shared-only\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz) # non-shared-only\n",
    "long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder = track_templates.get_decoders()\n",
    "\n",
    "# Unpack all directional variables:\n",
    "## {\"even\": \"RL\", \"odd\": \"LR\"}\n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "# Unpacking for `(long_LR_name, long_RL_name, short_LR_name, short_RL_name)`\n",
    "(long_LR_context, long_RL_context, short_LR_context, short_RL_context) = [curr_active_pipeline.filtered_contexts[a_name] for a_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj, global_any_laps_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name, global_any_name)] # note has global also\n",
    "(long_LR_session, long_RL_session, short_LR_session, short_RL_session) = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # sessions are correct at least, seems like just the computation parameters are messed up\n",
    "(long_LR_results, long_RL_results, short_LR_results, short_RL_results) = [curr_active_pipeline.computation_results[an_epoch_name].computed_data for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_computation_config, long_RL_computation_config, short_LR_computation_config, short_RL_computation_config) = [curr_active_pipeline.computation_results[an_epoch_name].computation_config for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)]\n",
    "(long_LR_pf1D, long_RL_pf1D, short_LR_pf1D, short_RL_pf1D) = (long_LR_results.pf1D, long_RL_results.pf1D, short_LR_results.pf1D, short_RL_results.pf1D)\n",
    "(long_LR_pf2D, long_RL_pf2D, short_LR_pf2D, short_RL_pf2D) = (long_LR_results.pf2D, long_RL_results.pf2D, short_LR_results.pf2D, short_RL_results.pf2D)\n",
    "(long_LR_pf1D_Decoder, long_RL_pf1D_Decoder, short_LR_pf1D_Decoder, short_RL_pf1D_Decoder) = (long_LR_results.pf1D_Decoder, long_RL_results.pf1D_Decoder, short_LR_results.pf1D_Decoder, short_RL_results.pf1D_Decoder)\n",
    "\n",
    "if rank_order_results is not None:\n",
    "    # `LongShortStatsItem` form (2024-01-02):\n",
    "    # LR_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "    # RL_results_real_values = np.array([(a_result_item.long_stats_z_scorer.real_value, a_result_item.short_stats_z_scorer.real_value) for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n",
    "    LR_results_long_short_z_diffs = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.LR_ripple.ranked_aclus_stats_dict.items()])\n",
    "    RL_results_long_short_z_diff = np.array([a_result_item.long_short_z_diff for epoch_id, a_result_item in rank_order_results.RL_ripple.ranked_aclus_stats_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c260739a4f36c662",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestSplitResult\n",
    "\n",
    "if 'TrainTestSplit' in curr_active_pipeline.global_computation_results.computed_data:\n",
    "    directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data.get('TrainTestSplit', None)\n",
    "    training_data_portion: float = directional_train_test_split_result.training_data_portion\n",
    "    test_data_portion: float = directional_train_test_split_result.test_data_portion\n",
    "    test_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.test_epochs_dict\n",
    "    train_epochs_dict: Dict[str, pd.DataFrame] = directional_train_test_split_result.train_epochs_dict\n",
    "    train_lap_specific_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_train_test_split_result.train_lap_specific_pf1D_Decoder_dict\n",
    "    \n",
    "long_LR_name, short_LR_name, global_LR_name, long_RL_name, short_RL_name, global_RL_name, long_any_name, short_any_name, global_any_name = ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj = [curr_active_pipeline.computation_results[an_epoch_name].computation_config.pf_params.computation_epochs for an_epoch_name in (long_LR_name, long_RL_name, short_LR_name, short_RL_name)] # note has global also\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec18cf18",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "if 'burst_detection' in curr_active_pipeline.computation_results[global_epoch_name].computed_data:\n",
    "    active_burst_intervals = curr_active_pipeline.computation_results[global_epoch_name].computed_data['burst_detection']['burst_intervals']\n",
    "# active_burst_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d31f37d",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_extended_stats = global_results.get('extended_stats', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9554d3bf5955d9d3",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c62d5c18c556",
   "metadata": {
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis: checks for cells localized to the endcaps that have their placefields truncated after shortening the track\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "disappearing_endcap_aclus = truncation_checking_result.disappearing_endcap_aclus\n",
    "# disappearing_endcap_aclus\n",
    "trivially_remapping_endcap_aclus = truncation_checking_result.minor_remapping_endcap_aclus\n",
    "# trivially_remapping_endcap_aclus\n",
    "significant_distant_remapping_endcap_aclus = truncation_checking_result.significant_distant_remapping_endcap_aclus\n",
    "# significant_distant_remapping_endcap_aclus\n",
    "appearing_aclus = jonathan_firing_rate_analysis_result.neuron_replay_stats_df[jonathan_firing_rate_analysis_result.neuron_replay_stats_df['track_membership'] == SplitPartitionMembership.RIGHT_ONLY].index\n",
    "appearing_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec24467335a760",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 1️⃣ POST-Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c46e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": [
     "unwrap",
     "initial",
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalDisplayFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multi_sort_raster_browser\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import paired_separately_sort_neurons, paired_incremental_sort_neurons # _display_directional_template_debugger\n",
    "from neuropy.utils.indexing_helpers import paired_incremental_sorting, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ScrollBarWithSpinBox.ScrollBarWithSpinBox import ScrollBarWithSpinBox\n",
    "\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_SerializationMixin\n",
    "from pyphoplacecellanalysis.General.Model.ComputationResults import ComputedResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses, RankOrderResult, ShuffleHelper, Zscorer, LongShortStatsTuple, DirectionalRankOrderLikelihoods, DirectionalRankOrderResult, RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import TimeColumnAliasesProtocol\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPseudo2DDecodersResult\n",
    "\n",
    "## Display Testing\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph import QtGui\n",
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.Extensions.pyqtgraph_helpers import pyqtplot_build_image_bounds_extent, pyqtplot_plot_image\n",
    "\n",
    "spikes_df = curr_active_pipeline.sess.spikes_df\n",
    "rank_order_results: RankOrderComputationsContainer = curr_active_pipeline.global_computation_results.computed_data.get('RankOrder', None)\n",
    "if rank_order_results is not None:\n",
    "    minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "    ripple_result_tuple, laps_result_tuple = rank_order_results.ripple_most_likely_result_tuple, rank_order_results.laps_most_likely_result_tuple\n",
    "\n",
    "else:        \n",
    "    ## get from parameters:\n",
    "    minimum_inclusion_fr_Hz: float = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.included_qclu_values\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "track_templates: TrackTemplates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values) # non-shared-only -- !! Is minimum_inclusion_fr_Hz=None the issue/difference?\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n",
    "# ripple_result_tuple\n",
    "\n",
    "## Unpacks `rank_order_results`: \n",
    "# global_replays = Epoch(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# global_replays = TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].replay))\n",
    "# active_replay_epochs, active_epochs_df, active_selected_spikes_df = combine_rank_order_results(rank_order_results, global_replays, track_templates=track_templates)\n",
    "# active_epochs_df\n",
    "\n",
    "# ripple_result_tuple.directional_likelihoods_tuple.long_best_direction_indices\n",
    "dir_index_to_direction_name_map: Dict[int, str] = {0:'LR', 1:\"RL\"}\n",
    "\n",
    "if rank_order_results is not None:\n",
    "    ## All three DataFrames are the same number of rows, each with one row corresponding to an Epoch:\n",
    "    active_replay_epochs_df = deepcopy(rank_order_results.LR_ripple.epochs_df)\n",
    "    # active_replay_epochs_df\n",
    "\n",
    "    # Change column type to int8 for columns: 'long_best_direction_indices', 'short_best_direction_indices'\n",
    "    # directional_likelihoods_df = pd.DataFrame.from_dict(ripple_result_tuple.directional_likelihoods_tuple._asdict()).astype({'long_best_direction_indices': 'int8', 'short_best_direction_indices': 'int8'})\n",
    "    directional_likelihoods_df = ripple_result_tuple.directional_likelihoods_df\n",
    "    # directional_likelihoods_df\n",
    "\n",
    "    # 2023-12-15 - Newest method:\n",
    "    # laps_combined_epoch_stats_df = rank_order_results.laps_combined_epoch_stats_df\n",
    "\n",
    "    # ripple_combined_epoch_stats_df: pd.DataFrame  = rank_order_results.ripple_combined_epoch_stats_df\n",
    "    # ripple_combined_epoch_stats_df\n",
    "\n",
    "\n",
    "    # # Concatenate the three DataFrames along the columns axis:\n",
    "    # # Assert that all DataFrames have the same number of rows:\n",
    "    # assert len(active_replay_epochs_df) == len(directional_likelihoods_df) == len(ripple_combined_epoch_stats_df), \"DataFrames have different numbers of rows.\"\n",
    "    # # Assert that all DataFrames have at least one row:\n",
    "    # assert len(active_replay_epochs_df) > 0, \"active_replay_epochs_df is empty.\"\n",
    "    # assert len(directional_likelihoods_df) > 0, \"directional_likelihoods_df is empty.\"\n",
    "    # assert len(ripple_combined_epoch_stats_df) > 0, \"ripple_combined_epoch_stats_df is empty.\"\n",
    "    # merged_complete_epoch_stats_df: pd.DataFrame = pd.concat([active_replay_epochs_df.reset_index(drop=True, inplace=False), directional_likelihoods_df.reset_index(drop=True, inplace=False), ripple_combined_epoch_stats_df.reset_index(drop=True, inplace=False)], axis=1)\n",
    "    # merged_complete_epoch_stats_df = merged_complete_epoch_stats_df.set_index(active_replay_epochs_df.index, inplace=False)\n",
    "\n",
    "    # merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "    # merged_complete_epoch_stats_df.to_csv('output/2023-12-21_merged_complete_epoch_stats_df.csv')\n",
    "    # merged_complete_epoch_stats_df\n",
    "\n",
    "    laps_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.laps_merged_complete_epoch_stats_df ## New method\n",
    "    ripple_merged_complete_epoch_stats_df: pd.DataFrame = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "\n",
    "\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "all_directional_decoder_dict_value = directional_merged_decoders_result.all_directional_decoder_dict\n",
    "all_directional_pf1D_Decoder_value = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "# long_directional_pf1D_Decoder_value = directional_merged_decoders_result.long_directional_pf1D_Decoder\n",
    "# long_directional_decoder_dict_value = directional_merged_decoders_result.long_directional_decoder_dict\n",
    "# short_directional_pf1D_Decoder_value = directional_merged_decoders_result.short_directional_pf1D_Decoder\n",
    "# short_directional_decoder_dict_value = directional_merged_decoders_result.short_directional_decoder_dict\n",
    "\n",
    "all_directional_laps_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_laps_filter_epochs_decoder_result\n",
    "all_directional_ripple_filter_epochs_decoder_result_value = directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = directional_merged_decoders_result.laps_track_identity_marginals_tuple\n",
    "ripple_directional_marginals, ripple_directional_all_epoch_bins_marginal, ripple_most_likely_direction_from_decoder, ripple_is_most_likely_direction_LR_dir  = directional_merged_decoders_result.ripple_directional_marginals_tuple\n",
    "ripple_track_identity_marginals, ripple_track_identity_all_epoch_bins_marginal, ripple_most_likely_track_identity_from_decoder, ripple_is_most_likely_track_identity_Long = directional_merged_decoders_result.ripple_track_identity_marginals_tuple\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}')\n",
    "\n",
    "laps_all_epoch_bins_marginals_df = directional_merged_decoders_result.laps_all_epoch_bins_marginals_df\n",
    "ripple_all_epoch_bins_marginals_df = directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ca336",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "run-group-end-run",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import HeuristicReplayScoring\n",
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_filter_replay_epochs\n",
    "\n",
    "filtered_epochs_df, filtered_decoder_filter_epochs_decoder_result_dict, filtered_ripple_all_epoch_bins_marginals_df = _perform_filter_replay_epochs(curr_active_pipeline, global_epoch_name, track_templates, decoder_ripple_filter_epochs_decoder_result_dict, ripple_all_epoch_bins_marginals_df, ripple_decoding_time_bin_size=ripple_decoding_time_bin_size,\n",
    "                                                                                                                            should_only_include_user_selected_epochs=False)\n",
    "filtered_epochs_df\n",
    "# filtered_ripple_all_epoch_bins_marginals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6e077",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "#### 2024-02-29 - 4pm - Filter the events for those meeting wcorr criteria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8cd0c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "min_wcorr_threshold: float = 0.33\n",
    "min_wcorr_diff_threshold: float = 0.2\n",
    "\n",
    "is_included_large_wcorr_diff = np.any((filtered_ripple_all_epoch_bins_marginals_df[['wcorr_abs_diff']].abs() > min_wcorr_diff_threshold), axis=1)\n",
    "# is_included_large_wcorr_diff\n",
    "is_included_high_wcorr = np.any((filtered_ripple_all_epoch_bins_marginals_df[['long_best_wcorr', 'short_best_wcorr']].abs() > min_wcorr_threshold), axis=1)\n",
    "\n",
    "df = filtered_ripple_all_epoch_bins_marginals_df[is_included_large_wcorr_diff]\n",
    "# df = filtered_ripple_all_epoch_bins_marginals_df[is_included_high_wcorr]\n",
    "df\n",
    "\n",
    "# delta_aligned_start_t\n",
    "\n",
    "significant_epochs_start_ts = np.squeeze(df['ripple_start_t'].to_numpy()) ## for filtering\n",
    "\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(significant_epochs_start_ts) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict\n",
    "filtered_epochs_df = filtered_epochs_df.epochs.matching_epoch_times_slice(significant_epochs_start_ts)\n",
    "# filtered_ripple_all_epoch_bins_marginals_df = filtered_ripple_all_epoch_bins_marginals_df.epochs.matching_epoch_times_slice(significant_epochs_start_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da049a90",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### 2024-06-25 - Advanced Time-dependent decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f702d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Directional Versions: 'long_LR':\n",
    "from neuropy.core.epoch import subdivide_epochs, ensure_dataframe\n",
    "\n",
    "\n",
    "## INPUTS: long_LR_epochs_obj, long_LR_results\n",
    "\n",
    "a_pf1D_dt: PfND_TimeDependent = deepcopy(long_LR_results.pf1D_dt)\n",
    "a_pf2D_dt: PfND_TimeDependent = deepcopy(long_LR_results.pf2D_dt)\n",
    "\n",
    "# Example usage\n",
    "df: pd.DataFrame = ensure_dataframe(deepcopy(long_LR_epochs_obj)) \n",
    "df['epoch_type'] = 'lap'\n",
    "df['interval_type_id'] = 666\n",
    "\n",
    "subdivide_bin_size = 0.200  # Specify the size of each sub-epoch in seconds\n",
    "subdivided_df: pd.DataFrame = subdivide_epochs(df, subdivide_bin_size)\n",
    "# print(subdivided_df)\n",
    "\n",
    "## Evolve the ratemaps:\n",
    "_a_pf1D_dt_snapshots = a_pf1D_dt.batch_snapshotting(subdivided_df, reset_at_start=True)\n",
    "_a_pf2D_dt_snapshots = a_pf2D_dt.batch_snapshotting(subdivided_df, reset_at_start=True)\n",
    "# a_pf2D_dt.plot_ratemaps_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78041bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_pf1D_dt.filtered_spikes_df\n",
    "# a_pf1D_dt.neuron_extended_ids\n",
    "\n",
    "# [NeuronExtendedIdentity(shank=1, cluster=3, aclu=3, qclu=9), NeuronExtendedIdentity(shank=1, cluster=6, aclu=5, qclu=1), NeuronExtendedIdentity(shank=1, cluster=7, aclu=6, qclu=2), NeuronExtendedIdentity(shank=1, cluster=9, aclu=7, qclu=2), NeuronExtendedIdentity(shank=1, cluster=12, aclu=8, qclu=2), NeuronExtendedIdentity(shank=2, cluster=3, aclu=10, qclu=4), NeuronExtendedIdentity(shank=2, cluster=4, aclu=11, qclu=2), NeuronExtendedIdentity(shank=2, cluster=6, aclu=12, qclu=4), NeuronExtendedIdentity(shank=2, cluster=12, aclu=15, qclu=2), NeuronExtendedIdentity(shank=2, cluster=14, aclu=17, qclu=2), NeuronExtendedIdentity(shank=2, cluster=18, aclu=20, qclu=4), NeuronExtendedIdentity(shank=2, cluster=24, aclu=21, qclu=4), NeuronExtendedIdentity(shank=2, cluster=27, aclu=23, qclu=9), NeuronExtendedIdentity(shank=2, cluster=31, aclu=24, qclu=2), NeuronExtendedIdentity(shank=2, cluster=32, aclu=25, qclu=4), NeuronExtendedIdentity(shank=2, cluster=33, aclu=26, qclu=2), NeuronExtendedIdentity(shank=2, cluster=34, aclu=27, qclu=2), NeuronExtendedIdentity(shank=3, cluster=4, aclu=28, qclu=2), NeuronExtendedIdentity(shank=3, cluster=9, aclu=31, qclu=2), NeuronExtendedIdentity(shank=3, cluster=10, aclu=32, qclu=9), NeuronExtendedIdentity(shank=3, cluster=11, aclu=33, qclu=9), NeuronExtendedIdentity(shank=3, cluster=12, aclu=34, qclu=4), NeuronExtendedIdentity(shank=4, cluster=2, aclu=35, qclu=4), NeuronExtendedIdentity(shank=7, cluster=5, aclu=39, qclu=2), NeuronExtendedIdentity(shank=7, cluster=6, aclu=40, qclu=1), NeuronExtendedIdentity(shank=7, cluster=8, aclu=41, qclu=4), NeuronExtendedIdentity(shank=7, cluster=10, aclu=43, qclu=4), NeuronExtendedIdentity(shank=7, cluster=11, aclu=44, qclu=4), NeuronExtendedIdentity(shank=8, cluster=2, aclu=45, qclu=2), NeuronExtendedIdentity(shank=8, cluster=4, aclu=46, qclu=9), NeuronExtendedIdentity(shank=8, cluster=6, aclu=48, qclu=4), NeuronExtendedIdentity(shank=8, cluster=7, aclu=49, qclu=2), NeuronExtendedIdentity(shank=8, cluster=11, aclu=50, qclu=4), NeuronExtendedIdentity(shank=8, cluster=12, aclu=51, qclu=4), NeuronExtendedIdentity(shank=8, cluster=13, aclu=52, qclu=2), NeuronExtendedIdentity(shank=8, cluster=16, aclu=53, qclu=2), NeuronExtendedIdentity(shank=8, cluster=20, aclu=54, qclu=9), NeuronExtendedIdentity(shank=8, cluster=23, aclu=55, qclu=2), NeuronExtendedIdentity(shank=8, cluster=25, aclu=56, qclu=2), NeuronExtendedIdentity(shank=11, cluster=8, aclu=60, qclu=2), NeuronExtendedIdentity(shank=11, cluster=10, aclu=62, qclu=2), NeuronExtendedIdentity(shank=11, cluster=13, aclu=63, qclu=2), NeuronExtendedIdentity(shank=12, cluster=4, aclu=64, qclu=4), NeuronExtendedIdentity(shank=12, cluster=5, aclu=65, qclu=2)]\n",
    "included_qclu_values = [1, 2]\n",
    "\n",
    "# a_pf1D_dt.neuron_extended_ids[0].qclu\n",
    "# np.isin(included_qclu_values)\n",
    "\n",
    "included_neuron_extended_identities = [v for v in a_pf1D_dt.neuron_extended_ids if v.qclu in included_qclu_values]\n",
    "included_neuron_aclus = [v.aclu for v in included_neuron_extended_identities] # [5, 6, 7, 8, 11, 15, 17, 24, 26, 27, 28, 31, 39, 40, 45, 49, 52, 53, 55, 56, 60, 62, 63, 65]\n",
    "\n",
    "included_neuron_extended_identities\n",
    "included_neuron_aclus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607a444",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# / 🛑 End Run Section 🛑\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433b81f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Find the time series of Long-likely events\n",
    "# type(long_RL_results) # DynamicParameters\n",
    "long_LR_pf1D_Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2363d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "type(all_directional_decoder_dict_value)\n",
    "list(all_directional_decoder_dict_value.keys()) # ['long_LR', 'long_RL', 'short_LR', 'short_RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e6027",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_all_epoch_bins_marginals_df\n",
    "laps_most_likely_direction_from_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdabd71",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "type(ripple_result_tuple) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations.DirectionalRankOrderResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca534c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "\n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629dae",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import DirectionalRankOrderResult\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots \n",
    "\n",
    "# @register_type_display(DirectionalRankOrderResult)\n",
    "def plot_histograms(self: DirectionalRankOrderResult, **kwargs) -> \"MatplotlibRenderPlots\":\n",
    "    \"\"\" \n",
    "    num='RipplesRankOrderZscore'\n",
    "    \"\"\"\n",
    "    print(f'.plot_histograms(..., kwargs: {kwargs})')\n",
    "    fig = plt.figure(layout=\"constrained\", **kwargs)\n",
    "    ax_dict = fig.subplot_mosaic(\n",
    "        [\n",
    "            [\"long_short_best_z_score_diff\", \"long_short_best_z_score_diff\"],\n",
    "            [\"long_best_z_scores\", \"short_best_z_scores\"],\n",
    "        ],\n",
    "    )\n",
    "    plots = (pd.DataFrame({'long_best_z_scores': self.long_best_dir_z_score_values}).hist(ax=ax_dict['long_best_z_scores'], bins=21, alpha=0.8),\n",
    "        pd.DataFrame({'short_best_z_scores': self.short_best_dir_z_score_values}).hist(ax=ax_dict['short_best_z_scores'], bins=21, alpha=0.8),\n",
    "        pd.DataFrame({'long_short_best_z_score_diff': self.long_short_best_dir_z_score_diff_values}).hist(ax=ax_dict['long_short_best_z_score_diff'], bins=21, alpha=0.8),\n",
    "    )\n",
    "    return MatplotlibRenderPlots(name='plot_histogram_figure', figures=[fig], axes=ax_dict)\n",
    "\n",
    "\n",
    "# register_type_display(plot_histograms, DirectionalRankOrderResult)\n",
    "## Call the newly added `plot_histograms` function on the `ripple_result_tuple` object which is of type `DirectionalRankOrderResult`:\n",
    "assert isinstance(ripple_result_tuple, DirectionalRankOrderResult) \n",
    "ripple_result_tuple.plot_histograms(num='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291690",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ripple_result_tuple.plot_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30bcb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# 💾 CSVs \n",
    "print(f'\\t try saving to CSV...')\n",
    "merged_complete_epoch_stats_df = rank_order_results.ripple_merged_complete_epoch_stats_df ## New method\n",
    "merged_complete_epoch_stats_df\n",
    "merged_complete_ripple_epoch_stats_df_output_path = curr_active_pipeline.get_output_path().joinpath(f'{DAY_DATE_TO_USE}_merged_complete_epoch_stats_df.csv').resolve()\n",
    "merged_complete_epoch_stats_df.to_csv(merged_complete_ripple_epoch_stats_df_output_path)\n",
    "print(f'\\t saving to CSV: {merged_complete_ripple_epoch_stats_df_output_path} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60749347",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print(f'\\tdone. building global result.')\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']\n",
    "selected_spikes_df = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.selected_spikes_df)\n",
    "# active_epochs = global_computation_results.computed_data['RankOrder'].ripple_most_likely_result_tuple.active_epochs\n",
    "active_epochs = deepcopy(curr_active_pipeline.global_computation_results.computed_data['RankOrder'].LR_ripple.epochs_df)\n",
    "track_templates = directional_laps_results.get_templates(minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz)\n",
    "\n",
    "ripple_combined_epoch_stats_df, ripple_new_output_tuple = RankOrderAnalyses.pandas_df_based_correlation_computations(selected_spikes_df=selected_spikes_df, active_epochs_df=active_epochs, track_templates=track_templates, num_shuffles=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886d9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# new_output_tuple (output_active_epoch_computed_values, valid_stacked_arrays, real_stacked_arrays, n_valid_shuffles) = ripple_new_output_tuple\n",
    "curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_combined_epoch_stats_df, curr_active_pipeline.global_computation_results.computed_data['RankOrder'].ripple_new_output_tuple = ripple_combined_epoch_stats_df, ripple_new_output_tuple\n",
    "print(f'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display the `TrainTestSplitResult` in a `PhoPaginatedMultiDecoderDecodedEpochsWindow`\n",
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import add_laps_groundtruth_information_to_dataframe\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "\n",
    "## INPUTS: train_decoded_results_dict\n",
    "# decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs # looks like 'lap_dir' column is wrong\n",
    "\n",
    "# active_results: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(decoder_laps_filter_epochs_decoder_result_dict)\n",
    "active_results: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy({k:v.decoder_result for k, v in _out_separate_decoder_results[0].items()})\n",
    "\n",
    "laps_app, laps_paginated_multi_decoder_decoded_epochs_window, laps_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                            decoder_decoded_epochs_result_dict=active_results, epochs_name='laps', included_epoch_indicies=None, \n",
    "    params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "    'skip_plotting_most_likely_positions': False, 'skip_plotting_measured_positions': False, \n",
    "    # 'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': False,\n",
    "    'enable_decoded_most_likely_position_curve': True, 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "    # 'disable_y_label': True,\n",
    "    # 'isPaginatorControlWidgetBackedMode': True,\n",
    "    # 'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "    # 'debug_print': True,\n",
    "    'max_subplots_per_page': 10,\n",
    "    'scrollable_figure': True,\n",
    "    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "    'use_AnchoredCustomText': False,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _perform_build_individual_time_bin_decoded_posteriors_df\n",
    "\n",
    "transfer_column_names_list: List[str] = ['maze_id', 'lap_dir', 'lap_id']\n",
    "filtered_laps_time_bin_marginals_df = _perform_build_individual_time_bin_decoded_posteriors_df(curr_active_pipeline, track_templates=track_templates, all_directional_laps_filter_epochs_decoder_result=all_directional_laps_filter_epochs_decoder_result, transfer_column_names_list=transfer_column_names_list)\n",
    "filtered_laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import co_filter_epochs_and_spikes\n",
    "\n",
    "## INPUTS: all_directional_laps_filter_epochs_decoder_result\n",
    "transfer_column_names_list: List[str] = ['maze_id', 'lap_dir', 'lap_id']\n",
    "TIME_OVERLAP_PREVENTION_EPSILON: float = 1e-12\n",
    "(laps_directional_marginals_tuple, laps_track_identity_marginals_tuple, laps_non_marginalized_decoder_marginals_tuple), laps_marginals_df = all_directional_laps_filter_epochs_decoder_result.compute_marginals(epoch_idx_col_name='lap_idx', epoch_start_t_col_name='lap_start_t',\n",
    "                                                                                                                                                    additional_transfer_column_names=['start','stop','label','duration','lap_id','lap_dir','maze_id','is_LR_dir'])\n",
    "laps_directional_marginals, laps_directional_all_epoch_bins_marginal, laps_most_likely_direction_from_decoder, laps_is_most_likely_direction_LR_dir  = laps_directional_marginals_tuple\n",
    "laps_track_identity_marginals, laps_track_identity_all_epoch_bins_marginal, laps_most_likely_track_identity_from_decoder, laps_is_most_likely_track_identity_Long = laps_track_identity_marginals_tuple\n",
    "non_marginalized_decoder_marginals, non_marginalized_decoder_all_epoch_bins_marginal, most_likely_decoder_idxs, non_marginalized_decoder_all_epoch_bins_decoder_probs_df = laps_non_marginalized_decoder_marginals_tuple\n",
    "laps_time_bin_marginals_df: pd.DataFrame = all_directional_laps_filter_epochs_decoder_result.build_per_time_bin_marginals_df(active_marginals_tuple=(laps_directional_marginals, laps_track_identity_marginals, non_marginalized_decoder_marginals),\n",
    "                                                                                                                              columns_tuple=(['P_LR', 'P_RL'], ['P_Long', 'P_Short'], ['long_LR', 'long_RL', 'short_LR', 'short_RL']), transfer_column_names_list=transfer_column_names_list)\n",
    "laps_time_bin_marginals_df['start'] = laps_time_bin_marginals_df['start'] + TIME_OVERLAP_PREVENTION_EPSILON ## ENSURE NON-OVERLAPPING\n",
    "\n",
    "## INPUTS: laps_time_bin_marginals_df\n",
    "# active_min_num_unique_aclu_inclusions_requirement: int = track_templates.min_num_unique_aclu_inclusions_requirement(curr_active_pipeline, required_min_percentage_of_active_cells=0.33333333333333)\n",
    "active_min_num_unique_aclu_inclusions_requirement = None # must be none for individual `time_bin` periods\n",
    "filtered_laps_time_bin_marginals_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=curr_active_pipeline.global_computation_config.rank_order_shuffle_analysis.minimum_inclusion_fr_Hz),\n",
    "                                                                  active_epochs_df=laps_time_bin_marginals_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement,\n",
    "                                                                epoch_id_key_name='lap_individual_time_bin_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "filtered_laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and max number of active aclus per time bin for each epoch (lap)\n",
    "filtered_laps_time_bin_marginals_df.groupby(['lap_id']).agg(n_unique_aclus_mean=('n_unique_aclus', 'mean'), n_unique_aclus_max=('n_unique_aclus', 'max')).reset_index()\n",
    "filtered_laps_time_bin_marginals_df.groupby(['maze_id']).agg(n_unique_aclus_mean=('n_unique_aclus', 'mean'), n_unique_aclus_max=('n_unique_aclus', 'max')).reset_index() ## per maze\n",
    "filtered_laps_time_bin_marginals_df.groupby(['maze_id', 'lap_dir']).agg(n_unique_aclus_mean=('n_unique_aclus', 'mean'), n_unique_aclus_max=('n_unique_aclus', 'max')).reset_index() # per maze x lap_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb39d3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# {frozenset({('desired_shared_decoding_time_bin_size', 0.025), ('minimum_event_duration', 0.05), ('use_single_time_bin_per_epoch', False)}): 0.025,\n",
    "#  frozenset({('desired_shared_decoding_time_bin_size', 0.03), ('minimum_event_duration', 0.05), ('use_single_time_bin_per_epoch', False)}): 0.03,\n",
    "#  frozenset({('desired_shared_decoding_time_bin_size', 0.044), ('minimum_event_duration', 0.05), ('use_single_time_bin_per_epoch', False)}): 0.044,\n",
    "#  frozenset({('desired_shared_decoding_time_bin_size', 0.05), ('minimum_event_duration', 0.05), ('use_single_time_bin_per_epoch', False)}): 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_trial_by_trial_result.directional_active_lap_pf_results_dicts\n",
    "a_trial_by_trial_result.directional_lap_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c59a0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "several_time_bin_sizes_ripple_df\n",
    "\n",
    "ripple_out_path # 'K:/scratch/collected_outputs/2024-07-05-kdiba_gor01_two_2006-6-07_16-40-19__withNewKamranExportedReplays-(ripple_marginals_df).csv'\n",
    "# 'K:/scratch/collected_outputs/2024-07-05-kdiba_gor01_two_2006-6-07_16-40-19__withNewComputedReplays-qclu_[1, 2]-frateThresh_5.0-(ripple_marginals_df).csv'\n",
    "several_time_bin_sizes_time_bin_ripple_df\n",
    "\n",
    "ripple_time_bin_marginals_out_path # 'K:/scratch/collected_outputs/2024-07-05-kdiba_gor01_two_2006-6-07_16-40-19__withNewKamranExportedReplays-(ripple_time_bin_marginals_df).csv'\n",
    "# 'K:/scratch/collected_outputs/2024-07-05-kdiba_gor01_two_2006-6-07_16-40-19__withNewComputedReplays-qclu_[1, 2]-frateThresh_5.0-(ripple_time_bin_marginals_df).csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dfa3e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "v: DecoderDecodedEpochsResult = list(output_directional_decoders_epochs_decode_results_dict.values())[0]\n",
    "v.add_all_extra_epoch_columns(curr_active_pipeline=curr_active_pipeline, track_templates=track_templates)\n",
    "# _out = v.export_csvs(parent_output_path=collected_outputs_path, active_context=curr_active_pipeline.get_session_context(), session_name=curr_active_pipeline.session_name, curr_session_t_delta=t_delta)\n",
    "\n",
    "# assert self.collected_outputs_path.exists()\n",
    "# curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "# CURR_BATCH_OUTPUT_PREFIX: str = f\"{self.BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "# print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "# from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_computations\n",
    "# curr_active_pipeline.reload_default_computation_functions()\n",
    "# batch_extended_computations(curr_active_pipeline, include_includelist=['merged_directional_placefields'], include_global_functions=True, fail_on_exception=True, force_recompute=False)\n",
    "# directional_merged_decoders_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "\n",
    "# active_context = curr_active_pipeline.get_session_context()\n",
    "# _out = directional_merged_decoders_result.compute_and_export_marginals_df_csvs(parent_output_path=self.collected_outputs_path, active_context=active_context)\n",
    "# print(f'successfully exported marginals_df_csvs to {self.collected_outputs_path}!')\n",
    "# (laps_marginals_df, laps_out_path), (ripple_marginals_df, ripple_out_path) = _out\n",
    "# (laps_marginals_df, laps_out_path, laps_time_bin_marginals_df, laps_time_bin_marginals_out_path), (ripple_marginals_df, ripple_out_path, ripple_time_bin_marginals_df, ripple_time_bin_marginals_out_path) = _out\n",
    "# print(f'\\tlaps_out_path: {laps_out_path}\\n\\tripple_out_path: {ripple_out_path}\\n\\tdone.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b6311",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa94d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a8b74",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Take extra computations from `_decode_and_evaluate_epochs_using_directional_decoders` and integrate into the multi-time-bin results from `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_all_df_score_metrics\n",
    "\n",
    "should_skip_radon_transform = True\n",
    "## Recompute the epoch scores/metrics such as radon transform and wcorr:\n",
    "\n",
    "a_sweep_tuple, a_pseudo_2D_result = list(output_full_directional_merged_decoders_result.items())[0]\n",
    "a_decoder_laps_filter_epochs_decoder_result_dict = deepcopy(a_pseudo_2D_result.all_directional_laps_filter_epochs_decoder_result)\n",
    "a_decoder_ripple_filter_epochs_decoder_result_dict = deepcopy(a_pseudo_2D_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "\n",
    "(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict), merged_df_outputs_tuple, raw_dict_outputs_tuple = _compute_all_df_score_metrics(directional_merged_decoders_result, track_templates,\n",
    "                                                                                                                                                                                    decoder_laps_filter_epochs_decoder_result_dict=a_decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict=a_decoder_ripple_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                                                                                    spikes_df=deepcopy(curr_active_pipeline.sess.spikes_df),\n",
    "                                                                                                                                                                                    should_skip_radon_transform=should_skip_radon_transform)\n",
    "laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df, laps_simple_pf_pearson_merged_df, ripple_simple_pf_pearson_merged_df = merged_df_outputs_tuple\n",
    "decoder_laps_radon_transform_df_dict, decoder_ripple_radon_transform_df_dict, decoder_laps_radon_transform_extras_dict, decoder_ripple_radon_transform_extras_dict, decoder_laps_weighted_corr_df_dict, decoder_ripple_weighted_corr_df_dict = raw_dict_outputs_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfda868",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# `_perform_compute_custom_epoch_decoding`\n",
    "\n",
    "a_sweep_tuple\n",
    "# a_pseudo_2D_result.all_directional_laps_filter_epochs_decoder_result\n",
    "# a_pseudo_2D_result\n",
    "# a_pseudo_2D_result.short_directional_decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9603a0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# print_keys_if_possible('several_time_bin_sizes_laps_df', several_time_bin_sizes_laps_df)\n",
    "print_keys_if_possible('output_full_directional_merged_decoders_result', output_full_directional_merged_decoders_result, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a71abd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# get_file_pat\n",
    "collected_outputs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0f606",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "output_laps_decoding_accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6f87",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# from neuropy.utils.matplotlib_helpers import pho_jointplot\n",
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import pho_jointplot, plot_histograms\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# def pho_jointplot(*args, **kwargs):\n",
    "# \t\"\"\" wraps sns.jointplot to allow adding titles/axis labels/etc.\"\"\"\n",
    "# \ttitle = kwargs.pop('title', None)\n",
    "# \t_out = sns.jointplot(*args, **kwargs)\n",
    "# \tif title is not None:\n",
    "# \t\tplt.suptitle(title)\n",
    "# \treturn _out\n",
    "\n",
    "common_kwargs = dict(ylim=(0,1), hue='time_bin_size') # , marginal_kws=dict(bins=25, fill=True)\n",
    "# sns.jointplot(data=a_laps_all_epoch_bins_marginals_df, x='lap_start_t', y='P_Long', kind=\"scatter\", color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per epoch') #color=\"#4CB391\")\n",
    "pho_jointplot(data=several_time_bin_sizes_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per epoch')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_ripple_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Ripple: per time bin')\n",
    "pho_jointplot(data=several_time_bin_sizes_time_bin_laps_df, x='delta_aligned_start_t', y='P_Long', kind=\"scatter\", **common_kwargs, title='Laps: per time bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43311ee",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_histograms\n",
    "\n",
    "# You can use it like this:\n",
    "plot_histograms('Laps', 'One Session', several_time_bin_sizes_time_bin_laps_df, \"several\")\n",
    "plot_histograms('Ripples', 'One Session', several_time_bin_sizes_time_bin_ripple_df, \"several\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33b924",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "several_time_bin_sizes_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102212a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# sns.displot(\n",
    "#     several_time_bin_sizes_laps_df, x=\"P_Long\", col=\"species\", row=\"time_bin_size\",\n",
    "#     binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    "# )\n",
    "\n",
    "sns.displot(\n",
    "    several_time_bin_sizes_laps_df, x='delta_aligned_start_t', y='P_Long', row=\"time_bin_size\",\n",
    "    binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fcb1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🎨 2024-02-06 - Other Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5623a2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "all",
     "run-group-display"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.TimeSynchronizedPlotters.TimeSynchronizedPlacefieldsPlotter import TimeSynchronizedPlacefieldsPlotter\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "#  Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "curr_active_pipeline.prepare_for_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968df7ce",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Display import DisplayFunctionItem\n",
    "from pyphocorehelpers.gui.Qt.tree_helpers import find_tree_item_by_text\n",
    "from pyphoplacecellanalysis.GUI.Qt.MainApplicationWindows.LauncherWidget.LauncherWidget import LauncherWidget\n",
    "\n",
    "widget = LauncherWidget()\n",
    "treeWidget = widget.mainTreeWidget # QTreeWidget\n",
    "widget.build_for_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to separate out the main-track vs.the platforms so that I can impose continuity constraints (for filtering replays via step-sizes) only on the bins of the main track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.sess.epochs\n",
    "# global_session.epochs\n",
    "# long_session.epochs\n",
    "# short_session.epochs\n",
    "## find first lap\n",
    "global_laps\n",
    "\n",
    "# curr_active_pipeline.sess.position.to_dataframe()\n",
    "long_session.position.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca411f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_sess_config = deepcopy(curr_active_pipeline.active_sess_config)\n",
    "# absolute_start_timestamp: float = active_sess_config.absolute_start_timestamp\n",
    "loaded_track_limits = active_sess_config.loaded_track_limits # x_midpoint, \n",
    "\n",
    "(first_valid_pos_time, last_valid_pos_time) = _find_first_and_last_valid_position_times(pos_df, loaded_track_limits)\n",
    "(first_valid_pos_time, last_valid_pos_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "(first_valid_pos_time, last_valid_pos_time) = curr_active_pipeline.find_first_and_last_valid_position_times()\n",
    "first_valid_pos_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f464ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_all_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "curr_active_pipeline.get_session_context().get_initialization_code_string()\n",
    "\n",
    "10.98\n",
    "\n",
    "override_dict = UserAnnotationsManager.get_hardcoded_specific_session_override_dict().get(curr_active_pipeline.get_session_context(), {})\n",
    "override_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68bec8",
   "metadata": {
    "tags": [
     "active",
     "great"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.plotting.figure import pretty_plot\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.placefield import plot_1d_placecell_validations\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.placefield import plot_single_cell_1D_placecell_validation\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.placefield import _subfn_plot_pf1D_placefield\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import test_plotRaw_v_time\n",
    "\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "\n",
    "# global_session.config.plotting_config\n",
    "active_config = deepcopy(curr_active_pipeline.active_configs[global_epoch_name])\n",
    "active_pf1D = deepcopy(global_pf1D)\n",
    "\n",
    "fig = plt.figure(figsize=(23, 9.7), clear=True, num='test_plotRaw_v_time')\n",
    "# Need axes:\n",
    "# Layout Subplots in Figure:\n",
    "gs = fig.add_gridspec(1, 8)\n",
    "gs.update(wspace=0, hspace=0.05) # set the spacing between axes. # `wspace=0`` is responsible for sticking the pf and the activity axes together with no spacing\n",
    "ax_activity_v_time = fig.add_subplot(gs[0, :-1]) # all except the last element are the trajectory over time\n",
    "ax_pf_tuning_curve = fig.add_subplot(gs[0, -1], sharey=ax_activity_v_time) # The last element is the tuning curve\n",
    "# if should_include_labels:\n",
    "    # ax_pf_tuning_curve.set_title('Normalized Placefield', fontsize='14')\n",
    "ax_pf_tuning_curve.set_xticklabels([])\n",
    "ax_pf_tuning_curve.set_yticklabels([])\n",
    "\n",
    "\n",
    "cellind: int = 2\n",
    "\n",
    "kwargs = {}\n",
    "# jitter the curve_value for each spike based on the time it occured along the curve:\n",
    "spikes_color_RGB = kwargs.get('spikes_color', (0, 0, 0))\n",
    "spikes_alpha = kwargs.get('spikes_alpha', 0.8)\n",
    "# print(f'spikes_color: {spikes_color_RGB}')\n",
    "should_plot_bins_grid = kwargs.get('should_plot_bins_grid', False)\n",
    "\n",
    "should_include_trajectory = kwargs.get('should_include_trajectory', True) # whether the plot should include \n",
    "should_include_labels = kwargs.get('should_include_labels', True) # whether the plot should include text labels, like the title, axes labels, etc\n",
    "should_include_plotRaw_v_time_spikes = kwargs.get('should_include_spikes', True) # whether the plot should include plotRaw_v_time-spikes, should be set to False to plot completely with the new all spikes mode\n",
    "use_filtered_positions: bool = kwargs.pop('use_filtered_positions', False)\n",
    "\n",
    "# position_plot_kwargs = {'color': '#393939c8', 'linewidth': 1.0, 'zorder':5} | kwargs.get('position_plot_kwargs', {}) # passed into `active_epoch_placefields1D.plotRaw_v_time`\n",
    "position_plot_kwargs = {'color': '#757575c8', 'linewidth': 1.0, 'zorder':5} | kwargs.get('position_plot_kwargs', {}) # passed into `active_epoch_placefields1D.plotRaw_v_time`\n",
    "\n",
    "\n",
    "# _out = test_plotRaw_v_time(active_pf1D=active_pf1D, cellind=cellind)\n",
    "# spike_plot_kwargs = {'linestyle':'none', 'markersize':5.0, 'marker': '.', 'markerfacecolor':spikes_color_RGB, 'markeredgecolor':spikes_color_RGB, 'zorder':10} ## OLDER\n",
    "spike_plot_kwargs = {'zorder':10} ## OLDER\n",
    "\n",
    "\n",
    "# active_pf1D.plotRaw_v_time(cellind, ax=ax_activity_v_time, spikes_alpha=spikes_alpha,\n",
    "# \tposition_plot_kwargs=position_plot_kwargs,\n",
    "# \tspike_plot_kwargs=spike_plot_kwargs,\n",
    "# \tshould_include_labels=should_include_labels, should_include_trajectory=should_include_trajectory, should_include_spikes=should_include_plotRaw_v_time_spikes,\n",
    "# \tuse_filtered_positions=use_filtered_positions,\n",
    "# ) # , spikes_color=spikes_color, spikes_alpha=spikes_alpha\n",
    "\n",
    "_out = test_plotRaw_v_time(active_pf1D=active_pf1D, cellind=cellind, ax=ax_activity_v_time, spikes_alpha=spikes_alpha,\n",
    "    position_plot_kwargs=position_plot_kwargs,\n",
    "    spike_plot_kwargs=spike_plot_kwargs,\n",
    "    should_include_labels=should_include_labels, should_include_trajectory=should_include_trajectory, should_include_spikes=should_include_plotRaw_v_time_spikes,\n",
    "    use_filtered_positions=use_filtered_positions,\n",
    ")\n",
    "\n",
    "_out = _subfn_plot_pf1D_placefield(active_epoch_placefields1D=active_pf1D, placefield_cell_index=cellind,\n",
    "                                ax_activity_v_time=ax_activity_v_time, ax_pf_tuning_curve=ax_pf_tuning_curve, pf_tuning_curve_ax_position='right')\n",
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ee8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_pf1D.run_spk_pos\n",
    "active_pf1D.spk_pos\n",
    "active_pf1D.run_spk_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_pf1D.ratemap_spiketrains\n",
    "active_pf1D.ratemap_spiketrains_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = plot_1d_placecell_validations(active_pf1D, active_config.plotting_config, modifier_string='lap_only', should_save=False)\n",
    "# _out = curr_active_pipeline.display('_display_1d_placefield_validations', 'maze_any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out.ui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plot_single_cell_1D_placecell_validation(active_pf1D, 1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71088cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib_configuration_update(\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e362334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with matplotlib_interactivity(is_interactive=True):\n",
    "_out.figures[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc6123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qclu_included_aclus = curr_active_pipeline.determine_good_aclus_by_qclu(included_qclu_values=[1,2,4,9])\n",
    "qclu_included_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_neuron_ids_list = [a_decoder.pf.ratemap.neuron_ids for a_decoder in (long_LR_decoder, long_RL_decoder, short_LR_decoder, short_RL_decoder)]\n",
    "original_neuron_ids_list = [a_decoder.pf.ratemap.neuron_ids for a_decoder in track_templates.get_decoders_dict().values()]\n",
    "original_neuron_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_names = track_templates.get_decoder_names() # ('long_LR', 'long_RL', 'short_LR', 'short_RL')\n",
    "decoder_names = TrackTemplates.get_decoder_names() # ('long_LR', 'long_RL', 'short_LR', 'short_RL')\n",
    "\n",
    "link = #00fff7\n",
    "link_visited = #ffaaff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4fd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.decoder_neuron_IDs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_print = True\n",
    "## INPUTS: included_qclu_values\n",
    "included_qclu_values = [1, 2]\n",
    "\n",
    "# modified_neuron_ids_dict = track_templates.determine_decoder_aclus_filtered_by_qclu(included_qclu_values=included_qclu_values)\n",
    "\n",
    "# filtered_track_templates = track_templates.filtered_by_frate_and_qclu(minimum_inclusion_fr_Hz=None, included_qclu_values=None)\n",
    "# filtered_track_templates = track_templates.filtered_by_frate_and_qclu(included_qclu_values=included_qclu_values)\n",
    "filtered_track_templates = track_templates.filtered_by_frate_and_qclu(minimum_inclusion_fr_Hz=5.0, included_qclu_values=[1, 2])\n",
    "\n",
    "# modified_neuron_ids_dict\n",
    "filtered_track_templates.decoder_neuron_IDs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_included_aclus_dict = {}\n",
    "# for a_decoder in track_templates.get_decoders_dict().values():\n",
    "for a_decoder_name, a_decoder in track_templates.get_decoders_dict().items():\n",
    "    # a_decoder.pf.spikes_df\n",
    "    neuron_identities: pd.DataFrame = deepcopy(a_decoder.pf.filtered_spikes_df).spikes.extract_unique_neuron_identities()\n",
    "    if debug_print:\n",
    "        print(f\"original {len(neuron_identities)}\")\n",
    "    filtered_neuron_identities: pd.DataFrame = neuron_identities[neuron_identities.neuron_type == NeuronType.PYRAMIDAL]\n",
    "    if debug_print:\n",
    "        print(f\"post PYRAMIDAL filtering {len(filtered_neuron_identities)}\")\n",
    "    filtered_neuron_identities = filtered_neuron_identities[['aclu', 'shank', 'cluster', 'qclu']]\n",
    "    filtered_neuron_identities = filtered_neuron_identities[np.isin(filtered_neuron_identities.qclu, included_qclu_values)] # drop [6, 7], which are said to have double fields - 80 remain\n",
    "    if debug_print:\n",
    "        print(f\"post (qclu != [6, 7]) filtering {len(filtered_neuron_identities)}\")\n",
    "    # filtered_neuron_identities\n",
    "    final_included_aclus = filtered_neuron_identities['aclu'].to_numpy()\n",
    "    final_included_aclus_dict[a_decoder_name] = final_included_aclus.tolist()\n",
    "\n",
    "\n",
    "final_included_aclus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e743ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "qclu_included_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.display_output_history_list\n",
    "curr_active_pipeline.display_output_last_added_context\n",
    "curr_active_pipeline.last_added_display_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd636253",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.any_decoder_neuron_IDs\n",
    "track_templates.decoder_neuron_IDs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650eea58",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n",
    "\n",
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True)\n",
    "spike_raster_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0c7f0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "add_renderables_menu = active_2d_plot.ui.menus.custom_context_menus.add_renderables[0].programmatic_actions_dict\n",
    "menu_commands = ['AddTimeIntervals.PBEs', 'AddTimeIntervals.Ripples', 'AddTimeIntervals.Replays', 'AddTimeIntervals.Laps'] # , 'AddTimeIntervals.SessionEpochs'\n",
    "for a_command in menu_commands:\n",
    "    add_renderables_menu[a_command].trigger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95289eb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "add_renderables_menu = active_2d_plot.ui.menus.custom_context_menus.add_renderables[0].programmatic_actions_dict\n",
    "menu_commands = ['AddTimeIntervals.PBEs', 'AddTimeIntervals.Ripples', 'AddTimeIntervals.Replays', 'AddTimeIntervals.Laps'] # , 'AddTimeIntervals.SessionEpochs'\n",
    "for a_command in menu_commands:\n",
    "    add_renderables_menu[a_command].trigger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d06e5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print_keys_if_possible('active_2d_plot.ui.menus.custom_context_menus', active_2d_plot.ui.menus.custom_context_menus, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bf9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.get_session_context()\n",
    "\n",
    "## Bad/Icky Bimodal Cells:\n",
    "{IdentifyingContext(format_name= 'kdiba', animal= 'vvp01', exper_name= 'one', session_name= '2006-4-10_12-25-50'): [7, 36, 31, 4, 32, 27, 13, ],\n",
    " \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d0bf7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.Qt.Menus.PhoMenuHelper import PhoMenuHelper\n",
    "\n",
    "_menu_commands_dict = PhoMenuHelper.build_programmatic_menu_command_dict(active_2d_plot)\n",
    "print_keys_if_possible('_menu_commands_dict', _menu_commands_dict, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e1345",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "add_renderables_menu    \n",
    "menu_commands = ['AddMatplotlibPlot.DecodedPosition', 'AddTimeIntervals.Ripples', 'AddTimeIntervals.Replays', 'AddTimeIntervals.Laps'] # , 'AddTimeIntervals.SessionEpochs'\n",
    "for a_command in menu_commands:\n",
    "    add_renderables_menu[a_command].trigger()\n",
    "\n",
    "# ['AddMatplotlibPlot'\n",
    "#  'DecodedPosition'\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb6d31",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "[f'AddTimeCurves.{k}' for k in add_renderables_menu['AddTimeCurves']] # ['AddTimeCurves.Position', 'AddTimeCurves.Velocity', 'AddTimeCurves.Random', 'AddTimeCurves.RelativeEntropySurprise', 'AddTimeCurves.Custom']\n",
    "[f'AddMatplotlibPlot.{k}' for k in add_renderables_menu['AddMatplotlibPlot']] # ['AddMatplotlibPlot.DecodedPosition', 'AddMatplotlibPlot.Custom']\n",
    "[f'Clear.{k}' for k in add_renderables_menu['Clear']] # ['Clear.all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13876513",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out = curr_active_pipeline.display(display_function='_display_trial_to_trial_reliability', active_session_configuration_context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac20997",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "win = _out.root_render_widget\n",
    "# Set column stretches to adjust column widths\n",
    "# win.ci.setColumnStretch(0, 5)  # First column, stretch factor of 5\n",
    "# win.ci.setColumnStretch(1, 5)  # Second column, stretch factor of 5\n",
    "# win.ci.setColumnStretch(6, 1)  # Last column, stretch factor of 1 (smaller width)\n",
    "\n",
    "max_col_idx: int = 5\n",
    "# for i in np.arange(max_col_idx+1):\n",
    "# \twin.ci.layout.setColumnPreferredWidth(i, 250) # larger\n",
    "win.ci.layout.setColumnPreferredWidth(max_col_idx, 5)   # Last column width (smaller)\n",
    "win.ci.layout.setColumnFixedWidth(max_col_idx, 5)\n",
    "win.ci.layout.setColumnMaximumWidth(max_col_idx, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c762d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a label item for the footer\n",
    "footer = pg.LabelItem(justify='center')\n",
    "footer.setText('Footer Text Here')\n",
    "\n",
    "# Add the footer label below the plot\n",
    "win.addItem(footer, row=2, col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2bb17",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print_keys_if_possible('add_renderables_menu', add_renderables_menu, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a70360",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "spike_raster_window.build_epoch_intervals_visual_configs_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41f353",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Downsample the preview background scroller for more fluid scrolling? Or is that not the problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab62cd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Disconnect the connection to see if that's what lagging out the scrolling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571368d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "spike_raster_window.connection_man.active_connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce9f36",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_2d_plot.rate_limited_signal_scrolled_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e77367",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_2d_plot.enable_debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf6288",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Add the legends:\n",
    "legends_dict = active_2d_plot.build_or_update_all_epoch_interval_rect_legends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468776b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Remove the legends\n",
    "active_2d_plot.remove_all_epoch_interval_rect_legends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72734c24",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.epochs_plotting_mixins import EpochDisplayConfig, _get_default_epoch_configs\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.EpochRenderConfigWidget.EpochRenderConfigWidget import EpochRenderConfigWidget, EpochRenderConfigsListWidget\n",
    "\n",
    "## Build right-sidebar epoch interval configs widget:\n",
    "spike_raster_window.build_epoch_intervals_visual_configs_widget()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702de2a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" `Plotted Rects` -> `configs widget`\"\"\" \n",
    "active_2d_plot.build_or_update_epoch_render_configs_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25608e70",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Update plots from configs:\n",
    "#     configs widget -> `Plotted Rects` \n",
    "active_2d_plot.update_epochs_from_configs_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd250fb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "an_epochs_display_list_widget = active_2d_plot.ui['epochs_render_configs_widget']\n",
    "_out_configs = deepcopy(an_epochs_display_list_widget.configs_from_states())\n",
    "_out_configs\n",
    "\n",
    "# {'diba_evt_file': EpochDisplayConfig(brush_color='#008000', brush_opacity=0.7843137254901961, desired_height_ratio=1.0, height=10.0, isVisible=True, name='diba_evt_file', pen_color='#008000', pen_opacity=0.6078431372549019, y_location=-52.0),\n",
    "#  'initial_loaded': EpochDisplayConfig(brush_color='#ffffff', brush_opacity=0.7843137254901961, desired_height_ratio=1.0, height=10.0, isVisible=True, name='initial_loaded', pen_color='#ffffff', pen_opacity=0.6078431372549019, y_location=-42.0),\n",
    "#  'PBEs': EpochDisplayConfig(brush_color='#aa55ff', brush_opacity=0.7843137254901961, desired_height_ratio=1.0, height=10.0, isVisible=True, name='PBEs', pen_color='#aaaaff', pen_opacity=0.6078431372549019, y_location=-32.0),\n",
    "#  'Ripples': EpochDisplayConfig(brush_color='#0000ff', brush_opacity=0.7843137254901961, desired_height_ratio=1.0, height=10.0, isVisible=True, name='Ripples', pen_color='#0000ff', pen_opacity=0.6078431372549019, y_location=-22.0),\n",
    "#  'Laps': EpochDisplayConfig(brush_color='#ff0000', brush_opacity=0.7843137254901961, desired_height_ratio=1.0, height=10.0, isVisible=True, name='Laps', pen_color='#ff0000', pen_opacity=0.6078431372549019, y_location=-12.0),\n",
    "#  'normal_computed': EpochDisplayConfig(brush_color='#800080', brush_opacity=0.7843137254901961, desired_height_ratio=1.0, height=10.0, isVisible=True, name='normal_computed', pen_color='#800080', pen_opacity=0.6078431372549019, y_location=-62.0),\n",
    "#  'diba_quiescent_method_replay_epochs': EpochDisplayConfig(brush_color='#ffa500', brush_opacity=0.7843137254901961, desired_height_ratio=1.0, height=10.0, isVisible=True, name='diba_quiescent_method_replay_epochs', pen_color='#ffa500', pen_opacity=0.6078431372549019, y_location=-72.0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86465b06",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "update_dict = {k:v.to_dict() for k, v in _out_configs.items()}\n",
    "update_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187827b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "def _on_update_rendered_intervals(active_2d_plot):\n",
    "    print(f'_on_update_rendered_intervals(...)')\n",
    "    _legends_dict = active_2d_plot.build_or_update_all_epoch_interval_rect_legends()\n",
    "    epoch_display_configs = active_2d_plot.extract_interval_display_config_lists()\n",
    "    an_epochs_display_list_widget = active_2d_plot.ui.get('epochs_render_configs_widget', None)\n",
    "    if an_epochs_display_list_widget is None:\n",
    "        # create a new one:    \n",
    "        an_epochs_display_list_widget:EpochRenderConfigsListWidget = EpochRenderConfigsListWidget(epoch_display_configs, parent=a_layout_widget)\n",
    "        active_2d_plot.ui.epochs_render_configs_widget = an_epochs_display_list_widget\n",
    "    else:\n",
    "        an_epochs_display_list_widget.update_from_configs(configs=epoch_display_configs)\n",
    "\n",
    "_a_connection = active_2d_plot.sigRenderedIntervalsListChanged.connect(_on_update_rendered_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3096fc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import EpochRenderingMixin\n",
    "\n",
    "# @function_attributes(short_name=None, tags=['epoch_intervals', 'layout', 'update', 'IMPORTANT'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-07-03 05:21', related_items=[])\n",
    "def rebuild_epoch_interval_layouts_given_normalized_heights(active_2d_plot, desired_epoch_render_stack_height:float=70.0):\n",
    "    \"\"\" Re-builds the stacked epoch layout to prevent them from overlapping and to normalize their height\n",
    "    \n",
    "    desired_epoch_render_stack_height: total height for all of the epochs\n",
    "    \n",
    "    \"\"\"\n",
    "    from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import EpochRenderingMixin\n",
    "    active_epochs_formatting_dict = active_2d_plot.extract_interval_display_config_lists() ## gets existing formatting dict\n",
    "\n",
    "    # extracts only the height, considers only the first config if the entry is a list:\n",
    "    # original_epoch_display_config_heights = {k:v[0].to_dict()['height'] for k, v in active_epochs_formatting_dict.items()} # {'Replays': 1.9, 'Laps': 0.9, 'diba_evt_file': 10.0, 'initial_loaded': 10.0, 'diba_quiescent_method_replay_epochs': 10.0, 'Ripples': 0.9, 'normal_computed': 10.0}\n",
    "    # original_epoch_display_config_heights ## original heights\n",
    "    required_vertical_offsets, required_interval_heights = EpochRenderingMixin.build_stacked_epoch_layout((len(active_epochs_formatting_dict) * [1.0]), epoch_render_stack_height=desired_epoch_render_stack_height, interval_stack_location='below') # ratio of heights to each interval\n",
    "    stacked_epoch_layout_dict = {interval_key:dict(y_location=y_location, height=height) for interval_key, y_location, height in zip(list(active_epochs_formatting_dict.keys()), required_vertical_offsets, required_interval_heights)} # Build a stacked_epoch_layout_dict to update the display\n",
    "    # stacked_epoch_layout_dict # {'LapsAll': {'y_location': -3.6363636363636367, 'height': 3.6363636363636367}, 'LapsTrain': {'y_location': -21.818181818181817, 'height': 18.18181818181818}, 'LapsTest': {'y_location': -40.0, 'height': 18.18181818181818}}\n",
    "    # stacked_epoch_layout_dict\n",
    "\n",
    "    # replaces 'y_location', 'position' for each dict:\n",
    "    update_dict = {k:(v[0].to_dict()|stacked_epoch_layout_dict[k]) for k, v in active_epochs_formatting_dict.items()} # builds a proper update dict from the `active_epochs_formatting_dict` and the new position and height adjustments\n",
    "    # update_dict\n",
    "    active_2d_plot.update_rendered_intervals_visualization_properties(update_dict=update_dict)\n",
    "\n",
    "rebuild_epoch_interval_layouts_given_normalized_heights(active_2d_plot, desired_epoch_render_stack_height=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd03f18",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# epoch_display_configs = {k:get_dict_subset(v[0].to_dict(), ['height', 'y_location']) for k, v in active_2d_plot.extract_interval_display_config_lists().items()}\n",
    "# epoch_display_configs\n",
    "\n",
    "## Re-build the stacked epochs to prevent them from overlapping:\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import EpochRenderingMixin\n",
    "\n",
    "\n",
    "active_epochs_formatting_dict = active_2d_plot.extract_interval_display_config_lists()\n",
    "\n",
    "epoch_display_config_heights = {k:v[0].to_dict()['height'] for k, v in active_epochs_formatting_dict.items()} # {'Replays': 1.9, 'Laps': 0.9, 'diba_evt_file': 10.0, 'initial_loaded': 10.0, 'diba_quiescent_method_replay_epochs': 10.0, 'Ripples': 0.9, 'normal_computed': 10.0}\n",
    "epoch_display_config_heights\n",
    "required_vertical_offsets, required_interval_heights = EpochRenderingMixin.build_stacked_epoch_layout((len(active_epochs_formatting_dict) * [1.0]), epoch_render_stack_height=70.0, interval_stack_location='below') # ratio of heights to each interval\n",
    "stacked_epoch_layout_dict = {interval_key:dict(y_location=y_location, height=height) for interval_key, y_location, height in zip(list(active_epochs_formatting_dict.keys()), required_vertical_offsets, required_interval_heights)} # Build a stacked_epoch_layout_dict to update the display\n",
    "# stacked_epoch_layout_dict # {'LapsAll': {'y_location': -3.6363636363636367, 'height': 3.6363636363636367}, 'LapsTrain': {'y_location': -21.818181818181817, 'height': 18.18181818181818}, 'LapsTest': {'y_location': -40.0, 'height': 18.18181818181818}}\n",
    "# stacked_epoch_layout_dict\n",
    "\n",
    "# replaces 'y_location', 'position' for each dict:\n",
    "update_dict = {k:(v[0].to_dict()|stacked_epoch_layout_dict[k]) for k, v in active_epochs_formatting_dict.items()}\n",
    "update_dict\n",
    "\n",
    "\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties(update_dict=update_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c93e8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Extract/Save all active epochs:\n",
    "active_epochs_formatting_dict: Dict[str, List[EpochDisplayConfig]] = deepcopy(active_2d_plot.extract_interval_display_config_lists())\n",
    "active_epochs_formatting_dict\n",
    "\n",
    "# an_epochs_display_list_widget.configs_from_states()\n",
    "\n",
    "\n",
    "an_epochs_display_list_widget = active_2d_plot.ui.get('epochs_render_configs_widget', None)\n",
    "if an_epochs_display_list_widget is None:\n",
    "    raise NotImplementedError\n",
    "    # create a new one:    \n",
    "    an_epochs_display_list_widget:EpochRenderConfigsListWidget = EpochRenderConfigsListWidget(active_epochs_formatting_dict, parent=a_layout_widget)\n",
    "    active_2d_plot.ui.epochs_render_configs_widget = an_epochs_display_list_widget\n",
    "else:\n",
    "    an_epochs_display_list_widget.update_from_configs(configs=active_epochs_formatting_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad3bd7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_epochs_confgs_dict: Dict[str, EpochDisplayConfig] = deepcopy(an_epochs_display_list_widget.configs_from_states())\n",
    "active_epochs_confgs_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f93d7e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "saveData('SpikeRaster2D_saved_Epochs.pkl', active_epochs_confgs_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d679d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_epochs_formatting_dict['Replays'][0].brush_QColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7318fbb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Restore/Load all active epochs:\n",
    "# update_dict = {k:(v[0].to_dict()|stacked_epoch_layout_dict[k]) for k, v in active_epochs_formatting_dict.items()}\n",
    "\n",
    "update_dict = {k:v.to_dict() for k, v in active_epochs_confgs_dict.items()} ## from active_epochs_confgs_dict\n",
    "update_dict\n",
    "\n",
    "## Updates intervals themselves\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties(update_dict=update_dict)\n",
    "\n",
    "## updates configs:\n",
    "# active_2d_plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673515f9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out_all_rendered_intervals_dict = active_2d_plot.get_all_rendered_intervals_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1292a3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_epochs_interval_datasources_dict: Dict[str, IntervalsDatasource] = active_2d_plot.interval_datasources\n",
    "active_epochs_interval_datasources_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58bafd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "rendered_epoch_names = active_2d_plot.interval_datasource_names\n",
    "print(f'rendered_epoch_names: {rendered_epoch_names}')\n",
    "for a_name in rendered_epoch_names:\n",
    "    a_render_container = active_2d_plot.rendered_epochs[a_name]\n",
    "    out_dict[a_name] = a_render_container\n",
    "\n",
    "out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e0ccf",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "main_plot_widget.setVisible(False) ## top plot disappeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfcbfd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "main_plot_widget.setVisible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c7a9b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Find Connections\n",
    "active_2d_plot.setVisible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdaaca1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# active_2d_plot.get_all_rendered_intervals_dict()\n",
    "active_2d_plot.interval_datasources\n",
    "# active_2d_plot.interval_rendering_plots\n",
    "active_2d_plot.interval_datasource_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaca430",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_2d_plot.setVisible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1badb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "spike_raster_window.isVisible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d558a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_Epoch, Epoch, ensure_dataframe\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.Specific2DRenderTimeEpochs import General2DRenderTimeEpochs, inline_mkColor\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.RenderTimeEpochs.EpochRenderingMixin import EpochRenderingMixin, RenderedEpochsItemsContainer\n",
    "from pyphoplacecellanalysis.General.Model.Datasources.IntervalDatasource import IntervalsDatasource\n",
    "from neuropy.utils.mixins.time_slicing import TimeColumnAliasesProtocol\n",
    "\n",
    "## Add various replay epochs as interval rects:\n",
    "\n",
    "## INPUTS: replay_epoch_variations\n",
    "\n",
    "# replay_epoch_variations\n",
    "\n",
    "\n",
    "## Use the three dataframes as separate Epoch series:\n",
    "custom_replay_dfs_dict = {k:ensure_dataframe(deepcopy(v)) for k, v in replay_epoch_variations.items()}\n",
    "custom_replay_keys = list(custom_replay_dfs_dict.keys()) # \n",
    "print(f'{custom_replay_keys}') # ['initial_loaded', 'normal_computed', 'diba_evt_file', 'diba_quiescent_method_replay_epochs']\n",
    "\n",
    "\n",
    "_color_rotation_order = ['white', 'purple', 'green', 'orange', 'pink', 'red']\n",
    "\n",
    "custom_replay_epochs_formatting_dict = {\n",
    "    'initial_loaded':dict(pen_color=inline_mkColor('white', 0.8), brush_color=inline_mkColor('white', 0.5)),\n",
    "    'normal_computed':dict(pen_color=inline_mkColor('purple', 0.8), brush_color=inline_mkColor('purple', 0.5)),\n",
    "    'diba_evt_file':dict(pen_color=inline_mkColor('green', 0.8), brush_color=inline_mkColor('green', 0.5)),\n",
    "    'diba_quiescent_method_replay_epochs':dict(pen_color=inline_mkColor('orange', 0.8), brush_color=inline_mkColor('orange', 0.5)),\n",
    "}\n",
    "\n",
    "# required_vertical_offsets, required_interval_heights = EpochRenderingMixin.build_stacked_epoch_layout((len(custom_replay_dfs_dict) * [1.0]), epoch_render_stack_height=40.0, interval_stack_location='below') # ratio of heights to each interval\n",
    "# stacked_epoch_layout_dict = {interval_key:dict(y_location=y_location, height=height) for interval_key, y_location, height in zip(list(custom_replay_epochs_formatting_dict.keys()), required_vertical_offsets, required_interval_heights)} # Build a stacked_epoch_layout_dict to update the display\n",
    "stacked_epoch_layout_dict = {interval_key:dict(y_location=y_location, height=height) for interval_key, y_location, height in zip(list(custom_replay_epochs_formatting_dict.keys()), *EpochRenderingMixin.build_stacked_epoch_layout((len(custom_replay_dfs_dict) * [1.0]), epoch_render_stack_height=40.0, interval_stack_location='below'))} # Build a stacked_epoch_layout_dict to update the display\n",
    "# replaces 'y_location', 'position' for each dict:\n",
    "custom_replay_epochs_formatting_dict = {k:(v|stacked_epoch_layout_dict[k]) for k, v in custom_replay_epochs_formatting_dict.items()}\n",
    "# custom_replay_epochs_formatting_dict\n",
    "\n",
    "# OUTPUTS: train_test_split_laps_dfs_dict, custom_replay_epochs_formatting_dict\n",
    "## INPUTS: train_test_split_laps_dfs_dict\n",
    "custom_replay_dfs_dict = {k:TimeColumnAliasesProtocol.renaming_synonym_columns_if_needed(df=v, required_columns_synonym_dict=IntervalsDatasource._time_column_name_synonyms) for k, v in custom_replay_dfs_dict.items()}\n",
    "\n",
    "## Build interval datasources for them:\n",
    "custom_replay_dfs_datasources_dict = {k:General2DRenderTimeEpochs.build_render_time_epochs_datasource(v) for k, v in custom_replay_dfs_dict.items()}\n",
    "## INPUTS: active_2d_plot, train_test_split_laps_epochs_formatting_dict, train_test_split_laps_dfs_datasources_dict\n",
    "assert len(custom_replay_epochs_formatting_dict) == len(custom_replay_dfs_datasources_dict)\n",
    "for k, an_interval_ds in custom_replay_dfs_datasources_dict.items():\n",
    "    an_interval_ds.update_visualization_properties(lambda active_df, **kwargs: General2DRenderTimeEpochs._update_df_visualization_columns(active_df, **(custom_replay_epochs_formatting_dict[k] | kwargs)))\n",
    "\n",
    "\n",
    "## Full output: train_test_split_laps_dfs_datasources_dict\n",
    "\n",
    "\n",
    "# actually add the epochs:\n",
    "for k, an_interval_ds in custom_replay_dfs_datasources_dict.items():\n",
    "    active_2d_plot.add_rendered_intervals(an_interval_ds, name=f'{k}', debug_print=False) # adds the interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34342d9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_2d_plot.params.enable_time_interval_legend_in_right_margin = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51663689",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## They can later be updated via:\n",
    "active_2d_plot.update_rendered_intervals_visualization_properties(custom_replay_epochs_formatting_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073b9ae",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# new_replay_epochs.to_file('new_replays.csv')\n",
    "new_replay_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680b0e1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "rank_order_results.minimum_inclusion_fr_Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7f309",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "track_templates.long_LR_decoder.neuron_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e8b3d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a672c4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg # Used to get the app for TopLevelWindowHelper.top_level_windows\n",
    "## For searching with `TopLevelWindowHelper.all_widgets(...)`:\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike3DRaster import Spike3DRaster\n",
    "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n",
    "\n",
    "found_spike_raster_windows = TopLevelWindowHelper.all_widgets(pg.mkQApp(), searchType=Spike3DRasterWindowWidget)\n",
    "\n",
    "if len(found_spike_raster_windows) < 1:\n",
    "    # no existing spike_raster_windows. Make a new one\n",
    "    print(f'no existing SpikeRasterWindow. Creating a new one.')\n",
    "    # Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "    active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "else:\n",
    "    print(f'found {len(found_spike_raster_windows)} existing Spike3DRasterWindowWidget windows using TopLevelWindowHelper.all_widgets(...). Will use the most recent.')\n",
    "    # assert len(found_spike_raster_windows) == 1, f\"found {len(found_spike_raster_windows)} Spike3DRasterWindowWidget windows using TopLevelWindowHelper.all_widgets(...) but require exactly one.\"\n",
    "    # Get the most recent existing one and reuse that:\n",
    "    spike_raster_window = found_spike_raster_windows[0]\n",
    "\n",
    "\n",
    "# Extras:\n",
    "active_2d_plot = spike_raster_window.spike_raster_plt_2d # <pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster.Spike2DRaster at 0x196c7244280>\n",
    "active_3d_plot = spike_raster_window.spike_raster_plt_3d # <pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster.Spike2DRaster at 0x196c7244280>\n",
    "main_graphics_layout_widget = active_2d_plot.ui.main_graphics_layout_widget # GraphicsLayoutWidget\n",
    "main_plot_widget = active_2d_plot.plots.main_plot_widget # PlotItem\n",
    "background_static_scroll_plot_widget = active_2d_plot.plots.background_static_scroll_window_plot # PlotItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2f20e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_display_items = widget.get_display_function_items()\n",
    "_display_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df0c9d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_fcn_name = '_display_batch_pho_jonathan_replay_firing_rate_comparison'\n",
    "a_fn_handle = widget._perform_get_display_function_code(a_fcn_name=a_fcn_name)\n",
    "assert a_fn_handle is not None\n",
    "# args = []\n",
    "# kwargs = {}\n",
    "a_disp_fn_item = widget.get_display_function_item(a_fn_name=a_fcn_name)\n",
    "assert a_disp_fn_item is not None, f\"a_disp_fn_item is None! for a_fn_name='{a_fcn_name}'\"\n",
    "\n",
    "a_disp_fn_item.is_global\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91ed7b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display(display_function=a_fcn_name, active_session_configuration_context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d0cff",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "long_short_display_config_manager = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4fb13",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_display_spike_rasters_pyqtplot_3D_with_2D_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c70b4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print(list(_display_items.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29925698",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import FigureCollector\n",
    "from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import fig_remapping_cells\n",
    "\n",
    "collector: FigureCollector = fig_remapping_cells(curr_active_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606a58c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "if not isinstance(curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis, JonathanFiringRateAnalysisResult):\n",
    "    jonathan_firing_rate_analysis_result = JonathanFiringRateAnalysisResult(**curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis.to_dict())\n",
    "else:\n",
    "    jonathan_firing_rate_analysis_result = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "\n",
    "neuron_replay_stats_df = jonathan_firing_rate_analysis_result.neuron_replay_stats_df.copy()\n",
    "neuron_replay_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b348a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_sorted_neuron_stats_df = neuron_replay_stats_df.sort_values(by=sortby, ascending=[True, True, True], inplace=False).copy() # also did test_df = neuron_replay_stats_df.sort_values(by=['long_pf_peak_x'], inplace=False, ascending=True).copy()\n",
    "_sorted_neuron_stats_df = _sorted_neuron_stats_df[np.isin(_sorted_neuron_stats_df.index, curr_any_context_neurons)] # clip to only those neurons included in `curr_any_context_neurons`\n",
    "_sorted_aclus = _sorted_neuron_stats_df.index.to_numpy()\n",
    "_sorted_neuron_IDXs = _sorted_neuron_stats_df.neuron_IDX.to_numpy()\n",
    "if debug_print:\n",
    "    print(f'_sorted_aclus: {_sorted_aclus}')\n",
    "    print(f'_sorted_neuron_IDXs: {_sorted_neuron_IDXs}')\n",
    "\n",
    "## Use this sort for the 'curr_any_context_neurons' sort order:\n",
    "new_all_aclus_sort_indicies, desired_sort_arr = find_desired_sort_indicies(curr_any_context_neurons, _sorted_aclus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2fa1dc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# _directional_laps_overview = curr_active_pipeline.plot._display_directional_laps_overview(curr_active_pipeline.computation_results, a)\n",
    "# _directional_laps_overview = curr_active_pipeline.display('_display_directional_laps_overview')\n",
    "# _directional_laps_overview = curr_active_pipeline.display('_display_grid_bin_bounds_validation')\n",
    "_directional_laps_overview = curr_active_pipeline.display('_display_long_short_pf1D_comparison')\n",
    "\n",
    "_directional_laps_overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9992e42",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### ✅🖼️🎨 2024-06-06 - Works to render the contour curve at a fixed promenence (the shape of the placefield's cap/crest) for each placefield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff6d67",
   "metadata": {
    "tags": [
     "3d",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.peak_prominences import render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter\n",
    "\n",
    "display_output = {}\n",
    "active_config_name = long_LR_name\n",
    "print(f'active_config_name: {active_config_name}')\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "pActiveTuningCurvesPlotter = None\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "active_config_modifiying_kwargs = {\n",
    "    'plotting_config': {'should_use_linear_track_geometry': True, \n",
    "                        't_start': t_start, 't_delta': t_delta, 't_end': t_end,\n",
    "                        }\n",
    "}\n",
    "display_output = display_output | curr_active_pipeline.display('_display_3d_interactive_tuning_curves_plotter', active_config_name, extant_plotter=display_output.get('pActiveTuningCurvesPlotter', None),\n",
    "                                                panel_controls_mode='Qt', should_nan_non_visited_elements=False, zScalingFactor=2000.0, active_config_modifiying_kwargs=active_config_modifiying_kwargs,\n",
    "                                                params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                            ) # Works now!\n",
    "ipcDataExplorer = display_output['ipcDataExplorer']\n",
    "display_output['pActiveTuningCurvesPlotter'] = display_output.pop('plotter') # rename the key from the generic \"plotter\" to \"pActiveSpikesBehaviorPlotter\" to avoid collisions with others\n",
    "pActiveTuningCurvesPlotter = display_output['pActiveTuningCurvesPlotter']\n",
    "root_dockAreaWindow, placefieldControlsContainerWidget, pf_widgets = display_output['pane'] # for Qt mode\n",
    "\n",
    "active_peak_prominence_2d_results = curr_active_pipeline.computation_results[active_config_name].computed_data.get('RatemapPeaksAnalysis', {}).get('PeakProminence2D', None)\n",
    "render_all_neuron_peak_prominence_2d_results_on_pyvista_plotter(ipcDataExplorer, active_peak_prominence_2d_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecde7d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### 2024-06-06 - Works to disable/hide all elements except the contour curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ff05b",
   "metadata": {
    "tags": [
     "3d",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "all_placefield_surfaces_are_hidden: bool = True\n",
    "all_placefield_points_are_hidden: bool = True\n",
    "\n",
    "disabled_peak_subactors_names_list = ['boxes', 'text', 'peak_points']\n",
    "# disabled_peak_subactors_names_list = ['text', 'peak_points']\n",
    "for active_neuron_id, a_plot_dict in ipcDataExplorer.plots['tuningCurvePlotActors'].items():\n",
    "    if a_plot_dict is not None:\n",
    "        # a_plot_dict.peaks\n",
    "        print(f'active_neuron_id: {active_neuron_id}, a_plot_dict.keys(): {list(a_plot_dict.keys())}')\n",
    "        # ['main', 'points', 'peaks']\n",
    "        if a_plot_dict.main is not None:\n",
    "            if all_placefield_surfaces_are_hidden:\n",
    "                a_plot_dict.main.SetVisibility(False)\n",
    "                # pass\n",
    "            \n",
    "        if a_plot_dict.points is not None:\n",
    "            if all_placefield_points_are_hidden:\n",
    "                a_plot_dict.points.SetVisibility(False)\n",
    "                # pass\n",
    "\n",
    "        if a_plot_dict.peaks is not None:\n",
    "            print(f'active_neuron_id: {active_neuron_id}, a_plot_dict.peaks: {list(a_plot_dict.peaks.keys())}')\n",
    "            for a_subactor_name in disabled_peak_subactors_names_list:\n",
    "                a_subactor = a_plot_dict.peaks.get(a_subactor_name, None)\n",
    "                if a_subactor is not None:\n",
    "                    a_subactor.SetVisibility(False)\n",
    "            # if all_placefield_surfaces_are_hidden:\n",
    "            #     a_plot_dict.main.SetVisibility(False) # Change the visibility to match the current tuning_curve_visibility_state\n",
    "\n",
    "# Once done, render\n",
    "ipcDataExplorer.p.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7be129",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "graphics_output_dict = curr_active_pipeline.display('_display_long_short_laps')\n",
    "graphics_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedac3f3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs, plot_data = graphics_output_dict['fig'], graphics_output_dict['axs'], graphics_output_dict['plot_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb3e9c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_display_grid_bin_bounds_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68008d6e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_long_short_laps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685113bf",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a new `SpikeRaster2D` instance using `_display_spike_raster_pyqtplot_2D` and capture its outputs:\n",
    "# active_2d_plot, active_3d_plot, spike_raster_window = curr_active_pipeline.plot._display_spike_rasters_pyqtplot_2D()\n",
    "\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_spike_rasters_pyqtplot_2D', 'maze_any') # 'maze_any'\n",
    "assert isinstance(_out_graphics_dict, dict)\n",
    "active_2d_plot, active_3d_plot, spike_raster_window = _out_graphics_dict['spike_raster_plt_2d'], _out_graphics_dict['spike_raster_plt_3d'], _out_graphics_dict['spike_raster_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3db27",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "add_renderables_menu = active_2d_plot.ui.menus.custom_context_menus.add_renderables[0].programmatic_actions_dict\n",
    "menu_commands = ['AddTimeIntervals.PBEs', 'AddTimeIntervals.Ripples', 'AddTimeIntervals.Replays', 'AddTimeIntervals.Laps', 'AddTimeIntervals.SessionEpochs']\n",
    "for a_command in menu_commands:\n",
    "    add_renderables_menu[a_command].trigger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5650ac",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print(list(add_renderables_menu.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a83aa",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print_keys_if_possible('add_renderables_menu', add_renderables_menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbad2e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# 3d_interactive_tuning_curves_plotter\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "active_config_modifiying_kwargs = {\n",
    "    'plotting_config': {'should_use_linear_track_geometry': True, \n",
    "                        't_start': t_start, 't_delta': t_delta, 't_end': t_end,\n",
    "                        }\n",
    "}\n",
    "_out_graphics_dict = curr_active_pipeline.display('_display_3d_interactive_tuning_curves_plotter', active_session_configuration_context=global_epoch_context,\n",
    "                                            active_config_modifiying_kwargs=active_config_modifiying_kwargs,\n",
    "                                            params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                           )\n",
    "ipcDataExplorer = _out_graphics_dict['ipcDataExplorer'] # InteractivePlaceCellTuningCurvesDataExplorer \n",
    "p = _out_graphics_dict['plotter']\n",
    "pane = _out_graphics_dict['pane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16e010",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_spike_and_behavior_browser', active_session_configuration_context=global_epoch_context) # , computation_kwargs_list=[{'laps_decoding_time_bin_size': 0.025}]\n",
    "ipspikesDataExplorer = _out['ipspikesDataExplorer']\n",
    "p = _out['plotter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc5550",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "iplapsDataExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d3f4b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "\n",
    "an_image_file_path = Path('an_image.png').resolve()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_image_plotter', active_session_configuration_context=global_epoch_context, image_file=an_image_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ce93e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for a_name, a_config in curr_active_pipeline.active_configs.items():\n",
    "    print(f'a_config.plotting_config.should_use_linear_track_geometry: {a_config.plotting_config.should_use_linear_track_geometry}')\n",
    "    a_config.plotting_config.should_use_linear_track_geometry = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d560b4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.pyqtgraph_extensions.PlotWidget.CustomPlotWidget import CustomPlotWidget\n",
    "from pyphoplacecellanalysis.External.pyqtgraph_extensions.graphicsItems.SelectableTextItem import SelectableTextItem\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "\n",
    "_out: TemplateDebugger = TemplateDebugger.init_templates_debugger(track_templates) # , included_any_context_neuron_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out.get_selected_aclus(return_only_selected_aclus=True) # 'long_LR': [45, 24, 18, 35, 32], 'long_RL': [], 'short_LR': [], 'short_RL': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c64cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_win, an_img_item = _out.pf1D_heatmaps['long_LR']\n",
    "# an_img_item.sceneBoundingRect()\n",
    "# a_win.getViewBox()\n",
    "# an_img_item.getViewBox()\n",
    "# a_win.setObjectName()\n",
    "a_win.objectName()\n",
    "an_img_item.objectName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_pfs_ymin_ymax_tuple_list_dict = _out.plots_data.active_pfs_ymin_ymax_tuple_list_dict\n",
    "active_pfs_ymin_ymax_tuple_list_dict['long_LR']\n",
    "active_pfs_ymin_ymax_tuple_list_dict['long_RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_data.sorted_neuron_IDs_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8213de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.External.pyqtgraph.GraphicsScene.mouseEvents import MouseClickEvent\n",
    "\n",
    "# clicked plot 0x7bf9b6060040, event: <MouseClickEvent (176,249) button=1>\n",
    "# self.on_mouse_click(...)\n",
    "# \tevent: <MouseClickEvent (176,249) button=1>\n",
    "def custom_on_mouse_clicked(self, custom_plot_widget, event):\n",
    "    \n",
    "    debug_print: bool = False\n",
    "    if debug_print:\n",
    "        print(f'custom_on_mouse_clicked(event: {event})')\n",
    "    # if not isinstance(event, MouseClickEvent):\n",
    "    if not hasattr(event, 'scenePos'):\n",
    "        if debug_print:\n",
    "            print(f'not MouseClickEvent. skipping.')\n",
    "        return\n",
    "    else:    \n",
    "        pos = event.scenePos() # 'QMouseEvent' object has no attribute 'scenePos'\n",
    "\n",
    "        if debug_print:\n",
    "            print(f'\\tscenePos: {pos}')\n",
    "            print(f'\\tscreenPos: {event.screenPos()}')\n",
    "            print(f'\\tpos: {event.pos()}')\n",
    "            \n",
    "        item_data = custom_plot_widget.item_data\n",
    "        if debug_print:\n",
    "            print(f'\\titem_data: {item_data}')\n",
    "        found_decoder_idx = item_data.get('decoder_idx', None)\n",
    "        found_decoder_name = item_data.get('decoder_name', None)\n",
    "        \n",
    "        ## find the clicked decoder\n",
    "        # found_decoder_idx = None\n",
    "        # found_decoder_name = None\n",
    "        # for a_decoder_idx, (a_decoder_name, (a_win, an_img_item)) in enumerate(self.pf1D_heatmaps.items()):\n",
    "        #     if ((found_decoder_idx is None) and (found_decoder_name is None)):\n",
    "        #         if an_img_item.sceneBoundingRect().contains(pos):\n",
    "        #             ## found correct decoder here:\n",
    "        #             found_decoder_idx = a_decoder_idx\n",
    "        #             found_decoder_name = a_decoder_name\n",
    "                    \n",
    "        if ((found_decoder_idx is None) and (found_decoder_name is None)):\n",
    "            print(f'WARNING: could not find correct decoder name/idx')\n",
    "        else:\n",
    "            if debug_print:\n",
    "                print(f'found valid decoder: found_decoder_name: \"{found_decoder_name}\", found_decoder_idx\" {found_decoder_idx}')\n",
    "            a_win, an_img_item = self.pf1D_heatmaps[found_decoder_name]\n",
    "            mouse_point = a_win.getViewBox().mapSceneToView(pos)\n",
    "            if debug_print:\n",
    "                print(f\"Clicked at: x={mouse_point.x()}, y={mouse_point.y()}\")\n",
    "            found_y_point: float = mouse_point.y()\n",
    "            ## round down\n",
    "            found_y_idx: int = int(found_y_point)\n",
    "            if debug_print:\n",
    "                print(f'found_y_idx: {found_y_idx}')\n",
    "            found_aclu: int = self.plots_data.sorted_neuron_IDs_lists[found_decoder_idx][found_y_idx]\n",
    "            print(f'found_aclu: {found_aclu}')\n",
    "            prev_selected_aclus = self.get_any_decoder_selected_aclus().tolist()\n",
    "            # prev_selected_aclus\n",
    "            prev_selected_aclus.append(found_aclu)\n",
    "            self.set_selected_aclus_for_all_decoders(any_selected_aclus=prev_selected_aclus)\n",
    "\n",
    "    # a_win, an_img_item = self.pf1D_heatmaps['long_LR']\n",
    "    # plot = an_img_item\n",
    "    # if plot.sceneBoundingRect().contains(pos):\n",
    "    #     # mouse_point = plot.vb.mapSceneToView(pos)\n",
    "    #     mouse_point = a_win.getViewBox().mapSceneToView(pos)\n",
    "    #     if debug_print:\n",
    "    #         print(f\"Clicked at: x={mouse_point.x()}, y={mouse_point.y()}\")\n",
    "    #     found_y_point: float = mouse_point.y()\n",
    "    #     ## round down\n",
    "    #     found_y_idx: int = int(found_y_point)\n",
    "    #     print(f'found_y_idx: {found_y_idx}')\n",
    "    #     found_aclu: int = self.plots_data.sorted_neuron_IDs_lists[0][found_y_idx]\n",
    "    #     print(f'found_aclu: {found_aclu}')\n",
    "\n",
    "_out.params.on_mouse_clicked_callback_fn_dict = {\n",
    "'custom_on_mouse_clicked': custom_on_mouse_clicked,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18950a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_pfs_img_extents = _out.plots_data.active_pfs_img_extents_dict[a_decoder_name] # [37.0773897438341, 0, 213.87855429166422, 25.0] # these extents are  (x, y, w, h)\n",
    "x, y, w, h = active_pfs_img_extents\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf253d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out.get_any_decoder_selected_aclus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe6956",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_selected_aclus = _out.get_any_decoder_selected_aclus().tolist()\n",
    "prev_selected_aclus\n",
    "prev_selected_aclus.append(found_aclu)\n",
    "_out.set_selected_aclus_for_all_decoders(any_selected_aclus=[18, 24, 31, 32, 35, 45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "synchronize_selected_aclus_across_decoders: bool = _out.params.setdefault('synchronize_selected_aclus_across_decoders', True)\n",
    "synchronize_selected_aclus_across_decoders\n",
    "\n",
    "curr_selected_aclus_dict = _out.get_selected_aclus(return_only_selected_aclus=True) # 'long_LR': [45, 24, 18, 35, 32], 'long_RL': [], 'short_LR': [], 'short_RL': []}\n",
    "if synchronize_selected_aclus_across_decoders:\n",
    "    any_decoder_selectioned_aclus = union_of_arrays(*list(curr_selected_aclus_dict.values()))\n",
    "    any_decoder_selectioned_aclus\n",
    "    # for a_decoder_name, a_decoder_selections in curr_selected_aclus_dict.items():\n",
    "    #     # select missing selections\n",
    "    #     curr_missing_selections = any_decoder_selectioned_aclus[np.isin(any_decoder_selectioned_aclus, a_decoder_selections)]\n",
    "    #     curr_missing_selections\n",
    "        \n",
    "    # for a_decoder_name, a_text_items_dict in _out.ui.text_items_dict.items():\n",
    "    #     for aclu in any_decoder_selectioned_aclus:\n",
    "    #         a_text_item = a_text_items_dict.get(aclu, None)\n",
    "    #         if a_text_item is not None:\n",
    "    #             # set the selection\n",
    "    #             # a_text_item.is_selected = True\n",
    "    #             a_text_item.perform_update_selected(new_is_selected=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab600bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_pfs_img_extents = _out.plots_data.active_pfs_img_extents_dict[a_decoder_name] # [37.0773897438341, 0, 213.87855429166422, 25.0] # these extents are  (x, y, w, h)\n",
    "x, y, w, h = active_pfs_img_extents\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu = 7\n",
    "a_decoder_name = 'long_LR'\n",
    "text: SelectableTextItem = _out.ui.text_items_dict[a_decoder_name][aclu]\n",
    "active_pfs_img_extents = _out.plots_data.active_pfs_img_extents_dict[a_decoder_name] # [37.0773897438341, 0, 213.87855429166422, 25.0] # these extents are  (x, y, w, h)\n",
    "x, y, w, h = active_pfs_img_extents\n",
    "rect = text.boundingRect()\n",
    "rect\n",
    "# # text.getViewBox().boundingRect()\n",
    "# text.getBoundingParents()\n",
    "# text.anchor\n",
    "# text\n",
    "\n",
    "# # Get the bounding rectangle of the text\n",
    "# br = text.textItem.boundingRect()\n",
    "# br\n",
    "# # Calculate the position adjustment based on the anchor\n",
    "# anchor_x = -br.left() - br.width() * text.anchor[0]\n",
    "# anchor_y = -br.top() - br.height() * text.anchor[1]\n",
    "\n",
    "# anchor_x\n",
    "# anchor_y\n",
    "\n",
    "# # text.rect_item.setRect(text.boundingRect())\n",
    "# transformed_rect = text.mapRectToParent(text.boundingRect())\n",
    "# transformed_rect\n",
    "rect.setWidth(text.parentWidget().width())\n",
    "rect.setHeight(h)\n",
    "\n",
    "# test_rect = QRectF(-10.104885544514367, -0.3724311354808627, 9.104885544514367, 1.3724311354808627)\n",
    "rect\n",
    "# text.rect_item.setRect(transformed_rect)\n",
    "text.rect_item.setRect(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtCore import QRectF\n",
    "\n",
    "# test_rect = QRectF(-10.104885544514367, -0.3724311354808627, 9.104885544514367, 1.3724311354808627)\n",
    "# test_rect = QRectF(-14.0, 0.0, 14.0, 21.0) # looks good\n",
    "# test_rect = text.boundingRect()\n",
    "rect = text.boundingRect()\n",
    "# parent_test_rect = text.parentWidget().boundingRect()\n",
    "if text.parentWidget() is not None:\n",
    "    parent_width = text.parentWidget().width()\n",
    "    if parent_width > rect.width():\n",
    "        rect.setWidth(parent_width)\n",
    "        \n",
    "rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab736093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_rect = QRectF(0.0, 0.0, 14.0, 21.0) # misaligned\n",
    "# test_rect\n",
    "\n",
    "# test_rect.setWidth(100.0)\n",
    "test_rect.setWidth(text.parentWidget().width())\n",
    "test_rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272095b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.rect_item.setRect(rect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.rect_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out.update_cell_emphasis(solo_emphasized_aclus=[7, 40, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out._build_internal_callback_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = _out.ui.setdefault('connections', {})\n",
    "connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_on_mouse_clicked(event):\n",
    "    print(f'_test_on_mouse_clicked(...)')\n",
    "    print(f'\\tevent: {event}')\n",
    "    print('\\tend.')\n",
    "\n",
    "def _test_on_mouse_moved(pos):\n",
    "    print(f'_test_on_mouse_moved(pos: {pos})')\n",
    "    \n",
    "\n",
    "_out.params.on_mouse_clicked_callback_fn_dict = {'_test_on_mouse_clicked': _test_on_mouse_clicked}\n",
    "_out.params.on_mouse_moved_callback_fn_dict = {'_test_on_mouse_moved': _test_on_mouse_moved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be160b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out.params.on_mouse_clicked_callback_fn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_on_mouse_clicked(event):\n",
    "    print(f'_test_on_mouse_clicked(...)')\n",
    "    print(f'\\tevent: {event}')\n",
    "    print('\\tend.')\n",
    "\n",
    "def _test_on_mouse_moved(pos):\n",
    "    print(f'_test_on_mouse_moved(pos: {pos})')\n",
    "    \n",
    "\n",
    "# _connections = {}\n",
    "for a_decoder_name, (curr_win, curr_img) in _out.pf1D_heatmaps.items():\n",
    "    print(f'a_decoder_name: {a_decoder_name}')\n",
    "    print(f'\\t curr_win: {curr_win}')\n",
    "    print(f'\\t curr_img: {curr_img}')\n",
    "    curr_win.sigMouseClicked.connect(_out.on_mouse_click)\n",
    "    view_box: pg.ViewBox = curr_win.getViewBox()\n",
    "    print(f'\\t view_box: {view_box}')\n",
    "    a_scene: pg.GraphicsScene = view_box.scene()\n",
    "    print(f'\\t a_scene: {a_scene}')\n",
    "    a_scene.setClickRadius(4.0)\n",
    "    # _connections[a_decoder_name] = view_box.scene().sigMouseClicked.connect(_test_on_mouse_clicked)\n",
    "    # _connections[a_decoder_name] = a_scene.sigMouseClicked.connect(_test_on_mouse_clicked)\n",
    "    # _connections[a_decoder_name] = \n",
    "    # a_scene.sigMouseClicked.connect(_test_on_mouse_clicked)\n",
    "    # view_box.scene().sigMouseMoved.connect(_test_on_mouse_moved)\n",
    "    print(f'\\t \"{a_decoder_name}\" connections done.')\n",
    "    \n",
    "    # view_box.scene().sigSceneMousePress.connect(_test_on_SceneMousePress)\n",
    "    # view_box.scene().sigMousePressed.connect(lambda event: print(f'Mouse pressed at: {event.scenePos()}'))\n",
    "\n",
    "print(f'all connections done.')\n",
    "# a_scene.setClickRadius(4.0)\n",
    "# a_scene.sigMouseClicked\n",
    "# a_scene.sigMouseMoved\n",
    "# a_scene.sigMouseHover\n",
    "\n",
    "# GraphicsView:\n",
    "# sigSceneMouseMoved\n",
    "# sigMouseReleased\n",
    "\n",
    "# PlotWidget(GraphicsView)\n",
    "# sigRangeChanged = QtCore.Signal(object, object)\n",
    "# sigTransformChanged = QtCore.Signal(object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in _connections.items():\n",
    "    v.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f91c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_ui.order_location_lines_dict[a_decoder_name][aclu] # Dict[types.DecoderName, Dict[types.aclu, pg.TextItem]]\n",
    "\n",
    "_out.pf1D_heatmaps\n",
    "\n",
    "# Dict[types.DecoderName, Tuple[pg.PlotWidget, pg.ImageItem]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4757c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791edb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1323cc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "\n",
    "\n",
    "_out = batch_perform_all_plots(curr_active_pipeline=curr_active_pipeline, enable_neptune=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d9015",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample 2D matrix\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import pv\n",
    "\n",
    "matrix = np.random.rand(10, 10)\n",
    "\n",
    "# Coordinates\n",
    "x, y = np.meshgrid(np.arange(matrix.shape[1]), np.arange(matrix.shape[0]))\n",
    "z = matrix.flatten()\n",
    "\n",
    "# Colors based on recency of updates (for example purposes, random values)\n",
    "colors = np.random.rand(matrix.size)\n",
    "\n",
    "# Create the plotter\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "# Add points (dots)\n",
    "points = np.column_stack((x.flatten(), y.flatten(), z))\n",
    "point_cloud = pv.PolyData(points)\n",
    "point_cloud['colors'] = colors\n",
    "plotter.add_mesh(point_cloud, render_points_as_spheres=True, point_size=10, scalars='colors', cmap='viridis')\n",
    "\n",
    "# Add stems\n",
    "for i in range(len(z)):\n",
    "    line = pv.Line([x.flatten()[i], y.flatten()[i], 0], [x.flatten()[i], y.flatten()[i], z[i]])\n",
    "    plotter.add_mesh(line, color='black')\n",
    "\n",
    "# Show plot\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23611eac",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot.display_function_items\n",
    "\n",
    "# '_display_directional_template_debugger'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d9ea5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b60f6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.prepare_for_display()\n",
    "directional_laps_overview = curr_active_pipeline.display(display_function='_display_directional_laps_overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ee6cb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_pic_placefields = curr_active_pipeline.display('_display_1d_placefields', long_LR_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867cad3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_pic_placefields_short_LR = curr_active_pipeline.display('_display_1d_placefields', short_LR_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c334080",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f93040",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59bafd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🖼️🎨 2024-02-28 - WE gotta see the replays on the 3D track. Or the 2D track.\n",
    "2024-04-28 - This is working in both 3D and 2D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ad549",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: directional_laps_results, global_replays, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "# global_pf1D\n",
    "# long_replays\n",
    "# direction_max_indices = ripple_all_epoch_bins_marginals_df[['P_Long', 'P_Short']].values.argmax(axis=1)\n",
    "# track_identity_max_indices = ripple_all_epoch_bins_marginals_df[['P_Long', 'P_Short']].values.argmax(axis=1)\n",
    "\n",
    "## How do I get the replays?\n",
    "# long_replay_df: pd.DataFrame = long_replays.to_dataframe() ## These work.\n",
    "# global_replay_df: pd.DataFrame = global_replays.to_dataframe() ## These work.\n",
    "# global_replay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e8e2b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## 1D version:\n",
    "## INPUTS: directional_laps_results, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "xbin = deepcopy(directional_laps_results.get_decoders()[0].xbin)\n",
    "xbin_centers = deepcopy(directional_laps_results.get_decoders()[0].xbin_centers)\n",
    "ybin_centers = None\n",
    "ybin = None\n",
    "\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_laps_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "## 1D:\n",
    "a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "\n",
    "## OUTPUTS: a_decoded_filter_epochs_decoder_result_dict, xbin_centers, ybin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cdc1d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## 2D version:\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _compute_lap_and_ripple_epochs_decoding_for_decoder\n",
    "\n",
    "## INPUTS: long_results, short_results\n",
    "# long_one_step_decoder_2D\n",
    "\n",
    "long_one_step_decoder_2D, short_one_step_decoder_2D  = [results_data.get('pf2D_Decoder', None) for results_data in (long_results, short_results)]\n",
    "one_step_decoder_dict_2D: Dict[str, BayesianPlacemapPositionDecoder] = dict(zip(('long', 'short'), (long_one_step_decoder_2D, short_one_step_decoder_2D)))\n",
    "long_pf2D = long_results.pf2D\n",
    "# short_pf2D = short_results.pf2D\n",
    "\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "xbin_centers = deepcopy(long_pf2D.xbin_centers)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "ybin_centers = deepcopy(long_pf2D.ybin_centers)\n",
    "\n",
    "## OUTPUTS: one_step_decoder_dict_2D, xbin_centers, ybin_centers\n",
    "\n",
    "## INPUTS: one_step_decoder_dict_2D\n",
    "\n",
    "# DirectionalMergedDecoders: Get the result after computation:\n",
    "directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "ripple_decoding_time_bin_size: float = directional_merged_decoders_result.ripple_decoding_time_bin_size\n",
    "laps_decoding_time_bin_size: float = directional_merged_decoders_result.laps_decoding_time_bin_size\n",
    "pos_bin_size: Tuple[float, float] = list(one_step_decoder_dict_2D.values())[0].pos_bin_size\n",
    "\n",
    "print(f'laps_decoding_time_bin_size: {laps_decoding_time_bin_size}, ripple_decoding_time_bin_size: {ripple_decoding_time_bin_size}, pos_bin_size: {pos_bin_size}')\n",
    "\n",
    "## Decode epochs for the two decoders ('long', 'short'):\n",
    "LS_decoder_laps_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {}\n",
    "LS_decoder_ripple_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {}\n",
    "\n",
    "for a_name, a_decoder in one_step_decoder_dict_2D.items():\n",
    "    LS_decoder_laps_filter_epochs_decoder_result_dict[a_name], LS_decoder_ripple_filter_epochs_decoder_result_dict[a_name] = _compute_lap_and_ripple_epochs_decoding_for_decoder(a_decoder, curr_active_pipeline, desired_laps_decoding_time_bin_size=laps_decoding_time_bin_size, desired_ripple_decoding_time_bin_size=ripple_decoding_time_bin_size)\n",
    "\n",
    "# LS_decoder_ripple_filter_epochs_decoder_result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591738c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## 2D:\n",
    "# Choose the ripple epochs to plot:\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(LS_decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long'] # 2D\n",
    "# Choose the laps epochs to plot:\n",
    "# a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(LS_decoder_laps_filter_epochs_decoder_result_dict)\n",
    "# a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "\n",
    "# a_result: DecodedFilterEpochsResult = LS_decoder_laps_filter_epochs_decoder_result_dict['long'] # 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_merged_decoders_result.perform_compute_marginals()\n",
    "directional_merged_decoders_result.laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd1b42",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryMatplotlibPlotter\n",
    "\n",
    "## INPUTS: a_result: DecodedFilterEpochsResult, an_epoch_idx: int = 18\n",
    "# e.g. `a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR']`\n",
    "\n",
    "# a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "\n",
    "## Convert to plottable posteriors\n",
    "# an_epoch_idx: int = 0\n",
    "\n",
    "# valid_aclus = deepcopy(decoder_aclu_peak_location_df_merged.aclu.unique())\n",
    "num_filter_epochs: int = a_result.num_filter_epochs\n",
    "a_decoded_traj_plotter = DecodedTrajectoryMatplotlibPlotter(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers)\n",
    "fig, axs, laps_pages = a_decoded_traj_plotter.plot_decoded_trajectories_2d(global_session, curr_num_subplots=8, active_page_index=0, plot_actual_lap_lines=False, use_theoretical_tracks_instead=True)\n",
    "\n",
    "integer_slider = a_decoded_traj_plotter.plot_epoch_with_slider_widget(an_epoch_idx=6)\n",
    "integer_slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60583e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "type(laps_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff87b5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "heatmaps[0].remove()\n",
    "\n",
    "# an_ax.remove(heatmaps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd94ef",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "an_ax = axs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe2a86",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plotActors, data_dict = plot_3d_stem_points(pCustom, active_epoch_placefields2D.ratemap.xbin, active_epoch_placefields2D.ratemap.ybin, active_epoch_placefields2D.ratemap.occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4627224",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "update_plot(value=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cfc42",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## add to 3D plotter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12058a97",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyVista.InteractivePlotter.InteractiveCustomDataExplorer import InteractiveCustomDataExplorer\n",
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecodedTrajectoryPyVistaPlotter\n",
    "from pyphoplacecellanalysis.Pho3D.PyVista.graphs import plot_3d_stem_points, plot_3d_binned_bars\n",
    "\n",
    "curr_active_pipeline.prepare_for_display()\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_out = curr_active_pipeline.display(display_function='_display_3d_interactive_custom_data_explorer', active_session_configuration_context=global_epoch_context,\n",
    "                                    params_kwargs=dict(should_use_linear_track_geometry=True, **{'t_start': t_start, 't_delta': t_delta, 't_end': t_end}),\n",
    "                                    )\n",
    "iplapsDataExplorer: InteractiveCustomDataExplorer = _out['iplapsDataExplorer']\n",
    "pActiveInteractiveLapsPlotter = _out['plotter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b011ca5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: a_result, xbin_centers, ybin_centers, iplapsDataExplorer\n",
    "# a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = DecodedTrajectoryPyVistaPlotter(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, p=iplapsDataExplorer.p)\n",
    "# a_decoded_trajectory_pyvista_plotter.build_ui()\n",
    "# a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=True)\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=False, active_plot_fn=plot_3d_stem_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cd81d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter: DecodedTrajectoryPyVistaPlotter = iplapsDataExplorer.add_decoded_posterior_bars(a_result=a_result, xbin=xbin, xbin_centers=xbin_centers, ybin=ybin, ybin_centers=ybin_centers, enable_plot_all_time_bins_in_epoch_mode=False, active_plot_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0e317",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "iplapsDataExplorer.clear_all_added_decoded_posterior_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968821ca",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d6646",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "update_plot_fn = a_decoded_trajectory_pyvista_plotter.data_dict['plot_3d_binned_bars[55.63197815967686]']['update_plot_fn']\n",
    "# update_plot_fn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038982b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# a_posterior_p_x_given_n, n_epoch_timebins = a_decoded_trajectory_pyvista_plotter._perform_get_curr_posterior(a_result=a_result, an_epoch_idx=a_decoded_trajectory_pyvista_plotter.curr_epoch_idx, time_bin_index=np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins))\n",
    "# np.shape(a_posterior_p_x_given_n)\n",
    "\n",
    "\n",
    "a_posterior_p_x_given_n, n_epoch_timebins = a_decoded_trajectory_pyvista_plotter.get_curr_posterior(an_epoch_idx=a_decoded_trajectory_pyvista_plotter.curr_epoch_idx, time_bin_index=np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins))\n",
    "np.shape(a_posterior_p_x_given_n)\n",
    "\n",
    "n_epoch_timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878ed8b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "v = a_decoded_trajectory_pyvista_plotter.plotActors['plot_3d_binned_bars[49.11980797704307]']\n",
    "# v['main'].remove()\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter.p.remove_actor(v['main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc498",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho3D.PyVista.graphs import clear_3d_binned_bars_plots\n",
    "\n",
    "clear_3d_binned_bars_plots(p=a_decoded_trajectory_pyvista_plotter.p, plotActors=a_decoded_trajectory_pyvista_plotter.plotActors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026b38b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.plotActors_CenterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec258c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.perform_update_plot_epoch_time_bin_range(value=None) # select all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721a086",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.perform_clear_existing_decoded_trajectory_plots()\n",
    "iplapsDataExplorer.p.update()\n",
    "iplapsDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795182a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "time_bin_index = np.arange(a_decoded_trajectory_pyvista_plotter.curr_n_time_bins)\n",
    "type(time_bin_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059eeac",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoded_trajectory_pyvista_plotter.slider_epoch.RemoveAllObservers()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch.Off()\n",
    "# a_decoded_trajectory_pyvista_plotter.slider_epoch.FastDelete()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch = None\n",
    "\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.RemoveAllObservers()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.Off()\n",
    "# a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin.FastDelete()\n",
    "a_decoded_trajectory_pyvista_plotter.slider_epoch_time_bin = None\n",
    "iplapsDataExplorer.p.clear_slider_widgets()\n",
    "iplapsDataExplorer.p.update()\n",
    "iplapsDataExplorer.p.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66de9f2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.PhoPositionalData.plotting.mixins.decoder_plotting_mixins import DecoderRenderingPyVistaMixin\n",
    "\n",
    "(plotActors, data_dict), (plotActors_CenterLabels, data_dict_CenterLabels) = DecoderRenderingPyVistaMixin.perform_plot_posterior_bars(iplapsDataExplorer.p, xbin=xbin, ybin=ybin, xbin_centers=xbin_centers, ybin_centers=ybin_centers,\n",
    "                                               posterior_p_x_given_n=a_posterior_p_x_given_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab8473",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# Other Misc Plotting Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bba12",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.plot._display_directional_template_debugger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7fec5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_template_debugger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c5e24",
   "metadata": {},
   "source": [
    "### Enable emphasizing/demphasizing aclus for TemplateDebugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719eba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "template_debugger: TemplateDebugger = _out['obj']\n",
    "\n",
    "# a_decoder_idx = 0\n",
    "# a_decoder_name = 'long_LR'\n",
    "\n",
    "\n",
    "# template_debugger.params.solo_emphasized_aclus = None # remove custom emphasis/demphasis\n",
    "# template_debugger.params.solo_emphasized_aclus = [31, 26, 14, 29, 11]\n",
    "\n",
    "\n",
    "# plots_data = template_debugger.plots_data\n",
    "# template_debugger.plots_data.unsorted_included_any_context_neuron_ids\n",
    "\n",
    "\n",
    "# out_colors_heatmap_image_matrix_dicts = plots_data['out_colors_heatmap_image_matrix_dicts'][a_decoder_name]\n",
    "# sort_helper_neuron_id_to_neuron_colors_dict = plots_data['sort_helper_neuron_id_to_neuron_colors_dicts'][a_decoder_idx] # Only one for all decoders, which is good actually\n",
    "# sorted_neuron_IDs_list = plots_data['sorted_neuron_IDs_lists'][a_decoder_idx] # Only one for all decoders, which is good actually\n",
    "\n",
    "# out_data['out_colors_heatmap_image_matrix_dicts'][a_decoder_name]\n",
    "#  'sort_helper_neuron_id_to_sort_IDX_dicts'\n",
    "# 'sort_helper_neuron_id_to_neuron_colors_dicts'\n",
    "# 'sorted_neuron_IDs_lists'\n",
    "# sort_helper_neuron_id_to_neuron_colors_dict\n",
    "\n",
    "template_debugger.update_cell_emphasis(solo_emphasized_aclus=[31, 26, 14, 29, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_debugger.update_cell_emphasis(solo_emphasized_aclus=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d69c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_debugger.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_debugger.params.included_any_context_neuron_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adda874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted_sort_helper_neuron_id_to_neuron_colors_dict = deepcopy(sort_helper_neuron_id_to_neuron_colors_dict) # a list of four dicts in it for some reason??\n",
    "# adjusted_sort_helper_neuron_id_to_neuron_colors_dict\n",
    "\n",
    "template_debugger.params.solo_emphasized_aclus = [31, 26, 14, 29, 11]\n",
    "# demphasized_aclus = ## build from the non-solo_emphasized_aclus\n",
    "# len(adjusted_sort_helper_neuron_id_to_neuron_colors_dicts)\n",
    "\n",
    "adjusted_sort_helper_neuron_id_to_neuron_colors_dict = {}\n",
    "for aclu, a_color in sort_helper_neuron_id_to_neuron_colors_dict.items():\n",
    "    if aclu in template_debugger.params.solo_emphasized_aclus:\n",
    "        # original color:\n",
    "        adjusted_sort_helper_neuron_id_to_neuron_colors_dict[aclu] = deepcopy(a_color)\n",
    "    else:\n",
    "        # desaturate the color:\n",
    "        desaturated_color = build_adjusted_color(deepcopy(a_color), saturation_scale=0.02, value_scale=0.1)\n",
    "        adjusted_sort_helper_neuron_id_to_neuron_colors_dict[aclu] = desaturated_color\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b3178",
   "metadata": {},
   "source": [
    "### Resume display stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f2ef8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from flexitext import flexitext\n",
    "from neuropy.utils.matplotlib_helpers import FormattedFigureText, FigureMargins ## flexitext version\n",
    "\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out = curr_active_pipeline.display('_display_directional_track_template_pf1Ds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2b444",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c1041",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_two_step_decoder_prediction_error_2D', global_epoch_context, variable_name='p_x_given_n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05b045",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_plot_most_likely_position_comparisons', global_epoch_context) # , variable_name='p_x_given_n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddebd9e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d805c37",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_laps_overview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c3cea",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "'_display_directional_laps_overview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98e796",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# '_display_directional_merged_pfs'\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pfs', plot_all_directions=False, plot_long_directional=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47076a61",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "'_display_1d_placefield_occupancy'\n",
    "'_display_placemaps_pyqtplot_2D'\n",
    " '_display_2d_placefield_occupancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481df233",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_2d_placefield_occupancy', global_any_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d0a20",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_grid_bin_bounds_validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbe6ee",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "_out = curr_active_pipeline.display('_display_running_and_replay_speeds_over_time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58951b11",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import add_rectangular_selector, add_range_selector\n",
    "\n",
    "\n",
    "# epoch_name = global_any_name\n",
    "epoch_name = short_epoch_name\n",
    "computation_result = curr_active_pipeline.computation_results[epoch_name]\n",
    "grid_bin_bounds = computation_result.computation_config['pf_params'].grid_bin_bounds\n",
    "epoch_context = curr_active_pipeline.filtered_contexts[epoch_name]\n",
    "            \n",
    "fig, ax = computation_result.computed_data.pf2D.plot_occupancy(identifier_details_list=[epoch_name], active_context=epoch_context) \n",
    "\n",
    "# rect_selector, set_extents, reset_extents = add_rectangular_selector(fig, ax, initial_selection=grid_bin_bounds) # (24.82, 257.88), (125.52, 149.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4720b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import add_vertical_track_bounds_lines\n",
    "\n",
    "grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "long_track_line_collection, short_track_line_collection = add_vertical_track_bounds_lines(grid_bin_bounds=grid_bin_bounds, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b862a66",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import compute_placefield_center_of_mass_positions\n",
    "\n",
    "\n",
    "epoch_name = global_any_name\n",
    "computation_result = curr_active_pipeline.computation_results[epoch_name]\n",
    "grid_bin_bounds = deepcopy(computation_result.computation_config['pf_params'].grid_bin_bounds)\n",
    "epoch_context = curr_active_pipeline.filtered_contexts[epoch_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905e90b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "grid_bin_bounds = deepcopy(long_pf2D.config.grid_bin_bounds)\n",
    "long_pf2D.xbin\n",
    "long_pf2D.ybin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc3d1d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "occupancy = deepcopy(long_pf2D.occupancy) # occupancy.shape # (60, 15)\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0416d4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage # used for `compute_placefield_center_of_masses`\n",
    "from neuropy.utils.mixins.peak_location_representing import compute_occupancy_center_of_mass_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6352663",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "occupancy_x_center_dict = {k:compute_occupancy_center_of_mass_positions(v.pf.occupancy, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n",
    "occupancy_x_center_dict # {'long_LR': 162.99271603199625, 'long_RL': 112.79866056603696, 'short_LR': 138.45611791646, 'short_RL': 130.78889937230684}\n",
    "\n",
    "occupancy_mask_x_center_dict = {k:compute_occupancy_center_of_mass_positions(v.pf.visited_occupancy_mask, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n",
    "occupancy_mask_x_center_dict # {'long_LR': 135.66781520875904, 'long_RL': 130.0042755113645, 'short_LR': 133.77996864296085, 'short_RL': 143.21920147195175}\n",
    "\n",
    "\n",
    "# {k:compute_occupancy_center_of_mass_positions(v.pf.occupancy, xbin=v.pf.xbin, ybin=v.pf.ybin).item() for k, v in track_templates.get_decoders_dict().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb029d8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "occupancy = deepcopy(long_pf2D.occupancy) # occupancy.shape # (60, 15)\n",
    "xbin = deepcopy(long_pf2D.xbin)\n",
    "ybin = deepcopy(long_pf2D.ybin)\n",
    "\n",
    "# masked_nonzero_occupancy = deepcopy(long_pf2D.nan_never_visited_occupancy)\n",
    "\n",
    "masked_nonzero_occupancy = deepcopy(long_pf2D.visited_occupancy_mask)\n",
    "\n",
    "# occupancy_CoM_positions = compute_occupancy_center_of_mass_positions(occupancy, xbin=long_pf2D.xbin, ybin=long_pf2D.ybin)\n",
    "occupancy_CoM_positions = compute_occupancy_center_of_mass_positions(masked_nonzero_occupancy, xbin=long_pf2D.xbin, ybin=long_pf2D.ybin) # array([127.704, 145.63])\n",
    "occupancy_CoM_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c3b06",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "long_pf2D.nan_never_visited_occupancy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4caa06",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.registered_display_function_docs_dict# '_display_grid_bin_bounds_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f963a7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Extracting on 2024-02-06 to display the LR/RL directions instead of the All/Long/Short pfs:\n",
    "def _display_directional_merged_pfs(owning_pipeline_reference, global_computation_results, computation_results, active_configs, include_includelist=None, save_figure=True, included_any_context_neuron_ids=None,\n",
    "                                    plot_all_directions=True, plot_long_directional=False, plot_short_directional=False, **kwargs):\n",
    "    \"\"\" Plots the merged pseduo-2D pfs/ratemaps. Plots: All-Directions, Long-Directional, Short-Directional in seperate windows. \n",
    "    \n",
    "    History: this is the Post 2022-10-22 display_all_pf_2D_pyqtgraph_binned_image_rendering-based method:\n",
    "    \"\"\"\n",
    "    from pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import pyqtplot_plot_image_array, display_all_pf_2D_pyqtgraph_binned_image_rendering\n",
    "    from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow \n",
    "    from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import LayoutScrollability\n",
    "\n",
    "    defer_render = kwargs.pop('defer_render', False)\n",
    "    directional_merged_decoders_result: DirectionalPseudo2DDecodersResult = global_computation_results.computed_data['DirectionalMergedDecoders']\n",
    "    active_merged_pf_plots_data_dict = {} #empty dict\n",
    "    \n",
    "    if plot_all_directions:\n",
    "        active_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='All-Directions', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.all_directional_pf1D_Decoder.pf # all-directions\n",
    "    if plot_long_directional:\n",
    "        active_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Long-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.long_directional_pf1D_Decoder.pf # Long-only\n",
    "    if plot_short_directional:\n",
    "        active_merged_pf_plots_data_dict[owning_pipeline_reference.build_display_context_for_session(track_config='Short-Directional', display_fn_name='display_all_pf_2D_pyqtgraph_binned_image_rendering')] = directional_merged_decoders_result.short_directional_pf1D_Decoder.pf # Short-only\n",
    "\n",
    "    out_plots_dict = {}\n",
    "    \n",
    "    for active_context, active_pf_2D in active_merged_pf_plots_data_dict.items():\n",
    "        # figure_format_config = {} # empty dict for config\n",
    "        figure_format_config = {'scrollability_mode': LayoutScrollability.NON_SCROLLABLE} # kwargs # kwargs as default figure_format_config\n",
    "        out_all_pf_2D_pyqtgraph_binned_image_fig: BasicBinnedImageRenderingWindow  = display_all_pf_2D_pyqtgraph_binned_image_rendering(active_pf_2D, figure_format_config) # output is BasicBinnedImageRenderingWindow\n",
    "    \n",
    "        # Set the window title from the context\n",
    "        out_all_pf_2D_pyqtgraph_binned_image_fig.setWindowTitle(f'{active_context.get_description()}')\n",
    "        out_plots_dict[active_context] = out_all_pf_2D_pyqtgraph_binned_image_fig\n",
    "\n",
    "        # Tries to update the display of the item:\n",
    "        names_list = [v for v in list(out_all_pf_2D_pyqtgraph_binned_image_fig.plots.keys()) if v not in ('name', 'context')]\n",
    "        for a_name in names_list:\n",
    "            # Adjust the size of the text for the item by passing formatted text\n",
    "            a_plot: pg.PlotItem = out_all_pf_2D_pyqtgraph_binned_image_fig.plots[a_name].mainPlotItem # PlotItem \n",
    "            # no clue why 2 is a good value for this...\n",
    "            a_plot.titleLabel.setMaximumHeight(2)\n",
    "            a_plot.layout.setRowFixedHeight(0, 2)\n",
    "            \n",
    "\n",
    "        if not defer_render:\n",
    "            out_all_pf_2D_pyqtgraph_binned_image_fig.show()\n",
    "\n",
    "    return out_plots_dict\n",
    "\n",
    "\n",
    "_display_directional_merged_pfs(curr_active_pipeline, curr_active_pipeline.global_computation_results, computation_results, active_configs, include_includelist=None, save_figure=True, included_any_context_neuron_ids=None,\n",
    "                                    plot_all_directions=True, plot_long_directional=False, plot_short_directional=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfeb03c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "# _out = curr_active_pipeline.display('_display_directional_merged_pfs', plot_all_directions=True, plot_long_directional=False, plot_short_directional=False)\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs') # scrollable_figure=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7268a75",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs_marginals') # scrollable_figure=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c9e39",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🖼️🎨 2024-02-08 - `PhoPaginatedMultiDecoderDecodedEpochsWindow` - Plot Ripple Metrics like Radon Transforms, WCorr, Simple Pearson, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf989bf6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": [
     "PhoPaginatedMultiDecoderDecodedEpochsWindow",
     "all",
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import RadonTransformPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "## INPUTS: directional_decoders_epochs_decode_result, filtered_epochs_df\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "unfiltered_epochs_df = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }')\n",
    "\n",
    "#  ripple_decoding_time_bin_size = 0.025 \n",
    "# 0.025\n",
    "\n",
    "## OUTPUTS: unfiltered_epochs_df, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "## OUTPUTS: filtered_epochs_df, filtered_decoder_filter_epochs_decoder_result_dict\n",
    "## INPUTS: directional_decoders_epochs_decode_result, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "## UPDATES: filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "\n",
    "## filter the epochs by something and only show those:\n",
    "# INPUTS: filtered_epochs_df\n",
    "# filtered_ripple_simple_pf_pearson_merged_df = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(active_epochs_df[['start', 'stop']].to_numpy())\n",
    "decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Update the `decoder_ripple_filter_epochs_decoder_result_dict` with the included epochs:\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# print(f\"any_good_selected_epoch_times.shape: {any_good_selected_epoch_times.shape}\") # (142, 2)\n",
    "\n",
    "pre_cols = {a_name:set(a_result.filter_epochs.columns) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "\n",
    "# 🟪 2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "filtered_decoder_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "## 2024-03-08 - Also constrain the user-selected ones (just to try it):\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "# ## Constrain again now by the user selections\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(any_good_selected_epoch_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()}\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Instead, add in the 'is_user_annotated_epoch' column instead of filtering\n",
    "## INPUTS: any_good_selected_epoch_times\n",
    "num_user_selected_times: int = len(any_good_selected_epoch_times)\n",
    "print(f'num_user_selected_times: {num_user_selected_times}')\n",
    "any_good_selected_epoch_indicies = None\n",
    "print(f'adding user annotation column!')\n",
    "\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "\n",
    "## OUT: filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "# ## specifically long_LR\n",
    "# filter_epochs: pd.DataFrame = deepcopy(ensure_dataframe(filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs))\n",
    "\n",
    "## OUTPUTS: filtered_epochs_df\n",
    "# filtered_epochs_df\n",
    "\n",
    "# a_decoder_decoded_epochs_result.filter_epochs\n",
    "a_decoder_decoded_epochs_result: DecodedFilterEpochsResult = decoder_ripple_filter_epochs_decoder_result_dict['long_LR']\n",
    "num_filter_epochs: int = a_decoder_decoded_epochs_result.num_filter_epochs\n",
    "active_epoch_idx: int = 6 #28\n",
    "active_captured_single_epoch_result: SingleEpochDecodedResult = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=active_epoch_idx)\n",
    "most_likely_position_indicies = deepcopy(active_captured_single_epoch_result.most_likely_position_indicies)\n",
    "most_likely_position_indicies = np.squeeze(most_likely_position_indicies)\n",
    "t_bin_centers = deepcopy(active_captured_single_epoch_result.time_bin_container.centers)\n",
    "t_bin_indicies = np.arange(len(np.squeeze(most_likely_position_indicies)))\n",
    "# most_likely_position_indicies\n",
    "p_x_given_n = deepcopy(active_captured_single_epoch_result.marginal_x.p_x_given_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd98310",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### 2024-05-09 - get the most-likely decoder for each epoch using the sequenceless probabilities and used this to selected the appopriate column for each of the heuristic measures.\n",
    "Modifies `extracted_merged_scores_df`, adding \"*_BEST\" columns for each specified heuristic score column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3434d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### Filter 1: Only very long-like replays post-delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## All Separate: \n",
    "# # INPUTS: filtered_decoder_filter_epochs_decoder_result_dict: Dict[decoder_name, DecodedFilterEpochsResult]\n",
    "# directional_decoders_epochs_decode_result\n",
    "# ## INPUTS: curr_active_pipeline, directional_decoders_epochs_decode_result\n",
    "# directional_decoders_epochs_decode_result\n",
    "# directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "# directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import co_filter_epochs_and_spikes\n",
    "\n",
    "# INPUTS: directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult\n",
    "# P_Long_threshold: float = 0.0\n",
    "# P_Long_threshold: float = 0.5\n",
    "P_Long_threshold: float = 0.80\n",
    "\n",
    "session_name: str = curr_active_pipeline.session_name\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "    \n",
    "ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## UPDATES: directional_decoders_epochs_decode_result\n",
    "## OUTPUTS: ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df\n",
    "## Specificy only the long-like replays occuring post-delta are of interest\n",
    "# df_is_included_criteria = lambda df: np.logical_and((df['P_Long'] > P_Long_threshold), (df['pre_post_delta_category'] == 'post-delta'))\n",
    "# df_is_included_criteria = lambda df: np.logical_and((df['P_Long'] > P_Long_threshold), (df['pre_post_delta_category'] == 'pre-delta'))\n",
    "# df_is_included_criteria = lambda df: np.logical_and((df['P_Short'] > P_Long_threshold), (df['pre_post_delta_category'] == 'pre-delta'))\n",
    "df_is_included_criteria = lambda df: np.logical_and((df['P_Short'] > P_Long_threshold), (df['pre_post_delta_category'] == 'post-delta'))\n",
    "included_ripple_start_times = ripple_simple_pf_pearson_merged_df[df_is_included_criteria(ripple_simple_pf_pearson_merged_df)]['ripple_start_t'].values\n",
    "# included_ripple_start_times\n",
    "\n",
    "## INPUTS: included_ripple_start_times\n",
    "# 1D_search (only for start times):\n",
    "long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict\n",
    "long_like_during_post_delta_only_filter_epochs_df = deepcopy(long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "long_like_during_post_delta_only_filter_epochs_df\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "\n",
    "active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "active_min_num_unique_aclu_inclusions_requirement: int = track_templates.min_num_unique_aclu_inclusions_requirement(curr_active_pipeline, required_min_percentage_of_active_cells=0.333333333)\n",
    "long_like_during_post_delta_only_filter_epochs_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=long_like_during_post_delta_only_filter_epochs_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "filtered_epochs_ripple_simple_pf_pearson_merged_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=ripple_simple_pf_pearson_merged_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "filtered_epochs_ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## OUTPUTS: long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict, long_like_during_post_delta_only_filter_epochs_df, filtered_epochs_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c5cc6",
   "metadata": {},
   "source": [
    "### Filter 2: Find events that have a good sequence score and one or more extreme-probabability bins (NOT FINISHED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95512dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## All Separate: \n",
    "# # INPUTS: filtered_decoder_filter_epochs_decoder_result_dict: Dict[decoder_name, DecodedFilterEpochsResult]\n",
    "# directional_decoders_epochs_decode_result\n",
    "# ## INPUTS: curr_active_pipeline, directional_decoders_epochs_decode_result\n",
    "# directional_decoders_epochs_decode_result\n",
    "# directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "# directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import co_filter_epochs_and_spikes\n",
    "\n",
    "# INPUTS: directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult\n",
    "n_long_extreme_bins_threshold: int = 2\n",
    "extreme_probabability_threshold: float = 0.9\n",
    "\n",
    "\n",
    "session_name: str = curr_active_pipeline.session_name\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "    \n",
    "ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "ripple_merged_complete_epoch_stats_df = directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df()\n",
    "\n",
    "\n",
    "## have to get the marginals from the merged_decoder\n",
    "## INPUTS: ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "\n",
    "## ripple_simple_pf_pearson_merged_df: epochs to include in the filtering\n",
    "all_directional_ripple_filter_epochs_decoder_result: DecodedFilterEpochsResult = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result).filtered_by_epoch_times(ripple_simple_pf_pearson_merged_df['ripple_start_t'].values) # DecodedFilterEpochsResult\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "trackID_marginals: List[NDArray] = [x.p_x_given_n for x in DirectionalPseudo2DDecodersResult.build_custom_marginal_over_long_short(all_directional_ripple_filter_epochs_decoder_result)] # these work if I want all of them\n",
    "\n",
    "# n_long_extreme_bins, n_short_extreme_bins = np.sum(trackID_marginals[0] > extreme_probabability_threshold, axis=1)\n",
    "trackID_marginals_num_extreme_bins: List[NDArray] = np.vstack([np.sum(x > extreme_probabability_threshold, axis=1).T for x in trackID_marginals]) # np.shape(data): (n_epoch_indicies, 2)\n",
    "# trackID_marginals_num_extreme_bins\n",
    "\n",
    "num_time_bins_per_epoch = [np.shape(x)[1] for x in trackID_marginals]\n",
    "ripple_simple_pf_pearson_merged_df['n_total_bins'] = num_time_bins_per_epoch\n",
    "ripple_simple_pf_pearson_merged_df['n_long_extreme_bins'] = np.squeeze(trackID_marginals_num_extreme_bins[:,0])\n",
    "ripple_simple_pf_pearson_merged_df['n_short_extreme_bins'] = np.squeeze(trackID_marginals_num_extreme_bins[:,1])\n",
    "\n",
    "ripple_simple_pf_pearson_merged_df\n",
    "## OUTPUTS: good_epochs_df, all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "## UPDATES: directional_decoders_epochs_decode_result\n",
    "## OUTPUTS: ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df\n",
    "## Specificy only the long-like replays occuring post-delta are of interest\n",
    "df_is_included_criteria = lambda df: np.logical_and((df['n_long_extreme_bins'] > n_long_extreme_bins_threshold), (df['pre_post_delta_category'] == 'post-delta'))\n",
    "included_ripple_start_times = ripple_simple_pf_pearson_merged_df[df_is_included_criteria(ripple_simple_pf_pearson_merged_df)]['ripple_start_t'].values\n",
    "# included_ripple_start_times\n",
    "\n",
    "## INPUTS: included_ripple_start_times\n",
    "# 1D_search (only for start times):\n",
    "high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict\n",
    "long_extreme_bins_during_post_delta_only_filter_epochs_df = deepcopy(long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "long_extreme_bins_during_post_delta_only_filter_epochs_df\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "\n",
    "active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "active_min_num_unique_aclu_inclusions_requirement: int = track_templates.min_num_unique_aclu_inclusions_requirement(curr_active_pipeline, required_min_percentage_of_active_cells=0.333333333)\n",
    "long_like_during_post_delta_only_filter_epochs_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=long_like_during_post_delta_only_filter_epochs_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "filtered_epochs_ripple_simple_pf_pearson_merged_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=ripple_simple_pf_pearson_merged_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "filtered_epochs_ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## OUTPUTS: long_extreme_bins_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict, long_extreme_bins_during_post_delta_only_filter_epochs_df, filtered_epochs_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb7bfa",
   "metadata": {},
   "source": [
    "### Filter 3: 2024-10-24 - Get all time bins with extreme values for both long/short to compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60243b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: all_directional_ripple_filter_epochs_decoder_result, active_decoder, trackID_marginals, extreme_probabability_threshold\n",
    "\n",
    "def find_extreme_filtered_time_bin_posteriors(extreme_probabability_threshold_condition: Callable):\n",
    "    \"\"\" captures: all_directional_ripple_filter_epochs_decoder_result, trackID_marginals \"\"\"\n",
    "    # n_long_extreme_bins, n_short_extreme_bins = np.sum(trackID_marginals[0] > extreme_probabability_threshold, axis=1)\n",
    "    # trackID_marginals_flattened_extreme_time_bin_indicies: List[NDArray] = [np.where(extreme_probabability_threshold_condition(x))[0] for x in trackID_marginals] # np.shape(trackID_marginals): (n_epoch_indicies, 2)\n",
    "    P_Long_trackID_marginals = [x[0, :] for x, nbins in zip(trackID_marginals, all_directional_ripple_filter_epochs_decoder_result.nbins)]\n",
    "    \n",
    "    # trackID_marginals_flattened_extreme_time_bin_indicies: List[NDArray] = [(extreme_probabability_threshold_condition(x)) for x in P_Long_trackID_marginals]\n",
    "    trackID_marginals_flattened_extreme_time_bin_indicies: List[NDArray] = [np.where(extreme_probabability_threshold_condition(x)) for x in P_Long_trackID_marginals]\n",
    "    \n",
    "    # trackID_marginals_flattened_extreme_time_bin_indicies: List[NDArray] = [(np.squeeze(extreme_probabability_threshold_condition(x))) for x in P_Long_trackID_marginals]\n",
    "    # [len(a_time_bin_edges) == len(included_time_bin_idxs) for included_time_bin_idxs, a_time_bin_edges in zip(trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.time_bin_edges)]\n",
    "    # [(np.shape(a_time_bin_edges), np.shape(included_time_bin_idxs)) for included_time_bin_idxs, a_time_bin_edges in zip(trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.time_bin_edges)]\n",
    "    trackID_marginals_flattened_extreme_time_bin_starts: List[NDArray] = None\n",
    "    # trackID_marginals_flattened_extreme_time_bin_starts: List[NDArray] = [np.atleast_1d(np.squeeze(np.array(a_time_bin_edges)[included_time_bin_idxs])).tolist() for included_time_bin_idxs, a_time_bin_edges in zip(trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.time_bin_edges)]\n",
    "    trackID_marginals_flattened_extreme_time_bin_p_x_given_ns: List[NDArray] = [np.squeeze(a_p_x_given_n[:, included_time_bin_idxs]) for included_time_bin_idxs, a_p_x_given_n in zip(trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.p_x_given_n_list)]  # np.shape(data): (57, 4, 1), (n_epoch_indicies, 2)\n",
    "\n",
    "    # trackID_marginals_flattened_extreme_time_bin_p_x_given_ns: List[NDArray] = [np.squeeze(a_p_x_given_n.p_x_given_n[:, included_time_bin_idxs]) for included_time_bin_idxs, a_p_x_given_n in zip(trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.marginal_x_list)]\n",
    "    \n",
    "    return trackID_marginals_flattened_extreme_time_bin_indicies, trackID_marginals_flattened_extreme_time_bin_starts, trackID_marginals_flattened_extreme_time_bin_p_x_given_ns\n",
    "\n",
    "\n",
    "\n",
    "# INPUTS: extreme_probabability_threshold: float\n",
    "# short_extreme_prob_thresh: float = extreme_probabability_threshold # 0.8\n",
    "# long_extreme_prob_thresh: float = 1.0 - short_extreme_prob_thresh # 0.2\n",
    "short_extreme_prob_thresh_condition = lambda x: (x > extreme_probabability_threshold) # x > 0.8\n",
    "long_extreme_prob_thresh_condition = lambda x: (x < (1.0 - extreme_probabability_threshold))  # x < 0.2\n",
    "\n",
    "short_extreme_time_bins_tuple = find_extreme_filtered_time_bin_posteriors(extreme_probabability_threshold_condition=short_extreme_prob_thresh_condition)\n",
    "long_extreme_time_bins_tuple = find_extreme_filtered_time_bin_posteriors(extreme_probabability_threshold_condition=long_extreme_prob_thresh_condition)\n",
    "\n",
    "## Unpack\n",
    "short_trackID_marginals_flattened_extreme_time_bin_indicies, short_trackID_marginals_flattened_extreme_time_bin_starts, short_trackID_marginals_flattened_extreme_time_bin_p_x_given_ns = short_extreme_time_bins_tuple\n",
    "long_trackID_marginals_flattened_extreme_time_bin_indicies, long_trackID_marginals_flattened_extreme_time_bin_starts, long_trackID_marginals_flattened_extreme_time_bin_p_x_given_ns = long_extreme_time_bins_tuple\n",
    "\n",
    "## Plot to compare them, mnaybe just truncate them all out flat or take an average or sum event?\n",
    "\n",
    "# [np.shape(a_p_x_given_n) for included_time_bin_idxs, a_p_x_given_n in zip(short_trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.p_x_given_n_list)]\n",
    "\n",
    "[np.shape(a_p_x_given_n.p_x_given_n[:, included_time_bin_idxs]) for included_time_bin_idxs, a_p_x_given_n in zip(short_trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.marginal_x_list)] #  (n_pos_bins, n_epoch_t_bins),\n",
    "\n",
    "    # trackID_marginals_flattened_extreme_time_bin_p_x_given_ns: List[NDArray] = [np.squeeze(a_p_x_given_n.p_x_given_n[:, included_time_bin_idxs]) for included_time_bin_idxs, a_p_x_given_n in zip(trackID_marginals_flattened_extreme_time_bin_indicies, all_directional_ripple_filter_epochs_decoder_result.marginal_x_list)]\n",
    "\n",
    "\n",
    "# short_trackID_marginals_flattened_extreme_time_bin_p_x_given_ns[2] # (9, 57)\n",
    "# short_trackID_marginals_flattened_extreme_time_bin_p_x_given_ns[1]\n",
    "# np.nanmean(np.vstack(short_trackID_marginals_flattened_extreme_time_bin_p_x_given_ns), axis=1)\n",
    "\n",
    "# [np.atleast_1d(x).shape for x in trackID_marginals_flattened_extreme_time_bin_p_x_given_ns if np.size(x) > 0]\n",
    "\n",
    "# trackID_marginals_num_extreme_bins\n",
    "# trackID_marginals_flattened_extreme_time_bin_indicies[0]\n",
    "# n_timebins, flat_time_bin_containers, timebins_p_x_given_n = all_directional_ripple_filter_epochs_decoder_result.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b17f9",
   "metadata": {},
   "source": [
    "### Filter 4: Find events that have a good sequence score (2024-11-28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90857a5",
   "metadata": {
    "tags": [
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "# ## All Separate: \n",
    "# # INPUTS: filtered_decoder_filter_epochs_decoder_result_dict: Dict[decoder_name, DecodedFilterEpochsResult]\n",
    "# directional_decoders_epochs_decode_result\n",
    "# ## INPUTS: curr_active_pipeline, directional_decoders_epochs_decode_result\n",
    "# directional_decoders_epochs_decode_result\n",
    "# directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "# directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import co_filter_epochs_and_spikes\n",
    "from neuropy.utils.indexing_helpers import flatten, NumpyHelpers, PandasHelpers\n",
    "# INPUTS: directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult\n",
    "best_continuous_seq_sort_threshold: float = 0.2\n",
    "# best_continuous_seq_sort_threshold: float = 0.95\n",
    "main_contiguous_subsequence_len_threshold: int = 6\n",
    "\n",
    "session_name: str = curr_active_pipeline.session_name\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "    \n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df = PandasHelpers.dropping_duplicated_df_columns(df=directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df)\n",
    "directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df = PandasHelpers.dropping_duplicated_df_columns(df=directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df)\n",
    "# duplicated_columns, duplicated_columns_dict = PandasHelpers.find_duplicated_df_columns(df=ripple_weighted_corr_merged_df, print_duplicated_columns=True)\n",
    "ripple_weighted_corr_merged_df = PandasHelpers.dropping_duplicated_df_columns(df=directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df)\n",
    "ripple_simple_pf_pearson_merged_df = PandasHelpers.dropping_duplicated_df_columns(df=directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df)\n",
    "\n",
    "ripple_merged_complete_epoch_stats_df: pd.DataFrame = PandasHelpers.dropping_duplicated_df_columns(df=directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df())\n",
    "# duplicated_columns, duplicated_columns_dict = PandasHelpers.find_duplicated_df_columns(df=ripple_merged_complete_epoch_stats_df, print_duplicated_columns=True)\n",
    "\n",
    "# ripple_merged_complete_epoch_stats_df\n",
    "# ripple_merged_complete_epoch_stats_df['overall_best_longest_sequence_length_ratio']\n",
    "\n",
    "## have to get the marginals from the merged_decoder\n",
    "## INPUTS: ripple_simple_pf_pearson_merged_df\n",
    "df: pd.DataFrame = deepcopy(ripple_merged_complete_epoch_stats_df)\n",
    "## INPUTS: df, duplicated_columns, duplicated_columns_dict\n",
    "# dropping_duplicated_df_columns\n",
    "\n",
    "## ripple_simple_pf_pearson_merged_df: epochs to include in the filtering\n",
    "all_directional_ripple_filter_epochs_decoder_result: DecodedFilterEpochsResult = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result).filtered_by_epoch_times(ripple_merged_complete_epoch_stats_df['ripple_start_t'].values) # DecodedFilterEpochsResult\n",
    "active_decoder = directional_merged_decoders_result.all_directional_pf1D_Decoder\n",
    "trackID_marginals: List[NDArray] = [x.p_x_given_n for x in DirectionalPseudo2DDecodersResult.build_custom_marginal_over_long_short(all_directional_ripple_filter_epochs_decoder_result)] # these work if I want all of them\n",
    "\n",
    "# ripple_merged_complete_epoch_stats_df['overall_best_continuous_seq_sort'] = np.nanmax(ripple_merged_complete_epoch_stats_df['long_best_continuous_seq_sort'].values, ripple_merged_complete_epoch_stats_df['short_best_continuous_seq_sort'].values) #.max()\n",
    "#TODO 2024-11-28 14:32: - [ ] np.nanmax sucks apparently, it doesn't compute the element-wise maximum across two arrays ever.\n",
    "\n",
    "if 'overall_best_continuous_seq_sort' not in ripple_merged_complete_epoch_stats_df:\n",
    "    ripple_merged_complete_epoch_stats_df['overall_best_continuous_seq_sort'] = ripple_merged_complete_epoch_stats_df[['long_best_continuous_seq_sort', 'short_best_continuous_seq_sort']].max(axis=1, skipna=True)\n",
    "\n",
    "assert 'overall_best_continuous_seq_len_ratio_no_repeats' in ripple_merged_complete_epoch_stats_df\n",
    "\n",
    "# ripple_merged_complete_epoch_stats_df\n",
    "\n",
    "if 'overall_best_main_contiguous_subsequence_len' not in ripple_merged_complete_epoch_stats_df:\n",
    "    ripple_merged_complete_epoch_stats_df['overall_best_main_contiguous_subsequence_len'] = ripple_merged_complete_epoch_stats_df[['long_best_main_contiguous_subsequence_len', 'short_best_main_contiguous_subsequence_len']].max(axis=1, skipna=True)\n",
    "\n",
    "assert 'overall_best_main_contiguous_subsequence_len' in ripple_merged_complete_epoch_stats_df\n",
    "\n",
    "\n",
    "# 'main_contiguous_subsequence_len'\n",
    "\n",
    "# ripple_merged_complete_epoch_stats_df['overall_best_main_contiguous_subsequence_len'].plot.hist()\n",
    "\n",
    "## OUTPUTS: good_epochs_df, all_directional_ripple_filter_epochs_decoder_result\n",
    "\n",
    "## UPDATES: directional_decoders_epochs_decode_result\n",
    "## OUTPUTS: ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df\n",
    "## Specificy only the long-like replays occuring post-delta are of interest\n",
    "# df_is_included_criteria = lambda df: (df['overall_best_continuous_seq_sort'] > best_continuous_seq_sort_threshold)\n",
    "\n",
    "# df_is_included_criteria = lambda df: NumpyHelpers.logical_and((df['overall_best_continuous_seq_sort'] >= best_continuous_seq_sort_threshold), (df['pre_post_delta_category'] == 'post-delta'), (df['P_Long'] > 0.5))\n",
    "# df_is_included_criteria = lambda df: (df['overall_best_continuous_seq_sort'] >= best_continuous_seq_sort_threshold) # , (df['pre_post_delta_category'] == 'post-delta')\n",
    "# df_is_included_criteria = lambda df: NumpyHelpers.logical_and((df['overall_best_continuous_seq_sort'].to_numpy() >= best_continuous_seq_sort_threshold), (df['pre_post_delta_category'].to_numpy() == 'post-delta'), (df['P_Long'].to_numpy() > 0.5)) # ValueError: operands could not be broadcast together with shapes (412,) (412,2) \n",
    "# df_is_included_criteria = lambda df: np.logical_and(np.logical_and((df['overall_best_continuous_seq_sort'].to_numpy() > best_continuous_seq_sort_threshold), (df['pre_post_delta_category'].to_numpy() == 'post-delta')), (df['P_Long'].to_numpy() > 0.5)) # ValueError: operands could not be broadcast together with shapes (412,) (412,2) \n",
    "\n",
    "# df_is_included_criteria = lambda df: (df['overall_best_continuous_seq_sort'].to_numpy() > best_continuous_seq_sort_threshold) & (df['pre_post_delta_category'].to_numpy() == 'post-delta') & (df['P_Long'].to_numpy() > 0.5) # ValueError: operands could not be broadcast together with shapes (412,) (412,2) \n",
    "\n",
    "# df_is_included_criteria = lambda df: ((df['pre_post_delta_category'] == 'post-delta') & (df['P_Long'] > 0.5)) # ValueError: operands could not be broadcast together with shapes (412,) (412,2) \n",
    "# df_is_included_criteria = lambda df: (df['overall_best_continuous_seq_len_ratio_no_repeats'] >= best_continuous_seq_sort_threshold) \n",
    "df_is_included_criteria = lambda df: (df['overall_best_main_contiguous_subsequence_len'] >= main_contiguous_subsequence_len_threshold) \n",
    "df_is_icky_included_criteria = lambda df: (df[['main_contiguous_subsequence_len_long_LR', 'main_contiguous_subsequence_len_long_RL', 'main_contiguous_subsequence_len_short_LR', 'main_contiguous_subsequence_len_short_RL']].max(axis=1, skipna=True) < main_contiguous_subsequence_len_threshold) \n",
    "\n",
    "\n",
    "included_ripple_start_times = ripple_merged_complete_epoch_stats_df[df_is_included_criteria(ripple_merged_complete_epoch_stats_df)]['ripple_start_t'].values\n",
    "# included_icky_ripple_start_times = ripple_merged_complete_epoch_stats_df[np.logical_not(df_is_included_criteria(ripple_merged_complete_epoch_stats_df))]['ripple_start_t'].values\n",
    "included_icky_ripple_start_times = ripple_merged_complete_epoch_stats_df[df_is_icky_included_criteria(ripple_merged_complete_epoch_stats_df)]['ripple_start_t'].values\n",
    "\n",
    "# included_ripple_start_times\n",
    "\n",
    "## INPUTS: included_ripple_start_times\n",
    "# 1D_search (only for start times):\n",
    "high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "low_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_icky_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict -> high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# high_continuous_seq_sort_only_filter_epochs_df = deepcopy(high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "# high_continuous_seq_sort_only_filter_epochs_df\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "\n",
    "# active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5, included_qclu_values=)\n",
    "# active_min_num_unique_aclu_inclusions_requirement: int = track_templates.min_num_unique_aclu_inclusions_requirement(curr_active_pipeline, required_min_percentage_of_active_cells=0.333333333)\n",
    "# # long_like_during_post_delta_only_filter_epochs_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=long_like_during_post_delta_only_filter_epochs_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "# filtered_epochs_ripple_simple_pf_pearson_merged_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=ripple_merged_complete_epoch_stats_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "# filtered_epochs_ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## OUTPUTS: high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict, high_continuous_seq_sort_only_filter_epochs_df, included_ripple_start_times\n",
    "included_icky_ripple_start_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84755c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_icky_ripple_start_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851356d",
   "metadata": {
    "tags": [
     "now"
    ]
   },
   "source": [
    "# 2024-11-20 - Find specific posterior from a start_t (e.g. 747.3501248767134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1b3300f",
   "metadata": {
    "tags": [
     "now",
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import RadonTransformPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow, DecodedEpochSlicesPaginatedFigureController, EpochSelectionsObject, ClickActionCallbacks\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import co_filter_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import get_proper_global_spikes_df\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_t: float = 747.3501248767134\n",
    "\n",
    "# included_ripple_start_times = [734.2202499993145, 811.4449451802066, 892.33579400, 972.578]\n",
    "\n",
    "# included_ripple_start_times = [1013.39]\n",
    "# included_ripple_start_times = [\n",
    "# # \t683.0109885382699,\n",
    "# #  685.3902820401127,\n",
    "# #  694.509939450887,\n",
    "# #  697.9841853519902,\n",
    "# #  701.9943720988231,\n",
    "# #  705.2988593669143,\n",
    "# #  706.6135825337842,\n",
    "# #  710.4212631442351,\n",
    "# #  712.0747355778003,\n",
    "# #  713.5096046030521,\n",
    "# #  717.7937214495614,\n",
    "# #  721.2997318145353,\n",
    "# #  734.2202499993145,\n",
    "# #  735.2181886882754,\n",
    "# #  738.1171107239788,\n",
    "# #  761.1802617956419,\n",
    "# #  769.0905348656233,\n",
    "# #  794.9678822564892,\n",
    "# #  812.6678770340513,\n",
    "# #  827.2364609349752,\n",
    "# #  835.6428003108595,\n",
    "# #  863.0844400207279,\n",
    "# #  869.0441169693368,\n",
    "# #  892.7914328108309,\n",
    "# #  906.0226529163774,\n",
    "# #  907.6281407343922,\n",
    "# #  909.8543565545696,\n",
    "# #  926.7004279292888,\n",
    "# #  946.198432666366,\n",
    "# #  958.0087306194472,\n",
    "# #  1011.5683166369564,\n",
    "# #  1013.3905032241018,\n",
    "# #  1028.1721302157966,\n",
    "# #  1030.4905367088504,\n",
    "# #  1064.2788637292106,\n",
    "# #  1064.9692339358153,\n",
    "# #  1072.363319110009,\n",
    "# #  1078.64460357395,\n",
    "# #  1079.5288168812403,\n",
    "# #  1107.1022146036848,\n",
    " \n",
    "# 1568.0800317029934,\n",
    "# ]\n",
    "\n",
    "## INPUTS: included_ripple_start_times\n",
    "\n",
    "\n",
    "\n",
    "## INPUTS: included_ripple_start_times\n",
    "# 1D_search (only for start times):\n",
    "# matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:deepcopy(a_result) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "# matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict\n",
    "matching_specific_start_ts_only_filter_epochs_df = deepcopy(matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "matching_specific_start_ts_only_filter_epochs_df\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "\n",
    "active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "active_min_num_unique_aclu_inclusions_requirement: int = track_templates.min_num_unique_aclu_inclusions_requirement(curr_active_pipeline, required_min_percentage_of_active_cells=0.333333333)\n",
    "matching_specific_start_ts_only_filter_epochs_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=matching_specific_start_ts_only_filter_epochs_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "filtered_epochs_ripple_simple_pf_pearson_merged_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=ripple_simple_pf_pearson_merged_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "filtered_epochs_ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## INPUTS: directional_decoders_epochs_decode_result, filtered_epochs_df\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "unfiltered_epochs_df = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }')\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# BEGIN FCN BODY                                                                                                       #\n",
    "# ==================================================================================================================== #\n",
    "## INPUTS filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "\n",
    "active_decoder_decoded_epochs_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "active_filter_epochs_df: pd.DataFrame = deepcopy(matching_specific_start_ts_only_filter_epochs_df)\n",
    "epochs_name='ripple'\n",
    "title='Specificed Start_t PBEs Only'\n",
    "known_epochs_type = 'ripple'\n",
    "\n",
    "active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline)\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations'] ## GENERAL\n",
    "(app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict), ripple_rasters_plot_tuple, yellow_blue_trackID_marginals_plot_tuple = PhoPaginatedMultiDecoderDecodedEpochsWindow.plot_full_paginated_decoded_epochs_window(curr_active_pipeline=curr_active_pipeline, track_templates=track_templates, active_spikes_df=active_spikes_df,\n",
    "                                                                                                                                                                                                   active_decoder_decoded_epochs_result_dict=deepcopy(active_decoder_decoded_epochs_result_dict),\n",
    "                                                                                                                                                                                                   directional_decoders_epochs_decode_result=deepcopy(directional_decoders_epochs_decode_result),\n",
    "                                                                                                                                                                                                   active_filter_epochs_df=active_filter_epochs_df, known_epochs_type=known_epochs_type, title=title,\n",
    "                                                                                                                                                                                                   )\n",
    "attached_yellow_blue_marginals_viewer_widget: DecodedEpochSlicesPaginatedFigureController = paginated_multi_decoder_decoded_epochs_window.attached_yellow_blue_marginals_viewer_widget\n",
    "attached_ripple_rasters_widget: RankOrderRastersDebugger = paginated_multi_decoder_decoded_epochs_window.attached_ripple_rasters_widget\n",
    "attached_directional_template_pfs_debugger: TemplateDebugger = paginated_multi_decoder_decoded_epochs_window.attached_directional_template_pfs_debugger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f390",
   "metadata": {},
   "source": [
    "# 2024-11-25 - New Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442394e",
   "metadata": {
    "tags": [
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import SerializationHelper_AllCustomDecodingResults, SerializationHelper_CustomDecodingResults\n",
    "from numpy import ma\n",
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "max_jump_distance_cm: float = 60.0\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "print(f'max_jump_distance_cm: {max_jump_distance_cm}')\n",
    "# print(list(HeuristicReplayScoring.build_all_score_computations_fn_dict().keys())) # ['jump', 'max_jump_cm', 'max_jump_cm_per_sec', 'ratio_jump_valid_bins', 'travel', 'coverage', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness']\n",
    "\n",
    "directional_laps_results: DirectionalLapsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps'] # DirectionalLapsResult\n",
    "# a_name: str = 'long_LR'\n",
    "# a_directional_decoders_epochs_decode_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict[a_name]\n",
    "\n",
    "## INPUTS: curr_active_pipeline, track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = deepcopy(curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']) ## GENERAL\n",
    "a_decoded_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "a_decoded_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=a_decoded_filter_epochs_decoder_result_dict,\n",
    "                                                                                                                    max_ignore_bins=2, same_thresh_cm=a_same_thresh_cm, max_jump_distance_cm=max_jump_distance_cm)\n",
    "# a_decoded_filter_epochs_decoder_result_dict\n",
    "\n",
    "directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict = deepcopy(a_decoded_filter_epochs_decoder_result_dict)\n",
    "# directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df\n",
    "curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations'] = directional_decoders_epochs_decode_result ## MIGHT NEED SAVING\n",
    "print(f'PIPELINE MIGHT NEED SAVING')\n",
    "## INPUTS: curr_active_pipeline, track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "# directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = deepcopy(curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']) ## GENERAL\n",
    "## INPUTS: directional_decoders_epochs_decode_result, filtered_epochs_df\n",
    "\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "unfiltered_epochs_df = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "if filtered_epochs_df is not None:\n",
    "    ## filter\n",
    "    filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "else:\n",
    "    filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(unfiltered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working unfiltered\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }')\n",
    "\n",
    "## OUTPUT: filtered_decoder_filter_epochs_decoder_result_dict, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ed804",
   "metadata": {
    "tags": [
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "save_path = curr_active_pipeline.get_output_path().joinpath(f\"{DAY_DATE_TO_USE}_CustomDecodingResults.pkl\").resolve()\n",
    "save_path = SerializationHelper_CustomDecodingResults.save(a_directional_decoders_epochs_decode_result=directional_decoders_epochs_decode_result, long_pf2D=long_pf2D,\n",
    "                                        save_path=save_path)\n",
    "save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea96a61",
   "metadata": {
    "tags": [
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "save_path = curr_active_pipeline.get_output_path().joinpath(f\"{DAY_DATE_TO_USE}_AllCustomDecodingResults.pkl\").resolve()\n",
    "save_path = SerializationHelper_AllCustomDecodingResults.save(track_templates=track_templates, a_directional_decoders_epochs_decode_result=directional_decoders_epochs_decode_result, \n",
    "                                        #    a_decoded_filter_epochs_decoder_result_dict=deepcopy(a_decoded_filter_epochs_decoder_result_dict),\n",
    "                                           pos_bin_size=directional_decoders_epochs_decode_result.pos_bin_size, ripple_decoding_time_bin_size=directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size, \n",
    "                                           save_path=save_path)\n",
    "save_path\n",
    "\n",
    "\n",
    "# load_path = Path(\"W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/2024-11-25_AllCustomDecodingResults.pkl\")\n",
    "# track_templates, directional_decoders_epochs_decode_result, xbin, xbin_centers =  SerializationHelper_AllCustomDecodingResults.save(load_path=load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "list(HeuristicReplayScoring.build_all_score_computations_fn_dict().keys()) # ['jump', 'avg_jump_cm', 'max_jump_cm', 'max_jump_cm_per_sec', 'ratio_jump_valid_bins', 'travel', 'coverage', 'continuous_seq_sort', 'sequential_correlation', 'monotonicity_score', 'laplacian_smoothness']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdda19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_all_scores_merged_df: pd.DataFrame = directional_decoders_epochs_decode_result.build_complete_all_scores_merged_df()\n",
    "# complete_all_scores_merged_df['overall_best_no'\n",
    "\n",
    "# ['long_best_longest_sequence_length_ratio', 'short_best_longest_sequence_length_ratio', 'short_best_longest_sequence_length_ratio']\n",
    "# ['longest_sequence_length_ratio_diff'] # not valid for this (and some other) columns\n",
    "\n",
    "# complete_all_scores_merged_df['long_best_longest_sequence_length_ratio']\n",
    "\n",
    "# complete_all_scores_merged_df['long_best_continuous_seq_sort']\n",
    "# complete_all_scores_merged_df['long_best_continuous_seq_len_ratio_no_repeats']\n",
    "# complete_all_scores_merged_df['long_best_main_contiguous_subsequence_len']\n",
    "complete_all_scores_merged_df['main_contiguous_subsequence_len_short_LR'].iloc[:10]\n",
    "# complete_all_scores_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs\n",
    "\n",
    "# 'avg_jump_cm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0269e23",
   "metadata": {},
   "source": [
    "## Find indicies that are included in `high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict` from `filtered_decoder_filter_epochs_decoder_result_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667a680",
   "metadata": {
    "tags": [
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict\n",
    "example_decoder_name = 'long_LR'\n",
    "all_epoch_result: DecodedFilterEpochsResult = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict[example_decoder_name])\n",
    "all_filter_epochs_df: pd.DataFrame = deepcopy(all_epoch_result.filter_epochs)\n",
    "\n",
    "# included_filter_epoch_result: DecodedFilterEpochsResult = deepcopy(high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict[example_decoder_name])\n",
    "included_filter_epoch_result: DecodedFilterEpochsResult = deepcopy(low_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict[example_decoder_name])\n",
    "\n",
    "included_filter_epochs_df: pd.DataFrame = deepcopy(included_filter_epoch_result.filter_epochs)\n",
    "included_filter_epochs_df\n",
    "\n",
    "# included_filter_epoch_times = included_filter_epochs_df[['start', 'stop']].to_numpy() # Both 'start', 'stop' column matching\n",
    "included_filter_epoch_times = included_filter_epochs_df['start'].to_numpy() # Both 'start', 'stop' column matching\n",
    "\n",
    "included_filter_epoch_times_to_all_epoch_index_map = included_filter_epoch_result.find_epoch_times_to_data_indicies_map(epoch_times=included_filter_epoch_times)\n",
    "included_filter_epoch_times_to_all_epoch_index_arr: NDArray = included_filter_epoch_result.find_data_indicies_from_epoch_times(epoch_times=included_filter_epoch_times)\n",
    "len(included_filter_epoch_times_to_all_epoch_index_arr)\n",
    "\n",
    "## OUTPUTS: included_filter_epoch_times_to_all_epoch_index_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ea2d7",
   "metadata": {},
   "source": [
    "### 🟢 🖼️ Plot specific `PhoPaginatedMultiDecoderDecodedEpochsWindow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff2dad",
   "metadata": {
    "tags": [
     "PhoPaginatedMultiDecoderDecodedEpochsWindow",
     "active-2024-12-11"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import get_heatmap_cmap\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColormapHelpers, ColorFormatConverter\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import FixedCustomColormaps\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow, DecodedEpochSlicesPaginatedFigureController, EpochSelectionsObject, ClickActionCallbacks\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ThinButtonBar.ThinButtonBarWidget import ThinButtonBarWidget\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.PaginationCtrl.PaginationControlWidget import PaginationControlWidget, PaginationControlWidgetState\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.Resources import GuiResources, ActionIcons, silx_resources_rc\n",
    "## INPUTS filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "\n",
    "# active_cmap = FixedCustomColormaps.get_custom_orange_with_low_values_dropped_cmap()\n",
    "# active_cmap = FixedCustomColormaps.get_custom_black_with_low_values_dropped_cmap(low_value_cutoff=0.05)\n",
    "# active_cmap = ColormapHelpers.create_colormap_transparent_below_value(active_cmap, low_value_cuttoff=0.1)\n",
    "active_cmap = FixedCustomColormaps.get_custom_greyscale_with_low_values_dropped_cmap(low_value_cutoff=0.05, full_opacity_threshold=0.4)\n",
    "active_cmap\n",
    "\n",
    "\n",
    "# ## pseudo-sorted `sorted_included_filter_epoch_times_to_all_epoch_index_arr`\n",
    "# sorted_included_filter_epoch_times_to_all_epoch_index_arr = deepcopy(included_filter_epoch_times_to_all_epoch_index_arr) ## unsorted\n",
    "# sorted_included_filter_epoch_times_to_all_epoch_index_arr = sorted_included_filter_epoch_times_to_all_epoch_index_arr[::-1]\n",
    "# sorted_included_filter_epoch_times_to_all_epoch_index_arr\n",
    "# reversed(sorted_included_filter_epoch_times_to_all_epoch_index_arr)\n",
    "\n",
    "# type(active_cmap) # matplotlib.colors.LinearSegmentedColormap\n",
    "# active_cmap.to_list()\n",
    "\n",
    "# color_list = [mcolors.rgb2hex(active_cmap(i)) for i in range(active_cmap.N)]\n",
    "# color_list\n",
    "\n",
    "# cmap = cm.get_cmap('viridis',nlevels)\n",
    "# active_cmap.set_under((1,1,1,0)) # Completely hide the underflow\n",
    "# active_cmap.colors[:,3] = np.linspace(0.3,0.9,13) # Choose a gradient for transparency\n",
    "\n",
    "# active_cmap.set_extremes(bad=None, under=None, over=None)\n",
    "\n",
    "\n",
    "# high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict, high_continuous_seq_sort_only_filter_epochs_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                decoder_decoded_epochs_result_dict=filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=filtered_ripple_simple_pf_pearson_merged_df, epochs_name='ripple',\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple', title='Long-like post-Delta Ripples Only', ## RIPPLE\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=high_continuous_seq_sort_only_filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple', title='High-sequence Score Ripples Only', ## RIPPLE\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', ## LAPS\n",
    "                                                                                                included_epoch_indicies=None,\n",
    "                                                                                                # included_epoch_indicies=included_filter_epoch_times_to_all_epoch_index_arr, ## unsorted\n",
    "                                                                                                # decoder_decoded_epochs_result_dict=sorted_filtered_decoder_filter_epochs_decoder_result_dict, epochs_name='ripple',  ## SORTED\n",
    "                                                                                                # included_epoch_indicies=sorted_included_filter_epoch_times_to_all_epoch_index_arr, ## SORTED\n",
    "                                                                                                debug_print=False,\n",
    "                                                                                                params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, \n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': False, \n",
    "                                                                                                    'enable_decoded_sequence_and_heuristics_curve': True, 'show_pre_merged_debug_sequences': True,\n",
    "                                                                                                     'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True, 'enable_weighted_corr_data_provider_modify_axes_rect': False,\n",
    "                                                                                                    # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 9,\n",
    "                                                                                                    # 'scrollable_figure': False,\n",
    "                                                                                                    'scrollable_figure': True,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': False,\n",
    "                                                                                                    'should_suppress_callback_exceptions': False,\n",
    "                                                                                                    # 'build_fn': 'insets_view',\n",
    "                                                                                                    'track_length_cm_dict': deepcopy(track_templates.get_track_length_dict()),\n",
    "                                                                                                    'posterior_heatmap_imshow_kwargs': dict(cmap=active_cmap), # , vmin=0.1, vmax=1.0\n",
    "                                                                                                    \n",
    "                                                                                                })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: included_ripple_start_times\n",
    "# 1D_search (only for start times):\n",
    "# matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(included_ripple_start_times) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict: Dict[str, DecodedFilterEpochsResult] = {a_name:deepcopy(a_result) for a_name, a_result in filtered_decoder_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "# # matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# matching_specific_start_ts_only_filter_epochs_df = deepcopy(matching_specific_start_ts_only_filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "# matching_specific_start_ts_only_filter_epochs_df\n",
    "\n",
    "# # 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "\n",
    "active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "active_min_num_unique_aclu_inclusions_requirement: int = track_templates.min_num_unique_aclu_inclusions_requirement(curr_active_pipeline, required_min_percentage_of_active_cells=0.333333333)\n",
    "# matching_specific_start_ts_only_filter_epochs_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=matching_specific_start_ts_only_filter_epochs_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "# filtered_epochs_ripple_simple_pf_pearson_merged_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=ripple_simple_pf_pearson_merged_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "# matching_specific_start_ts_only_filter_epochs_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=matching_specific_start_ts_only_filter_epochs_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "\n",
    "# filtered_epochs_ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "# ## INPUTS: directional_decoders_epochs_decode_result, filtered_epochs_df\n",
    "# decoder_ripple_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# unfiltered_epochs_df = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "# filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "## INPUTS: filtered_decoder_filter_epochs_decoder_result_dict\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }')\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# BEGIN FCN BODY                                                                                                       #\n",
    "# ==================================================================================================================== #\n",
    "## INPUTS filtered_decoder_filter_epochs_decoder_result_dict\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "\n",
    "active_decoder_decoded_epochs_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "active_filter_epochs_df: pd.DataFrame = deepcopy(active_decoder_decoded_epochs_result_dict['long_LR'].filter_epochs) # deepcopy(matching_specific_start_ts_only_filter_epochs_df)\n",
    "epochs_name='ripple'\n",
    "title='Filtered PBEs'\n",
    "known_epochs_type = 'ripple'\n",
    "\n",
    "active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline)\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations'] ## GENERAL\n",
    "(app, paginated_multi_decoder_decoded_epochs_window, pagination_controller_dict), ripple_rasters_plot_tuple, yellow_blue_trackID_marginals_plot_tuple = PhoPaginatedMultiDecoderDecodedEpochsWindow.plot_full_paginated_decoded_epochs_window(curr_active_pipeline=curr_active_pipeline, track_templates=track_templates, active_spikes_df=active_spikes_df,\n",
    "                                                                                                                                                                                                   active_decoder_decoded_epochs_result_dict=deepcopy(active_decoder_decoded_epochs_result_dict), # epochs_name='ripple',\n",
    "                                                                                                                                                                                                   directional_decoders_epochs_decode_result=deepcopy(directional_decoders_epochs_decode_result),\n",
    "                                                                                                                                                                                                   active_filter_epochs_df=active_filter_epochs_df, known_epochs_type=known_epochs_type, title=title,\n",
    "                                                                                                params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, \n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': False, \n",
    "                                                                                                    'enable_decoded_sequence_and_heuristics_curve': True, 'show_pre_merged_debug_sequences': False,\n",
    "                                                                                                     'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True, 'enable_weighted_corr_data_provider_modify_axes_rect': False,\n",
    "                                                                                                    # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 9,\n",
    "                                                                                                    # 'scrollable_figure': False,\n",
    "                                                                                                    'scrollable_figure': True,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': False,\n",
    "                                                                                                    'should_suppress_callback_exceptions': False,\n",
    "                                                                                                    # 'build_fn': 'insets_view',\n",
    "                                                                                                    'track_length_cm_dict': deepcopy(track_templates.get_track_length_dict()),\n",
    "                                                                                                    'posterior_heatmap_imshow_kwargs': dict(cmap=active_cmap), # , vmin=0.1, vmax=1.0\n",
    "                                                                                                    \n",
    "                                                                                                })\n",
    "attached_yellow_blue_marginals_viewer_widget: DecodedEpochSlicesPaginatedFigureController = paginated_multi_decoder_decoded_epochs_window.attached_yellow_blue_marginals_viewer_widget\n",
    "attached_ripple_rasters_widget: RankOrderRastersDebugger = paginated_multi_decoder_decoded_epochs_window.attached_ripple_rasters_widget\n",
    "attached_directional_template_pfs_debugger: TemplateDebugger = paginated_multi_decoder_decoded_epochs_window.attached_directional_template_pfs_debugger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fdfe63",
   "metadata": {
    "tags": [
     "PhoPaginatedMultiDecoderDecodedEpochsWindow"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: filtered_decoder_filter_epochs_decoder_result_dict\n",
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_ripple_filter_epochs_decoder_result_dict=deepcopy(filtered_decoder_filter_epochs_decoder_result_dict),\n",
    " included_columns=['P_decoder', 'ratio_jump_valid_bins', \n",
    "                #    'wcorr',\n",
    "#'avg_jump_cm', 'max_jump_cm',\n",
    "# 'longest_sequence_length', 'continuous_seq_sort', 'continuous_seq_len_ratio_no_repeats',\n",
    "'main_contiguous_subsequence_len'\n",
    "], defer_refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358b43a",
   "metadata": {
    "tags": [
     "active-2024-12-05"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()\n",
    "# paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.update_params(show_pre_merged_debug_sequences=False)\n",
    "# paginated_multi_decoder_decoded_epochs_window.update_params(show_pre_merged_debug_sequences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a252de",
   "metadata": {},
   "source": [
    "### Debug DecodedSequenceAndHeuristicsPlotData in the PhoPaginatedMultiDecoderDecodedEpochsWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequencesPartitioningResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import DecodedSequenceAndHeuristicsPlotData\n",
    "\n",
    "decoded_sequence_and_heuristics_curves_data_dict: Dict[types.DecoderName, Dict[float, DecodedSequenceAndHeuristicsPlotData]] = paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='plots_data.decoded_sequence_and_heuristics_curves_data')\n",
    "decoded_sequence_and_heuristics_partition_results_dict: Dict[types.DecoderName, Dict[float, SubsequencesPartitioningResult]] = {a_name:{k:v.partition_result for k, v in a_data_dict.items()} for a_name, a_data_dict in decoded_sequence_and_heuristics_curves_data_dict.items()}\n",
    "# decoded_sequence_and_heuristics_curves_data_dict\n",
    "# decoded_sequence_and_heuristics_partition_results_dict\n",
    "\n",
    "## OUTPUTS: decoded_sequence_and_heuristics_partition_results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_templates.get_track_length_dict()\n",
    "\n",
    "track_templates.short_LR_decoder.xbin\n",
    "\n",
    "# track_templates.decoder_dict\n",
    "\n",
    "pos_bounds = [np.min([track_templates.long_LR_decoder.xbin, track_templates.short_LR_decoder.xbin]),\n",
    "              np.max([track_templates.long_LR_decoder.xbin, track_templates.short_LR_decoder.xbin])] # [37.0773897438341, 253.98616538463315]\n",
    "pos_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_t_list = [113.37414696447586, ]\n",
    "decoder_name: types.DecoderName = 'long_LR'\n",
    "# decoder_name: types.DecoderName = 'short_LR'\n",
    "absolute_epoch_idx: int = 1\n",
    "a_partition_result: SubsequencesPartitioningResult = list(decoded_sequence_and_heuristics_partition_results_dict[decoder_name].values())[absolute_epoch_idx]\n",
    "a_partition_result.compute()\n",
    "with pd.option_context('display.max_rows', 35):\n",
    "    a_partition_result.position_bins_info_df\n",
    "    a_partition_result.position_changes_info_df\n",
    "\n",
    "a_partition_result.merged_split_positions_arrays\n",
    "a_partition_result.longest_sequence_subsequence\n",
    "a_partition_result._plot_step_by_step_subsequence_partition_process(non_main_sequence_alpha_multiplier=0.2, should_show_non_main_sequence_hlines=True, debug_print=False)\n",
    "# partition_result\n",
    "a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False)\n",
    "\n",
    "# subsequence_lengths: [6 1 1 1 1 2 3 1 3 8], subsequence_len_sort_indicies: [9 0 8 6 5 7 4 3 2 1]\n",
    "# subsequence_lengths: [6 1 1 1 1 2 1 2 1 3 0 8], subsequence_len_sort_indicies: [11  0  9  7  5  8  6  4  3  2  1 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_seq_length_dict = {'neither': a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=False, should_use_no_repeat_values=False),\n",
    "    'ignoring_intru': a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False),\n",
    "    '+no_repeat': a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=True),\n",
    "}\n",
    "\n",
    "longest_seq_length_multiline_label_str: str = '\\n'.join([': '.join([k, str(v)]) for k, v in longest_seq_length_dict.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=SubsequenceDetectionSamples.most_likely_positions_bad_single_main_seq, **SubsequencesPartitioningResult_common_init_kwargs,)\n",
    "# Access the partitioned subsequences\n",
    "subsequences = partition_result.split_positions_arrays\n",
    "merged_subsequences = partition_result.merged_split_positions_arrays\n",
    "print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "subsequences\n",
    "merged_subsequences\n",
    "\n",
    "position_bins_info_df = deepcopy(partition_result.position_bins_info_df)\n",
    "position_changes_info_df = deepcopy(partition_result.position_changes_info_df)\n",
    "position_bins_info_df\n",
    "position_changes_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_data_index: 8\n",
    "\n",
    "    data_x: 154.699102134922\n",
    "    data_y: 206.9536589448834\n",
    "    pixel_x: 260\n",
    "    pixel_y: 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da440cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with Ctx(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "    user_annotations[ctx + Ctx(decoder='long_LR')] = [[149.959, 150.254], [191.609, 191.949], [670.216, 670.418], [808.799, 808.948], [993.868, 994.185]]\n",
    "    user_annotations[ctx + Ctx(decoder='long_RL')] = [[251.417, 251.812], [624.226, 624.499], [637.785, 638.182], [1085.080, 1085.184], [1117.650, 1118.019], [1252.562, 1252.739], [1348.890, 1349.264], [1440.852, 1441.328]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_LR')] = [[105.400, 105.563], [564.149, 564.440], [1085.596, 1086.046], [1161.001, 1161.274], [1302.651, 1302.801], [1332.283, 1332.395], [1705.053, 1705.141], [1707.712, 1707.919], [1729.689, 1730.228]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_RL')] = [[132.511, 132.791], [154.499, 154.853], [1085.596, 1086.046], [1117.650, 1118.019], [1244.038, 1244.176], [1252.562, 1252.739], [1262.523, 1262.926], [1316.056, 1316.270], [1317.977, 1318.181], [1348.890, 1349.264], [1731.111, 1731.288]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e05bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "    user_annotations[ctx + Ctx(decoder='long_LR')] = []\n",
    "    user_annotations[ctx + Ctx(decoder='long_RL')] = [[223.204, 223.514], [235.576, 235.744]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_LR')] = [[223.204, 223.514]]\n",
    "    user_annotations[ctx + Ctx(decoder='short_RL')] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe53433",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_name = 'short_LR'\n",
    "a_pagination_controller = paginated_multi_decoder_decoded_epochs_window.pagination_controllers[a_name]\n",
    "a_plots = a_pagination_controller.plots.decoded_sequence_and_heuristics_curves\n",
    "# plots = a_plots.plots\n",
    "# plots\n",
    "\n",
    "list(a_pagination_controller.plots_data.keys())\n",
    "\n",
    "# a_pagination_controller.filter_epochs_decoder_result\n",
    "a_pagination_controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_filter_epochs_decoder_result: DecodedFilterEpochsResult = a_pagination_controller.plots_data.filter_epochs_decoder_result # .filter_epochs_decoder_result\n",
    "a_single_epoch_decoded_result = a_filter_epochs_decoder_result.get_result_for_epoch_at_time(epoch_start_time=105.40014315512963) # SingleEpochDecodedResult\n",
    "a_single_epoch_decoded_result.epoch_info_tuple\n",
    "\n",
    "a_single_epoch_decoded_result.p_x_given_n\n",
    "a_single_epoch_decoded_result.most_likely_positions\n",
    "# decoded_sequence_and_heuristics_curves_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a01af7",
   "metadata": {},
   "source": [
    "# Test :🔷🚧💯  `SubsequencesPartitioningResult` until it's FULLY WORKING - 2024-12-04 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "48fa82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequencesPartitioningResult\n",
    "from neuropy.utils.indexing_helpers import PandasHelpers\n",
    "\n",
    "class SubsequenceDetectionSamples:\n",
    "    intrusion_example_positions_list = [\n",
    "        [252.083,248.278,248.278,252.083,240.667,229.251,38.9801,198.808,175.975,160.753,153.143,149.337,214.029,145.532,183.586,118.894,77.0343],\n",
    "        [233.056,225.446,225.446,217.835,217.835,217.835,217.835,217.835,240.667,244.473,248.278,248.278,240.667,236.862,240.667,77.0343,214.029,175.975,164.559,153.143,141.726,183.586,58.0072,54.2018],\n",
    "        [217.835,217.835,206.418,195.002,195.002,99.8668,221.64,107.478,96.0614,80.8397,73.2289,80.8397,77.0343],\n",
    "        [92.2559,84.6451,84.6451,92.2559,134.116,126.505,126.505,141.726,149.337,38.9801],\n",
    "    ]\n",
    "\n",
    "    most_likely_positions_bad_single_main_seq = np.array([38.9801,225.446,225.446,145.532,137.921,38.9801,225.446,217.835,175.975,107.478,134.116,145.532,38.9801,38.9801,172.17,149.337,248.278,225.446,153.143,145.532,111.283,69.4234])\n",
    "\n",
    "    jump_bad_list = [\n",
    "     [227.75, 250.404, 182.442, 223.974, 231.526, 84.2736, 38.9652, 235.301, 223.974, 220.199, 227.75, 227.75, 182.442, 182.442, 152.236, 239.077, 239.077, 235.301, 231.526, 250.404],\n",
    "     [182.442, 193.769, 189.993, 174.89, 148.46, 54.068],\n",
    "     [186.217, 186.217, 186.217, 163.563, 91.8249, 69.1708, 186.217, 186.217, 193.769, 201.32, 193.769, 201.32],\n",
    "     [242.853, 242.853, 205.096, 72.9465, 76.7222, 110.703, 144.685, 186.217],\n",
    "    ]\n",
    "    \n",
    "    good_long_sequences_list = [\n",
    "        [84.2736, 80.4979, 133.358, 174.89, 174.89, 186.217, 201.32],\n",
    "    ]\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def get_all_examples(cls):\n",
    "        all_example_dict = {}\n",
    "        all_example_dict.update({f\"intrusion[{i}]\":np.array(arr) for i, arr in enumerate(SubsequenceDetectionSamples.intrusion_example_positions_list)})\n",
    "        all_example_dict.update({f\"jump[{i}]\":np.array(arr) for i, arr in enumerate(SubsequenceDetectionSamples.jump_bad_list)})\n",
    "        all_example_dict.update({f\"great[{i}]\":np.array(arr) for i, arr in enumerate(SubsequenceDetectionSamples.good_long_sequences_list)})\n",
    "        return all_example_dict\n",
    "\n",
    "        # plot_subplot_mosaic_dict = {f\"intrusion_example[{i}]\":dict(sharex=True, sharey=True, mosaic=[[\"ax_ungrouped_seq\"],[\"ax_grouped_seq\"],[\"ax_merged_grouped_seq\"],], gridspec_kw=dict(wspace=0, hspace=0.15)) for i, idx in enumerate(np.arange(num_tabs))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f615758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import SubsequencesPartitioningResult\n",
    "\n",
    "# n_pos_bins: int = 57\n",
    "# max_ignore_bins: int = 2\n",
    "# a_same_thresh_cm: float = 10.700000000000001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "\n",
    "SubsequencesPartitioningResult_common_init_kwargs = dict(same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=60.0, pos_bin_edges=deepcopy(track_templates.long_LR_decoder.xbin), debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = partition_result._plot_step_by_step_subsequence_partition_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=SubsequenceDetectionSamples.most_likely_positions_bad_single_main_seq, **SubsequencesPartitioningResult_common_init_kwargs,)\n",
    "# Access the partitioned subsequences\n",
    "subsequences = partition_result.split_positions_arrays\n",
    "merged_subsequences = partition_result.merged_split_positions_arrays\n",
    "print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "subsequences\n",
    "merged_subsequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333977a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_seq_length_dict = {'neither': partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=False, should_use_no_repeat_values=False),\n",
    "    'ignoring_intru': partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False),\n",
    "    '+no_repeat': partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=True),\n",
    "}\n",
    "\n",
    "longest_seq_length_multiline_label_str: str = '\\n'.join([': '.join([k, str(v)]) for k, v in longest_seq_length_dict.items()])\n",
    "print(longest_seq_length_multiline_label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_equiv_group_list, value_equiv_group_idxs_list = SubsequencesPartitioningResult.find_value_equiv_groups(longest_sequence_subsequence, same_thresh_cm=same_thresh_cm)\n",
    "\n",
    "# a_subsequence = deepcopy(partition_result.longest_sequence_subsequence)\n",
    "a_subsequence = deepcopy([38.9801, 225.446, 225.446])\n",
    "## INPUTS: a_subsequence\n",
    "total_num_values: int = len(a_subsequence)\n",
    "_, value_equiv_group_idxs_list = SubsequencesPartitioningResult.find_value_equiv_groups(a_subsequence, same_thresh_cm=partition_result.same_thresh)\n",
    "total_num_values_excluding_repeats: int = len(value_equiv_group_idxs_list) ## the total number of non-repeated values\n",
    "total_num_repeated_values: int = total_num_values - total_num_values_excluding_repeats\n",
    "print(f'value_equiv_group_idxs_list: {value_equiv_group_idxs_list}')\n",
    "print(f'total_num_values: {total_num_values}')\n",
    "print(f'total_num_values_excluding_repeats: {total_num_values_excluding_repeats}')\n",
    "print(f'total_num_repeated_values: {total_num_repeated_values}')\n",
    "\n",
    "num_items_per_equiv_list: List[int] = [len(v) for v in value_equiv_group_idxs_list] ## number of items in each equiv-list\n",
    "num_equiv_values: int = len(value_equiv_group_idxs_list) # the number of equivalence value sets in the longest subsequence\n",
    "\n",
    "\n",
    "num_equiv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9329a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_bins_info_df = deepcopy(partition_result.position_bins_info_df)\n",
    "position_changes_info_df = deepcopy(partition_result.position_changes_info_df)\n",
    "subsequences_df = deepcopy(partition_result.subsequences_df)\n",
    "position_bins_info_df\n",
    "position_changes_info_df\n",
    "subsequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(subsequences_df.columns)) # ['subsequence_idx', 'positions', 'total_distance_traveled', 'track_coverage_score', 'main_rank', 'is_main', 'flat_idx_first', 'flat_idx_last', 'start_t', 'end_t', 'n_intrusion_bins', 'len', 'len_excluding_repeats', 'len_excluding_intrusions', 'len_excluding_both']\n",
    "\n",
    "\n",
    "['total_distance_traveled', 'track_coverage_score', 'main_rank', 'is_main', 'n_intrusion_bins', 'len', 'len_excluding_repeats', 'len_excluding_intrusions', 'len_excluding_both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a94bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_bins_info_df\n",
    "\n",
    "# Performed 3 aggregations grouped on column: 'decoder_epoch_id'\n",
    "computed_subseq_properties_df = deepcopy(position_bins_info_df).groupby(['subsequence_idx']).agg(flat_idx_first=('flat_idx', 'first'), flat_idx_last=('flat_idx', 'last'),\n",
    "                                                                                            start_t=('t_bin_start', 'first'), end_t=('t_bin_end', 'last'),\n",
    "                                                                                             n_intrusion_bins=('is_intrusion', 'sum')).reset_index()\n",
    "\n",
    "## merge into `subsequences_df`:\n",
    "subsequences_df = subsequences_df.merge(computed_subseq_properties_df, on='subsequence_idx') ## drops missing entries unfortunately\n",
    "# subsequences_df = subsequences_df.merge(computed_subseq_properties_df, how='left', on='subsequence_idx') ## requires that the output dataframe has all rows that were in `subsequences_df`, filling in NaNs when no corresponding values are found in the right df\n",
    "subsequences_df['len_excluding_intrusions'] = subsequences_df['len'] - subsequences_df['n_intrusion_bins']\n",
    "subsequences_df['len_excluding_both'] = subsequences_df['len_excluding_repeats'] - subsequences_df['n_intrusion_bins']\n",
    "subsequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4cd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32686c11",
   "metadata": {
    "tags": [
     "active-2024-12-11"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_multitab import MplMultiTab, MplMultiTab2D, MplTabbedFigure\n",
    "from neuropy.utils.matplotlib_helpers import TabbedMatplotlibFigures\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.CustomMatplotlibTabbedWidget import CustomMplMultiTab\n",
    "\n",
    "all_examples_plot_data_positions_arr_dict = SubsequenceDetectionSamples.get_all_examples()\n",
    "# num_tabs: int = len(SubsequenceDetectionSamples.intrusion_example_positions_list)\n",
    "# plot_subplot_mosaic_dict = {f\"intrusion_example[{i}]\":dict(sharex=True, sharey=True, mosaic=[[\"ax_ungrouped_seq\"],[\"ax_grouped_seq\"],[\"ax_merged_grouped_seq\"],], gridspec_kw=dict(wspace=0, hspace=0.15)) for i, idx in enumerate(np.arange(num_tabs))}\n",
    "\n",
    "num_tabs: int = len(all_examples_plot_data_positions_arr_dict)\n",
    "plot_subplot_mosaic_dict = {a_name:dict(sharex=True, sharey=True, mosaic=[[\"ax_ungrouped_seq\"],[\"ax_grouped_seq\"],[\"ax_merged_grouped_seq\"],], gridspec_kw=dict(wspace=0, hspace=0.15)) for i, (a_name, arr) in enumerate(all_examples_plot_data_positions_arr_dict.items())}\n",
    "\n",
    "# ui = MplMultiTab()\n",
    "# ui, figures_dict, axs_dict = TabbedMatplotlibFigures.build_tabbed_multi_figure(plot_subplot_mosaic_dict, obj_class=MplMultiTab)\n",
    "ui, figures_dict, axs_dict = TabbedMatplotlibFigures.build_tabbed_multi_figure(plot_subplot_mosaic_dict, obj_class=None)\n",
    "\n",
    "# ui.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04634f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "\n",
    "SubsequencesPartitioningResult_common_init_kwargs = dict(same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=60.0, n_pos_bins=57, debug_print=False)\n",
    "\n",
    "## rebuild\n",
    "all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, **SubsequencesPartitioningResult_common_init_kwargs) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "# all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=15.0, n_pos_bins=57, debug_print=False) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "\n",
    "all_examples_plot_data_name_keys = list(all_examples_plot_data_dict.keys())\n",
    "\n",
    "# ui.show()\n",
    "\n",
    "# def _test():\n",
    "# for i, a_most_likely_positions_list in enumerate(SubsequenceDetectionSamples.intrusion_example_positions_list):\n",
    "# for i, (a_name, a_most_likely_positions_list) in enumerate(all_examples_plot_data_positions_arr_dict.items()):\n",
    "for i, (a_name, a_partition_result) in enumerate(all_examples_plot_data_dict.items()):\n",
    "    # a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "    # a_partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, **SubsequencesPartitioningResult_common_init_kwargs)\n",
    "    # Access the partitioned subsequences\n",
    "    subsequences = a_partition_result.split_positions_arrays\n",
    "    merged_subsequences = a_partition_result.merged_split_positions_arrays\n",
    "    print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "    print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "    print(a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False))\n",
    "    a_fig = figures_dict[a_name] # list(figures_dict.values())[i]\n",
    "    an_ax_dict = axs_dict[a_name] # list(axs_dict.values())[i]    \n",
    "    _out = a_partition_result._plot_step_by_step_subsequence_partition_process(extant_ax_dict=an_ax_dict)\n",
    "\n",
    "\n",
    "partition_result = deepcopy(a_partition_result) ## just copy the first one\n",
    "\n",
    "# create plotting function\n",
    "def _perform_plot(fig, indices):\n",
    "    \"\"\" captures: all_examples_plot_data_name_keys, \n",
    "    \"\"\"\n",
    "    print('Doing plot:', indices)\n",
    "    # i, j = indices\n",
    "    i = indices\n",
    "    a_name = all_examples_plot_data_name_keys[i]\n",
    "    print(f'\\t a_name: \"{a_name}\"')\n",
    "    # ax = fig.subplots()\n",
    "    a_partition_result = all_examples_plot_data_dict[a_name]\n",
    "    if isinstance(fig, (MplTabbedFigure, )): # , TabNode\n",
    "        fig = fig.figure ## get the real figure\n",
    "    _out = a_partition_result._plot_step_by_step_subsequence_partition_process(extant_fig=fig, extant_ax_dict=None)\n",
    "    return _out\n",
    "    # return ax.scatter(*np.random.randn(2, n), color=colours[i],  marker=f'${markers[j]}$')\n",
    "\n",
    "# ui.add_task(_perform_plot)   # add your plot worker\n",
    "\n",
    "# ui.set_focus(0)      # this will trigger the plotting for group 0 tab 0\n",
    "ui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf52e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_partition_result: SubsequencesPartitioningResult = all_examples_plot_data_dict['intrusion[2]']\n",
    "a_partition_result: SubsequencesPartitioningResult = all_examples_plot_data_dict['jump[3]']\n",
    "position_info_df, position_changes_info_df = a_partition_result.rebuild_sequence_info_df()\n",
    "position_info_df\n",
    "position_changes_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3096a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_partition_result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_split_indicies = position_changes_info_df[position_changes_info_df['should_split']].index # [1, 4, 5, 6, 8]\n",
    "diff_split_indicies\n",
    "\n",
    "split_indicies = position_changes_info_df[position_changes_info_df['should_split']]['prev_bin_flat_idx'].to_numpy()\n",
    "split_indicies\n",
    "\n",
    "active_split_indicies = deepcopy(split_indicies) ## this is what it should be, but all the splits are +1 later than they should be\n",
    "split_most_likely_positions_arrays = np.split(a_partition_result.flat_positions, active_split_indicies)\n",
    "# a_partition_result.split_positions_arrays = split_most_likely_positions_arrays\n",
    "split_most_likely_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1897c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_partition_result.diff_split_indicies\n",
    "a_partition_result.split_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1939680",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65801ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3219d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_positions_arrays = deepcopy(a_partition_result.split_positions_arrays)\n",
    "merged_split_positions_arrays = deepcopy(a_partition_result.merged_split_positions_arrays)\n",
    "\n",
    "split_positions_arrays\n",
    "merged_split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attrs\n",
    "from qtpy.QtWidgets import QWidget, QToolBar, QFormLayout, QSpinBox, QDoubleSpinBox, QAction\n",
    "\n",
    "included_field_names = ['max_ignore_bins', 'same_thresh', 'max_jump_distance_cm']\n",
    "\n",
    "def create_controls(obj, callback):\n",
    "    \"\"\" tries to build UI controls to dynamically explore the effect of changing the parameters on the detected sequences \"\"\"\n",
    "    w = QWidget()\n",
    "    layout = QFormLayout(w)\n",
    "    for a in attrs.fields(obj.__class__):\n",
    "        desc = a.metadata.get('desc', '')\n",
    "        if a.name in included_field_names:\n",
    "            if a.type == int:\n",
    "                ctrl = QSpinBox()\n",
    "            else:\n",
    "                ctrl = QDoubleSpinBox()\n",
    "\n",
    "            ctrl.setToolTip(desc)\n",
    "            val = getattr(obj, a.name)\n",
    "            if val is not None:\n",
    "                ctrl.setValue(val)\n",
    "\n",
    "            def on_value_changed(val, name=a.name):\n",
    "                print(f'on_value_changed(val: {val}, name: {name})')\n",
    "                setattr(obj, name, val)\n",
    "                callback(obj)\n",
    "\n",
    "            ctrl.valueChanged.connect(on_value_changed)\n",
    "            layout.addRow(a.name, ctrl)\n",
    "\n",
    "\t# END for a in at...\n",
    "\t# layout.addRow(a.name, refresh_action)\n",
    "    return w\n",
    "\n",
    "\n",
    "def on_update_callback(updated_partition_result):\n",
    "    \"\"\" captures: all_examples_plot_data_dict, axs_dict\n",
    "    \"\"\"\n",
    "    print(f'updated_partition_result: {updated_partition_result}')\n",
    "    # updated_partition_result.compute() ## recompute\n",
    "    \n",
    "    # all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=15.0, n_pos_bins=57, debug_print=False) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "    # all_examples_plot_data_dict = {a_name:SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, same_thresh=a_same_thresh_cm, max_ignore_bins=2, max_jump_distance_cm=15.0, n_pos_bins=57, debug_print=False) for a_name, a_most_likely_positions_list in all_examples_plot_data_positions_arr_dict.items()}\n",
    "\n",
    "    for i, (a_name, a_partition_result) in enumerate(all_examples_plot_data_dict.items()):\n",
    "        # a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "        a_partition_result.same_thresh = updated_partition_result.same_thresh\n",
    "        a_partition_result.max_ignore_bins = updated_partition_result.max_ignore_bins\n",
    "        a_partition_result.max_jump_distance_cm = updated_partition_result.max_jump_distance_cm\n",
    "        a_partition_result.compute()\n",
    "        \n",
    "        # partition_result = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list=a_most_likely_positions_list, **SubsequencesPartitioningResult_common_init_kwargs)\n",
    "        # Access the partitioned subsequences\n",
    "        subsequences = a_partition_result.split_positions_arrays\n",
    "        merged_subsequences = a_partition_result.merged_split_positions_arrays\n",
    "        print(\"Number of subsequences before merging:\", len(subsequences))\n",
    "        print(\"Number of subsequences after merging:\", len(merged_subsequences))\n",
    "        print(a_partition_result.get_longest_sequence_length(return_ratio=False, should_ignore_intrusion_bins=True, should_use_no_repeat_values=False))\n",
    "        \n",
    "        # a_fig = list(figures_dict.values())[i]\n",
    "        an_ax_dict = axs_dict[a_name] # list(axs_dict.values())[i]\n",
    "        for an_ax_name, an_ax in an_ax_dict.items():\n",
    "            an_ax.clear()\n",
    "\n",
    "        _out = a_partition_result._plot_step_by_step_subsequence_partition_process(extant_ax_dict=an_ax_dict)\n",
    "        ui.update()\n",
    "        ui.draw()\n",
    "\n",
    "_out_w = create_controls(deepcopy(partition_result), callback=on_update_callback)\n",
    "toolbar = QToolBar(\"Controls\", ui)\n",
    "toolbar.addWidget(_out_w)\n",
    "refresh_action = QAction(\"Refresh\", ui)\n",
    "refresh_action.triggered.connect(lambda: [ctrl.setValue(ctrl.value()) for ctrl in _out_w.findChildren((QSpinBox, QDoubleSpinBox))])\n",
    "toolbar.addAction(refresh_action)\n",
    "ui.addToolBar(toolbar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ead3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.tabs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8681e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ui.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbde0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ui.tabs.__dict__\n",
    "# ui.tabs._items\n",
    "for k, v in ui.tabs._items.items():\n",
    "    print(f\"k: {k}, v: {v}\")\n",
    "    # v.figure\n",
    "    v.canvas.draw()\n",
    "    v.canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ae8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.update()\n",
    "ui.updateGeometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd655382",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_w.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad08e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.compute()\n",
    "position_bins_info_df = deepcopy(partition_result.position_bins_info_df)\n",
    "position_changes_info_df = deepcopy(partition_result.position_changes_info_df)\n",
    "position_bins_info_df\n",
    "position_changes_info_df\n",
    "# partition_result.position_changes_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f398de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_changes_info_df['split_reason'] = '' # str column descripting why the split occured\n",
    "position_changes_info_df['direction'] = np.sign(position_changes_info_df['pos_diff']).astype(int)\n",
    "position_changes_info_df['exceeds_same_thresh'] = (np.abs(position_changes_info_df['pos_diff']) > partition_result.same_thresh)\n",
    "position_changes_info_df['did_accum_dir_change'] = (position_changes_info_df['direction'] != position_changes_info_df['direction'].shift()) & (position_changes_info_df['direction'] != 0)\n",
    "position_changes_info_df['should_split'] = np.logical_and(position_changes_info_df['did_accum_dir_change'], position_changes_info_df['exceeds_same_thresh'])\n",
    "if (partition_result.max_jump_distance_cm is not None):\n",
    "    position_changes_info_df['exceeds_jump_distance'] = (np.abs(position_changes_info_df['pos_diff']) > partition_result.max_jump_distance_cm)\n",
    "    position_changes_info_df['should_split'] = np.logical_or(position_changes_info_df['should_split'], position_changes_info_df['exceeds_jump_distance'])\n",
    "\n",
    "position_changes_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6927e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_changes_info_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e331718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee55301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_changes_info_df.diff\n",
    "# np.diff(position_bins_info_df['pos'])\n",
    "prev_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[:-1]\n",
    "next_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[1:]\n",
    "\n",
    "prev_bin_flat_idxs\n",
    "next_bin_flat_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[:-1]\n",
    "next_bin_flat_idxs = position_bins_info_df['flat_idx'].to_numpy()[1:]\n",
    "\n",
    "position_changes_info_df = pd.DataFrame({'pos_diff': np.diff(position_bins_info_df['pos']),\n",
    "                                         'prev_bin_flat_idx': prev_bin_flat_idxs, 'next_bin_flat_idxs': next_bin_flat_idxs,\n",
    "})\n",
    "position_changes_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVE-2024-12-05\n",
    "partition_result.num_merged_subsequence_bins\n",
    "\n",
    "\n",
    "\n",
    "position_bins_info_df, position_changes_info_df = deepcopy(partition_result.rebuild_sequence_info_df())\n",
    "sequence_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c824573",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_flat_indicies = position_bins_info_df[position_bins_info_df['is_intrusion']]['flat_idx'].to_numpy()\n",
    "is_longest_sequence_bin = (position_bins_info_df['subsequence_idx'] == partition_result.longest_sequence_subsequence_idx) #['flat_idx'].to_numpy()\n",
    "longest_sequence_flatindicies: NDArray = partition_result.longest_sequence_flatindicies\n",
    "longest_sequence_non_intrusion_flatindicies = np.setdiff1d(longest_sequence_flatindicies, intrusion_flat_indicies)\n",
    "longest_sequence_non_intrusion_flatindicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f80ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.split_position_flatindicies_arrays = np.split(partition_result.flat_position_indicies, partition_result.split_indicies)\n",
    "partition_result.merged_split_position_flatindicies_arrays = np.split(partition_result.flat_position_indicies, partition_result.split_indicies)\n",
    "\n",
    "print(partition_result.split_position_flatindicies_arrays) # [array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8, 9]), array([10, 11]), array([12, 13]), array([14]), array([15]), array([16]), array([17, 18, 19, 20, 21])]\n",
    "\n",
    "partition_result.longest_sequence_subsequence_excluding_intrusions\n",
    "partition_result.longest_subsequence_non_intrusion_nbins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ae4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_info_df: pd.DataFrame = partition_result.rebuild_sequence_info_df()\n",
    "sequence_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin by finding only the longest sequence\n",
    "flat_position_idxs = deepcopy(partition_result.flat_position_indicies)\n",
    "subsequences = deepcopy(partition_result.split_positions_arrays)\n",
    "n_tbins_list = np.array([len(v) for v in subsequences])\n",
    "longest_subsequence_idx: int = np.argmax(n_tbins_list)\n",
    "\n",
    "longest_subsequence_positions = subsequences[longest_subsequence_idx] \n",
    "# longest_subsequence_indicies = \n",
    "# longest_subsequence_positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_subsequence_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import LongestSupersequenceFinder\n",
    "\n",
    "# Sample subsequences\n",
    "# subsequences = [\n",
    "#     [38.9801, 225.446, 225.446],\n",
    "#     [145.532, 137.921, 38.9801],\n",
    "#     [225.446, 217.835, 175.975, 107.478],\n",
    "#     [134.116, 145.532],\n",
    "#     [38.9801, 38.9801],\n",
    "#     [172.17],\n",
    "#     [149.337],\n",
    "#     [248.278],\n",
    "#     [225.446, 153.143, 145.532, 111.283, 69.4234]\n",
    "# ]\n",
    "\n",
    "# subsequences =[[38.9801,225.446,225.446], [145.532,137.921,38.9801], [225.446,217.835,175.975,107.478], [134.116,145.532], [38.9801,38.9801], [172.17], [149.337], [248.278], [225.446,153.143,145.532,111.283,69.4234]]\n",
    "# subsequences\n",
    "\n",
    "max_ignore_bins = 2  # Define your maximum ignore bins\n",
    "\n",
    "# Create an instance of the finder\n",
    "finder = LongestSupersequenceFinder([v.tolist() for v in subsequences], max_ignore_bins)\n",
    "\n",
    "# Find the longest supersequence\n",
    "longest_supersequence = finder.find_longest_supersequence()\n",
    "\n",
    "print(\"Longest Supersequence:\")\n",
    "print(longest_supersequence)\n",
    "print(\"\\nLength of the longest supersequence:\", len(longest_supersequence))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079da887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "partition_result. idx\n",
    "\n",
    "\n",
    "\n",
    "n_total_tbins: int = np.sum(n_tbins_list)\n",
    "is_subsequence_potential_intrusion = (n_tbins_list <= partition_result.max_ignore_bins) ## any subsequence shorter than the max ignore distance\n",
    "ignored_subsequence_idxs = np.where(is_subsequence_potential_intrusion)[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ca2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the `most_likely_positions` from the start_t\n",
    "intrusion_example_epoch0 = np.array([624.225748876459, 624.4987573765684])\n",
    "intrusion_example_epoch1 = np.array([637.7847819341114, 638.1821449307026])\n",
    "a_pagination_controller = paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR']\n",
    "a_result = a_pagination_controller.plots_data['filter_epochs_decoder_result'] # DecodedFilterEpochsResult\n",
    "a_single_epoch_decoded_result = a_result.get_result_for_epoch_at_time(epoch_start_time=intrusion_example_epoch1[0]) # SingleEpochDecodedResult\n",
    "a_single_epoch_decoded_result.epoch_info_tuple\n",
    "# a_single_epoch_decoded_result.p_x_given_n\n",
    "a_single_epoch_decoded_result.most_likely_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2c3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f45a343",
   "metadata": {},
   "source": [
    "# <end Test `SubsequencesPartitioningResult`>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4288e0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e593ec20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8cc03",
   "metadata": {
    "tags": [
     "testing"
    ]
   },
   "outputs": [],
   "source": [
    "## 2024-12-03 Constrains existing axes using the figure's layout_engine. Used to create a right margin to render text in (but already factored out)\n",
    "\n",
    "# for an_ax in a_pagination_controller.matplotlib_widget.axes:\n",
    "    \n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    a_fig = a_pagination_controller.matplotlib_widget.fig\n",
    "    # Get current subplots_adjust positions\n",
    "    current_adjust = deepcopy(a_fig.subplotpars) # type(current_adjust): matplotlib.figure.SubplotParams\n",
    "    current_adjust_dict = deepcopy(current_adjust.__dict__) # {'left': 0.125, 'bottom': 0.11, 'right': 0.9, 'top': 0.88, 'wspace': 0.2, 'hspace': 0.2}\n",
    "    print(\"Left:\", current_adjust.left)\n",
    "    print(\"Right:\", current_adjust.right)\n",
    "    print(\"Bottom:\", current_adjust.bottom)\n",
    "    print(\"Top:\", current_adjust.top)\n",
    "    print(\"Wspace:\", current_adjust.wspace)\n",
    "    print(\"Hspace:\", current_adjust.hspace)\n",
    "\n",
    "    # Get the current layout engine\n",
    "    layout_engine = a_fig.get_layout_engine()\n",
    "    print(\"Current layout engine:\", layout_engine) # Current layout engine: <matplotlib.layout_engine.ConstrainedLayoutEngine object at 0x00000176E18FB700>\n",
    "\n",
    "    if isinstance(layout_engine, matplotlib.layout_engine.ConstrainedLayoutEngine):\n",
    "        print(\"Constrained layout is active.\")\n",
    "        # Get the current constrained layout pads\n",
    "        pads = a_fig.get_constrained_layout_pads() # (0.04167, 0.04167, 0.02, 0.02)\n",
    "        pads_dict = dict(zip(['w_pad', 'h_pad', 'wspace', 'hspace'], pads))\n",
    "        pads_dict\n",
    "        print(\"w_pad:\", pads_dict['w_pad'])\n",
    "        print(\"h_pad:\", pads_dict['h_pad'])\n",
    "        print(\"wspace:\", pads_dict['wspace'])\n",
    "        print(\"hspace:\", pads_dict['hspace'])\n",
    "        # Adjust the right margin\n",
    "        # a_fig.set_constrained_layout_pads(right=0.8) # wspace=0.05, hspace=0.05,\n",
    "        curr_layout_rect = deepcopy(layout_engine.__dict__['_params'].get('rect', None)) # {'_params': {'h_pad': 0.04167, 'w_pad': 0.04167, 'hspace': 0.02, 'wspace': 0.02, 'rect': (0, 0, 1, 1)}, '_compress': False}\n",
    "        curr_layout_rect\n",
    "        # layout_engine.get('rect')\n",
    "        layout_engine.set(rect=(0.0, 0.0, 0.8, 1.0)) # ConstrainedLayoutEngine uses rect = (left, bottom, width, height)\n",
    "        \n",
    "    else:\n",
    "        print(\"Other layout engine or none is active.\")\n",
    "\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()\n",
    "\n",
    "# Left: 0.125\n",
    "# Right: 0.9\n",
    "# Bottom: 0.11\n",
    "# Top: 0.88\n",
    "# Wspace: 0.2\n",
    "# Hspace: 0.2\n",
    "\n",
    "# Adjust the right margin\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995259d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.indexing_helpers import NumpyHelpers, PandasHelpers\n",
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import get_track_length_dict\n",
    "from neuropy.utils.indexing_helpers import ListHelpers\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring, HeuristicScoresTuple, SubsequencesPartitioningResult, is_valid_sequence_index\n",
    "from pyphocorehelpers.DataStructure.general_parameter_containers import RenderPlots\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "\n",
    "## INPUTS: track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "decoder_track_length_dict = track_templates.get_track_length_dict() # {a_name:idealized_track_length_dict[a_name.split('_', maxsplit=1)[0]] for a_name, a_result in a_decoded_filter_epochs_decoder_result_dict.items()} # \n",
    "decoder_track_length_dict # {'long_LR': 214.0, 'long_RL': 214.0, 'short_LR': 144.0, 'short_RL': 144.0}\n",
    "## OUTPUTS: decoder_track_length_dict\n",
    "\n",
    "same_thresh_fraction_of_track: float = 0.05 ## up to 5.0% of the track\n",
    "# same_thresh_fraction_of_track: float = 0.15 ## up to 15% of the track\n",
    "same_thresh_cm: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "# same_thresh_n_bin_units: float = {k:(v * same_thresh_fraction_of_track) for k, v in decoder_track_length_dict.items()}\n",
    "max_jump_distance_cm: float = (decoder_track_length_dict['short_LR'] * 0.1) # can't jump more than 45$ of the track\n",
    "print(f'max_jump_distance_cm: {max_jump_distance_cm}')\n",
    "\n",
    "a_result: DecodedFilterEpochsResult = filtered_decoder_filter_epochs_decoder_result_dict['long_LR']\n",
    "# a_result: DecodedFilterEpochsResult = a_decoded_filter_epochs_decoder_result_dict['long_LR'] # 1D\n",
    "an_epoch_idx: int = 36\n",
    "\n",
    "# intrusion_example_epoch0 = np.array([624.225748876459, 624.4987573765684])\n",
    "# intrusion_example_epoch1 = np.array([637.7847819341114, 638.1821449307026])\n",
    "# a_pagination_controller = paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR']\n",
    "# a_result = a_pagination_controller.plots_data['filter_epochs_decoder_result'] # DecodedFilterEpochsResult\n",
    "# a_single_epoch_decoded_result = a_result.get_result_for_epoch_at_time(epoch_start_time=intrusion_example_epoch0[0]) # SingleEpochDecodedResult\n",
    "# a_single_epoch_decoded_result.epoch_info_tuple\n",
    "\n",
    "## INPUTS: a_result: DecodedFilterEpochsResult, an_epoch_idx: int = 1, a_decoder_track_length: float\n",
    "a_most_likely_positions_list = a_result.most_likely_positions_list[an_epoch_idx]\n",
    "a_p_x_given_n = a_result.p_x_given_n_list[an_epoch_idx] # np.shape(a_p_x_given_n): (62, 9)\n",
    "n_time_bins: int = a_result.nbins[an_epoch_idx]\n",
    "n_pos_bins: int = np.shape(a_p_x_given_n)[0]\n",
    "time_window_centers = a_result.time_window_centers[an_epoch_idx]\n",
    "# a_track_length_cm: float = same_thresh_cm['long_LR']\n",
    "a_same_thresh_cm: float = same_thresh_cm['long_LR']\n",
    "# a_same_thresh_cm: float = 0.0\n",
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "\n",
    "print(f'n_time_bins: {n_time_bins}')\n",
    "\n",
    "# INPUTS: a_most_likely_positions_list, n_pos_bins\n",
    "\n",
    "a_first_order_diff = np.diff(a_most_likely_positions_list, n=1, prepend=[a_most_likely_positions_list[0]])\n",
    "assert len(a_first_order_diff) == len(a_most_likely_positions_list), f\"the prepend above should ensure that the sequence and its first-order diff are the same length.\"\n",
    "\n",
    "## 2024-05-09 Smarter method that can handle relatively constant decoded positions with jitter:\n",
    "# partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.partition_subsequences_ignoring_repeated_similar_positions(a_first_order_diff, same_thresh=same_thresh)  # Add 1 because np.diff reduces the index by 1\n",
    "# not_ignoring_similar_partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list, flat_time_window_centers=time_window_centers, n_pos_bins=n_pos_bins, max_ignore_bins=2, same_thresh=0.0)\n",
    "partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.init_from_positions_list(a_most_likely_positions_list, flat_time_window_centers=time_window_centers, n_pos_bins=n_pos_bins, max_ignore_bins=2, same_thresh=a_same_thresh_cm, max_jump_distance_cm=max_jump_distance_cm, debug_print=True)\n",
    "\n",
    "# Split the array at each index where a sign change occurs\n",
    "relative_indicies_arr = np.arange(n_pos_bins)\n",
    "\n",
    "# active_split_indicies = deepcopy(partition_result.split_indicies) ## this is what it should be, but all the splits are +1 later than they should be\n",
    "active_split_indicies = deepcopy(partition_result.diff_split_indicies) ## this is what it should be, but all the splits are +1 later than they should be\n",
    "\n",
    "split_relative_indicies = np.split(relative_indicies_arr, active_split_indicies)\n",
    "split_most_likely_positions_arrays = np.split(a_most_likely_positions_list, active_split_indicies)\n",
    "split_most_likely_positions_arrays\n",
    "\n",
    "# split_first_order_diff_arrays = np.split(a_first_order_diff, partition_result.split_indicies)\n",
    "split_first_order_diff_arrays = np.split(a_first_order_diff, partition_result.diff_split_indicies)\n",
    "\n",
    "# longest_sequence\n",
    "split_diff_index_subsequence_index_arrays = np.split(np.arange(partition_result.n_diff_bins), partition_result.diff_split_indicies) # subtract 1 again to get the diff_split_indicies instead\n",
    "no_low_magnitude_diff_index_subsequence_indicies = [v[np.isin(v, partition_result.low_magnitude_change_indicies, invert=True)] for v in split_diff_index_subsequence_index_arrays] # get the list of indicies for each subsequence without the low-magnitude ones\n",
    "num_subsequence_bins = np.array([len(v) for v in split_diff_index_subsequence_index_arrays]) # np.array([4, 6])\n",
    "num_subsequence_bins_no_repeats = np.array([len(v) for v in no_low_magnitude_diff_index_subsequence_indicies]) # np.array([1, 1])\n",
    "\n",
    "# num_subsequence_bins: number of tbins in each split sequence\n",
    "# num_subsequence_bins_no_repeats\n",
    "\n",
    "total_num_subsequence_bins = np.sum(num_subsequence_bins)\n",
    "total_num_subsequence_bins_no_repeats = np.sum(num_subsequence_bins_no_repeats)\n",
    "\n",
    "longest_sequence_length_no_repeats: int = int(np.nanmax(num_subsequence_bins_no_repeats)) # Now find the length of the longest non-changing sequence\n",
    "longest_sequence_no_repeats_start_idx: int = int(np.nanargmax(num_subsequence_bins_no_repeats)) ## the actual start index of the longest sequence!\n",
    "# longest_sequence_no_repeats_start_idx\n",
    "\n",
    "\n",
    "# _tmp_merge_split_positions_arrays, final_out_subsequences, (subsequence_replace_dict, subsequences_to_add, subsequences_to_remove, final_intrusion_idxs) = partition_result.merge_over_ignored_intrusions(max_ignore_bins=2, debug_print=True)\n",
    "# subsequence_replace_dict\n",
    "# print(subsequences_to_remove)\n",
    "# print(subsequences_to_add)\n",
    "# final_intrusion_idxs\n",
    "# final_out_subsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e70659",
   "metadata": {},
   "source": [
    "## Program Goal: Find the longest possible subsequence such that none of the following requirements are violated:\n",
    "1. the `candidate_subsequence` contains no intruding subsequence with length longer than `max_ignore_bins` bins  \n",
    "2. the total number of intruding subsequence time bins does not exceed `(len(candidate_subsequence)/2.0)` -- \"more sequence bins than intruding bins\"\n",
    "maxbridged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb785ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.merged_split_positions_arrays\n",
    "# partition_result.bridged_intrusion_bin_indicies\n",
    "partition_result.sequence_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d87693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out: MatplotlibRenderPlots = _plot_step_by_step_subsequence_partition_process(partition_result=partition_result)\n",
    "# out\n",
    "\n",
    "out: MatplotlibRenderPlots = partition_result._plot_step_by_step_subsequence_partition_process()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.partition_subsequences()\n",
    "partition_result.merge_intrusions()\n",
    "partition_result.merged_split_positions_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out2: MatplotlibRenderPlots = partition_result._plot_step_by_step_subsequence_partition_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example Sequences\n",
    "partition_result.list_parts\n",
    "\n",
    "with pd.option_context('display.max_rows', 100):\n",
    "    partition_result.sequence_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_info_df: pd.DataFrame = partition_result.rebuild_sequence_info_df()\n",
    "with pd.option_context('display.max_rows', 100):\n",
    "    display(sequence_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49441002",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6810d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_result.longest_subsequence_length\n",
    "partition_result.longest_sequence_length_no_repeats\n",
    "partition_result.longest_sequence_no_repeats_start_idx\n",
    "partition_result.longest_sequence_subsequence\n",
    "\n",
    "partition_result.first_order_diff_lst\n",
    "partition_result.low_magnitude_change_indicies\n",
    "partition_result.diff_split_indicies\n",
    "partition_result.split_indicies\n",
    "\n",
    "# get_longest_sequence_length_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73587c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'a_same_thresh_cm: {a_same_thresh_cm}')\n",
    "if isinstance(a_most_likely_positions_list, (list, tuple, )):\n",
    "    a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "\n",
    "(sub_change_equivalency_groups, sub_change_equivalency_group_values), (list_parts, list_split_indicies, sub_change_threshold_change_indicies) = SubsequencesPartitioningResult.detect_repeated_similar_positions(a_most_likely_positions_list, same_thresh=a_same_thresh_cm)\n",
    "sub_change_equivalency_groups\n",
    "# list_parts\n",
    "sub_change_equivalency_group_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sequence_subsequence = deepcopy(partition_result.longest_sequence_subsequence)\n",
    "longest_sequence_subsequence_partition_result: SubsequencesPartitioningResult = SubsequencesPartitioningResult.init_from_positions_list(longest_sequence_subsequence, n_pos_bins=n_pos_bins, max_ignore_bins=2, same_thresh=a_same_thresh_cm)\n",
    "longest_sequence_subsequence_partition_result.merged_split_positions_arrays ## makes things worse\n",
    "longest_sequence_subsequence_partition_result.list_parts\n",
    "longest_sequence_subsequence_partition_result.split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos_bins: int = 57\n",
    "a_same_thresh_cm: float = 32.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import function_attributes\n",
    "\n",
    "\n",
    "@function_attributes(short_name=None, tags=['split'], 'UNUSED', 'UNVALIDATED', 'fresh-reimpleemntation-attempt', input_requires=[], output_provides=[], uses=[], used_by=['_compute_should_split_arr'], creation_date='2024-12-04 04:02', related_items=[])\n",
    "def _compute_is_change_point_split_arr(first_order_diff_lst):\n",
    "    ## INPUTS: prev_accum_dir, first_order_diff_lst\n",
    "    prev_accum_dir = None # sentinal value\n",
    "    is_change_point_arr = []\n",
    "    did_accum_dir_change_arr = []\n",
    "    for i, v in enumerate(first_order_diff_lst):\n",
    "        curr_dir = np.sign(v)\n",
    "        did_accum_dir_change: bool = (prev_accum_dir != curr_dir)# and (prev_accum_dir is not None) and (prev_accum_dir != 0)\n",
    "        did_accum_dir_change_arr.append(did_accum_dir_change)\n",
    "        is_change_point: bool = True # (prev_accum_dir is None)\n",
    "        if did_accum_dir_change: \n",
    "            ## Exceeds the `same_thresh` indicating we want to use the change\n",
    "            ## sign changed, split here.\n",
    "            is_change_point = True\n",
    "            if (curr_dir != 0):\n",
    "                # only for non-zero directions should we set the prev_accum_dir, otherwise leave it what it was (or blank)\n",
    "                if prev_accum_dir is None:\n",
    "                    is_change_point = False # don't split for the first direction change (since it's a change from None/0.0\n",
    "                else:\n",
    "                    is_change_point = True\n",
    "                # ## either way update the prev_accum_dir\n",
    "                # prev_accum_dir = curr_dir\n",
    "            else:\n",
    "                print(f'debug: iteration[{i}] - v: {v} - curr_dir == 0')\n",
    "                ## #TODO 2024-12-04 04:15: - [ ] if it's 0, we consider it a change point\n",
    "                is_change_point = False\n",
    "\n",
    "            ## return should_split\n",
    "            # is_change_point_arr.append(is_change_point)\n",
    "            # END if (np.abs(v) > same_thresh)\n",
    "        else:\n",
    "            is_change_point = False # no change, shouldn't split\n",
    "            # is_change_point_arr.append(is_change_point)\n",
    "            ## normally continue accumulating without splitting\n",
    "        # END if did_accum_dir_change ...\n",
    "        is_change_point_arr.append(is_change_point) ## now `is_change_point` should be correct\n",
    "        ## either way update the prev_accum_dir\n",
    "        prev_accum_dir = curr_dir\n",
    "    \n",
    "\n",
    "    # end for i, v \n",
    "    is_change_point_arr = np.array(is_change_point_arr)\n",
    "    did_accum_dir_change_arr = np.array(did_accum_dir_change_arr)\n",
    "    return is_change_point_arr, did_accum_dir_change_arr\n",
    "    # return should_split_arr, did_accum_dir_change_arr\n",
    "\n",
    "\n",
    "\n",
    "@function_attributes(short_name=None, tags=['split', 'UNUSED', 'UNVALIDATED', 'fresh-reimpleemntation-attempt'], input_requires=[], output_provides=[], uses=['_compute_is_change_point_split_arr'], used_by=[], creation_date='2024-12-04 04:02', related_items=[])\n",
    "def _compute_should_split_arr(a_most_likely_positions_list, same_thresh: float):\n",
    "    \"\"\" \n",
    "    should_split = _compute_should_split_arr(first_order_diff_lst)\n",
    "    should_split\n",
    "    \n",
    "    \"\"\"\n",
    "    ## INPUTS: prev_accum_dir, first_order_diff_lst\n",
    "    ## INPUTS: a_most_likely_positions_list, same_thresh, \n",
    "    if isinstance(a_most_likely_positions_list, list):\n",
    "        a_most_likely_positions_list = np.array(a_most_likely_positions_list)\n",
    "\n",
    "    first_order_diff_lst = np.diff(a_most_likely_positions_list, n=1, prepend=[a_most_likely_positions_list[0]])\n",
    "    assert len(first_order_diff_lst) == len(a_most_likely_positions_list), f\"the prepend above should ensure that the sequence and its first-order diff are the same length.\"\n",
    "    is_change_point_arr, did_accum_dir_change_arr = _compute_is_change_point_split_arr(first_order_diff_lst)\n",
    "    is_subthreshold = (np.abs(first_order_diff_lst) <= same_thresh)\n",
    "    # np.logical_and(did_accum_dir_change_arr, np.logical_not(is_subthreshold))\n",
    "    should_split = np.logical_and(is_change_point_arr, np.logical_not(is_subthreshold))    \n",
    "    return should_split, (is_change_point_arr, is_subthreshold)\n",
    "\n",
    "\n",
    "# longest_sequence_subsequence = deepcopy(partition_result.longest_sequence_subsequence)\n",
    "# a_most_likely_positions_list = longest_sequence_subsequence\n",
    "a_most_likely_positions_list = deepcopy(partition_result.flat_positions)\n",
    "same_thresh = a_same_thresh_cm\n",
    "should_split, (is_change_point_arr, is_subthreshold) = _compute_should_split_arr(a_most_likely_positions_list, same_thresh=same_thresh)\n",
    "should_split\n",
    "is_change_point_arr\n",
    "is_subthreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06486aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "longest_sequence_subsequence\n",
    "(sub_change_equivalency_groups, sub_change_equivalency_group_values), (list_parts, list_split_indicies, sub_change_threshold_change_indicies) = SubsequencesPartitioningResult.detect_repeated_similar_positions(longest_sequence_subsequence, same_thresh=a_same_thresh_cm)\n",
    "sub_change_equivalency_groups\n",
    "# list_parts\n",
    "sub_change_equivalency_group_values\n",
    "sub_change_threshold_change_indicies\n",
    "list_split_indicies \n",
    "list_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.merged_split_positions_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c087abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_ignoring_similar_partition_result\n",
    "not_ignoring_similar_partition_result.merged_split_positions_arrays # subsequence_index_lists_omitting_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd465fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.total_num_subsequence_bins\n",
    "partition_result.total_num_subsequence_bins_no_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76744733",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.low_magnitude_change_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_sequence_info_df: pd.DataFrame = partition_result.rebuild_sequence_info_df()\n",
    "longest_subsequence_df: pd.DataFrame = rebuild_sequence_info_df[rebuild_sequence_info_df['subsequence_idx'] == 1]\n",
    "longest_subsequence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a89329",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.merged_split_positions_arrays\n",
    "split_arr_lengths = [len(v) for v in partition_result.split_positions_arrays]\n",
    "split_arr_lengths = flatten([[i] * len(v) for i, v in enumerate(partition_result.split_positions_arrays)])\n",
    "\n",
    "split_arr_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72569080",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.low_magnitude_change_indicies\n",
    "partition_result.split_indicies\n",
    "partition_result.num_merged_subsequence_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8370550",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ignore_bins: int = 2\n",
    "_tmp_merge_split_positions_arrays, final_out_subsequences, (subsequence_replace_dict, subsequences_to_add, subsequences_to_remove) = partition_result.merge_over_ignored_intrusions(max_ignore_bins=max_ignore_bins)\n",
    "_tmp_merge_split_positions_arrays\n",
    "subsequence_replace_dict\n",
    "final_out_subsequences\n",
    "\n",
    "# subsequences = deepcopy(partition_result.split_positions_arrays)\n",
    "# subsequences\n",
    "\n",
    "# remaining_subsequence_list= [[119.191, 142.107, 180.3, 191.757, 245.227], [84.8181, 84.8181, 84.8181]]\n",
    "# remaining_subsequence_list.reverse()\n",
    "# remaining_subsequence_list\n",
    "# curr_subsequence, remaining_subsequence_list = merge_subsequences([138.288, 134.469], remaining_subsequence_list=remaining_subsequence_list)\n",
    "\n",
    "# merged_subsequences = merge_subsequences(subsequences, max_ignore_bins=1)\n",
    "# merged_subsequences\n",
    "fig2, ax2 = _debug_plot_time_bins_multiple(positions_list=final_out_subsequences, num='debug_plot_merged_time_binned_positions')\n",
    "\n",
    "# array([138.288, 134.469]), array([69.5411]), array([249.046, 249.046, 249.046])\n",
    "# array([138.288, 134.469, 69.5411, 249.046, 249.046, 249.046])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (left_congruent_flanking_sequence, left_congruent_flanking_index), (right_congruent_flanking_sequence, right_congruent_flanking_index) = _compute_sequences_spanning_ignored_intrusions(split_first_order_diff_arrays, continuous_sequence_lengths, longest_sequence_start_idx=longest_sequence_start_idx, max_ignore_bins=max_ignore_bins)\n",
    "(left_congruent_flanking_sequence, left_congruent_flanking_index), (right_congruent_flanking_sequence, right_congruent_flanking_index) = _compute_sequences_spanning_ignored_intrusions(split_first_order_diff_arrays, num_subsequence_bins_no_repeats,\n",
    "                                                                                                                                                                                        target_subsequence_idx=longest_sequence_no_repeats_start_idx, max_ignore_bins=max_ignore_bins)\n",
    "print(f\"{left_congruent_flanking_sequence}: {left_congruent_flanking_sequence}\")\n",
    "print(f\"{right_congruent_flanking_sequence}: {right_congruent_flanking_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.low_magnitude_change_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5657fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_result.list_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb31299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_debug_plot_time_binned_positions(a_most_likely_positions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b32de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sequence_length_ratio = HeuristicReplayScoring.bin_wise_continuous_sequence_sort_score_fn(a_result=a_result, an_epoch_idx=an_epoch_idx, a_decoder_track_length=170.0, same_thresh=same_thresh)\n",
    "longest_sequence_length_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_filter_epochs_df = deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "active_filter_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4bb35",
   "metadata": {},
   "source": [
    "# 🖼️🎨`PhoPaginatedMultiDecoderDecodedEpochsWindow.plot_full_paginated_decoded_epochs_window(..)` combined windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949e48a",
   "metadata": {
    "tags": [
     "run-heuristic-filter"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import RadonTransformPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow, DecodedEpochSlicesPaginatedFigureController, EpochSelectionsObject, ClickActionCallbacks\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import co_filter_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import get_proper_global_spikes_df\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TemplateDebugger import TemplateDebugger\n",
    "\n",
    "## INPUTS: directional_decoders_epochs_decode_result, filtered_epochs_df\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "unfiltered_epochs_df = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }')\n",
    "\n",
    "## OUTPUTS: unfiltered_epochs_df, decoder_ripple_filter_epochs_decoder_result_dict\n",
    "## OUTPUTS: filtered_epochs_df, filtered_decoder_filter_epochs_decoder_result_dict\n",
    "\n",
    "# posterior_heatmap_imshow_kwargs = {'cmap': orange_posterior_cmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', render_track_identity_marginal_ripples=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46868c2c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### Custom click callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad5dfd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import ClickActionCallbacks\n",
    "\n",
    "is_enabled = True\n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # a_pagination_controller.params.debug_print = True    \n",
    "    print(f\"a_pagination_controller.params['on_middle_click_item_callbacks']: {a_pagination_controller.params['on_middle_click_item_callbacks']}\")\n",
    "    print(f\"a_pagination_controller.params['on_secondary_click_item_callbacks']: {a_pagination_controller.params.get('on_secondary_click_item_callbacks', {})}\")\n",
    "    a_pagination_controller.params.should_suppress_callback_exceptions = False\n",
    "    \n",
    "    if not a_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "        a_pagination_controller.params['on_middle_click_item_callbacks'] = {}\n",
    "        \n",
    "    if not a_pagination_controller.params.has_attr('on_secondary_click_item_callbacks'):\n",
    "        a_pagination_controller.params['on_secondary_click_item_callbacks'] = {}\n",
    "        \n",
    "    a_pagination_controller.params['on_secondary_click_item_callbacks'] = {}\n",
    "    \n",
    "    if is_enabled:\n",
    "        # a_pagination_controller.params.on_middle_click_item_callbacks['copy_click_time_to_clipboard_callback'] = ClickActionCallbacks.copy_click_time_to_clipboard_callback\n",
    "        # a_pagination_controller.params.on_secondary_click_item_callbacks['copy_click_time_to_clipboard_callback'] = ClickActionCallbacks.copy_click_time_to_clipboard_callback\n",
    "        # a_pagination_controller.params.on_secondary_click_item_callbacks['copy_axis_image_to_clipboard_callback'] = ClickActionCallbacks.copy_axis_image_to_clipboard_callback\n",
    "        a_pagination_controller.params.on_secondary_click_item_callbacks['copy_axis_image_to_clipboard_callback'] = ClickActionCallbacks.copy_selected_posterior_to_message_box_callback\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # a_pagination_controller.params.on_middle_click_item_callbacks.pop('copy_click_time_to_clipboard_callback', None)\n",
    "        # a_pagination_controller.params.on_secondary_click_item_callbacks.pop('copy_click_time_to_clipboard_callback', None)\n",
    "        a_pagination_controller.params.on_secondary_click_item_callbacks.pop('copy_axis_image_to_clipboard_callback', None)\n",
    "        \n",
    "    # a_pagination_controller.params.on_secondary_click_item_callbacks.pop('copy_epoch_times_to_clipboard_callback', None)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# paginated_multi_decoder_decoded_epochs_window.params.on_middle_click_item_callbacks['copy_axis_image_to_clipboard_callback'] = ClickActionCallbacks.copy_axis_image_to_clipboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd21b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import ClickActionCallbacks\n",
    "\n",
    "# dir(ClickActionCallbacks)\n",
    "# ClickActionCallbacks.copy_axis_image_to_clipboard_callback,\n",
    "\n",
    "object_methods = [method_name for method_name in dir(ClickActionCallbacks) if callable(getattr(ClickActionCallbacks, method_name)) and not method_name.startswith('_')]\n",
    "print(object_methods)\n",
    "# methods_list = [method[0] for method in inspect.getmembers(ClickActionCallbacks, predicate=inspect.isfunction)]\n",
    "possible_mouse_actions_dict: Dict[str, Callable] = {method[0]:method[1] for method in inspect.getmembers(ClickActionCallbacks, predicate=inspect.isfunction)}\n",
    "print(possible_mouse_actions_dict)\n",
    "# click_options = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.global_thin_button_bar_widget\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='params.possible_mouse_actions_dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b15c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='params.on_left_click_item_callbacks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window.add_mouse_action_controls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.global_thin_button_bar_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_combo_box_text: str = 'copy_axis_image_to_clipboard_callback'\n",
    "selected_callback_fn = ClickActionCallbacks.copy_axis_image_to_clipboard_callback\n",
    "on_middle_click_item_callbacks_dict = paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='params.on_middle_click_item_callbacks')\n",
    "on_middle_click_item_callbacks_dict\n",
    "\n",
    "for a_name, a_callback_dict in on_middle_click_item_callbacks_dict.items():\n",
    "    print(f'a_name: {a_name}')\n",
    "    a_callback_dict[selected_combo_box_text] = selected_callback_fn\n",
    "    for old_method_name in methods_list:\n",
    "        a_callback_dict.pop(old_method_name)\n",
    "    \n",
    "\n",
    "on_middle_click_item_callbacks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.get_children_props(prop_path='params.on_secondary_click_item_callbacks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.update_params(possible_mouse_actions_dict = {\n",
    "            'copy_axis_image_to_clipboard_callback': ClickActionCallbacks.copy_axis_image_to_clipboard_callback,\n",
    "            'copy_click_time_to_clipboard_callback': ClickActionCallbacks.copy_click_time_to_clipboard_callback,\n",
    "            'copy_epoch_times_to_clipboard_callback': ClickActionCallbacks.copy_epoch_times_to_clipboard_callback,\n",
    "            'log_clicked_epoch_times_to_message_box_callback': ClickActionCallbacks.log_clicked_epoch_times_to_message_box_callback\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# from PyQt5.QtWidgets import QApplication, QMainWindow, QToolBar, QComboBox, QAction\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.Qt import QT_LIB, QtCore, QtGui, QtWidgets\n",
    "\n",
    "QT_LIB\n",
    "# from qtpy import QtCore, QtWidgets\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph.QtWidgets as QToolBar, QComboBox, QAction\n",
    "# pg.QtWidgets\n",
    "# pg.QtWidgets.Q\n",
    "# from pg.QtWidgets import QToolBar, QComboBox, QAction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestMainWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the list of methods\n",
    "        self.methods_list = [\"Method1\", \"Method2\", \"Method3\", \"Method4\"]\n",
    "\n",
    "        # Initialize the toolbar\n",
    "        self.toolbar = QToolBar(\"Toolbar\", self)\n",
    "        self.toolbar.setMovable(False)\n",
    "        self.toolbar.setFixedHeight(30)  # Making the toolbar thin\n",
    "\n",
    "        # Create and add the left mouse button (LMB) combo box\n",
    "        self.lmb_combo = QComboBox()\n",
    "        self.lmb_combo.addItems(self.methods_list)\n",
    "        lmb_action = QAction(\"LMB Action\", self)\n",
    "        lmb_action.setStatusTip(\"Choose action for LMB\")\n",
    "        self.toolbar.addAction(lmb_action)\n",
    "        self.toolbar.addWidget(self.lmb_combo)\n",
    "\n",
    "        # Create and add the middle mouse button (MMB) combo box\n",
    "        self.mmb_combo = QComboBox()\n",
    "        self.mmb_combo.addItems(self.methods_list)\n",
    "        mmb_action = QAction(\"MMB Action\", self)\n",
    "        mmb_action.setStatusTip(\"Choose action for MMB\")\n",
    "        self.toolbar.addAction(mmb_action)\n",
    "        self.toolbar.addWidget(self.mmb_combo)\n",
    "\n",
    "        # Create and add the right mouse button (RMB) combo box\n",
    "        self.rmb_combo = QComboBox()\n",
    "        self.rmb_combo.addItems(self.methods_list)\n",
    "        rmb_action = QAction(\"RMB Action\", self)\n",
    "        rmb_action.setStatusTip(\"Choose action for RMB\")\n",
    "        self.toolbar.addAction(rmb_action)\n",
    "        self.toolbar.addWidget(self.rmb_combo)\n",
    "\n",
    "        # Adding the toolbar to the main window\n",
    "        self.addToolBar(self.toolbar)\n",
    "\n",
    "        # Setting the main window properties\n",
    "        self.setWindowTitle('Mouse Button Action Selector')\n",
    "        self.setGeometry(100, 100, 400, 200)\n",
    "        \n",
    "\n",
    "app = pg.mkQApp('test toolbar mouse button actions')\n",
    "mainWindow = TestMainWindow()\n",
    "mainWindow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769f9fd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## printing the callback values don't seem to work until after `paginated_multi_decoder_decoded_epochs_window.add_data_overlays(...)` is called.\n",
    "# paginated_multi_decoder_decoded_epochs_window.enable_middle_click_selected_epoch_times_to_clipboard(is_enabled=False)\n",
    "paginated_multi_decoder_decoded_epochs_window.enable_middle_click_selected_epoch_times_to_clipboard(is_enabled=True)\n",
    "\n",
    "# clicked_epoch = np.array([132.51138943410479, 132.79100273095537])\n",
    "\n",
    "# clicked_epoch = np.array([149.95935746072792, 150.25439218967222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_example_epoch0 = np.array([624.225748876459, 624.4987573765684])\n",
    "intrusion_example_epoch1 = np.array([637.7847819341114, 638.1821449307026])\n",
    "\n",
    "a_pagination_controller = paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR']\n",
    "a_result = a_pagination_controller.plots_data['filter_epochs_decoder_result'] # DecodedFilterEpochsResult\n",
    "a_single_epoch_decoded_result = a_result.get_result_for_epoch_at_time(epoch_start_time=intrusion_example_epoch0[0]) # SingleEpochDecodedResult\n",
    "a_single_epoch_decoded_result.epoch_info_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e3bba",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "clicked_epoch_start_stop_time = [488.296 488.484]\n",
    "start_t = 488.29642327222973\n",
    "found_IDX = 24\n",
    "\n",
    "# ripple_idx=80, ripple_start_t=488.29642327222973\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52a4f4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "@function_attributes(short_name=None, tags=['callback'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-29 17:16', related_items=[])\n",
    "def an_alt_clicked_epoch_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user middle-clicks an epoch \n",
    "    \n",
    "    captures: _out_ripple_rasters\n",
    "    \"\"\"\n",
    "    print(f'an_alt_clicked_epoch_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            print(f'start_t: {start_t}')\n",
    "            _out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19d578",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Enable programmatically updating the rasters viewer to the clicked epoch index when middle clicking on a posterior.\n",
    "@function_attributes(short_name=None, tags=['callback'], input_requires=[], output_provides=[], uses=[], used_by=[], creation_date='2024-04-29 17:16', related_items=[])\n",
    "def an_alt_clicked_epoch_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user middle-clicks an epoch \n",
    "    \n",
    "    captures: _out_ripple_rasters\n",
    "    \"\"\"\n",
    "    print(f'an_alt_clicked_epoch_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            print(f'start_t: {start_t}')\n",
    "            _out_ripple_rasters.programmatically_update_epoch_IDX_from_epoch_start_time(start_t)\n",
    "\n",
    "\n",
    "for a_name, a_pagination_controller in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # a_pagination_controller.params.debug_print = True\n",
    "    if not a_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "        a_pagination_controller.params['on_middle_click_item_callbacks'] = {}    \n",
    "    a_pagination_controller.params.on_middle_click_item_callbacks['an_alt_clicked_epoch_callback'] = an_alt_clicked_epoch_callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fa30c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ThinButtonBar.ThinButtonBarWidget import ThinButtonBarWidget\n",
    "\n",
    "a_name = 'long_RL'\n",
    "a_pagination_controller = paginated_multi_decoder_decoded_epochs_window.pagination_controllers[a_name]\n",
    "a_controlled_widget = a_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "\n",
    "thin_button_bar_widget: ThinButtonBarWidget = a_controlled_widget.ui.thin_button_bar_widget\n",
    "# thin_button_bar_widget.label_message = \"<controlled>\"\n",
    "# thin_button_bar_widget.txtLineEdit\n",
    "# thin_button_bar_widget.ui.txtLineEdit.setText('test')\n",
    "# thin_button_bar_widget.ui.txtLineEdit.text\n",
    "\n",
    "# thin_button_bar_widget.parent().update()\n",
    "# a_controlled_widget.update()\n",
    "# print_keys_if_possible('a_pagination_controller.ui', a_pagination_controller.ui, max_depth=2)\n",
    "# thin_button_bar_widget.label_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fbe1b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: a_pagination_controller\n",
    "a_decoder_decoded_epochs_result: DecodedFilterEpochsResult = a_pagination_controller.plots_data.filter_epochs_decoder_result\n",
    "\n",
    "active_epoch_data_idx: int = 28\n",
    "print(f'active_epoch_data_idx: {active_epoch_data_idx}')\n",
    "active_captured_single_epoch_result: SingleEpochDecodedResult = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=active_epoch_data_idx)\n",
    "active_captured_single_epoch_result\n",
    "\n",
    "## Outputs: active_captured_single_epoch_result\n",
    "\n",
    "# filter_epochs_decoder_result.filter_epochs\n",
    "# filter_epochs_decoder_result.p_x_given_n_list["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431df389",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "included_page_data_indicies, (curr_page_active_filter_epochs, curr_page_epoch_labels, curr_page_time_bin_containers, curr_page_posterior_containers) = a_pagination_controller.plots_data.paginator.get_page_data(page_idx=a_pagination_controller.current_page_idx)\n",
    "\n",
    "# for i, curr_ax in enumerate(self.plots.axs):\n",
    "    \n",
    "curr_page_rel_idx: int = 0\n",
    "curr_slice_idx: int = included_page_data_indicies[curr_page_rel_idx]\n",
    "curr_epoch_slice = curr_page_active_filter_epochs[curr_page_rel_idx]\n",
    "curr_time_bin_container = curr_page_time_bin_containers[curr_page_rel_idx]\n",
    "curr_posterior_container = curr_page_posterior_containers[curr_page_rel_idx]\n",
    "curr_time_bins = curr_time_bin_container.centers\n",
    "curr_posterior = curr_posterior_container.p_x_given_n\n",
    "curr_most_likely_positions = curr_posterior_container.most_likely_positions_1D\n",
    "\n",
    "curr_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fcc2c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_pagination_controller.get_total_pages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe4653",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for i, (a_name, a_pagination_controller) in enumerate(paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items()):\n",
    "    print(f'i: {i}, a_name: {a_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654374c6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81780963",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.show_message(\"test message\", durationMs=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca820df",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289385ce",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "filtered_decoder_filter_epochs_decoder_result_dict['long_LR'].filter_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6097ba",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Get radon transform data:\n",
    "a_pagination_controller = pagination_controller_dict['long_LR']\n",
    "radon_transform_data = a_pagination_controller.plots_data['radon_transform_data']\n",
    "radon_transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30830fa4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120293e6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# active_selections_dict = paginated_multi_decoder_decoded_epochs_window.save_selections()\n",
    "# paginated_multi_decoder_decoded_epochs_window.ui.print = print\n",
    "_annotations = paginated_multi_decoder_decoded_epochs_window.print_user_annotations()\n",
    "_annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d82308",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "pagination_controller_dict['long_LR'].params.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f785638",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_data_overlays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ee5dc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3435812",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.params.xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd64912",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Show crosshair at cursor position\n",
    "plt.connect('motion_notify_event', lambda event: plt.gcf().gca().format_coord(event.xdata, event.ydata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c382b69",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, filtered_decoder_filter_epochs_decoder_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e9ba0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "print_keys_if_possible('paginated_multi_decoder_decoded_epochs_window', paginated_multi_decoder_decoded_epochs_window.ui, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a14372",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.gui.Qt.widgets.toast_notification_widget import ToastWidget, ToastShowingWidgetMixin\n",
    "# paginated_multi_decoder_decoded_epochs_window.ui._contents.windows\n",
    "\n",
    "for a_name, a_window in paginated_multi_decoder_decoded_epochs_window.ui._contents.windows.items():\n",
    "    message = 'This is a toast message!'\n",
    "    a_window.toast.show_message(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83380a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "clicked_epoch = np.array([1316.0564141790383, 1316.2703788694926])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1ff44",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### Attached raster viewer widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c265cd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.RankOrderRastersDebugger import RankOrderRastersDebugger\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import build_attached_raster_viewer_widget\n",
    "\n",
    "_out_ripple_rasters, update_attached_raster_viewer_epoch_callback = build_attached_raster_viewer_widget(paginated_multi_decoder_decoded_epochs_window=paginated_multi_decoder_decoded_epochs_window, track_templates=track_templates, active_spikes_df=active_spikes_df, filtered_ripple_simple_pf_pearson_merged_df=filtered_ripple_simple_pf_pearson_merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2865a6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.export_decoder_pagination_controller_figure_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5b2c2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# type(_out_ripple_rasters) # RankOrderRastersDebugger\n",
    "# root_plots_dict: Dict[str, pg.PlotItem] = _out_ripple_rasters.root_plots_dict\n",
    "# root_plots_dict\n",
    "\n",
    "rasters_output_path = Path(r\"C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\EXTERNAL\\PhoDibaPaper2024Book\\FIGURES\").resolve()\n",
    "assert rasters_output_path.exists()\n",
    "example_replay_output_folder = rasters_output_path.joinpath('example_replay_2').resolve()\n",
    "example_replay_output_folder.mkdir(parents=False, exist_ok=True)\n",
    "_out_ripple_rasters.save_figure(export_path=example_replay_output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f3ff1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625daf82",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "win = _out_ripple_rasters.ui.root_dockAreaWindow\n",
    "# win.setWindowTitle(f'Debug Directional Template Rasters <Controlled by DecodedEpochSlices window>')\n",
    "win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae668b2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out_ripple_rasters.setWindowTitle(f'Debug Directional Template Rasters <Controlled by DecodedEpochSlices window>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db798a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Attempting to set identical low and high xlims makes transformation singular; automatically expanding. Is this what is causing the white posteriors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80c795",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddc065",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR'].params.posterior_heatmap_imshow_kwargs = dict(vmin=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23369f63",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# paginated_multi_decoder_decoded_epochs_window.update_params(posterior_heatmap_imshow_kwargs = dict(vmin=0.0))\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.update_params(enable_per_epoch_action_buttons=True)\n",
    "paginated_multi_decoder_decoded_epochs_window.refresh_current_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370bef3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.get_children_props('params')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('plots')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('plots.fig')\n",
    "paginated_multi_decoder_decoded_epochs_window.get_children_props('plots.fig')\n",
    "# paginated_multi_decoder_decoded_epochs_window.get_children_props('params.posterior_heatmap_imshow_kwargs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ca528",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# paginated_multi_decoder_decoded_epochs_window# AttributeError: 'PhoPaginatedMultiDecoderDecodedEpochsWindow' object has no attribute 'params'\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.pagination_controllers['long_LR'].params.should_suppress_callback_exceptions = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a19394",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.jump_to_page(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea69a1c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f136d94",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2150f30",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # v.params.enable_radon_transform_info = False\n",
    "    # v.params.enable_weighted_correlation_info = False\n",
    "    v._subfn_clear_selectability_rects()\n",
    "    \n",
    "# paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d5ece",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for a_name, a_ctrlr in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    a_ctrlr.perform_update_selections(defer_render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb8eea",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009775d7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[785.7379401021171, 785.9232737672282]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[427.4610240198672, 427.55720829055645]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[833.3391086903866, 833.4508065531263]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[491.7975491596153, 492.17844624456484], [940.0164351915009, 940.2191870877286]]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [array([785.738, 785.923])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [array([427.461, 427.557])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [array([833.339, 833.451])]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [array([491.798, 492.178]), array([940.016, 940.219])]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[785.7379401021171, 785.9232737672282]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[427.4610240198672, 427.55720829055645]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[833.3391086903866, 833.4508065531263]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[491.7975491596153, 492.17844624456484], [940.0164351915009, 940.2191870877286]]\n",
    "\n",
    "# with Ctx(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0',display_fn_name='DecodedEpochSlices',epochs='ripple',user_annotation='selections') as ctx:\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_LR')] = [[208.356, 208.523], [693.842, 693.975], [954.574, 954.679]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='long_RL')] = [[224.037, 224.312]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_LR')] = [[145.776, 146.022], [198.220, 198.582], [220.041, 220.259], [511.570, 511.874], [865.238, 865.373]]\n",
    "# \tuser_annotations[ctx + Ctx(decoder='short_RL')] = [[191.817, 192.100], [323.147, 323.297]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776e895",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "with VizTracer(output_file=f\"viztracer_{get_now_time_str()}-paginated_multi_decoder_decoded_epochs_window_page.json\", min_duration=200, tracer_entries=3000000, ignore_frozen=True) as tracer:\n",
    "    paginated_multi_decoder_decoded_epochs_window.jump_to_page(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f513296",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.jump_to_page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b6ed4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98478063",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "track_templates.get_decoder_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec6078",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    # v.params.enable_radon_transform_info = False\n",
    "    # v.params.enable_weighted_correlation_info = False\n",
    "    v.params.enable_radon_transform_info = True\n",
    "    v.params.enable_weighted_correlation_info = True\n",
    "    v.params.debug_enabled = True\n",
    "\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904027b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for k, v in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items():\n",
    "    print(f'decoder[{k}]:')\n",
    "    v.params.name\n",
    "    # v.params.on_render_page_callbacks\n",
    "    # v.params.enable_radon_transform_info\n",
    "    len(v.plots_data.radon_transform_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ff3b7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cc2a9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263a3d8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b447b8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.refresh_current_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3540",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "def _sub_subfn_wrapped_in_brackets(s: str, bracket_strings = (\"[\", \"]\")) -> str:\n",
    "        return bracket_strings[0] + s + bracket_strings[1]\n",
    "    \n",
    "def _sub_subfn_format_nested_list(arr, precision:int=3, num_sep=\", \", array_sep=', ') -> str:\n",
    "    \"\"\"\n",
    "    Converts a nested list of floats into a single string,\n",
    "    with each float formatted to the specified precision.\n",
    "    \n",
    "    arr = np.array([[491.798, 492.178], [940.016, 940.219]])\n",
    "    _sub_subfn_format_nested_list(arr)\n",
    "\n",
    "    >> '[[491.798, 492.178], [940.016, 940.219]]'\n",
    "\n",
    "    arr = np.array([[785.738, 785.923]])\n",
    "    _sub_subfn_format_nested_list(arr)\n",
    "    >> '[[785.738, 785.923]]'\n",
    "    \"\"\"\n",
    "    return _sub_subfn_wrapped_in_brackets(array_sep.join([_sub_subfn_wrapped_in_brackets(num_sep.join([f\"{num:.{precision}f}\" for num in row])) for row in arr]))\n",
    "    \n",
    "# arr = np.array([[491.798, 492.178], [940.016, 940.219]])\n",
    "arr = np.array([[785.738, 785.923]])\n",
    "_sub_subfn_format_nested_list(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0ab5d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### 2024-02-29 3pm - Get the active user-annotated epoch times from the `paginated_multi_decoder_decoded_epochs_window` and use these to filter `filtered_ripple_simple_pf_pearson_merged_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c982e52",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Inputs: paginated_multi_decoder_decoded_epochs_window, filtered_ripple_simple_pf_pearson_merged_df\n",
    "any_good_selected_epoch_times = deepcopy(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times)\n",
    "any_good_selected_epoch_indicies = deepcopy(paginated_multi_decoder_decoded_epochs_window.find_data_indicies_from_epoch_times(paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a7eda",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## :✅:🎯 2024-09-27 - Test programmatic/background saving of stacked decoded epoch figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b8840",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# using: perform_export_all_decoded_posteriors_as_images\n",
    "from pyphoplacecellanalysis.Pho2D.data_exporting import HeatmapExportConfig, PosteriorExporting\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import Assert\n",
    "\n",
    "## INPUTS:: filtered_decoder_filter_epochs_decoder_result_dict, long_like_during_post_delta_only_filter_epochs\n",
    "active_epochs_decoder_result_dict = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "parent_output_folder = Path('output/array_to_images').resolve()\n",
    "\n",
    "# active_epochs_decoder_result_dict = deepcopy(long_like_during_post_delta_only_filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "# parent_output_folder = Path('output/long_like_during_post_delta').resolve()\n",
    "\n",
    "\n",
    "active_epochs_decoder_result_dict = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict)\n",
    "\n",
    "\n",
    "parent_output_folder.mkdir(exist_ok=True)\n",
    "Assert.path_exists(parent_output_folder)\n",
    "posterior_out_folder = parent_output_folder.joinpath(DAY_DATE_TO_USE).resolve()\n",
    "posterior_out_folder.mkdir(parents=True, exist_ok=True)\n",
    "save_path = posterior_out_folder.resolve()\n",
    "_parent_save_context: IdentifyingContext = curr_active_pipeline.build_display_context_for_session('perform_export_all_decoded_posteriors_as_images')\n",
    "_specific_session_output_folder = save_path.joinpath(active_context.get_description(subset_excludelist=['format_name'])).resolve()\n",
    "_specific_session_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "print(f'\\tspecific_session_output_folder: \"{_specific_session_output_folder}\"')\n",
    "\n",
    "custom_export_formats: Dict[str, HeatmapExportConfig] = {\n",
    "    'greyscale': HeatmapExportConfig.init_greyscale(desired_height=1200),\n",
    "    'color': HeatmapExportConfig(colormap='Oranges', desired_height=1200),\n",
    "    # 'color': HeatmapExportConfig(colormap=additional_cmaps['long_LR']),\n",
    "    # 'color': HeatmapExportConfig(colormap=cmap1, desired_height=200),\n",
    "}\n",
    "custom_export_formats = None\n",
    "\n",
    "out_paths, out_custom_formats_dict = PosteriorExporting.perform_export_all_decoded_posteriors_as_images(decoder_laps_filter_epochs_decoder_result_dict=None, decoder_ripple_filter_epochs_decoder_result_dict=active_epochs_decoder_result_dict,\n",
    "                                                                                                            _save_context=_parent_save_context, parent_output_folder=_specific_session_output_folder,\n",
    "                                                                                                            desired_height=1200, custom_export_formats=custom_export_formats, combined_img_padding=6, combined_img_separator_color=(0, 0, 0, 255))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6569b",
   "metadata": {},
   "source": [
    "### 2024-11-26 - try HDF5 posterior export so they can be loaded more easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "from neuropy.utils.result_context import DisplaySpecifyingIdentifyingContext\n",
    "\n",
    "filtered_epochs_df = None\n",
    "\n",
    "## INPUTS: curr_active_pipeline, track_templates, a_decoded_filter_epochs_decoder_result_dict\n",
    "directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = deepcopy(curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']) ## GENERAL\n",
    "## INPUTS: directional_decoders_epochs_decode_result, filtered_epochs_df\n",
    "\n",
    "decoder_ripple_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "unfiltered_epochs_df = deepcopy(decoder_ripple_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)\n",
    "if filtered_epochs_df is not None:\n",
    "    ## filter\n",
    "    filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(filtered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working filtered\n",
    "else:\n",
    "    filtered_decoder_filter_epochs_decoder_result_dict: Dict[types.DecoderName, DecodedFilterEpochsResult] = {a_name:a_result.filtered_by_epoch_times(unfiltered_epochs_df[['start', 'stop']].to_numpy()) for a_name, a_result in decoder_ripple_filter_epochs_decoder_result_dict.items()} # working unfiltered\n",
    "\n",
    "ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert parent_output_path.exists(), f\"'{parent_output_path}' does not exist!\"\n",
    "output_date_str: str = get_now_rounded_time_str(rounded_minutes=10)\n",
    "# Export CSVs:\n",
    "# def export_df_to_csv(export_df: pd.DataFrame, data_identifier_str: str = f'(laps_marginals_df)', parent_output_path: Path=None):\n",
    "#     \"\"\" captures `active_context`, 'output_date_str'\n",
    "#     \"\"\"\n",
    "#     # parent_output_path: Path = Path('output').resolve()\n",
    "#     # active_context = curr_active_pipeline.get_session_context()\n",
    "#     session_identifier_str: str = active_context.get_description() # 'kdiba_gor01_two_2006-6-12_16-53-46__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_1.0normal_computed-frateThresh_1.0-qclu_[1, 2, 4, 6, 7, 9]'\n",
    "#     # session_identifier_str: str = active_context.get_description(subset_excludelist=['custom_suffix']) # no this is just the session\n",
    "#     assert output_date_str is not None\n",
    "#     out_basename = '-'.join([output_date_str, session_identifier_str, data_identifier_str]) # '2024-11-15_0200PM-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays_qclu_[1, 2, 4, 6, 7, 9]_frateThresh_5.0-(ripple_WCorrShuffle_df)_tbin-0.025'\n",
    "#     out_filename = f\"{out_basename}.csv\"\n",
    "#     out_path = parent_output_path.joinpath(out_filename).resolve()\n",
    "#     export_df.to_csv(out_path)\n",
    "#     return out_path \n",
    "\n",
    "\n",
    "def export_data_to_h5(data_identifier_str: str = f'(laps_marginals_df)', parent_output_path: Path=None):\n",
    "    \"\"\" captures `active_context`, 'output_date_str'\n",
    "    \"\"\"\n",
    "    # parent_output_path: Path = Path('output').resolve()\n",
    "    # active_context = curr_active_pipeline.get_session_context()\n",
    "    session_identifier_str: str = active_context.get_description() # 'kdiba_gor01_two_2006-6-12_16-53-46__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_1.0normal_computed-frateThresh_1.0-qclu_[1, 2, 4, 6, 7, 9]'\n",
    "    # session_identifier_str: str = active_context.get_description(subset_excludelist=['custom_suffix']) # no this is just the session\n",
    "    assert output_date_str is not None\n",
    "    out_basename = '-'.join([output_date_str, session_identifier_str, data_identifier_str]) # '2024-11-15_0200PM-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays_qclu_[1, 2, 4, 6, 7, 9]_frateThresh_5.0-(ripple_WCorrShuffle_df)_tbin-0.025'\n",
    "    out_filename = f\"{out_basename}.h5\"\n",
    "    out_path = parent_output_path.joinpath(out_filename).resolve()\n",
    "    ## can export here\n",
    "    \n",
    "    return out_path \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df93063",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(f'output/{BATCH_DATE_TO_USE}_newest_all_decoded_epoch_posteriors.h5').resolve()\n",
    "\n",
    "complete_session_context, (session_context, additional_session_context) = curr_active_pipeline.get_complete_session_context()\n",
    "_, _, custom_suffix = curr_active_pipeline.get_custom_pipeline_filenames_from_parameters()\n",
    "custom_params_hdf_key: str = custom_suffix.strip('_') # strip leading/trailing underscores\n",
    "# _parent_save_context: IdentifyingContext = curr_active_pipeline.build_display_context_for_session('save_decoded_posteriors_to_HDF5', custom_suffix=custom_suffix)\n",
    "_parent_save_context: DisplaySpecifyingIdentifyingContext = deepcopy(session_context).overwriting_context(custom_suffix=custom_params_hdf_key, display_fn_name='save_decoded_posteriors_to_HDF5')\n",
    "# _parent_save_context: DisplaySpecifyingIdentifyingContext = complete_session_context.overwriting_context(display_fn_name='save_decoded_posteriors_to_HDF5')\n",
    "_parent_save_context.display_dict = {\n",
    "    'custom_suffix': lambda k, v: f\"{v}\", # just include the name\n",
    "    'display_fn_name': lambda k, v: f\"{v}\", # just include the name\n",
    "}\n",
    "\n",
    "\n",
    "out_contexts, _flat_all_out_paths = PosteriorExporting.perform_save_all_decoded_posteriors_to_HDF5(decoder_laps_filter_epochs_decoder_result_dict=None,\n",
    "                                                                             decoder_ripple_filter_epochs_decoder_result_dict=filtered_decoder_filter_epochs_decoder_result_dict,\n",
    "                                                                             _save_context=_parent_save_context.get_raw_identifying_context(), save_path=save_path)\n",
    "out_contexts\n",
    "_flat_all_out_paths\n",
    "\n",
    "# DataTypeWarning: Unsupported type for attribute 'is_user_annotated_epoch' in node '002'. Offending HDF5 class: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "list(dict.fromkeys([v.as_posix() for v in _flat_all_out_paths]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91396e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'save_path: \"{save_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = Path('output/2024-11-26_Lab_newest_all_decoded_epoch_posteriors.h5')\n",
    "\n",
    "## used for reconstituting dataset:\n",
    "dataset_type_fields = ['p_x_given_n', 'p_x_given_n_grey', 'most_likely_positions', 'most_likely_position_indicies', 'time_bin_edges', 't_bin_centers']\n",
    "decoder_names = ['long_LR', 'long_RL', 'short_LR', 'short_RL']\n",
    "\n",
    "_out_dict, (session_key_parts, custom_replay_parts) = PosteriorExporting.load_decoded_posteriors_from_HDF5(load_path=load_path, debug_print=True)\n",
    "_out_ripple_only_dict = {k:v['ripple'] for k, v in _out_dict.items()} ## cut down to only the laps\n",
    "\n",
    "## build the final ripple data outputs:\n",
    "ripple_data_field_dict = {}\n",
    "# active_var_key: str = 'p_x_given_n' # dataset_type_fields\t\n",
    "\n",
    "for active_var_key in dataset_type_fields:\n",
    "    ripple_data_field_dict[active_var_key] = {\n",
    "        a_decoder_name: [v for v in _out_ripple_only_dict[a_decoder_name][active_var_key]] for a_decoder_name in decoder_names\n",
    "    }\n",
    "\n",
    "\n",
    "ripple_img_dict = ripple_data_field_dict['p_x_given_n_grey']\n",
    "ripple_img_dict['long_LR'][0]\n",
    "# ripple_0_img = _out_ripple_only_dict['long_LR'][active_var_key][0]\n",
    "# ripple_0_img\n",
    "# lap_0_img = _out_dict['long_LR']['laps']['p_x_given_n_grey'][0]\n",
    "# lap_0_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_keys_if_possible('loaded_posteriors_dict', _out_dict, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1f903",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 🔶 2024-03-01 - Get the active user-annotated epoch times from the `UserAnnotationsManager` and use these to filter `filtered_ripple_simple_pf_pearson_merged_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc751d6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.misc import numpyify_array\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.epoch import EpochsAccessor\n",
    "from neuropy.core.epoch import find_data_indicies_from_epoch_times\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "## Get from UserAnnotations directly instead of the intermediate viewer\n",
    "\n",
    "## # inputs: any_good_selected_epoch_times, any_good_selected_epoch_times, any_good_selected_epoch_indicies \n",
    "\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "# any_good_selected_epoch_indicies\n",
    "# Add user-selection columns to df\n",
    "a_df = deepcopy(filtered_ripple_simple_pf_pearson_merged_df)\n",
    "# a_df = deepcopy(ripple_weighted_corr_merged_df)\n",
    "a_df['is_user_annotated_epoch'] = False\n",
    "# any_good_selected_epoch_indicies = a_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, np.squeeze(any_good_selected_epoch_times[:,0]), t_column_names=['ripple_start_t',])\n",
    "# any_good_selected_epoch_indicies = find_data_indicies_from_epoch_times(a_df, any_good_selected_epoch_times, t_column_names=['ripple_start_t',])\n",
    "any_good_selected_epoch_indicies\n",
    "# a_df['is_user_annotated_epoch'] = np.isin(a_df.index.to_numpy(), any_good_selected_epoch_indicies)\n",
    "a_df['is_user_annotated_epoch'].loc[any_good_selected_epoch_indicies] = True # Here's another .iloc issue! Changing to .loc\n",
    "a_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3a540",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "df = DecoderDecodedEpochsResult.filter_epochs_dfs_by_annotation_times(curr_active_pipeline, any_good_selected_epoch_times, ripple_decoding_time_bin_size, filtered_ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dee7b3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### 2024-02-29 - 4pm - Filter the events for those meeting wcorr criteria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7e67e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "min_wcorr_threshold: float = 0.33\n",
    "min_wcorr_diff_threshold: float = 0.2\n",
    "\n",
    "is_included_large_wcorr_diff = np.any((df[['wcorr_abs_diff']].abs() > min_wcorr_diff_threshold), axis=1)\n",
    "is_included_high_wcorr = np.any((df[['long_best_wcorr', 'short_best_wcorr']].abs() > min_wcorr_threshold), axis=1)\n",
    "\n",
    "df = df[is_included_high_wcorr]\n",
    "df\n",
    "\n",
    "# delta_aligned_start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc554d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Shifts the absolute times to delta-relative values, as would be needed to draw on a 'delta_aligned_start_t' axis:\n",
    "delta_relative_t_start, delta_relative_t_delta, delta_relative_t_end = np.array([earliest_delta_aligned_t_start, t_delta, latest_delta_aligned_t_end]) - t_delta\n",
    "delta_relative_t_start, delta_relative_t_delta, delta_relative_t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f5cbf",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "df['_wcorr_y_col'] = df['long_best_wcorr'].abs()\n",
    "df['_wcorr_y_col_y_diff_col'] = df['long_best_wcorr'].abs() - df['short_best_wcorr'].abs()\n",
    "# df.plot.scatter(x='ripple_start_t', y='wcorr_y_col')\n",
    "df.plot.scatter(x='delta_aligned_start_t', y='_wcorr_y_col_y_diff_col')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5438dce",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# df['pearsonr_long_abs'] = df['long_best_pf_peak_x_pearsonr'].abs()\n",
    "# df['pearsonr_short_abs'] = df['short_best_pf_peak_x_pearsonr'].abs()\n",
    "# df['pearsonr_diff'] = df['long_best_pf_peak_x_pearsonr'].abs() - df['short_best_pf_peak_x_pearsonr'].abs()\n",
    "\n",
    "# df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_long_abs')\n",
    "# df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_short_abs')\n",
    "df.plot.scatter(x='delta_aligned_start_t', y='pearsonr_abs_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f951c0e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d716b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.debug_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02556ea",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### Add utility footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3888f6e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.GUI.PyQtPlot.DockingWidgets.DynamicDockDisplayAreaContent import CustomDockDisplayConfig, get_utility_dock_colors\n",
    "from pyphoplacecellanalysis.GUI.Qt.Widgets.ThinButtonBar.ThinButtonBarWidget import ThinButtonBarWidget\n",
    "\n",
    "\n",
    "def _add_utility_footer(paginated_multi_decoder_decoded_epochs_window):\n",
    "    ui = paginated_multi_decoder_decoded_epochs_window.ui._contents\n",
    "    # ui.dock_widgets\n",
    "    # ui.dock_configs\n",
    "\n",
    "\n",
    "    ## Build the utility controls at the bottom:\n",
    "    ctrls_dock_config = CustomDockDisplayConfig(custom_get_colors_callback_fn=get_utility_dock_colors, showCloseButton=True, orientation='horizontal')\n",
    "\n",
    "    button_bar_height = 21\n",
    "    ctrls_button_bar_widget = ThinButtonBarWidget()\n",
    "    ctrls_button_bar_widget.setObjectName(\"ctrls_button_bar\")\n",
    "    # Set the background color to blue with 40% opacity (RGBA)\n",
    "    ctrls_button_bar_widget.setStyleSheet(\"background-color: rgba(0, 0, 255, 102);\")\n",
    "\n",
    "    ctrl_layout = pg.LayoutWidget()\n",
    "    ctrl_layout.addWidget(ctrls_button_bar_widget, row=1, rowspan=1, col=1, colspan=2)\n",
    "    ctrl_widgets_dict = dict(ctrls_widget=ctrls_button_bar_widget)\n",
    "    # Set the background color to green with 40% opacity (RGBA)\n",
    "    ctrl_layout.setStyleSheet(\"background-color: rgba(0, 255, 10, 102);\")\n",
    "\n",
    "    # ctrl_layout.setSizePolicy(\n",
    "\n",
    "    def onCopySelectionsClicked():\n",
    "        print(f'onCopySelectionsClicked()')\n",
    "        saved_selections_contexts_dict = paginated_multi_decoder_decoded_epochs_window.print_user_annotations()\n",
    "\n",
    "    ctrl_widgets_dict['copy_selection_connection'] = ctrls_button_bar_widget.sigCopySelections.connect(onCopySelectionsClicked)\n",
    "\n",
    "    ui.dock_widgets['bottom_controls'] = paginated_multi_decoder_decoded_epochs_window.add_display_dock(identifier='bottom_controls', widget=ctrl_layout, dockSize=(600, button_bar_height), dockAddLocationOpts=['bottom'], display_config=ctrls_dock_config, autoOrientation=False)\n",
    "    # ui.dock_widgets['bottom_controls'][1].hideTitleBar()\n",
    "    ui.dock_widgets['bottom_controls']\n",
    "\n",
    "    button_bar_height = 21\n",
    "\n",
    "    a_layout = ui.dock_widgets['bottom_controls'][0]\n",
    "    a_layout.size()\n",
    "    a_layout.setContentsMargins(0,0,0,0)\n",
    "    a_layout.setFixedHeight(21)\n",
    "    ui.dock_widgets['bottom_controls'][1].size()\n",
    "    ui.dock_widgets['bottom_controls'][1].setContentsMargins(0,0,0,0)\n",
    "    ui.dock_widgets['bottom_controls'][1].setStyleSheet(\"background-color: rgba(255, 10, 10, 102);\") # RED\n",
    "\n",
    "    # ui.dock_widgets['bottom_controls'][1].hideTitleBar()\n",
    "    # ui.dock_widgets['bottom_controls'][1].size\n",
    "\n",
    "    return ctrl_layout, ctrls_dock_config, ui\n",
    "\n",
    "\n",
    "ctrl_layout, ctrls_dock_config, ui = _add_utility_footer(paginated_multi_decoder_decoded_epochs_window=new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6823e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window=new_wcorr_shuffle_paginated_multi_decoder_decoded_epochs_window\n",
    "ui = paginated_multi_decoder_decoded_epochs_window.ui._contents\n",
    "\n",
    "layout_widget, dock_item = ui.dock_widgets['bottom_controls']\n",
    "layout_widget.size()\n",
    "# Set the background color to light grey\n",
    "layout_widget.setStyleSheet(\"background-color: red;\")\n",
    "\n",
    "# layout_widget.setBackgroundColor('black')\n",
    "layout_widget.setAutoFillBackground(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3add584",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    " ui.dock_widgets['bottom_controls'][1].size()\n",
    " ui.dock_widgets['bottom_controls'][1].setFixedHeight(21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b47e37",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ui.dock_widgets['bottom_controls'][1].children()\n",
    "# [<pyphoplacecellanalysis.External.pyqtgraph.dockarea.DockDrop.DropAreaOverlay object at 0x00000175C7D24820>,\n",
    "#  <PyQt5.QtWidgets.QGridLayout object at 0x00000175C7D248B0>,\n",
    "#  <pyphoplacecellanalysis.External.pyqtgraph.dockarea.Dock.DockLabel object at 0x00000175C7D24E50>,\n",
    "#  <PyQt5.QtWidgets.QWidget object at 0x00000175C7D245E0>,\n",
    "#  <pyphoplacecellanalysis.External.pyqtgraph.dockarea.DockDrop.DropAreaOverlay object at 0x00000175C7D24B80>]\n",
    "\n",
    "ui.dock_widgets['bottom_controls'][1].layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a5afa",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "dock_item.showTitleBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d22cf",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "dock_item.setOrientation('horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17726a0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "dock_item.setContentsMargins(0,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91d13a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "layout_widget.setContentsMargins(0,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3d78a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ui.dock_widgets['bottom_controls'][0].resize(600, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39be52f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.find_display_dock('bottom_controls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f454f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.remove_display_dock('bottom_controls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426375f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "\n",
    "## Set epoch annotations from selections epochs \n",
    "annotations_man = UserAnnotationsManager()\n",
    "user_annotations = annotations_man.get_user_annotations()\n",
    "new_selections_dict = paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(user_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf471332",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "loaded_selections_objs_dict = {a_name:EpochSelectionsObject(epoch_times=a_selections_values) for a_name, a_selections_values in loaded_selections_dict.items()}\n",
    "loaded_selections_objs_dict\n",
    "\n",
    "## Select just the selected epoch times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0b733",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "saved_selections_context_dict = {a_name:v.figure_ctx.adding_context_if_missing(user_annotation='selections') for a_name, v in saved_selections_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74f8ee",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "user_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d312d9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.print_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ee6f5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Remove the excessively long plot titles?\n",
    "# root_dockAreaWindow.update\n",
    "pagination_controller_dict = paginated_multi_decoder_decoded_epochs_window.pagination_controllers\n",
    "all_widgets = {a_decoder_name:a_pagination_controller.ui.mw for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_windows = {a_decoder_name:a_pagination_controller.ui.mw.window() for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_plots = {a_decoder_name:a_pagination_controller.plots for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_plots_data = {a_decoder_name:a_pagination_controller.plots_data for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_params = {a_decoder_name:a_pagination_controller.params for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_current_page_idx = {a_decoder_name:a_pagination_controller.current_page_idx for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_current_page_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfebd7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# all_separate_plots\n",
    "\n",
    "all_separate_weighted_corr_plots = {a_decoder_name:a_pagination_controller.plots.get('weighted_corr', {}) for a_decoder_name, a_pagination_controller in pagination_controller_dict.items()}\n",
    "all_separate_weighted_corr_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d02b9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "self.ui.print = self.private_print # builtins.print # the print function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69313c31",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import EpochsAccessor\n",
    "\n",
    "# MLM\n",
    "# {a_name:a_ctrlr.params.is_selected for a_name, a_ctrlr in root_dockAreaWindow.pagination_controllers.items()}\n",
    "# {a_name:a_ctrlr.selected_epoch_times for a_name, a_ctrlr in root_dockAreaWindow.pagination_controllers.items()}\n",
    "\n",
    "any_good_selected_epoch_times: NDArray = paginated_multi_decoder_decoded_epochs_window.any_good_selected_epoch_times # drops duplicate rows (present in multiple decoders), and sorts them ascending\n",
    "# any_good_selected_epoch_times\n",
    "# Only at the decoder-level\n",
    "any_good_epoch_idxs_list = [a_ctrlr.find_data_indicies_from_epoch_times(any_good_selected_epoch_times) for a_name, a_ctrlr in paginated_multi_decoder_decoded_epochs_window.pagination_controllers.items()]\n",
    "any_good_epoch_idxs: NDArray = any_good_epoch_idxs_list[0]\n",
    "any_good_epoch_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29282203",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531db971",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(any_good_selected_epoch_times)\n",
    "# filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "\n",
    "found_data_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.find_data_indicies_from_epoch_times(epoch_times=any_good_selected_epoch_times)\n",
    "df = filtered_ripple_simple_pf_pearson_merged_df.epochs._obj.iloc[found_data_indicies].copy().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78ed2c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1677479",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "hand_selected_ripple_simple_pf_pearson_merged_df = filtered_ripple_simple_pf_pearson_merged_df.iloc[any_good_epoch_idxs, :].reset_index(drop=True)\n",
    "hand_selected_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d400bd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# hand_selected_ripple_simple_pf_pearson_merged_df['best_decoder_index']\n",
    "\n",
    "is_most_likely_long = (hand_selected_ripple_simple_pf_pearson_merged_df['P_Long'] >= 0.5)\n",
    "# is_most_likely_long\n",
    "\n",
    "long_likely_hand_selected_ripple_simple_pf_pearson_merged_df = hand_selected_ripple_simple_pf_pearson_merged_df[is_most_likely_long]\n",
    "long_likely_hand_selected_ripple_simple_pf_pearson_merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d33190",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 🖼️🎨 Plot laps to compare between decoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4873ebb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import add_laps_groundtruth_information_to_dataframe\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "\n",
    "# decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs # looks like 'lap_dir' column is wrong\n",
    "updated_laps_dfs_dict = {}\n",
    "\n",
    "## Update the .filter_epochs:\n",
    "for k, v in decoder_laps_filter_epochs_decoder_result_dict.items():\n",
    "    updated_laps_dfs_dict[k] = Epoch(add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=ensure_dataframe(v.filter_epochs)))\n",
    "    decoder_laps_filter_epochs_decoder_result_dict[k].filter_epochs =  updated_laps_dfs_dict[k]\n",
    "\n",
    "# updated_laps_dfs_dict['long_LR']\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs\n",
    "\n",
    "laps_app, laps_paginated_multi_decoder_decoded_epochs_window, laps_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                            decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps', included_epoch_indicies=None, \n",
    "    params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': False, \n",
    "    # 'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': False,\n",
    "    'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "    # 'disable_y_label': True,\n",
    "    # 'isPaginatorControlWidgetBackedMode': True,\n",
    "    # 'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "    # 'debug_print': True,\n",
    "    'max_subplots_per_page': 9,\n",
    "    'scrollable_figure': True,\n",
    "    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "    'use_AnchoredCustomText': False,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec05d21",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "\n",
    "## INPUTS: decoder_laps_filter_epochs_decoder_result_dict\n",
    "\n",
    "## Highlight the correct ones:\n",
    "# {k:Epoch(add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=ensure_dataframe(v.filter_epochs))) for k, v in decoder_laps_filter_epochs_decoder_result_dict.items()}\n",
    "\n",
    "## Select the true laps by emulating user_annotations:\n",
    "filter_epochs = ensure_dataframe(deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)) \n",
    "# filter_epochs\n",
    "\n",
    "decoder_name_idx_map = {'long_LR': 0, 'long_RL': 1, 'short_LR': 2, 'short_RL': 3} \n",
    "selections_dict = {}\n",
    "figure_ctx_dict = laps_paginated_multi_decoder_decoded_epochs_window.figure_ctx_dict\n",
    "loaded_selections_context_dict = {a_name:a_figure_ctx.adding_context_if_missing(user_annotation='selections') for a_name, a_figure_ctx in figure_ctx_dict.items()}\n",
    "\n",
    "for a_name, an_idx in decoder_name_idx_map.items():\n",
    "    a_selections_context = loaded_selections_context_dict[a_name]\n",
    "    selections_dict[a_selections_context] = filter_epochs[filter_epochs['true_decoder_index'] == an_idx][['start', 'stop']].to_numpy()\n",
    "\n",
    "\n",
    "## Clearing the existing selection rects and them having them rebuilt when the selection is updated fixes them being shifted.\n",
    "for k, v in laps_pagination_controller_dict.items():\n",
    "    v._subfn_clear_selectability_rects()\n",
    "\n",
    "# _tmp_out_selections = laps_paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations(user_annotations=selections_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6607cc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364cc14",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.remove_data_overlays(defer_refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e23e2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.remov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4fe0c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Clearing the existing selection rects and them having them rebuilt when the selection is updated fixes them being shifted.\n",
    "for k, v in laps_pagination_controller_dict.items():\n",
    "    v._subfn_clear_selectability_rects()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675f133",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603bc41",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "filtered_ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f847f0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1556870",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# list(decoder_laps_filter_epochs_decoder_result_dict.keys())\n",
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba2a38",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Get the figure from the axes:\n",
    "a_fig = ax.get_figure()\n",
    "a_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b177a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller = laps_paginated_multi_decoder_decoded_epochs_window.contents.pagination_controllers['long_LR'] # DecodedEpochSlicesPaginatedFigureController\n",
    "a_pagination_controller_figure_widget = paginator_controller_widget = a_controlling_pagination_controller.ui.mw # MatplotlibTimeSynchronizedWidget\n",
    "paginator_controller_widget = a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget # PaginationControlWidget\n",
    "# paginator_controller_widget\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92048bc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "axs = a_controlling_pagination_controller.plots.axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab273d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ax.get_figure().canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47481538",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "selection_rectangles_dict = a_controlling_pagination_controller.plots.get('selection_rectangles_dict', None)\n",
    "selection_rectangles_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa452e4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# a_controlling_pagination_controller.plots.fig.canvas.draw_idle()\n",
    "# a_controlling_pagination_controller.plots.fig.canvas.draw()\n",
    "# paginator_controller_widget.update()\n",
    "a_pagination_controller_figure_widget.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db1753",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginator_controller_widget.go_to_page(3)\n",
    "# paginator_controller_widget.jump_to_page(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fbe9f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_controlling_pagination_controller.ui.mw.ui.paginator_controller_widget.jump_to_page\n",
    "\n",
    "new_obj.plots_data.paginator\n",
    "new_obj.params.active_identifying_figure_ctx\n",
    "new_obj.on_paginator_control_widget_jump_to_page(page_idx=0)\n",
    "new_obj.ui.connections['paginator_controller_widget_jump_to_page']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c54dc7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for i, extant_plots in a_plots['weighted_corr'].items():\n",
    "    extant_wcorr_text = extant_plots.get('wcorr_text', None)\n",
    "    # extant_wcorr_text = extant_plots.pop('wcorr_text', None)\n",
    "    print(f'extant_wcorr_text: {extant_wcorr_text}')\n",
    "    # plot the radon transform line on the epoch:\n",
    "    if (extant_wcorr_text is not None):\n",
    "        # already exists, clear the existing ones. \n",
    "        # Let's assume we want to remove the 'Quadratic' line (line2)\n",
    "        print(f'removing extant text object at index: {i}.')\n",
    "        # extant_wcorr_text.remove()\n",
    "        extant_wcorr_text.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e27b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "for a_name, a_pagination_controller in pagination_controller_dict.items():\n",
    "    display_context = a_pagination_controller.params.get('active_identifying_figure_ctx', IdentifyingContext())\n",
    "\n",
    "    # Get context for current page of items:\n",
    "    current_page_idx: int = int(a_pagination_controller.current_page_idx)\n",
    "    a_paginator = a_pagination_controller.paginator\n",
    "    total_num_pages = int(a_paginator.num_pages)\n",
    "    page_context = display_context.overwriting_context(page=current_page_idx, num_pages=total_num_pages)\n",
    "    print(page_context)\n",
    "\n",
    "    ## Get the figure/axes:\n",
    "    a_plots = a_pagination_controller.plots # RenderPlots\n",
    "    a_plot_data = a_pagination_controller.plots_data\n",
    "\n",
    "    a_params = a_pagination_controller.params\n",
    "    a_params.skip_plotting_measured_positions\n",
    "\n",
    "    figs = a_plots.fig\n",
    "    axs = a_plots.axs\n",
    "\n",
    "    # # with mpl.rc_context({'figure.figsize': (8.4, 4.8), 'figure.dpi': '220', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    # with mpl.rc_context({'figure.figsize': (16.8, 4.8), 'figure.dpi': '420', 'savefig.transparent': True, 'ps.fonttype': 42, }):\n",
    "    #     curr_active_pipeline.output_figure(final_context=page_context, fig=figs, write_vector_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d32342",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 💾 Export Paginated Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec0488",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline)\n",
    "# paginated_multi_decoder_decoded_epochs_window.export_all_pages(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8a471",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.export_decoder_pagination_controller_figure_page(curr_active_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd2637",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 🔷🎨 Single Decoder Version (`DecodedEpochSlicesPaginatedFigureController`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d880d1c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import _subfn_update_decoded_epoch_slices\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import DecodedEpochSlicesPaginatedFigureController # `plot_decoded_epoch_slices_paginated`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import WeightedCorrelationPaginatedPlotDataProvider\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import DecodedPositionsPlotDataProvider, DecodedAndActualPositionsPlotData\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import perform_plot_1D_single_most_likely_position_curve\n",
    "\n",
    "# Inputs: epochs_name, decoder_ripple_filter_epochs_decoder_result_dict, curr_active_pipeline\n",
    "# epochs_name = 'ripple'\n",
    "epochs_name = 'laps'\n",
    "\n",
    "(a_name, a_decoder) = tuple(track_templates.get_decoders_dict().items())[0]\n",
    "\n",
    "# a_decoder_decoded_epochs_result = decoder_ripple_filter_epochs_decoder_result_dict[a_name]\n",
    "\n",
    "# a_decoder_decoded_epochs_result = decoder_ripple_filter_epochs_decoder_result_dict[a_name]\n",
    "a_decoder_decoded_epochs_result = deepcopy(filtered_decoder_filter_epochs_decoder_result_dict[a_name]) ## FILTERED\n",
    "\n",
    "_out_pagination_controller = DecodedEpochSlicesPaginatedFigureController.init_from_decoder_data(active_filter_epochs=a_decoder_decoded_epochs_result.filter_epochs,\n",
    "                                                                                    filter_epochs_decoder_result= a_decoder_decoded_epochs_result,\n",
    "                                                                                    xbin=a_decoder.xbin, global_pos_df=curr_active_pipeline.sess.position.df,\n",
    "                                                                                    a_name=f'DecodedEpochSlices[{a_name}]', active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs=epochs_name, decoder=a_name),\n",
    "                                                                                    max_subplots_per_page=32,\n",
    "                                                                                    params_kwargs={'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': True, 'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                    'enable_decoded_most_likely_position_curve': True, #'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                    # 'disable_y_label': True,\n",
    "                                                                                                    'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                    'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                    # 'debug_print': True,\n",
    "                                                                                                    'max_subplots_per_page': 32,\n",
    "                                                                                                    'scrollable_figure': True,\n",
    "                                                                                                    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "                                                                                                    'use_AnchoredCustomText': True,\n",
    "                                                                                                    'disable_toolbar': False,\n",
    "                                                                                                    # 'build_fn': 'insets_view',\n",
    "                                                                                                    'decoder_track_length': decoder_track_length_dict,\n",
    "                                                                                    }, \n",
    "                                                                                    # disable_toolbar=False\n",
    "                                                                                    )\n",
    "\n",
    "_out_pagination_controller.params.should_suppress_callback_exceptions = False\n",
    "_out_pagination_controller.add_data_overlays(a_decoder_decoded_epochs_result)\n",
    "_tmp_out_selections = _out_pagination_controller.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5004473",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_pagination_controller.add_data_overlays(a_decoder_decoded_epochs_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c780e52",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "fig = _out_pagination_controller.plots.fig\n",
    "# fig.toolbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f46ef",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# type(_out_pagination_controller)\n",
    "\n",
    "_out_pagination_controller.plot_widget._buildUI_setup_statusbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff74414",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "single_epoch_field_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e429a68",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# on_selected_epochs_changed\n",
    "\n",
    "active_captured_single_epoch_result: SingleEpochDecodedResult = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=3)\n",
    "\n",
    "def get_selected_posterior_on_secondary_clicked_callback(self, event, clicked_ax, clicked_data_index, clicked_epoch_is_selected, clicked_epoch_start_stop_time):\n",
    "    \"\"\" called when the user alt-clicks an epoch \n",
    "    \n",
    "    captures: active_captured_single_epoch_result\n",
    "    \"\"\"\n",
    "    global active_captured_single_epoch_result\n",
    "    if self.params.debug_print:\n",
    "        print(f'get_selected_posterior_on_secondary_clicked_callback(clicked_data_index: {clicked_data_index}, clicked_epoch_is_selected: {clicked_epoch_is_selected}, clicked_epoch_start_stop_time: {clicked_epoch_start_stop_time})')\n",
    "    if clicked_epoch_start_stop_time is not None:\n",
    "        if len(clicked_epoch_start_stop_time) == 2:\n",
    "            start_t, end_t = clicked_epoch_start_stop_time\n",
    "            # print(f'start_t: {start_t}')\n",
    "            clicked_data_index: int = _out_pagination_controller.find_data_indicies_from_epoch_times(epoch_times=np.array([start_t, end_t]))[0]\n",
    "            if self.params.debug_print:\n",
    "                print(f'\\tclicked_data_index: {clicked_data_index}')            \n",
    "            active_captured_single_epoch_result = a_decoder_decoded_epochs_result.get_result_for_epoch(active_epoch_idx=clicked_data_index)\n",
    "            if self.params.debug_print:\n",
    "                print(f'\\tactive_captured_single_epoch_result.epoch_info_tuple: {active_captured_single_epoch_result.epoch_info_tuple}')\n",
    "                print(f'\\tdone.')\n",
    "\n",
    "\n",
    "# BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "if not _out_pagination_controller.params.has_attr('on_middle_click_item_callbacks'):\n",
    "    _out_pagination_controller.params['on_middle_click_item_callbacks'] = {}\n",
    "\n",
    "_out_pagination_controller.params.on_middle_click_item_callbacks['get_selected_posterior_on_secondary_clicked_callback'] = get_selected_posterior_on_secondary_clicked_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a4cd3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_decoder_decoded_epochs_result.active_filter_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a619872",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.plotting.media_output_helpers import get_array_as_image\n",
    "\n",
    "posterior_image = active_captured_single_epoch_result.get_posterior_as_image(desired_width=2048)\n",
    "posterior_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8ca93",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "{i:col for i, col in enumerate(a_decoder_decoded_epochs_result.active_filter_epochs.columns)}\n",
    "\n",
    "column_indicies = np.arange(12, 19)\n",
    "column_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8451e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out_pagination_controller.params.debug_print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc81f8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 2024-04-30 Heuristic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21abb21",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# *position_relative\": mapped between the ends of the track, 0.0 to 1.0\n",
    "most_likely_position_relative = (np.squeeze(active_captured_single_epoch_result.most_likely_position_indicies) / float(active_captured_single_epoch_result.n_xbins-1))\n",
    "most_likely_position_relative\n",
    "\n",
    "\n",
    "plt.hlines([0], colors='k', xmin=active_captured_single_epoch_result.time_bin_edges[0], xmax=active_captured_single_epoch_result.time_bin_edges[-1])\n",
    "plt.step(active_captured_single_epoch_result.time_bin_container.centers[1:], np.diff(most_likely_position_relative))\n",
    "plt.scatter(active_captured_single_epoch_result.time_bin_container.centers, most_likely_position_relative, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c52d60",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.Qt import QtGui, QtCore, QtWidgets\n",
    "# from pyphoplacecellanalysis.External.pyqtgraph.parametertree.parameterTypes.file import popupFilePicker\n",
    "from pyphoplacecellanalysis.External.pyqtgraph.widgets.FileDialog import FileDialog\n",
    "\n",
    "from silx.gui import qt\n",
    "from silx.gui.dialog.ImageFileDialog import ImageFileDialog\n",
    "from silx.gui.dialog.DataFileDialog import DataFileDialog\n",
    "import silx.io\n",
    "\n",
    "from pyphoplacecellanalysis.GUI.IPyWidgets.pipeline_ipywidgets import saveFile\n",
    "\n",
    "app = pg.mkQApp('silx_testing')\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc644214",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from silx.gui.plot import Plot2D\n",
    "\n",
    "matrix = np.random.rand(10, 10)  # Example 2D matrix\n",
    "plot = Plot2D()\n",
    "plot.addImage(matrix, colormap=\"viridis\", vmin=0, vmax=1)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c941e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "HeuristicReplayScoring.bin_wise_track_coverage_score_fn(a_result=a_decoder_decoded_epochs_result, an_epoch_idx=active_captured_single_epoch_result.epoch_data_index, a_decoder_track_length=170.0)\n",
    "\n",
    "# np.diff(active_captured_single_epoch_result.most_likely_position_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549b6af",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ax = _out_pagination_controller.plots.axs[0]\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d52d45",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ax.format_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f28b01",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Find ascending sequences of most-likely positions\n",
    "def format_coord(x, y):\n",
    "    col = round(x)\n",
    "    row = round(y)\n",
    "    nrows, ncols = X.shape\n",
    "    if 0 <= col < ncols and 0 <= row < nrows:\n",
    "        z = X[row, col]\n",
    "        return f'x={x:1.4f}, y={y:1.4f}, z={z:1.4f}'\n",
    "    else:\n",
    "        return f'x={x:1.4f}, y={y:1.4f}'\n",
    "\n",
    "\n",
    "ax.format_coord = format_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de603c0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# _out_pagination_controller.plot_widget.setStatusTip('LONG STATUS TIP TEST')\n",
    "\n",
    "_out_pagination_controller.plot_widget.update_status('LONG STATUS TIP TEST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b187c8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# _out_pagination_controller.plots.radon_transform\n",
    "fig = _out_pagination_controller.plots.fig\n",
    "\n",
    "# plt.subplots_adjust(left=0.15, right=0.85, top=0.9, bottom=0.1)\n",
    "# Adjust the margins using subplots_adjust\n",
    "fig.subplots_adjust(left=0.15, right=0.85, bottom=0.15, top=0.85)\n",
    "\n",
    "# Adjust the margins using the Figure object\n",
    "# fig.set_tight_layout(dict(rect=[0.1, 0.2, 0.8, 0.8]))\n",
    "# fig.tight_layout(dict(rect=[0.1, 0.2, 0.8, 0.8]))\n",
    "# fig.tight_layout(pad=1.0, rect=[0.1, 0.1, 0.8, 0.8])\n",
    "_out_pagination_controller.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9643ed",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "(a_name, a_decoder) = tuple(track_templates.get_decoders_dict().items())[0]\n",
    "a_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d1a25",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## 🔷🎨 2024-03-06 - Uni Page Scrollable Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9e880",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "\n",
    "# decoder_decoded_epochs_result_dict: generic\n",
    "single_page_app, single_page_paginated_multi_decoder_decoded_epochs_window, single_page_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                                                                                                decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                                                                                                included_epoch_indicies=None, debug_print=False,\n",
    "                                                                                                params_kwargs={'skip_plotting_most_likely_positions': False, 'enable_per_epoch_action_buttons': False,\n",
    "                                                                                                               'enable_radon_transform_info': False, 'enable_weighted_correlation_info': True,\n",
    "                                                                                                                # 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "                                                                                                                # 'disable_y_label': True,\n",
    "                                                                                                                'isPaginatorControlWidgetBackedMode': True,\n",
    "                                                                                                                'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "                                                                                                                # 'debug_print': True,\n",
    "                                                                                                                'max_subplots_per_page': 64,\n",
    "                                                                                                                'scrollable_figure': True,\n",
    "                                                                                                                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2565e3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "single_page_paginated_multi_decoder_decoded_epochs_window.add_data_overlays(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "_tmp_out_selections = single_page_paginated_multi_decoder_decoded_epochs_window.restore_selections_from_user_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abee48",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# for curr_results_obj: LeaveOneOutDecodingAnalysisResult object\n",
    "num_filter_epochs:int = curr_results_obj.active_filter_epochs.n_epochs\n",
    "\n",
    "# `active_filter_epochs_df` native columns approach\n",
    "active_filter_epochs_df = curr_results_obj.active_filter_epochs.to_dataframe().copy()\n",
    "assert np.isin(['score', 'velocity', 'intercept', 'speed'], active_filter_epochs_df.columns).all()\n",
    "epochs_linear_fit_df = active_filter_epochs_df[['score', 'velocity', 'intercept', 'speed']].copy() # get the `epochs_linear_fit_df` as a subset of the filter epochs df\n",
    "# epochs_linear_fit_df approach\n",
    "assert curr_results_obj.all_included_filter_epochs_decoder_result.num_filter_epochs == np.shape(epochs_linear_fit_df)[0]\n",
    "\n",
    "num_filter_epochs:int = curr_results_obj.all_included_filter_epochs_decoder_result.num_filter_epochs # curr_results_obj.num_filter_epochs\n",
    "try:\n",
    "    time_bin_containers: List[BinningContainer] = deepcopy(curr_results_obj.time_bin_containers)\n",
    "except AttributeError as e:\n",
    "    # AttributeError: 'LeaveOneOutDecodingAnalysisResult' object has no attribute 'time_bin_containers' is expected when `curr_results_obj: LeaveOneOutDecodingAnalysisResult - for Long/Short plotting`\n",
    "    time_bin_containers: List[BinningContainer] = deepcopy(curr_results_obj.all_included_filter_epochs_decoder_result.time_bin_containers) # for curr_results_obj: LeaveOneOutDecodingAnalysisResult - for Long/Short plotting\n",
    "\n",
    "radon_transform_data = RadonTransformPlotDataProvider._subfn_build_radon_transform_plotting_data(active_filter_epochs_df=active_filter_epochs_df,\n",
    "            num_filter_epochs = num_filter_epochs, time_bin_containers = time_bin_containers, radon_transform_column_names=['score', 'velocity', 'intercept', 'speed'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17b087",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "paginated_multi_decoder_decoded_epochs_window.export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fa458",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# _display_long_and_short_stacked_epoch_slices\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out_dict = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f3190",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## Other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d1c13",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_out = _out_pagination_controller.plots['radon_transform'][7]\n",
    "extant_line = _out['line'] # matplotlib.lines.Line2D\n",
    "extant_line.linestyle = 'none'\n",
    "# extant_line.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e00165",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print(list(curr_active_pipeline.filtered_contexts.keys())) # ['maze1_odd', 'maze2_odd', 'maze_odd', 'maze1_even', 'maze2_even', 'maze_even', 'maze1_any', 'maze2_any', 'maze_any']\n",
    "\n",
    "# Converting between decoder names and filtered epoch names:\n",
    "# {'long':'maze1', 'short':'maze2'}\n",
    "# {'LR':'odd', 'RL':'even'}\n",
    "long_LR_name, short_LR_name, long_RL_name, short_RL_name = ['maze1_odd', 'maze2_odd', 'maze1_even', 'maze2_even']\n",
    "decoder_name_to_session_context_name: Dict[str,str] = dict(zip(track_templates.get_decoder_names(), (long_LR_name, long_RL_name, short_LR_name, short_RL_name))) # {'long_LR': 'maze1_odd', 'long_RL': 'maze1_even', 'short_LR': 'maze2_odd', 'short_RL': 'maze2_even'}\n",
    "session_context_to_decoder_name: Dict[str,str] = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), track_templates.get_decoder_names())) # {'maze1_odd': 'long_LR', 'maze1_even': 'long_RL', 'maze2_odd': 'short_LR', 'maze2_even': 'short_RL'}\n",
    "\n",
    "decoder_name_to_session_context_name\n",
    "session_context_to_decoder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896f620",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_num_slices: int = _out_pagination_controller.params.active_num_slices\n",
    "single_plot_fixed_height: float = _out_pagination_controller.params.single_plot_fixed_height\n",
    "all_plots_height: float = _out_pagination_controller.params.all_plots_height\n",
    "print(f'all_plots_height: {all_plots_height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67f5d1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "laps_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fed6b8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from PendingNotebookCode import _add_maze_id_to_epochs\n",
    "\n",
    "\n",
    "## Add new weighted correlation results as new columns in existing filter_epochs df:\n",
    "active_filter_epochs = long_results_obj.active_filter_epochs\n",
    "# Add the maze_id to the active_filter_epochs so we can see how properties change as a function of which track the replay event occured on:\n",
    "active_filter_epochs = _add_maze_id_to_epochs(active_filter_epochs, short_session.t_start)\n",
    "active_filter_epochs._df['weighted_corr_LONG'] = epoch_long_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_SHORT'] = epoch_short_weighted_corr_results[:,0]\n",
    "active_filter_epochs._df['weighted_corr_spearman_LONG'] = epoch_long_weighted_corr_results[:,1]\n",
    "active_filter_epochs._df['weighted_corr_spearman_SHORT'] = epoch_short_weighted_corr_results[:,1]\n",
    "\n",
    "\n",
    "active_filter_epochs\n",
    "active_filter_epochs.to_dataframe()\n",
    "## plot the `weighted_corr_LONG` over time\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=1, nrows=active_num_rows, sharex=True, sharey=sharey, figsize=figsize)\n",
    "\n",
    "## Weighted Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_LONG', title='weighted_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "## Weighted Spearman Correlation during replay epochs:\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_LONG', title='weighted_spearman_corr during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='weighted_corr_spearman_SHORT', xlabel='Replay Epoch Time', ylabel='Weighted Spearman Correlation', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n",
    "_out_ax = active_filter_epochs._df.plot.scatter(x='start', y='score_LONG', title='Radon Transform Score during replay events', marker=\"s\",  s=5, label=f'Long', alpha=0.8)\n",
    "active_filter_epochs._df.plot.scatter(x='start', y='score_SHORT', xlabel='Replay Epoch Time', ylabel='Replay Radon Transform Score', ax=_out_ax, marker=\"s\", c='r', s=5, label=f'Short', alpha=0.8)\n",
    "_out_ax.axhline(y=0.0, linewidth=1, color='k') # the y=0.0 line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3e18e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.reload_default_display_functions()\n",
    "example_stacked_epoch_graphics = curr_active_pipeline.display('_display_long_and_short_stacked_epoch_slices', defer_render=False, save_figure=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1e967",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "## TODO 2024-02-15 8pm - Add in to previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd539a1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "\n",
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "# (laps_radon_transform_merged_df, ripple_radon_transform_merged_df, laps_weighted_corr_merged_df, ripple_weighted_corr_merged_df)\n",
    "laps_simple_pf_pearson_merged_df\n",
    "# laps_radon_transform_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57565cbb",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "decoder_aclu_peak_location_df_merged = deepcopy(track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=False))\n",
    "# decoder_aclu_peak_location_df_merged[np.isin(decoder_aclu_peak_location_df_merged['aclu'], both_included_neuron_stats_df.aclu.to_numpy())]\n",
    "decoder_aclu_peak_location_df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74381521",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_result: TrialByTrialActivity = directional_active_lap_pf_results_dicts['long_LR']\n",
    "# a_result.sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421fd1a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 💾 2024-03-04 - Export `DecoderDecodedEpochsResult` CSVs with user annotations for epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0ae73",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "pho-run-2024",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.heuristic_replay_scoring import HeuristicReplayScoring\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "_, _, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_valid_epoch_times = filtered_epochs_df[['start', 'stop']].to_numpy()\n",
    "\n",
    "## 2024-03-08 - Also constrain the user-selected ones (just to try it):\n",
    "decoder_user_selected_epoch_times_dict, any_user_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "\n",
    "a_result_dict = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "# {a_name:ensure_dataframe(a_result.filter_epochs) for a_name, a_result in a_result_dict.items()}\n",
    "\n",
    "directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=True)\n",
    "\n",
    "# 🟪 2024-02-29 - `compute_pho_heuristic_replay_scores`\n",
    "directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict, _out_new_scores = HeuristicReplayScoring.compute_all_heuristic_scores(track_templates=track_templates, a_decoded_filter_epochs_decoder_result_dict=directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "## Merge the heuristic columns into the wcorr df columns for exports\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "\n",
    "# {a_name:DecoderDecodedEpochsResult.try_add_is_user_annotated_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=filtered_valid_epoch_times) for a_name, a_result in a_result_dict.items()}\n",
    "\n",
    "for a_name, a_result in a_result_dict.items():\n",
    "    # a_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=True)\n",
    "\n",
    "    ## Merge the heuristic columns into the wcorr df columns for exports\n",
    "    # directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "    a_wcorr_result = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict[a_name]\n",
    "    \n",
    "    # did_update_user_annotation_col = DecoderDecodedEpochsResult.try_add_is_user_annotated_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=any_user_selected_epoch_times, t_column_names=None)\n",
    "    # print(f'did_update_user_annotation_col: {did_update_user_annotation_col}')\n",
    "    # did_update_is_valid = DecoderDecodedEpochsResult.try_add_is_valid_epoch_column(ensure_dataframe(a_result.filter_epochs), any_good_selected_epoch_times=filtered_valid_epoch_times, t_column_names=None)\n",
    "    # print(f'did_update_is_valid: {did_update_is_valid}')\n",
    "\n",
    "# ['start',]\n",
    "\n",
    "a_result_dict = deepcopy(directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict)\n",
    "\n",
    "# {a_name:ensure_dataframe(a_result.filter_epochs) for a_name, a_result in a_result_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9f746",
   "metadata": {
    "tags": [
     "pho-run-2024",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pathlib import Path\n",
    "\n",
    "# 💾 export_csvs\n",
    "\n",
    "# BATCH_DATE_TO_USE: str = f'{get_now_day_str()}_APOGEE' # TODO: Change this as needed, templating isn't actually doing anything rn.\n",
    "\n",
    "known_collected_outputs_paths = [Path(v).resolve() for v in ('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs', '/Users/pho/Dropbox (University of Michigan)/MED-DibaLabDropbox/Data/Pho/Outputs/output/collected_outputs', '/home/halechr/cloud/turbo/Data/Output/collected_outputs', '/home/halechr/FastData/gen_scripts/', '/home/halechr/FastData/collected_outputs/', 'output/gen_scripts/', r'K:\\scratch\\collected_outputs')]\n",
    "collected_outputs_path = find_first_extant_path(known_collected_outputs_paths)\n",
    "assert collected_outputs_path.exists(), f\"collected_outputs_path: '{collected_outputs_path}' does not exist! Is the right computer's config commented out above?\"\n",
    "print(f'collected_outputs_path: \"{collected_outputs_path}\"')\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline, track_templates=track_templates)\n",
    "print(f'\\tComputation complete. Exporting .CSVs...')\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "_, _, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_valid_epoch_times = filtered_epochs_df[['start', 'stop']].to_numpy()\n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_output_csv_paths = directional_decoders_epochs_decode_result.export_csvs(parent_output_path=collected_outputs_path.resolve(), active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                              user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                              valid_epochs_selections={'ripple': filtered_valid_epoch_times})\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported directional_decoders_epochs_decode_result to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _output_csv_paths.items()])\n",
    "# print(f'\\t\\t\\tCSV Paths: {_output_csv_paths}\\n')\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')\n",
    "\n",
    "# {'laps_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_weighted_corr_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_weighted_corr_merged_df)_tbin-0.025.csv'),\n",
    "#  'laps_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(laps_simple_pf_pearson_merged_df)_tbin-0.025.csv'),\n",
    "#  'ripple_simple_pf_pearson_merged_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-02-16_0750PM-kdiba_gor01_two_2006-6-07_16-40-19-(ripple_simple_pf_pearson_merged_df)_tbin-0.025.csv')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b77273",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffc939",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88335249",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "any_good_selected_epoch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7420982",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 2024-03-04 - Filter out the epochs based on the criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb021f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# from neuropy.utils.mixins.time_slicing import add_epochs_id_identity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "\n",
    "# 2024-03-04 - Filter out the epochs based on the criteria:\n",
    "filtered_epochs_df, active_spikes_df = filter_and_update_epochs_and_spikes(curr_active_pipeline, global_epoch_name, track_templates, required_min_percentage_of_active_cells=0.333333, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1)\n",
    "filtered_epochs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68233f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🟢 2024-03-27 - Look at active set cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0cb5e",
   "metadata": {
    "tags": [
     "all",
     "required-validations-2024-10-07"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.HDF5_representable import HDFConvertableEnum\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import TruncationCheckingResults\n",
    "\n",
    "\n",
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_aclus_dict, jonathan_firing_rate_analysis_result.neuron_replay_stats_df = truncation_checking_result.build_truncation_checking_aclus_dict(neuron_replay_stats_df=jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "\n",
    "frs_index_inclusion_magnitude:float = 0.5\n",
    "\n",
    "jonathan_firing_rate_analysis_result = JonathanFiringRateAnalysisResult(**curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis.to_dict())\n",
    "\n",
    "## Unrefined:\n",
    "# neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "\n",
    "## Refine the LxC/SxC designators using the firing rate index metric:\n",
    "\n",
    "## Get global `long_short_fr_indicies_analysis`:\n",
    "long_short_fr_indicies_analysis_results = curr_active_pipeline.global_computation_results.computed_data['long_short_fr_indicies_analysis']\n",
    "long_short_fr_indicies_df = long_short_fr_indicies_analysis_results['long_short_fr_indicies_df']\n",
    "jonathan_firing_rate_analysis_result.refine_exclusivity_by_inst_frs_index(long_short_fr_indicies_df, frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "\n",
    "neuron_replay_stats_df, *exclusivity_tuple = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=frs_index_inclusion_magnitude)\n",
    "# short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = exclusivity_tuple\n",
    "exclusivity_aclus_tuple = [v.track_exclusive_aclus for v in exclusivity_tuple]\n",
    "exclusivity_aclus_dict = dict(zip(['short_exclusive', 'long_exclusive', 'BOTH', 'EITHER', 'XOR', 'NEITHER'], exclusivity_aclus_tuple))\n",
    "any_aclus = union_of_arrays(*exclusivity_aclus_tuple)\n",
    "exclusivity_aclus_dict['any'] = any_aclus\n",
    "refined_exclusivity_aclus_tuple = [v.get_refined_track_exclusive_aclus() for v in exclusivity_tuple]\n",
    "neuron_replay_stats_df: pd.DataFrame = HDFConvertableEnum.convert_dataframe_columns_for_hdf(neuron_replay_stats_df)\n",
    "\n",
    "# These keys exhaustively span all aclus:\n",
    "exhaustive_key_names = ['short_exclusive', 'long_exclusive', 'BOTH', 'NEITHER']\n",
    "assert np.all(any_aclus == union_of_arrays(*[exclusivity_aclus_dict[k] for k in exhaustive_key_names]))\n",
    "exhaustive_key_dict = {k:v for k, v in exclusivity_aclus_dict.items() if k in exhaustive_key_names}\n",
    "\n",
    "\n",
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294094c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "old_any_aclus = np.array([  3,   4,   5,   7,  10,  11,  13,  14,  15,  17,  23,  24,  25,  26,  31,  32,  33,  34,  45,  49,  50,  51,  52,  54,  55,  58,  61,  64,  68,  69,  70,  71,  73,  74,  75,  76,  78,  81,  82,  83,  84,  85,  87,  90,  92,  93,  96,  97, 102, 109])\n",
    "old_appearing_aclus = np.array([ 4, 11, 13, 23, 52, 58, 87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af361ab4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "any_aclus = union_of_arrays(*[v for v in truncation_checking_aclus_dict.values() if len(v) > 0])\n",
    "any_aclus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385b808",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520650ce",
   "metadata": {
    "tags": [
     "all",
     "required-validations-2024-10-07"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.analyses.placefields import PfND\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import perform_sweep_lap_groud_truth_performance_testing, _perform_variable_time_bin_lap_groud_truth_performance_testing\n",
    "\n",
    "# desired_laps_decoding_time_bin_size: float = 0.75\n",
    "desired_laps_decoding_time_bin_size: float = 0.5\n",
    "\n",
    "## INPUTS: exclusivity_aclus_tuple, desired_laps_decoding_time_bin_size: float\n",
    "# short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = exclusivity_aclus_tuple\n",
    "# included_neuron_ids_list = [short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset]\n",
    "\n",
    "# included_neuron_ids_list = [*exclusivity_aclus_tuple]\n",
    "\n",
    "## INPUTS: truncation_checking_aclus_dict\n",
    "included_neuron_ids_list = list(truncation_checking_aclus_dict.values())\n",
    "row_names = list(truncation_checking_aclus_dict.keys())\n",
    "\n",
    "_output_tuples_list = perform_sweep_lap_groud_truth_performance_testing(curr_active_pipeline, \n",
    "                                                                        included_neuron_ids_list=included_neuron_ids_list,\n",
    "                                                                        desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_size)\n",
    "\n",
    "percent_laps_correctness_df: pd.DataFrame = pd.DataFrame.from_records([complete_decoded_context_correctness_tuple.percent_correct_tuple for (a_directional_merged_decoders_result, result_laps_epochs_df, complete_decoded_context_correctness_tuple) in _output_tuples_list],\n",
    "                          columns=(\"track_ID_correct\", \"dir_correct\", \"complete_correct\"), index=row_names)\n",
    "percent_laps_correctness_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb00384",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# ❕🟢 2024-10-07 - Rigorous Decoder Performance assessment\n",
    "2024-03-29 - Quantify cell contributions to decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9715a6",
   "metadata": {
    "tags": [
     "all",
     "active-2024-10-08"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs: all_directional_pf1D_Decoder, alt_directional_merged_decoders_result\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrainTestSplitResult, TrainTestLapsSplitting, CustomDecodeEpochsResult, decoder_name, epoch_split_key, get_proper_global_spikes_df, DirectionalPseudo2DDecodersResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _do_train_test_split_decode_and_evaluate\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import PfND\n",
    "from neuropy.core.session.dataSession import Laps\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import compute_weighted_correlations\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _check_result_laps_epochs_df_performance\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "global_session = curr_active_pipeline.filtered_sessions[global_epoch_name]\n",
    "\n",
    "def _add_extra_epochs_df_columns(epochs_df: pd.DataFrame):\n",
    "    \"\"\" captures: global_session, t_start, t_delta, t_end\n",
    "    \"\"\"\n",
    "    epochs_df = epochs_df.sort_values(['start', 'stop', 'label']).reset_index(drop=True) # Sort by columns: 'start' (ascending), 'stop' (ascending), 'label' (ascending)\n",
    "    epochs_df = epochs_df.drop_duplicates(subset=['start', 'stop', 'label'])\n",
    "    epochs_df = epochs_df.epochs.adding_maze_id_if_needed(t_start=t_start, t_delta=t_delta, t_end=t_end)\n",
    "    epochs_df = Laps._compute_lap_dir_from_smoothed_velocity(laps_df=epochs_df, global_session=deepcopy(global_session), replace_existing=True)\n",
    "    return epochs_df\n",
    "\n",
    "directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data.get('TrainTestSplit', None)\n",
    "force_recompute_directional_train_test_split_result: bool = False\n",
    "if (directional_train_test_split_result is None) or force_recompute_directional_train_test_split_result:\n",
    "    ## recompute\n",
    "    print(f\"'TrainTestSplit' not computed, recomputing...\")\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['directional_train_test_split'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "    directional_train_test_split_result: TrainTestSplitResult = curr_active_pipeline.global_computation_results.computed_data['TrainTestSplit']\n",
    "    assert directional_train_test_split_result is not None, f\"faiiled even after recomputation\"\n",
    "    print('\\tdone.')\n",
    "\n",
    "training_data_portion: float = directional_train_test_split_result.training_data_portion\n",
    "test_data_portion: float = directional_train_test_split_result.test_data_portion\n",
    "print(f'training_data_portion: {training_data_portion}, test_data_portion: {test_data_portion}')\n",
    "\n",
    "test_epochs_dict: Dict[types.DecoderName, pd.DataFrame] = directional_train_test_split_result.test_epochs_dict\n",
    "train_epochs_dict: Dict[types.DecoderName, pd.DataFrame] = directional_train_test_split_result.train_epochs_dict\n",
    "train_lap_specific_pf1D_Decoder_dict: Dict[types.DecoderName, BasePositionDecoder] = directional_train_test_split_result.train_lap_specific_pf1D_Decoder_dict\n",
    "\n",
    "# OUTPUTS: train_test_split_laps_df_dict\n",
    "active_laps_decoding_time_bin_size: float = 0.25\n",
    "# active_laps_decoding_time_bin_size: float = 2.5\n",
    "# active_laps_decoding_time_bin_size: float = 5.5\n",
    "complete_decoded_context_correctness_tuple, laps_marginals_df, all_directional_pf1D_Decoder, all_test_epochs_df, all_directional_laps_filter_epochs_decoder_result, _out_separate_decoder_results = _do_train_test_split_decode_and_evaluate(curr_active_pipeline=curr_active_pipeline,\n",
    "                                                                                                                                                                                                                active_laps_decoding_time_bin_size=active_laps_decoding_time_bin_size, included_neuron_IDs=disappearing_aclus,\n",
    "                                                                                                                                                                                                                force_recompute_directional_train_test_split_result=False, compute_separate_decoder_results=True)\n",
    "(is_decoded_track_correct, is_decoded_dir_correct, are_both_decoded_properties_correct), (percent_laps_track_identity_estimated_correctly, percent_laps_direction_estimated_correctly, percent_laps_estimated_correctly) = complete_decoded_context_correctness_tuple\n",
    "print(f\"percent_laps_track_identity_estimated_correctly: {round(percent_laps_track_identity_estimated_correctly*100.0, ndigits=3)}%\")\n",
    "\n",
    "if _out_separate_decoder_results is not None:\n",
    "    assert len(_out_separate_decoder_results) == 3, f\"_out_separate_decoder_results: {_out_separate_decoder_results}\"\n",
    "    test_decoder_results_dict, train_decoded_results_dict, train_decoded_measured_diff_df_dict = _out_separate_decoder_results\n",
    "    ## OUTPUTS: test_decoder_results_dict, train_decoded_results_dict\n",
    "_remerged_laps_dfs_dict = {}\n",
    "for a_decoder_name, a_test_epochs_df in test_epochs_dict.items():\n",
    "    a_train_epochs_df = train_epochs_dict[a_decoder_name]\n",
    "    a_train_epochs_df['test_train_epoch_type'] = 'train'\n",
    "    a_test_epochs_df['test_train_epoch_type'] = 'test'\n",
    "    _remerged_laps_dfs_dict[a_decoder_name] = pd.concat([a_train_epochs_df, a_test_epochs_df], axis='index')\n",
    "    _remerged_laps_dfs_dict[a_decoder_name] = _add_extra_epochs_df_columns(epochs_df=_remerged_laps_dfs_dict[a_decoder_name])\n",
    "\n",
    "\n",
    "# _add_extra_epochs_df_columns\n",
    "# _remerged_laps_dfs_dict = {k: pd.concat([v, test_epochs_dict[k]], axis='index') for k, v in train_epochs_dict.items()}\t\n",
    "# _remerged_laps_dfs_dict['long_LR']\n",
    "\n",
    "\n",
    "## OUTPUTS: all_test_epochs_df, train_epochs_dict, test_epochs_dict, _remerged_laps_dfs_dict\n",
    "# all_test_epochs_df\n",
    "\n",
    "# Performed 3 aggregations grouped on column: 'lap_id'\n",
    "# all_test_epochs_df = all_test_epochs_df.groupby(['lap_id']).agg(start_min=('start', 'min'), stop_max=('stop', 'max'), maze_id_first=('maze_id', 'first')).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48688256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'disappearing_aclus: {disappearing_aclus}')\n",
    "_alt_directional_train_test_split_result = directional_train_test_split_result.sliced_by_neuron_id(included_neuron_ids=disappearing_aclus)\n",
    "_alt_directional_train_test_split_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "_alt_directional_train_test_split_result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_decoded_track_correct ## get an across_session_scatter output like we do for the ripples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8749913",
   "metadata": {},
   "source": [
    "### Display the `TrainTestSplitResult` in a `PhoPaginatedMultiDecoderDecodedEpochsWindow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d23249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.epoch import Epoch, ensure_dataframe\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import add_laps_groundtruth_information_to_dataframe\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow\n",
    "\n",
    "## INPUTS: train_decoded_results_dict\n",
    "# decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs # looks like 'lap_dir' column is wrong\n",
    "\n",
    "# active_results: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(decoder_laps_filter_epochs_decoder_result_dict)\n",
    "active_results: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy(train_decoded_results_dict)\n",
    "\n",
    "# updated_laps_dfs_dict = {}\n",
    "# ## Update the .filter_epochs:\n",
    "# for k, v in active_results.items():\n",
    "#     updated_laps_dfs_dict[k] = Epoch(add_laps_groundtruth_information_to_dataframe(curr_active_pipeline=curr_active_pipeline, result_laps_epochs_df=ensure_dataframe(v.filter_epochs)))\n",
    "#     active_results[k].filter_epochs =  updated_laps_dfs_dict[k]\n",
    "\n",
    "# updated_laps_dfs_dict['long_LR']\n",
    "# active_results['long_LR'].filter_epochs\n",
    "\n",
    "laps_app, laps_paginated_multi_decoder_decoded_epochs_window, laps_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                            decoder_decoded_epochs_result_dict=active_results, epochs_name='laps', included_epoch_indicies=None, \n",
    "    params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "    'skip_plotting_most_likely_positions': False, 'skip_plotting_measured_positions': False, \n",
    "    # 'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': False,\n",
    "    'enable_decoded_most_likely_position_curve': True, 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "    # 'disable_y_label': True,\n",
    "    # 'isPaginatorControlWidgetBackedMode': True,\n",
    "    # 'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "    # 'debug_print': True,\n",
    "    'max_subplots_per_page': 10,\n",
    "    'scrollable_figure': True,\n",
    "    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "    'use_AnchoredCustomText': False,\n",
    "    })\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import TrainTestSplitPlotDataProvider, TrainTestSplitPlotData\n",
    "\n",
    "\n",
    "## INPUTS: all_test_epochs_df, train_epochs_dict, test_epochs_dict, _remerged_laps_dfs_dict\n",
    "# a_decoder_name: str='long_LR'\n",
    "# a_ctrlr = laps_pagination_controller_dict[a_decoder_name]\n",
    "\n",
    "for a_decoder_name, a_ctrlr in laps_pagination_controller_dict.items():\n",
    "    # Build Radon Transforms and add them:\n",
    "    train_test_split_epochs_data = TrainTestSplitPlotDataProvider.decoder_build_single_decoded_position_curves_data(all_test_epochs_df=all_test_epochs_df, train_epochs_dict=train_epochs_dict, test_epochs_dict=test_epochs_dict, remerged_laps_dfs_dict=_remerged_laps_dfs_dict, a_decoder_name=a_decoder_name)\n",
    "    if train_test_split_epochs_data is not None:\n",
    "        TrainTestSplitPlotDataProvider.add_data_to_pagination_controller(a_ctrlr, train_test_split_epochs_data, update_controller_on_apply=True)\n",
    "        # TrainTestSplitPlotDataProvider.remove_data_from_pagination_controller(a_pagination_controller=a_ctrlr, should_remove_params=True, update_controller_on_apply=True)\n",
    "\n",
    "laps_paginated_multi_decoder_decoded_epochs_window.refresh_current_page()\n",
    "\n",
    "# on_render_page_callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce176e84",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import pho_jointplot\n",
    "import seaborn as sns\n",
    "\n",
    "plot_key: str = 'err_cm'\n",
    "\n",
    "# Plot each list as a separate time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, value in train_decoded_measured_diff_df_dict.items():\n",
    "    # sns.lineplot(x=range(len(value)), y=value, label=key)\n",
    "    _out_line = sns.lineplot(data=value, x='t', y=plot_key, label=key)\n",
    "    _out_scatter = sns.scatterplot(data=value, x='t', y=plot_key) # no `, label=key` because we only want one entry in the legend\n",
    "\n",
    "plt.xlabel('lap_center_t (sec)')\n",
    "plt.ylabel('mean_error [cm]')\n",
    "plt.title('LAp Decoding Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10117f91",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_epochs_dict = {k:Epoch(ensure_dataframe(v.measured_decoded_position_comparion.decoded_measured_diff_df)) for k, v in test_decoder_results_dict.items()}\n",
    "active_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20e747",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_epochs_dict = {k:Epoch(ensure_dataframe(v)) for k, v in train_decoded_measured_diff_df_dict.items()}\n",
    "active_epochs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8f168",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 2024-04-03 - Time-bin effect on lap decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00ca28",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from attrs import make_class\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "return_full_decoding_results: bool = True\n",
    "# return_full_decoding_results: bool = False\n",
    "desired_laps_decoding_time_bin_size = [0.025] #np.linspace(start=0.025, stop=, num=4)\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, False)\n",
    "custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_size,\n",
    "                                                                        use_single_time_bin_per_epoch=[False],\n",
    "                                                                        minimum_event_duration=[desired_laps_decoding_time_bin_size[-1]])\n",
    "\n",
    "\n",
    "_across_session_results_extended_dict = {}\n",
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict, return_full_decoding_results=return_full_decoding_results,\n",
    "                                                save_hdf=True, save_csvs=True,\n",
    "                                                # desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.5, num=4),\n",
    "                                                custom_all_param_sweep_options=custom_all_param_sweep_options, # directly provide the parameter sweeps\n",
    "                                                )\n",
    "if return_full_decoding_results:\n",
    "    # with `return_full_decoding_results == True`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_full_directional_merged_decoders_result = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    # validate the result:\n",
    "    # {k:v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size for k,v in output_full_directional_merged_decoders_result.items()}\n",
    "    # assert np.all([np.isclose(dict(k)['desired_shared_decoding_time_bin_size'], v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size) for k,v in output_full_directional_merged_decoders_result.items()]), f\"the desired time_bin_size in the parameters should match the one used that will appear in the decoded result\"\n",
    "\n",
    "\n",
    "else:\n",
    "    # with `return_full_decoding_results == False`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    output_full_directional_merged_decoders_result = None\n",
    "\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da04be4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d97fe2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _show_sweep_result\n",
    "\n",
    "## INPUTS: output_full_directional_merged_decoders_result\n",
    "\n",
    "\n",
    "## RUN\n",
    "global_measured_position_df: pd.DataFrame = deepcopy(curr_active_pipeline.sess.position.to_dataframe()).dropna(subset=['lap']) # computation_result.sess.position.to_dataframe()\n",
    "# sweep_key_name: str=\"desired_shared_decoding_time_bin_size\"\n",
    "sweep_key_name: str=\"desired_laps_decoding_time_bin_size\"\n",
    "_out_pagination_controller, (all_swept_measured_positions_dfs_dict, all_swept_decoded_positions_df_dict, all_swept_decoded_measured_diff_df_dict) = _show_sweep_result(output_full_directional_merged_decoders_result, global_measured_position_df=global_measured_position_df,\n",
    "                                                                                                                                                        xbin=long_results_obj.original_1D_decoder.xbin,\n",
    "                                                                                                                                                        active_context=curr_active_pipeline.build_display_context_for_session(display_fn_name='DecodedEpochSlices', epochs='laps', decoder='all_dir'),\n",
    "                                                                                                                                                        sweep_params_idx=2, sweep_key_name=sweep_key_name, max_subplots_per_page=4)\n",
    "# _out_pagination_controller\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed630cfc",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "desired_laps_decoding_time_bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e8f9a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Context Mask - provides additional information about an Identifying context, like whether a certain component of it should print:\n",
    "# has tags like 'print_debug', 'print_session', 'print_across_sessions'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b5597",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### Plot Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b205de6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "from pyphoplacecellanalysis.Pho2D.plotly.plotly_templates import PlotlyHelpers\n",
    "from pyphoplacecellanalysis.Pho2D.statistics_plotting_helpers import plot_histograms_across_sessions, plot_stacked_histograms\n",
    "from pyphoplacecellanalysis.Pho2D.plotly.Extensions.plotly_helpers import plotly_helper_save_figures, _helper_build_figure, plotly_pre_post_delta_scatter, plot_across_sessions_scatter_results\n",
    "from PIL import Image\n",
    "from pyphocorehelpers.programming_helpers import copy_image_to_clipboard\n",
    "from pyphocorehelpers.Filesystem.path_helpers import sanitize_filename_for_Windows\n",
    "\n",
    "# fig_size_kwargs = {'width': 1650, 'height': 480}\n",
    "resolution_multiplier = 1\n",
    "# fig_size_kwargs = {'width': resolution_multiplier*1650, 'height': resolution_multiplier*480}\n",
    "fig_size_kwargs = {'width': resolution_multiplier*1920, 'height': resolution_multiplier*480}\n",
    "is_dark_mode, template = PlotlyHelpers.get_plotly_template(is_dark_mode=False)\n",
    "pio.templates.default = template\n",
    "\n",
    "# figure_export_path = Path(r'E:\\Dropbox (Personal)\\Active\\Kamran Diba Lab\\Presentations\\2024-05-30 - Pho iNAV Poster\\Figures').resolve()\n",
    "# figure_export_path = Path('/Users/pho/Dropbox (Personal)/Active/Kamran Diba Lab/Presentations/2024-05-30 - Pho iNAV Poster/Figures').resolve()\n",
    "# assert figure_export_path.exists()\n",
    "\n",
    "figures_folder = Path('output').resolve()\n",
    "\n",
    "def save_plotly(a_fig, a_fig_context):\n",
    "    \"\"\" \n",
    "    captures: TODAY_DAY_DATE, figures_folder, neptuner_run\n",
    "    \"\"\"\n",
    "    fig_save_path: Path = figures_folder.joinpath('_'.join([BATCH_DATE_TO_USE, sanitize_filename_for_Windows(a_fig_context.get_description())])).resolve()\n",
    "    figure_out_paths = {'.html': fig_save_path.with_suffix('.html'), '.png': fig_save_path.with_suffix('.png')}\n",
    "    a_fig.write_html(figure_out_paths['.html'])\n",
    "    display(fullwidth_path_widget(figure_out_paths['.html'], file_name_label='.html'))\n",
    "    # print(file_uri_from_path(figure_out_paths['.html']))\n",
    "    a_fig.write_image(figure_out_paths['.png'])\n",
    "    # print(file_uri_from_path(figure_out_paths['.png']))\n",
    "    display(fullwidth_path_widget(figure_out_paths['.png'], file_name_label='.png'))\n",
    "        \n",
    "    return figure_out_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d0b4d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: wcorr_ripple_shuffle_all_df, all_shuffles_only_best_decoder_wcorr_df, custom_suffix\n",
    "\n",
    "histogram_bins = 25\n",
    "num_sessions = 1\n",
    "# plot_var_name: str = 'abs_best_wcorr'\n",
    "# plot_var_name: str = 'wcorr_z_long'\n",
    "# plot_var_name: str = 'wcorr_long_LR'\n",
    "plot_var_name: str = 'wcorr_long_RL'\n",
    "\n",
    "# plot_y_zero: float = 0.5\n",
    "plot_y_zero: float = 0.0\n",
    "\n",
    "# concatenated_ripple_df = deepcopy(wcorr_ripple_shuffle_all_df)\n",
    "concatenated_ripple_df = deepcopy(ripple_directional_all_epoch_bins_marginal)\n",
    "desired_ripple_decoding_time_bin_size: float = wcorr_shuffle_results.wcorr_ripple_shuffle.all_templates_decode_kwargs['desired_ripple_decoding_time_bin_size']\n",
    "print(f'{desired_ripple_decoding_time_bin_size = }')\n",
    "concatenated_ripple_df['time_bin_size'] = desired_ripple_decoding_time_bin_size\n",
    "display(concatenated_ripple_df)\n",
    "if 'delta_aligned_start_t' not in concatenated_ripple_df.columns:\n",
    "    concatenated_ripple_df['delta_aligned_start_t'] = concatenated_ripple_df['start'] - t_delta\n",
    "\n",
    "# px_scatter_kwargs = {'x': 'delta_aligned_start_t', 'y': variable_name, 'color':\"is_user_annotated_epoch\", 'title': f\"'{variable_name}'\"} # , 'color': 'time_bin_size', 'range_y': [-1.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size', 'is_user_annotated_epoch':'user_sel'}\n",
    "px_scatter_kwargs = {'x': 'delta_aligned_start_t', 'y': plot_var_name, 'title': f\"'{plot_var_name}'\"} # , 'color': 'time_bin_size', 'range_y': [-1.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size', 'is_user_annotated_epoch':'user_sel'}\n",
    "#\n",
    "# hist_kwargs = dict(color=\"time_bin_size\")\n",
    "hist_kwargs = dict(color=\"is_user_annotated_epoch\") # , histnorm='probability density'\n",
    "hist_kwargs.pop('color')\n",
    "\n",
    "# px_scatter_kwargs['color'] = 'custom_replay_name'\n",
    "# hist_kwargs['color'] = 'custom_replay_name'\n",
    "\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\n",
    "new_fig_ripples, new_fig_ripples_context = plotly_pre_post_delta_scatter(data_results_df=concatenated_ripple_df, out_scatter_fig=None, histogram_bins=histogram_bins,\n",
    "                        px_scatter_kwargs=px_scatter_kwargs, histogram_variable_name=plot_var_name, hist_kwargs=hist_kwargs, forced_range_y=None,\n",
    "                        time_delta_tuple=(t_start, 0.0, (t_end-t_delta)), legend_title_text=None, is_dark_mode=is_dark_mode)\n",
    "\n",
    "new_fig_ripples = new_fig_ripples.update_layout(fig_size_kwargs)\n",
    "_extras_output_dict = {}\n",
    "if is_dark_mode:\n",
    "    _extras_output_dict[\"y_mid_line\"] = new_fig_ripples.add_hline(y=plot_y_zero, line=dict(color=\"rgba(0.8,0.8,0.8,.75)\", width=2), row='all', col='all')\n",
    "else:\n",
    "    _extras_output_dict[\"y_mid_line\"] = new_fig_ripples.add_hline(y=plot_y_zero, line=dict(color=\"rgba(0.2,0.2,0.2,.75)\", width=2), row='all', col='all')\n",
    "\n",
    "# # Update layout to add a title to the legend\n",
    "# new_fig_ripples.update_layout(\n",
    "#     legend_title_text='Is User Selected'  # Add a title to the legend\n",
    "# )\n",
    "\n",
    "# fig_to_clipboard(new_fig_ripples, **fig_size_kwargs)\n",
    "new_fig_ripples_context = new_fig_ripples_context.adding_context_if_missing(num_sessions=num_sessions, plot_type='scatter+hist', comparison='pre-post-delta', variable_name=plot_var_name)\n",
    "figure_out_paths = save_plotly(a_fig=new_fig_ripples, a_fig_context=new_fig_ripples_context)\n",
    "new_fig_ripples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82089d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🟢 2024-05-29 - Trial-by-Trial Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3cd82",
   "metadata": {
    "tags": [
     "all",
     "required-validations-2024-10-07"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent\n",
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrialByTrialActivityResult\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "## INPUTS: curr_active_pipeline, track_templates, global_epoch_name, (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)\n",
    "any_decoder_neuron_IDs: NDArray = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "# long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "# ## Directional Trial-by-Trial Activity:\n",
    "if 'pf1D_dt' not in curr_active_pipeline.computation_results[global_epoch_name].computed_data:\n",
    "    # if `KeyError: 'pf1D_dt'` recompute\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "active_pf_1D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D_dt'])\n",
    "# active_pf_2D_dt: PfND_TimeDependent = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D_dt'])\n",
    "\n",
    "active_pf_dt: PfND_TimeDependent = active_pf_1D_dt\n",
    "# Limit only to the placefield aclus:\n",
    "active_pf_dt = active_pf_dt.get_by_id(ids=any_decoder_neuron_IDs)\n",
    "\n",
    "# active_pf_dt: PfND_TimeDependent = deepcopy(active_pf_2D_dt) # 2D\n",
    "long_LR_name, long_RL_name, short_LR_name, short_RL_name = track_templates.get_decoder_names()\n",
    "directional_lap_epochs_dict = dict(zip((long_LR_name, long_RL_name, short_LR_name, short_RL_name), (long_LR_epochs_obj, long_RL_epochs_obj, short_LR_epochs_obj, short_RL_epochs_obj)))\n",
    "directional_active_lap_pf_results_dicts: Dict[types.DecoderName, TrialByTrialActivity] = TrialByTrialActivity.directional_compute_trial_by_trial_correlation_matrix(active_pf_dt=active_pf_dt, directional_lap_epochs_dict=directional_lap_epochs_dict, included_neuron_IDs=any_decoder_neuron_IDs)\n",
    "\n",
    "## OUTPUTS: directional_active_lap_pf_results_dicts\n",
    "a_trial_by_trial_result: TrialByTrialActivityResult = TrialByTrialActivityResult(any_decoder_neuron_IDs=any_decoder_neuron_IDs,\n",
    "                                                                                active_pf_dt=active_pf_dt,\n",
    "                                                                                directional_lap_epochs_dict=directional_lap_epochs_dict,\n",
    "                                                                                directional_active_lap_pf_results_dicts=directional_active_lap_pf_results_dicts,\n",
    "                                                                                is_global=True)  # type: Tuple[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, BasePositionDecoder], Any]\n",
    "\n",
    "directional_lap_epochs_dict: Dict[str, Epoch] = directional_trial_by_trial_activity_result.directional_lap_epochs_dict\n",
    "stability_df, stability_dict = a_trial_by_trial_result.get_stability_df()\n",
    "# appearing_or_disappearing_aclus, appearing_stability_df, appearing_aclus, disappearing_stability_df, disappearing_aclus, (stable_both_aclus, stable_neither_aclus, stable_long_aclus, stable_short_aclus) = a_trial_by_trial_result.get_cell_stability_info(minimum_one_point_stability=0.6, zero_point_stability=0.1)\n",
    "_neuron_group_split_stability_dfs_tuple, _neuron_group_split_stability_aclus_tuple = a_trial_by_trial_result.get_cell_stability_info(minimum_one_point_stability=0.6, zero_point_stability=0.1)\n",
    "appearing_stability_df, disappearing_stability_df, appearing_or_disappearing_stability_df, stable_both_stability_df, stable_neither_stability_df, stable_long_stability_df, stable_short_stability_df = _neuron_group_split_stability_dfs_tuple\n",
    "appearing_aclus, disappearing_aclus, appearing_or_disappearing_aclus, stable_both_aclus, stable_neither_aclus, stable_long_aclus, stable_short_aclus = _neuron_group_split_stability_aclus_tuple\n",
    "override_active_neuron_IDs = deepcopy(appearing_or_disappearing_aclus)\n",
    "override_active_neuron_IDs\n",
    "\n",
    "# stability_df\n",
    "\n",
    "# a_trial_by_trial_result\n",
    "\n",
    "# Time-dependent\n",
    "long_pf1D_dt, short_pf1D_dt, global_pf1D_dt = long_results.pf1D_dt, short_results.pf1D_dt, global_results.pf1D_dt\n",
    "# long_pf2D_dt, short_pf2D_dt, global_pf2D_dt = long_results.pf2D_dt, short_results.pf2D_dt, global_results.pf2D_dt\n",
    "global_pf1D_dt: PfND_TimeDependent = global_results.pf1D_dt\n",
    "# global_pf2D_dt: PfND_TimeDependent = global_results.pf2D_dt\n",
    "_flat_z_scored_tuning_map_matrix, _flat_decoder_identity_arr = a_trial_by_trial_result.build_combined_decoded_epoch_z_scored_tuning_map_matrix() # .shape: (n_epochs, n_neurons, n_pos_bins) \n",
    "modified_directional_active_lap_pf_results_dicts: Dict[types.DecoderName, TrialByTrialActivity] = a_trial_by_trial_result.build_separated_nan_filled_decoded_epoch_z_scored_tuning_map_matrix()\n",
    "# _flat_z_scored_tuning_map_matrix\n",
    "\n",
    "\n",
    "## OUTPUTS: override_active_neuron_IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_df\n",
    "appearing_stability_df\n",
    "disappearing_stability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a45bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pf_computation', 'pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_trial_by_trial_activity_result: TrialByTrialActivityResult = curr_active_pipeline.global_computation_results.computed_data.get('TrialByTrialActivity', None)\n",
    "assert directional_trial_by_trial_activity_result is not None\n",
    "\n",
    "any_decoder_neuron_IDs: NDArray = deepcopy(directional_trial_by_trial_activity_result.any_decoder_neuron_IDs)\n",
    "    \n",
    "## OUTPUTS: directional_trial_by_trial_activity_result, directional_active_lap_pf_results_dicts\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "\n",
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pf_computation', 'pfdt_computation'], enabled_filter_names=[global_epoch_name], fail_on_exception=True, debug_print=False)\n",
    "\n",
    "# active_pf = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D_dt']) # PfND_TimeDependent\n",
    "\n",
    "# active_pf = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D'])\n",
    "active_pf = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D'])\n",
    "\n",
    "# active_pf = deepcopy(directional_trial_by_trial_activity_result.active_pf_dt) # IndexError: index 65 is out of bounds for axis 0 with size 65\n",
    "\n",
    "any_decoder_neuron_IDs: NDArray = deepcopy(directional_trial_by_trial_activity_result.any_decoder_neuron_IDs)\n",
    "override_active_neuron_IDs = deepcopy(any_decoder_neuron_IDs)\n",
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['pf_computation', 'pfdt_computation'], enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "## Uses `plot_trial_to_trial_reliability_all_decoders_image_stack` to plot the reliability trial-by-trial indicators over time\n",
    "## INPUTS: a_pf2D_dt, z_scored_tuning_map_matrix\n",
    "# directional_active_lap_pf_results_dicts: Dict[types.DecoderName, TrialByTrialActivity] = deepcopy(directional_trial_by_trial_activity_result.directional_active_lap_pf_results_dicts)\n",
    "modified_directional_active_lap_pf_results_dicts: Dict[types.DecoderName, TrialByTrialActivity] = directional_trial_by_trial_activity_result.build_separated_nan_filled_decoded_epoch_z_scored_tuning_map_matrix()\n",
    "modified_directional_active_lap_pf_results_dicts = {k:v.sliced_by_neuron_id(included_neuron_ids=override_active_neuron_IDs) for k, v in modified_directional_active_lap_pf_results_dicts.items()}\n",
    "_a_trial_by_trial_window = TrialByTrialActivityWindow.plot_trial_to_trial_reliability_all_decoders_image_stack(directional_active_lap_pf_results_dicts=modified_directional_active_lap_pf_results_dicts,\n",
    "                                                                                                                active_one_step_decoder=deepcopy(active_pf), drop_below_threshold=drop_below_threshold,\n",
    "                                                                                                                override_active_neuron_IDs=override_active_neuron_IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_session_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148cc20",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### ✅ 2024-08-14-:🖼️  Normal Matplotlib-based figure output for the `trial_by_trial_correlation_matrix.z_scored_tuning_map_matrix` to show the reliably of each place cell across laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f022a",
   "metadata": {
    "tags": [
     "all",
     "required-validations-2024-10-07"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.PyQtPlots.plot_placefields import display_all_pf_2D_pyqtgraph_binned_image_rendering, pyqtplot_plot_image_array\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.ContainerBased.TrialByTrialActivityWindow import TrialByTrialActivityWindow\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "## Uses `plot_trial_to_trial_reliability_all_decoders_image_stack` to plot the reliability trial-by-trial indicators over time\n",
    "# active_pf_dt = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf1D_dt']) # PfND_TimeDependent\n",
    "# active_pf_dt = a_pf2D_dt\n",
    "\n",
    "# active_pf_dt = deepcopy(global_pf1D_dt)\n",
    "# active_pf_dt = deepcopy(global_pf1D)\n",
    "active_pf_dt = deepcopy(curr_active_pipeline.computation_results[global_epoch_name].computed_data['pf2D'])\n",
    "np.sum(active_pf_dt.occupancy)\n",
    "\n",
    "drop_below_threshold = 0.0000001\n",
    "override_active_neuron_IDs = deepcopy(any_decoder_neuron_IDs)\n",
    "## INPUTS: a_pf2D_dt, z_scored_tuning_map_matrix\n",
    "# directional_active_lap_pf_results_dicts: Dict[types.DecoderName, TrialByTrialActivity] = deepcopy(a_trial_by_trial_result.directional_active_lap_pf_results_dicts)\n",
    "# app, parent_root_widget, root_render_widget, plot_array, img_item_array, other_components_array, plot_data_array, additional_img_items_dict, legend_layout = plot_trial_to_trial_reliability_all_decoders_image_stack(directional_active_lap_pf_results_dicts=directional_active_lap_pf_results_dicts, active_one_step_decoder=deepcopy(active_pf_dt), drop_below_threshold=drop_below_threshold)\n",
    "# _a_trial_by_trial_window = TrialByTrialActivityWindow.plot_trial_to_trial_reliability_all_decoders_image_stack(directional_active_lap_pf_results_dicts=directional_active_lap_pf_results_dicts, active_one_step_decoder=deepcopy(active_pf_dt), drop_below_threshold=drop_below_threshold,\n",
    "#                                                                                                                is_overlaid_heatmaps_mode=False,\n",
    "#                                                                                                                )\n",
    "\n",
    "modified_directional_active_lap_pf_results_dicts: Dict[types.DecoderName, TrialByTrialActivity] = a_trial_by_trial_result.build_separated_nan_filled_decoded_epoch_z_scored_tuning_map_matrix()\n",
    "modified_directional_active_lap_pf_results_dicts = {k:v.sliced_by_neuron_id(included_neuron_ids=override_active_neuron_IDs) for k, v in modified_directional_active_lap_pf_results_dicts.items()}\n",
    "# modified_directional_active_lap_pf_results_dicts['long_RL'] = deepcopy(modified_directional_active_lap_pf_results_dicts['long_LR'])\n",
    "_a_trial_by_trial_window: TrialByTrialActivityWindow = TrialByTrialActivityWindow.plot_trial_to_trial_reliability_all_decoders_image_stack(directional_active_lap_pf_results_dicts=modified_directional_active_lap_pf_results_dicts,\n",
    "                                                                                                                active_one_step_decoder=deepcopy(active_pf_dt), drop_below_threshold=drop_below_threshold,\n",
    "                                                                                                                override_active_neuron_IDs=override_active_neuron_IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_plot = _a_trial_by_trial_window.plots.position_plot # PlotItem\n",
    "pos_df: pd.DataFrame = deepcopy(active_pf_dt.position.to_dataframe())\n",
    "position_plot.clearPlots()\n",
    "position_plot.plot(x=pos_df['x'].to_numpy(), y=pos_df['t'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36009e",
   "metadata": {},
   "source": [
    "## 2024-10-14 - Add Track Shapes to the Trial-by-Trial figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654233df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "\n",
    "## get grid_bin_bounds\n",
    "loaded_track_limits = deepcopy(curr_active_pipeline.active_sess_config.loaded_track_limits)\n",
    "# loaded_track_limits\n",
    "\n",
    "# .x_midpoint\n",
    "# .pix2cm\n",
    "loaded_track_limits['long_xlim']\n",
    "loaded_track_limits['short_xlim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.track_shape_drawing import LinearTrackInstance, LinearTrackDimensions, test_LinearTrackDimensions_2D_pyqtgraph\n",
    "\n",
    "plot_array: List[pg.PlotItem] = _a_trial_by_trial_window.plots.plot_array\n",
    "plot_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get grid_bin_bounds\n",
    "loaded_track_limits = deepcopy(curr_active_pipeline.active_sess_config.loaded_track_limits)\n",
    "# loaded_track_limits\n",
    "\n",
    "# .x_midpoint\n",
    "# .pix2cm\n",
    "loaded_track_limits['long_xlim']\n",
    "loaded_track_limits['short_xlim']\n",
    "\n",
    "grid_bin_bounds = [loaded_track_limits['long_xlim'], [0.0, 0.0]]\n",
    "grid_bin_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf55eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.geometry_helpers import point_tuple_mid_point\n",
    "\n",
    "long_track_instance, short_track_instance = LinearTrackInstance.init_tracks_from_session_config(a_sess_config=curr_active_pipeline.sess.config)\n",
    "# _out_temp = long_track_instance.plot_rects(plot_item=plot_array[0])\n",
    "\n",
    "long_track_dims: LinearTrackDimensions = deepcopy(long_track_instance.track_dimensions)\n",
    "short_track_dims: LinearTrackDimensions = deepcopy(short_track_instance.track_dimensions)\n",
    "\n",
    "\n",
    "# Find center from `grid_bin_bounds` using `point_tuple_mid_point`\n",
    "x_midpoint, y_midpoint = (point_tuple_mid_point(grid_bin_bounds[0]), point_tuple_mid_point(grid_bin_bounds[1])) # grid_bin_bounds_center_point: (145.43, 140.61)\n",
    "\n",
    "long_notable_x_positions, _long_notable_y_positions = long_track_dims._build_component_notable_positions(offset_point=(x_midpoint, y_midpoint))\n",
    "short_notable_x_positions, _short_notable_y_positions = short_track_dims._build_component_notable_positions(offset_point=(x_midpoint, y_midpoint))\n",
    "\n",
    "# Omit the midpoint\n",
    "long_notable_x_platform_positions = long_notable_x_positions[[0,1,3,4]] # [37.0774 59.0774 228.69 250.69]\n",
    "short_notable_x_platform_positions = short_notable_x_positions[[0,1,3,4]] # [72.0132 94.0132 193.754 215.754]\n",
    "\n",
    "long_notable_x_platform_positions\n",
    "short_notable_x_platform_positions\n",
    "# app, w, cw, (ax0, ax1), (long_track_dims, long_rect_items, long_rects), (short_track_dims, short_rect_items, short_rects) = test_LinearTrackDimensions_2D_pyqtgraph(long_track_dims=long_track_instance.track_dimensions,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshort_track_dims=short_track_instance.track_dimensions)\n",
    "\n",
    "\n",
    "_out_temp = short_track_dims.plot_rects(plot_item=plot_array[0], offset=[x_midpoint, 0])\n",
    "\n",
    "# _out_temp = long_track_dims.plot_rects(plot_item=plot_array[0], offset=[x_midpoint, 0])\n",
    "# _out_items = long_track_dims.plot_line_collections(plot_item=plot_array[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _out_temp\n",
    "new_pen = pg.mkPen({'color': \"#ffd9001A\", 'width': 2})\n",
    "new_brush = pg.mkBrush(\"#ffd90010\")\n",
    "new_rendering_properties_tuple = (new_pen, new_brush)\n",
    "new_pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c257be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_item, rect_items, rects = _out_temp\n",
    "# rect_items\n",
    "for i, ((x, y, w, h, pen, brush), an_item) in enumerate(zip(rects, rect_items)):\n",
    "    # rects\n",
    "    # pen.setColor(\"#ffd9001A\")\n",
    "    # brush.setColor(\"#ffd90010\")\n",
    "    print(f'item[{i}]: {an_item}')\n",
    "    # an_item.set\n",
    "    an_item.setPen(pg.mkPen({'color': \"#ffd9001A\", 'width': 2}))\n",
    "    an_item.setBrush(pg.mkBrush(\"#ffd90010\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b26cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for an_item in rect_items:\n",
    "    plot_array[0].removeItem(an_item)\n",
    "    # an_item.deleteLater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d907a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'long_LR' not in _a_trial_by_trial_window.plots.additional_img_items_dict:\n",
    "    print(f'added \"long_LR\" to _a_trial_by_trial_window.plots.additional_img_items_dict')\n",
    "    _a_trial_by_trial_window.plots.additional_img_items_dict['long_LR'] = _a_trial_by_trial_window.plots.img_item_array\n",
    "    \n",
    "\n",
    "# _a_trial_by_trial_window.plots.additional_img_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_decoder_name: str = 'short_LR'\n",
    "# _a_trial_by_trial_window.set_series_opacity(target_decoder_name='long_LR', target_opacity=1.0)\n",
    "# _a_trial_by_trial_window.restore_all_series_opacity()\n",
    "_a_trial_by_trial_window.restore_all_series_opacity(override_all_opacity=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd31088",
   "metadata": {},
   "outputs": [],
   "source": [
    "_a_trial_by_trial_window.set_series_opacity(target_decoder_name='long_LR', target_opacity=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523071",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 2024-05-30 - Continuous decoded posterior output videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc2a01",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "directional_decoders_decode_result: DirectionalDecodersContinuouslyDecodedResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersDecoded']\n",
    "all_directional_pf1D_Decoder_dict: Dict[str, BasePositionDecoder] = directional_decoders_decode_result.pf1D_Decoder_dict\n",
    "pseudo2D_decoder: BasePositionDecoder = directional_decoders_decode_result.pseudo2D_decoder\n",
    "spikes_df = directional_decoders_decode_result.spikes_df\n",
    "continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "previously_decoded_keys: List[float] = list(continuously_decoded_result_cache_dict.keys()) # [0.03333]\n",
    "print(F'previously_decoded time_bin_sizes: {previously_decoded_keys}')\n",
    "# continuously_decoded_result_cache_dict = directional_decoders_decode_result.continuously_decoded_result_cache_dict\n",
    "time_bin_size: float = directional_decoders_decode_result.most_recent_decoding_time_bin_size\n",
    "print(f'time_bin_size: {time_bin_size}')\n",
    "\n",
    "continuously_decoded_dict = directional_decoders_decode_result.most_recent_continuously_decoded_dict\n",
    "pseudo2D_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('pseudo2D', None)\n",
    "pseudo2D_decoder_continuously_decoded_result\n",
    "\n",
    "a_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('long_LR', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a8ed8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.data_exporting import PosteriorExporting\n",
    "\n",
    "a_decoder_continuously_decoded_result: DecodedFilterEpochsResult = continuously_decoded_dict.get('long_LR', None)\n",
    "PosteriorExporting.save_posterior_to_video(a_decoder_continuously_decoded_result=a_decoder_continuously_decoded_result, result_name='continuous_long_LR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8a190",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "PosteriorExporting.save_posterior_to_video(a_decoder_continuously_decoded_result=pseudo2D_decoder_continuously_decoded_result, result_name='continuous_pseudo2D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f828c7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## INPUTS: global_results, global_epoch_name\n",
    "\n",
    "# Get the decoders from the computation result:\n",
    "active_one_step_decoder = global_results['pf2D_Decoder']\n",
    "active_two_step_decoder = global_results.get('pf2D_TwoStepDecoder', None)\n",
    "if active_two_step_decoder is None:\n",
    "    curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['position_decoding_two_step'], computation_kwargs_list=[{}], enabled_filter_names=[global_epoch_name, global_LR_name, global_RL_name], fail_on_exception=True, debug_print=False)\n",
    "    active_two_step_decoder = global_results.get('pf2D_TwoStepDecoder', None)\n",
    "    assert active_two_step_decoder is not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125463a",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pyphocorehelpers.plotting.media_output_helpers import get_array_as_image, colormap_and_save_as_video, save_array_as_video\n",
    "\n",
    "# image = get_array_as_image(img_data, desired_height=100, desired_width=None, skip_img_normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9390e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "an_input_posterior = deepcopy(active_two_step_decoder.p_x_given_n_and_x_prev)\n",
    "result_name: str = f'two_step_maze_all'\n",
    "\n",
    "# an_input_posterior = deepcopy(active_one_step_decoder.p_x_given_n)\n",
    "# result_name: str = f'one_step_2D_maze_all'\n",
    "\n",
    "n_x_bins, n_y_bins, n_time_bins = np.shape(an_input_posterior)\n",
    "transpose_axes_tuple = (2, 1, 0,)\n",
    "an_input_posterior = np.transpose(an_input_posterior, transpose_axes_tuple)\n",
    "decoding_realtime_FPS: float = 1.0 / float(active_one_step_decoder.time_bin_size)\n",
    "print(f'decoding_realtime_FPS: {decoding_realtime_FPS}')\n",
    "## save video\n",
    "video_out_path = save_array_as_video(array=an_input_posterior, video_filename=f'output/videos/{result_name}.avi', isColor=True, fps=decoding_realtime_FPS, colormap=cv2.COLORMAP_VIRIDIS)\n",
    "# video_out_path = colormap_and_save_as_video(array=an_input_posterior, video_filename=f'output/videos/{result_name}.avi', fps=decoding_realtime_FPS)\n",
    "\n",
    "print(f'video_out_path: {video_out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc44dd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['position_decoding_two_step'], computation_kwargs_list=[{}], enabled_filter_names=[global_epoch_name, global_LR_name, global_RL_name], fail_on_exception=True, debug_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96151def",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_binned_position_df: pd.DataFrame = global_results.get('extended_stats', {}).get('time_binned_position_df', None)\n",
    "time_binned_position_df\n",
    "# active_measured_positions = computation_result.sess.position.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde2025",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 2024-06-07 - PhoDiba2023Paper figure generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9ab23",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import main_complete_figure_generations\n",
    "\n",
    "main_complete_figure_generations(curr_active_pipeline, save_figure=True, save_figures_only=True, enable_default_neptune_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c41f7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 2024-06-10 - Across Sessions Bar Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae95fcd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "## long_short_post_decoding:\n",
    "inst_spike_rate_groups_result: InstantaneousSpikeRateGroupsComputation = curr_active_pipeline.global_computation_results.computed_data.long_short_inst_spike_rate_groups\n",
    "inst_spike_rate_groups_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19e3c5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import InstantaneousSpikeRateGroupsComputation\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_session_instantaneous_spike_rates_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "instantaneous_time_bin_size_seconds_list: float = [1000.000] # 10ms #TODO 2024-09-12 19:48: - [ ] This is where the time_bin_size is changed\n",
    "save_pickle = True\n",
    "save_hdf = False\n",
    "save_across_session_hdf = False\n",
    "\n",
    "_across_session_results_extended_dict = {}\n",
    "## Combine the output of `compute_and_export_session_instantaneous_spike_rates_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_session_instantaneous_spike_rates_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict, instantaneous_time_bin_size_seconds_list=instantaneous_time_bin_size_seconds_list,\n",
    "                                                save_hdf=save_hdf, save_pickle=save_pickle, save_across_session_hdf=save_across_session_hdf,\n",
    "                                                epoch_handling_mode='UseAllEpochsMode',\n",
    "                                            )\n",
    "\n",
    "# '_perform_long_short_instantaneous_spike_rate_groups_analysis'\n",
    "# global_computation_results = curr_active_pipeline.global_computation_results\n",
    "# global_computation_results.get('computation_config', {})\n",
    "\n",
    "\n",
    "# instantaneous_time_bin_size_seconds_list: float = global_computation_results.computation_config.instantaneous_time_bin_size_seconds_list # 0.01 # 10ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a7920",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict\n",
    "# '2024-09-12_recomputed_inst_fr_comps_0.001.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff872f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Specify the output file:\n",
    "common_file_path = Path('output/active_across_session_scatter_plot_results.h5')\n",
    "print(f'common_file_path: {common_file_path}')\n",
    "InstantaneousFiringRatesDataframeAccessor.add_results_to_inst_fr_results_table(inst_fr_comps=inst_spike_rate_groups_result, curr_active_pipeline=curr_active_pipeline, common_file_path=common_file_path, file_mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22878176",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## Specify the output file:\n",
    "common_file_path = Path('output/active_across_session_scatter_plot_results.h5')\n",
    "print(f'common_file_path: {common_file_path}')\n",
    "InstantaneousFiringRatesDataframeAccessor.add_results_to_inst_fr_results_table(inst_fr_comps=inst_spike_rate_groups_result, curr_active_pipeline=curr_active_pipeline, common_file_path=common_file_path, file_mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a76fe",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print_keys_if_possible(curr_key='pipeline', curr_value=_out, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df735b",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 🔷 2024-07-02 - New epoch decoding and CSV export: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb4c68",
   "metadata": {
    "tags": [
     "pho-run-2024",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import filter_and_update_epochs_and_spikes\n",
    "\n",
    "if ('DirectionalDecodersEpochsEvaluations' in curr_active_pipeline.global_computation_results.computed_data) and (curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations'] is not None):\n",
    "    directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult = curr_active_pipeline.global_computation_results.computed_data['DirectionalDecodersEpochsEvaluations']\n",
    "    directional_decoders_epochs_decode_result.add_all_extra_epoch_columns(curr_active_pipeline, track_templates=track_templates, required_min_percentage_of_active_cells=0.33333333, debug_print=False)\n",
    "\n",
    "    ## UNPACK HERE via direct property access:\n",
    "    pos_bin_size: float = directional_decoders_epochs_decode_result.pos_bin_size\n",
    "    ripple_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.ripple_decoding_time_bin_size\n",
    "    laps_decoding_time_bin_size: float = directional_decoders_epochs_decode_result.laps_decoding_time_bin_size\n",
    "    print(f'{pos_bin_size = }, {ripple_decoding_time_bin_size = }, {laps_decoding_time_bin_size = }') # pos_bin_size = 3.8054171165052444, ripple_decoding_time_bin_size = 0.025, laps_decoding_time_bin_size = 0.2\n",
    "    decoder_laps_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_laps_filter_epochs_decoder_result_dict\n",
    "    decoder_ripple_filter_epochs_decoder_result_dict = directional_decoders_epochs_decode_result.decoder_ripple_filter_epochs_decoder_result_dict\n",
    "    decoder_laps_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_df_dict\n",
    "    decoder_ripple_radon_transform_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_df_dict\n",
    "\n",
    "    # New items:\n",
    "    decoder_laps_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_laps_radon_transform_extras_dict\n",
    "    decoder_ripple_radon_transform_extras_dict = directional_decoders_epochs_decode_result.decoder_ripple_radon_transform_extras_dict\n",
    "\n",
    "    # Weighted correlations:\n",
    "    laps_weighted_corr_merged_df = directional_decoders_epochs_decode_result.laps_weighted_corr_merged_df\n",
    "    ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "    decoder_laps_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_laps_weighted_corr_df_dict\n",
    "    decoder_ripple_weighted_corr_df_dict = directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict\n",
    "\n",
    "    # Pearson's correlations:\n",
    "    laps_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.laps_simple_pf_pearson_merged_df\n",
    "    ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bc1db",
   "metadata": {
    "tags": [
     "pho-run-2024",
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_session_name: str = curr_active_pipeline.session_name # '2006-6-08_14-26-15'\n",
    "CURR_BATCH_OUTPUT_PREFIX: str = f\"{BATCH_DATE_TO_USE}-{curr_session_name}\"\n",
    "print(f'CURR_BATCH_OUTPUT_PREFIX: {CURR_BATCH_OUTPUT_PREFIX}')\n",
    "\n",
    "# active_context = curr_active_pipeline.get_session_context().adding_context_if_missing(custom_\n",
    "\n",
    "# session_name: str = curr_active_pipeline.session_name\n",
    "\n",
    "active_context = curr_active_pipeline.get_session_context()\n",
    "session_name: str = f\"{curr_active_pipeline.session_name}{custom_suffix}\" ## appending this here is a hack, but it makes the correct filename\n",
    "active_context = active_context.adding_context_if_missing(suffix=custom_suffix)\n",
    "session_ctxt_key:str = active_context.get_description(separator='|', subset_includelist=(IdentifyingContext._get_session_context_keys() + ['suffix']))\n",
    "\n",
    "earliest_delta_aligned_t_start, t_delta, latest_delta_aligned_t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\n",
    "active_context\n",
    "session_ctxt_key\n",
    "# Shifts the absolute times to delta-relative values, as would be needed to draw on a 'delta_aligned_start_t' axis:\n",
    "delta_relative_t_start, delta_relative_t_delta, delta_relative_t_end = np.array([earliest_delta_aligned_t_start, t_delta, latest_delta_aligned_t_end]) - t_delta\n",
    "# decoder_user_selected_epoch_times_dict, any_good_selected_epoch_times = DecoderDecodedEpochsResult.load_user_selected_epoch_times(curr_active_pipeline)\n",
    "# any_good_selected_epoch_indicies = filtered_ripple_simple_pf_pearson_merged_df.epochs.matching_epoch_times_slice(any_good_selected_epoch_times)\n",
    "# df = filter_epochs_dfs_by_annotation_times(curr_active_pipeline, any_good_selected_epoch_times, ripple_decoding_time_bin_size=ripple_decoding_time_bin_size, filtered_ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df)\n",
    "# df\n",
    "\n",
    "# collected_outputs_path = self.collected_outputs_path.resolve()\n",
    "\n",
    "collected_outputs_path = collected_outputs_path.resolve()\n",
    "\n",
    "## Export CSVs:\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "_output_csv_paths = directional_decoders_epochs_decode_result.export_csvs(parent_output_path=collected_outputs_path, active_context=active_context, session_name=curr_session_name, curr_session_t_delta=t_delta,\n",
    "                                                                        # user_annotation_selections={'ripple': any_good_selected_epoch_times},\n",
    "                                                                        # valid_epochs_selections={'ripple': filtered_valid_epoch_times},\n",
    "                                                                        )\n",
    "\n",
    "print(f'\\t\\tsuccessfully exported directional_decoders_epochs_decode_result to {collected_outputs_path}!')\n",
    "_output_csv_paths_info_str: str = '\\n'.join([f'{a_name}: \"{file_uri_from_path(a_path)}\"' for a_name, a_path in _output_csv_paths.items()])\n",
    "# print(f'\\t\\t\\tCSV Paths: {_output_csv_paths}\\n')\n",
    "print(f'\\t\\t\\tCSV Paths: {_output_csv_paths_info_str}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fa269",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "session_name: str = curr_active_pipeline.session_name\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\n",
    "def _update_ripple_df(a_ripple_df):\n",
    "    \"\"\" captures: session_name, t_start, t_delta, t_end, ripple_decoding_time_bin_size \"\"\"\n",
    "    if ('time_bin_size' not in a_ripple_df.columns) and (ripple_decoding_time_bin_size is not None):\n",
    "        ## add the column\n",
    "        a_ripple_df['time_bin_size'] = ripple_decoding_time_bin_size\n",
    "    # Add the maze_id to the active_filter_epochs so we can see how properties change as a function of which track the replay event occured on:\n",
    "    a_ripple_df = DecoderDecodedEpochsResult.add_session_df_columns(a_ripple_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "    return a_ripple_df\n",
    "\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df = _update_ripple_df(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df)\n",
    "directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df = _update_ripple_df(directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df)\n",
    "    \n",
    "ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "## UPDATES: directional_decoders_epochs_decode_result\n",
    "## OUTPUTS: ripple_simple_pf_pearson_merged_df, ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72763c5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "ripple_simple_pf_pearson_merged_df\n",
    "ripple_weighted_corr_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe34dd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "# directional_decoders_epochs_decode_result.decoder_ripple_weighted_corr_df_dict # vector for each decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea151325",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Plot: directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "from pyphoplacecellanalysis.Pho2D.plotly.Extensions.plotly_helpers import plotly_pre_post_delta_scatter\n",
    "\n",
    "ripple_weighted_corr_merged_df = deepcopy(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df)\n",
    "ripple_weighted_corr_merged_df\n",
    "\n",
    "session_name: str = curr_active_pipeline.session_name\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c440a69",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# histogram_bins = 'auto'\n",
    "histogram_bins: int = 25\n",
    "\n",
    "# ripple_weighted_corr_merged_df = ripple_weighted_corr_merged_df[['P_Short','delta_aligned_start_t', 'time_bin_size']]\n",
    "ripple_weighted_corr_merged_df = ripple_weighted_corr_merged_df[['P_Short','delta_aligned_start_t', 'time_bin_size']]\n",
    "new_ripple_fig, new_ripple_fig_context = plotly_pre_post_delta_scatter(data_results_df=ripple_weighted_corr_merged_df, out_scatter_fig=None, histogram_bins=histogram_bins,\n",
    "                                                                        px_scatter_kwargs=dict(title='Ripple'), histogram_variable_name='P_Short')\n",
    "\n",
    "# new_laps_fig = new_laps_fig.update_layout(fig_size_kwargs, \n",
    "#     xaxis_title=\"X Axis Title\",\n",
    "#     yaxis_title=\"Y Axis Title\",\n",
    "#     legend_title=\"Legend Title\",\n",
    "#     font=dict(\n",
    "#         family=\"Courier New, monospace\",\n",
    "#         size=18,\n",
    "#         color=\"RebeccaPurple\"\n",
    "#     ),\n",
    "# )\n",
    "# Update x-axis labels\n",
    "# new_laps_fig.update_xaxes(title_text=\"Num Time Bins\", row=1, col=1)\n",
    "# new_laps_fig.update_xaxes(title_text=\"Delta-aligned Event Time (seconds)\", row=1, col=2)\n",
    "# new_laps_fig.update_xaxes(title_text=\"Num Time Bins\", row=1, col=3)\n",
    "\n",
    "\n",
    "_extras_output_dict = {}\n",
    "_extras_output_dict[\"y_mid_line\"] = new_ripple_fig.add_hline(y=0.5, line=dict(color=\"rgba(0.8,0.8,0.8,.75)\", width=2), row='all', col='all')\n",
    "\n",
    "new_ripple_fig\n",
    "\n",
    "\n",
    "\n",
    "# # Update layout to add a title to the legend\n",
    "# new_fig_ripples.update_layout(\n",
    "#     legend_title_text='Is User Selected'  # Add a title to the legend\n",
    "# )\n",
    "\n",
    "# fig_to_clipboard(new_fig_ripples, **fig_size_kwargs)\n",
    "\n",
    "# new_laps_fig_context: IdentifyingContext = new_laps_fig_context.adding_context_if_missing(epoch='withNewKamranExportedReplays', num_sessions=num_sessions, plot_type='scatter+hist', comparison='pre-post-delta', variable_name=variable_name)\n",
    "# figure_out_paths = save_plotly(a_fig=new_laps_fig, a_fig_context=new_laps_fig_context)\n",
    "# new_laps_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2236f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# curr_active_pipeline.__getstate__()\n",
    "curr_active_pipeline.sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c774c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# curr_active_pipeline.__getstate__()\n",
    "\n",
    "# _temp_pipeline_dict = get_dict_subset(curr_active_pipeline.__getstate__(), dummy_pipeline_attrs_names_list)\n",
    "_temp_pipeline_dict = get_dict_subset(curr_active_pipeline.stage.__getstate__(), dummy_pipeline_attrs_names_list) | {'sess': deepcopy(curr_active_pipeline.sess)}\n",
    "_temp_pipeline_dict\n",
    "\n",
    "print_keys_if_possible('curr_active_pipeline.stage.__getstate__()', _temp_pipeline_dict, max_depth=2)\n",
    "\n",
    "a_dummy_pipeline: SimpleCurrActivePipelineComputationDummy = SimpleCurrActivePipelineComputationDummy(**_temp_pipeline_dict)\n",
    "a_dummy_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39ec66",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "a_dummy_pipeline = SimpleCurrActivePipelineComputationDummy(**curr_active_pipeline.__getstate__())\n",
    "a_dummy_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689d97d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### 🖼️🎨 Plot laps via `PhoPaginatedMultiDecoderDecodedEpochsWindow`:\n",
    "TODO 💯❗ 2024-08-15 22:58: - [ ] PhoPaginatedMultiDecoderDecodedEpochsWindow renders the list of subplots on a page with the first being on the BOTTOM and then increasing up towards the top. This is very counter-intuitive and potentially explains issues with ordering and indexing of plots. 💯❗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e8dd9",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "from pyphoplacecellanalysis.Pho2D.stacked_epoch_slices import PhoPaginatedMultiDecoderDecodedEpochsWindow, DecodedEpochSlicesPaginatedFigureController, EpochSelectionsObject, ClickActionCallbacks\n",
    "\n",
    "laps_app, laps_paginated_multi_decoder_decoded_epochs_window, laps_pagination_controller_dict = PhoPaginatedMultiDecoderDecodedEpochsWindow.init_from_track_templates(curr_active_pipeline, track_templates,\n",
    "                            decoder_decoded_epochs_result_dict=decoder_laps_filter_epochs_decoder_result_dict, epochs_name='laps',\n",
    "                            # decoder_decoded_epochs_result_dict=decoder_ripple_filter_epochs_decoder_result_dict, epochs_name='ripple',\n",
    "                            included_epoch_indicies=None, \n",
    "    params_kwargs={'enable_per_epoch_action_buttons': False,\n",
    "    'skip_plotting_most_likely_positions': True, 'skip_plotting_measured_positions': False, \n",
    "    'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': False, 'enable_weighted_correlation_info': False,\n",
    "    # 'enable_decoded_most_likely_position_curve': False, 'enable_radon_transform_info': True, 'enable_weighted_correlation_info': True,\n",
    "    # 'disable_y_label': True,\n",
    "    # 'isPaginatorControlWidgetBackedMode': True,\n",
    "    # 'enable_update_window_title_on_page_change': False, 'build_internal_callbacks': True,\n",
    "    # 'debug_print': True,\n",
    "    # 'max_subplots_per_page': 10,\n",
    "    # 'scrollable_figure': False,\n",
    "    'max_subplots_per_page': 50,\n",
    "    'scrollable_figure': True,\n",
    "    # 'posterior_heatmap_imshow_kwargs': dict(vmin=0.0075),\n",
    "    'use_AnchoredCustomText': False,\n",
    "    # 'build_fn': 'insets_view',\n",
    "    })\n",
    "\n",
    "#TODO 💯❗ 2024-08-15 22:58: - [ ] PhoPaginatedMultiDecoderDecodedEpochsWindow renders the list of subplots on a page with the first being on the BOTTOM and then increasing up towards the top. This is very counter-intuitive and potentially explains issues with ordering and indexing of plots. 💯❗\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531682d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult, SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.computer_vision import ComputerVisionComputations\n",
    "from pyphocorehelpers.plotting.media_output_helpers import img_data_to_greyscale\n",
    "\n",
    "parent_output_folder = Path(r'K:/scratch/collected_outputs/figures/_temp_individual_posteriors').resolve()\n",
    "# parent_output_folder = Path(r\"E:\\Dropbox (Personal)\\Active\\Kamran Diba Lab\\Pho-Kamran-Meetings\\2024-08-20 - Finalizing Transition Matrix\\_temp_individual_posteriors\").resolve()\n",
    "posterior_out_folder = parent_output_folder.joinpath(DAY_DATE_TO_USE).resolve()\n",
    "posterior_out_folder.mkdir(parents=True, exist_ok=True)\n",
    "save_path = posterior_out_folder.resolve()\n",
    "_parent_save_context: IdentifyingContext = curr_active_pipeline.build_display_context_for_session('perform_export_all_decoded_posteriors_as_images')\n",
    "out_paths = ComputerVisionComputations.perform_export_all_decoded_posteriors_as_images(decoder_laps_filter_epochs_decoder_result_dict, decoder_ripple_filter_epochs_decoder_result_dict, _save_context=_parent_save_context, parent_output_folder=save_path, desired_height=None)\n",
    "# out_paths\n",
    "fullwidth_path_widget(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91449226",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "out_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3a479",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 2024-09-03 - Testing Output Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f794f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# Non-interactive:\n",
    "_non_interactive_mode_kwargs = dict(should_use_MatplotlibTimeSynchronizedWidget=False, scrollable_figure=False, defer_render=True)\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=False, backend='AGG')\n",
    "_curr_interaction_mode_kwargs = _non_interactive_mode_kwargs # non-interactive mode\n",
    "\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_epochs', curr_active_pipeline.get_session_context(),\n",
    "            max_num_lap_epochs = 240, max_num_ripple_epochs = 500,\n",
    "            render_directional_marginal_laps=True, render_directional_marginal_ripples=True, render_track_identity_marginal_laps=True, render_track_identity_marginal_ripples=True,\n",
    "            # render_directional_marginal_laps=True, render_directional_marginal_ripples=False, render_track_identity_marginal_laps=False, render_track_identity_marginal_ripples=False,\n",
    "            # constrained_layout=True, # layout='none',\n",
    "            # build_fn='basic_view', constrained_layout=True, \n",
    "            build_fn='insets_view', constrained_layout=None, layout='none', # , constrained_layout=False constrained_layout=None, layout='none', # , constrained_layout=None, layout='none' extrodinarily fast\n",
    "            **_curr_interaction_mode_kwargs, # interactive mode\n",
    "            skip_plotting_measured_positions=True, skip_plotting_most_likely_positions=True, save_figure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41422f07",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_full\n",
    "\n",
    "pf1d_compare_graphics, (example_epoch_rasters_L, example_epoch_rasters_S), example_stacked_epoch_graphics, fig_1c_figures_out_dict = PAPER_FIGURE_figure_1_full(curr_active_pipeline,\n",
    "                                                                                                        should_plot_example_rasters=False, should_plot_pf1d_compare=False,\n",
    "                                                                                                        ) # did not display the pf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c183a3d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# example_epoch_rasters_L.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4644e1",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### 💾🖼️ 2024-04-27 - Save Posteriors as Yellow-Blue plots to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04910321",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.data_exporting import PosteriorExporting\n",
    "from pyphocorehelpers.plotting.media_output_helpers import vertical_image_stack, horizontal_image_stack, image_grid\n",
    "\n",
    "# Open the images\n",
    "_raster_imgs = [Image.open(i) for i in _out_rasters_save_paths]\n",
    "# _out_vstack = vertical_image_stack(_raster_imgs, padding=5)\n",
    "# _out_vstack\n",
    "_out_hstack = horizontal_image_stack(_raster_imgs, padding=5)\n",
    "_out_hstack\n",
    "\n",
    "directional_merged_decoders_result.perform_compute_marginals()\n",
    "directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n",
    "\n",
    "# parent_array_as_image_output_folder = Path(r'E:\\Dropbox (Personal)\\Active\\Kamran Diba Lab\\Presentations\\2024-05-30 - Pho iNAV Poster\\Exports\\array_as_image').resolve()\n",
    "parent_array_as_image_output_folder = Path('output/Exports/array_as_image').resolve()\n",
    "parent_array_as_image_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "assert parent_array_as_image_output_folder.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d42335",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# clicked_epoch = np.array([169.95631618227344, 170.15983607806265])\n",
    "clicked_epoch = np.array([91.57839279191103, 91.857145929])\n",
    "clicked_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe39b46",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# np.shape(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.p_x_given_n_list[1])\n",
    "         \n",
    "# directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.marginal_x_list\n",
    "active_marginals_df: pd.DataFrame = deepcopy(directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df)\n",
    "# active_marginals_df.ripple_idx\n",
    "# directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result.marginal_x_list\n",
    "active_filter_epochs_decoder_result: DecodedFilterEpochsResult = deepcopy(directional_merged_decoders_result.all_directional_ripple_filter_epochs_decoder_result)\n",
    "active_filter_epochs_decoder_result.filter_epochs.epochs.find_data_indicies_from_epoch_times(np.atleast_1d(clicked_epoch[0]))\n",
    "\n",
    "# active_filter_epochs_decoder_result.all_directional_ripple_filter_epochs_decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e64e6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import save_marginals_arrays_as_image\n",
    "\n",
    "PosteriorExporting.save_marginals_arrays_as_image(directional_merged_decoders_result=directional_merged_decoders_result, parent_array_as_image_output_folder=parent_array_as_image_output_folder, epoch_id_identifier_str='ripple', epoch_ids=[31])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040f44c",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.DecoderPredictionError import plot_decoded_epoch_slices\n",
    "\n",
    "debug_print = False\n",
    "enable_flat_line_drawing = False\n",
    "# enable_flat_line_drawing = True\n",
    "debug_test_max_num_slices = 16\n",
    "\n",
    "decoding_time_bin_size = 0.05\n",
    "sess = deepcopy(global_session)\n",
    "\n",
    "enable_flat_line_drawing = True\n",
    "## Testing PBE Decoding\n",
    "# active_decoder = new_2D_decoder\n",
    "active_decoder = new_1D_decoder\n",
    "# filter_epochs = sess.laps.as_epoch_obj() # epoch object\n",
    "filter_epochs = sess.ripple # epoch object\n",
    "filter_epochs_decoder_result = active_decoder.decode_specific_epochs(sess.spikes_df, filter_epochs=filter_epochs, decoding_time_bin_size=decoding_time_bin_size, debug_print=False)\n",
    "\n",
    "params, plots_data, plots, ui = plot_decoded_epoch_slices(filter_epochs, filter_epochs_decoder_result, global_pos_df=sess.position.to_dataframe(), xbin=active_decoder.xbin, enable_flat_line_drawing=False, debug_test_max_num_slices=20, debug_print=False)\n",
    "\n",
    "\n",
    "# Laps Example:\n",
    "## Lap-Epochs Decoding:\n",
    "laps_copy = deepcopy(sess.laps)\n",
    "laps_filter_epochs = laps_copy.filtered_by_lap_flat_index(np.arange(6)).as_epoch_obj() # epoch object\n",
    "laps_filter_epochs_decoder_result = active_decoder.decode_specific_epochs(sess.spikes_df, filter_epochs=laps_filter_epochs, decoding_time_bin_size=decoding_time_bin_size, debug_print=False)\n",
    "laps_plot_tuple = plot_decoded_epoch_slices(laps_filter_epochs, laps_filter_epochs_decoder_result, global_pos_df=sess.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "                                                        enable_flat_line_drawing=enable_flat_line_drawing, debug_test_max_num_slices=debug_test_max_num_slices, name='stacked_epoch_slices_matplotlib_subplots_LAPS', debug_print=debug_print)\n",
    "                                                        \n",
    "\n",
    "# Ripples Example:                                                        \n",
    "params, plots_data, plots, ui = plot_decoded_epoch_slices(filter_epochs, filter_epochs_decoder_result, global_pos_df=sess.position.to_dataframe(), xbin=active_decoder.xbin,\n",
    "                                                        enable_flat_line_drawing=enable_flat_line_drawing, debug_test_max_num_slices=debug_test_max_num_slices, name='stacked_epoch_slices_matplotlib_subplots_RIPPLES', debug_print=debug_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf1be5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# 2024-09-16 - LxC and SxC\n",
    "- [ ] Unfortunately the manually selected LxCs/SxCs do not match those computed based on thresholds for firing rate differences, albiet with both laps and replays included. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74958a12",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import compute_spatial_information\n",
    "\n",
    "global_spikes_df: pd.DataFrame = deepcopy(curr_active_pipeline.filtered_sessions[global_epoch_name].spikes_df).drop(columns=['neuron_type'], inplace=False)\n",
    "an_active_pf = deepcopy(global_pf1D)\n",
    "spatial_information, all_spikes_df, epoch_averaged_activity_per_pos_bin, global_all_spikes_counts = compute_spatial_information(all_spikes_df=global_spikes_df, an_active_pf=an_active_pf, global_session_duration=global_session.duration)\n",
    "spatial_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_averaged_activity_per_pos_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526371d",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "epoch_averaged_activity_per_pos_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5a463",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## compute number of spikes in each epoch\n",
    "long_pf1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943db46e",
   "metadata": {
    "tags": [
     "all",
     "exclusivity"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.LongShortTrackComparingDisplayFunctions import add_spikes_df_placefield_inclusion_columns\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import compute_all_cells_long_short_firing_rate_df, determine_neuron_exclusivity_from_firing_rate\n",
    "\n",
    "df_combined = compute_all_cells_long_short_firing_rate_df(global_spikes_df=global_spikes_df)\n",
    "firing_rate_required_diff_Hz: float = 1.0 # minimum difference required for a cell to be considered Long- or Short-\"preferring\"\n",
    "maximum_opposite_period_firing_rate_Hz: float = 1.0 # maximum allowed firing rate in the opposite period to be considered exclusive\n",
    "(LpC_df, SpC_df, LxC_df, SxC_df), (LpC_aclus, SpC_aclus, LxC_aclus, SxC_aclus) = determine_neuron_exclusivity_from_firing_rate(df_combined=df_combined, firing_rate_required_diff_Hz=firing_rate_required_diff_Hz, \n",
    "                                                                                                                               maximum_opposite_period_firing_rate_Hz=maximum_opposite_period_firing_rate_Hz)\n",
    "\n",
    "## Extract the aclus\n",
    "print(f'LpC_aclus: {LpC_aclus}')\n",
    "print(f'SpC_aclus: {SpC_aclus}')\n",
    "\n",
    "print(f'LxC_aclus: {LxC_aclus}')\n",
    "print(f'SxC_aclus: {SxC_aclus}')\n",
    "\n",
    "## OUTPUTS: LpC_aclus, SpC_aclus, LxC_aclus, SxC_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e781fbf",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### User Hand-Selected LxCs and SxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06cbb4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager, SessionCellExclusivityRecord\n",
    "\n",
    "## Extract from manual user-annotations:\n",
    "session_cell_exclusivity_annotations: Dict[IdentifyingContext, SessionCellExclusivityRecord] = UserAnnotationsManager.get_hardcoded_specific_session_cell_exclusivity_annotations_dict()\n",
    "curr_session_cell_exclusivity_annotation: SessionCellExclusivityRecord = session_cell_exclusivity_annotations[curr_context] # SessionCellExclusivityRecord(LxC=[109], LpC=[], Others=[], SpC=[67, 52], SxC=[23, 4, 58])\n",
    "\n",
    "df_SxC = df_combined[np.isin(df_combined.index, curr_session_cell_exclusivity_annotation.SxC)]\n",
    "df_SpC = df_combined[np.isin(df_combined.index, curr_session_cell_exclusivity_annotation.SpC)]\n",
    "df_LxC = df_combined[np.isin(df_combined.index, curr_session_cell_exclusivity_annotation.LxC)]\n",
    "df_LpC = df_combined[np.isin(df_combined.index, curr_session_cell_exclusivity_annotation.LpC)]\n",
    "\n",
    "df_SxC\n",
    "df_SpC\n",
    "df_LxC\n",
    "df_LpC\n",
    "\n",
    "# [23,4,58]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2adf2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "### Find PBEs that include XxC cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664af32",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import co_filter_epochs_and_spikes\n",
    "\n",
    "# INPUTS: directional_decoders_epochs_decode_result: DecoderDecodedEpochsResult, filtered_decoder_filter_epochs_decoder_result_dict\n",
    "session_name: str = curr_active_pipeline.session_name\n",
    "t_start, t_delta, t_end = curr_active_pipeline.find_LongShortDelta_times()\n",
    "directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df = DecoderDecodedEpochsResult.add_session_df_columns(directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df, session_name=session_name, time_bin_size=None, t_start=t_start, curr_session_t_delta=t_delta, t_end=t_end, time_col='ripple_start_t')\n",
    "    \n",
    "ripple_weighted_corr_merged_df = directional_decoders_epochs_decode_result.ripple_weighted_corr_merged_df\n",
    "ripple_simple_pf_pearson_merged_df = directional_decoders_epochs_decode_result.ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "active_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "active_min_num_unique_aclu_inclusions_requirement: int = track_templates.min_num_unique_aclu_inclusions_requirement(curr_active_pipeline, required_min_percentage_of_active_cells=0.333333333)\n",
    "ripple_simple_pf_pearson_merged_df, active_spikes_df = co_filter_epochs_and_spikes(active_spikes_df=active_spikes_df, active_epochs_df=ripple_simple_pf_pearson_merged_df, included_aclus=track_templates.any_decoder_neuron_IDs, min_num_unique_aclu_inclusions=active_min_num_unique_aclu_inclusions_requirement, epoch_id_key_name='ripple_epoch_id', no_interval_fill_value=-1, add_unique_aclus_list_column=True, drop_non_epoch_spikes=True)\n",
    "ripple_simple_pf_pearson_merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b95b0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "## Count up the number of each XpC/XxC cell in each epoch. Updates `filtered_epochs_df`\n",
    "\n",
    "## INPUTS: LpC_aclus, SpC_aclus, LxC_aclus, SxC_aclus, filtered_epochs_ripple_simple_pf_pearson_merged_df\n",
    "\n",
    "filtered_epochs_df: pd.DataFrame = deepcopy(ripple_simple_pf_pearson_merged_df)\n",
    "\n",
    "# ADDS columns: ['n_LpC_aclus', 'n_SpC_aclus', 'n_LxC_aclus', 'n_SxC_aclus']\n",
    "\n",
    "filtered_epochs_df['n_LpC_aclus'] = 0\n",
    "filtered_epochs_df['n_SpC_aclus'] = 0\n",
    "filtered_epochs_df['n_LxC_aclus'] = 0\n",
    "filtered_epochs_df['n_SxC_aclus'] = 0\n",
    "for a_row in filtered_epochs_df.itertuples(index=True):\n",
    "    for an_aclu in list(a_row.unique_active_aclus):\n",
    "        if an_aclu in LpC_aclus:\n",
    "            filtered_epochs_df.loc[a_row.Index, 'n_LpC_aclus'] += 1\n",
    "        if an_aclu in SpC_aclus:\n",
    "            filtered_epochs_df.loc[a_row.Index, 'n_SpC_aclus'] += 1\n",
    "        if an_aclu in LxC_aclus:\n",
    "            filtered_epochs_df.loc[a_row.Index, 'n_LxC_aclus'] += 1\n",
    "        if an_aclu in SxC_aclus:\n",
    "            filtered_epochs_df.loc[a_row.Index, 'n_SxC_aclus'] += 1\n",
    "\n",
    "\n",
    "filtered_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107cec5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# filtered_epochs_df.plot.scatter(x='delta_aligned_start_t', y='n_LxC_aclus')\n",
    "# filtered_epochs_df.plot.scatter(x='delta_aligned_start_t', y='n_LpC_aclus')\n",
    "# filtered_epochs_df.plot.scatter(x='n_LpC_aclus', y='n_SpC_aclus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daed89ab",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# PhoJonathanPlotHelpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fa610",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.MultiContextComparingDisplayFunctions.LongShortTrackComparingDisplayFunctions import LongShortTrackComparingDisplayFunctions, PhoJonathanPlotHelpers\n",
    "\n",
    "curr_active_pipeline.reload_default_display_functions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35d5ad",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'\n",
    "\n",
    "graphics_output_dict = curr_active_pipeline.display('_display_batch_pho_jonathan_replay_firing_rate_comparison', active_identifying_session_ctx) # MatplotlibRenderPlots\n",
    "# graphics_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4fd56",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "debug_print = True\n",
    "## Get global 'jonathan_firing_rate_analysis' results:\n",
    "curr_jonathan_firing_rate_analysis = curr_active_pipeline.global_computation_results.computed_data['jonathan_firing_rate_analysis']\n",
    "neuron_replay_stats_df, rdf, aclu_to_idx, irdf = curr_jonathan_firing_rate_analysis.neuron_replay_stats_df, curr_jonathan_firing_rate_analysis.rdf.rdf, curr_jonathan_firing_rate_analysis.rdf.aclu_to_idx, curr_jonathan_firing_rate_analysis.irdf.irdf\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Batch Output of Figures                                                                                              #\n",
    "# ==================================================================================================================== #\n",
    "## 🗨️🟢 2022-11-05 - Pho-Jonathan Batch Outputs of Firing Rate Figures\n",
    "# %matplotlib qt\n",
    "short_only_df = neuron_replay_stats_df[neuron_replay_stats_df.track_membership == SplitPartitionMembership.RIGHT_ONLY]\n",
    "short_only_aclus = short_only_df.index.values.tolist()\n",
    "long_only_df = neuron_replay_stats_df[neuron_replay_stats_df.track_membership == SplitPartitionMembership.LEFT_ONLY]\n",
    "long_only_aclus = long_only_df.index.values.tolist()\n",
    "shared_df = neuron_replay_stats_df[neuron_replay_stats_df.track_membership == SplitPartitionMembership.SHARED]\n",
    "shared_aclus = shared_df.index.values.tolist()\n",
    "if debug_print:\n",
    "    print(f'shared_aclus: {shared_aclus}')\n",
    "    print(f'long_only_aclus: {long_only_aclus}')\n",
    "    print(f'short_only_aclus: {short_only_aclus}')\n",
    "\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'    \n",
    "## MODE: this mode creates a special folder to contain the outputs for this session.\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Output Figures to File                                                                                               #\n",
    "# ==================================================================================================================== #\n",
    "active_out_figures_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, n_max_page_rows=10, included_unit_neuron_IDs=[49], disable_top_row=True)\n",
    "\n",
    "# /home/halechr/repos/Spike3D/EXTERNAL/Screenshots/ProgrammaticDisplayFunctionTesting/2024-09-24/kdiba/vvp01/two/2006-4-17_12-52-15/BatchPhoJonathanReplayFRC_shared_4of4_(39,41,42).png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_only_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d518bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_print = True\n",
    "## Get global 'jonathan_firing_rate_analysis' results:\n",
    "curr_jonathan_firing_rate_analysis = curr_active_pipeline.global_computation_results.computed_data['jonathan_firing_rate_analysis']\n",
    "neuron_replay_stats_df, rdf, aclu_to_idx, irdf = curr_jonathan_firing_rate_analysis.neuron_replay_stats_df, curr_jonathan_firing_rate_analysis.rdf.rdf, curr_jonathan_firing_rate_analysis.rdf.aclu_to_idx, curr_jonathan_firing_rate_analysis.irdf.irdf\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Batch Output of Figures                                                                                              #\n",
    "# ==================================================================================================================== #\n",
    "## 🗨️🟢 2022-11-05 - Pho-Jonathan Batch Outputs of Firing Rate Figures\n",
    "# %matplotlib qt\n",
    "short_only_df = neuron_replay_stats_df[neuron_replay_stats_df.track_membership == SplitPartitionMembership.RIGHT_ONLY]\n",
    "short_only_aclus = short_only_df.index.values.tolist()\n",
    "long_only_df = neuron_replay_stats_df[neuron_replay_stats_df.track_membership == SplitPartitionMembership.LEFT_ONLY]\n",
    "long_only_aclus = long_only_df.index.values.tolist()\n",
    "shared_df = neuron_replay_stats_df[neuron_replay_stats_df.track_membership == SplitPartitionMembership.SHARED]\n",
    "shared_aclus = shared_df.index.values.tolist()\n",
    "if debug_print:\n",
    "    print(f'shared_aclus: {shared_aclus}')\n",
    "    print(f'long_only_aclus: {long_only_aclus}')\n",
    "    print(f'short_only_aclus: {short_only_aclus}')\n",
    "\n",
    "active_identifying_session_ctx = curr_active_pipeline.sess.get_context() # 'bapun_RatN_Day4_2019-10-15_11-30-06'    \n",
    "## MODE: this mode creates a special folder to contain the outputs for this session.\n",
    "\n",
    "active_out_figures_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, n_max_page_rows=10, included_unit_neuron_IDs=[49], disable_top_row=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bc009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113cbbf",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "jonathan_firing_rate_analysis_result = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.2)\n",
    "## all cells:\n",
    "# fig_1c_figures_all_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, included_unit_neuron_IDs=None, n_max_page_rows=20, write_vector_format=False, write_png=True, show_only_refined_cells=False, disable_top_row=False)\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "fig_1c_figures_all_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, included_unit_neuron_IDs=any_decoder_neuron_IDs, n_max_page_rows=20, write_vector_format=False, write_png=True, show_only_refined_cells=False, disable_top_row=False)\n",
    "# fig_1c_figures_all_dict\n",
    "\n",
    "## find the output figures from the `curr_active_pipeline.registered_output_files`\n",
    "_found_contexts_dict: Dict[IdentifyingContext, Path] = {}\n",
    "for a_figure_path, an_output_dict in curr_active_pipeline.registered_output_files.items():\n",
    "    a_ctxt = an_output_dict['context']\n",
    "    _found_contexts_dict[a_ctxt] = a_figure_path\n",
    "\n",
    "\n",
    "relevant_figures_dict: Dict[IdentifyingContext, Path] = IdentifyingContext.matching(_found_contexts_dict, criteria={'display_fn_name': 'BatchPhoJonathanReplayFRC'})\n",
    "relevant_figures_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b3270",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "fig_1c_figures_all_dict\n",
    "\n",
    "# print_keys_if_possible('registered_output_files', curr_active_pipeline.registered_output_files, max_depth=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90f781",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "{k:active_out_figures_dict[k] for k in relevant_figures_dict.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ba646",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import BatchPhoJonathanFiguresHelper\n",
    "\n",
    "# PhoJonathan Results:\n",
    "long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "long_results, short_results, global_results = [curr_active_pipeline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "jonathan_firing_rate_analysis_result = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.2)\n",
    "## all cells:\n",
    "fig_1c_figures_all_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, included_unit_neuron_IDs=LpC_aclus, n_max_page_rows=20, write_vector_format=False, write_png=False, show_only_refined_cells=False, disable_top_row=False, split_by_short_long_shared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab2057",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# global_spikes_df\n",
    "global_results.sess.spikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b05b88",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# Recover Session Parameters ( from `preprocessing_parameters`, default function kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.session.Formats.BaseDataSessionFormats import ParametersContainer\n",
    "from neuropy.core.session.Formats.SessionSpecifications import SessionConfig\n",
    "from neuropy.utils.indexing_helpers import flatten_dict\n",
    "from pyphocorehelpers.DataStructure.dynamic_parameters import DynamicParameters\n",
    "from pyphoplacecellanalysis.General.PipelineParameterClassTemplating import GlobalComputationParametersAttrsClassTemplating\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters, merged_directional_placefields_Parameters, rank_order_shuffle_analysis_Parameters, directional_decoders_decode_continuous_Parameters, directional_decoders_evaluate_epochs_Parameters, directional_train_test_split_Parameters, long_short_decoding_analyses_Parameters, long_short_rate_remapping_Parameters, long_short_inst_spike_rate_groups_Parameters, wcorr_shuffle_analysis_Parameters, perform_specific_epochs_decoding_Parameters, DEP_ratemap_peaks_Parameters, ratemap_peaks_prominence2d_Parameters\n",
    "import tables as tb\n",
    "import h5py\n",
    "\n",
    "# curr_active_pipeline.active_sess_config # 'preprocessing_parameters'\n",
    "\n",
    "# BEGIN FUNCTION BODY ________________________________________________________________________________________________ #\n",
    "\n",
    "# preprocessing_parameters: ParametersContainer = deepcopy(curr_active_pipeline.active_sess_config)\n",
    "\n",
    "\n",
    "a_sess_config: SessionConfig = deepcopy(curr_active_pipeline.active_sess_config)\n",
    "preprocessing_parameters: ParametersContainer = deepcopy(a_sess_config.preprocessing_parameters)\n",
    "preprocessing_parameters\n",
    "\n",
    "# preprocessing_parameters\n",
    "# preprocessing_parameters_dict = {'epoch_estimation_parameters': preprocessing_parameters.epoch_estimation_parameters.to_dict(),\n",
    "# }\n",
    "preprocessing_parameters_dict = preprocessing_parameters.to_dict()\n",
    "# preprocessing_parameters_dict\n",
    "\n",
    "## save dict to HDF\n",
    "# preprocessing_parameters_dict\n",
    "\n",
    "\n",
    "## Version with nested keys expressed as '/' separated flat strings\n",
    "preprocessing_parameters_dict = flatten_dict(preprocessing_parameters.to_dict())\n",
    "# preprocessing_parameters_dict\n",
    "# _defn_lines, _flat_fields_tuples_list, _base_variable_name_only_values_dict, _base_variable_name_only_types_dict = GlobalComputationParametersAttrsClassTemplating._build_kwargs_class_defns(flat_computation_function_default_kwargs_values_dict=flat_computation_function_default_kwargs_values_dict)\n",
    "# print(_defn_lines)\n",
    "# _flat_fields_tuples_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_parameters.epoch_estimation_parameters.laps\n",
    "\n",
    "# {k:v for k, v in flatten_dict(preprocessing_parameters.epoch_estimation_parameters.to_dict()).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_str, nested_classes_dict, imports_dict = GlobalComputationParametersAttrsClassTemplating._subfn_build_attrs_parameters_classes(registered_merged_computation_function_default_kwargs_dict=preprocessing_parameters_dict, \n",
    "                                                                                                        params_defn_save_path=None, should_build_hdf_class=True, print_defns=True)\n",
    "imports_list = list(imports_dict.keys())\n",
    "imports_string: str = 'import ' + ', '.join(imports_list)\n",
    "print(imports_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebeec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.parameters import BaseConfig\n",
    "\n",
    "def override_convert_dict_to_hdf_attrs_fn(f, key, value):\n",
    "    \"\"\"Converts a dictionary into HDF5 datasets under the given key in the HDF5 file or group `f`.\"\"\"\n",
    "    group = f.require_group(key)\n",
    "    for sub_k, sub_v in value.items():\n",
    "        if isinstance(sub_v, str):\n",
    "            # Handle strings explicitly\n",
    "            dt = h5py.string_dtype(encoding='utf-8')\n",
    "            group.create_dataset(sub_k, data=sub_v, dtype=dt)\n",
    "        elif isinstance(sub_v, np.ndarray) and sub_v.dtype.kind in {'U', 'S'}:\n",
    "            # Convert Unicode arrays to ASCII if possible\n",
    "            sub_v_bytes = sub_v.astype('S')\n",
    "            group.create_dataset(sub_k, data=sub_v_bytes)\n",
    "        else:\n",
    "            group[sub_k] = sub_v\n",
    "            \n",
    "# override_convert_dict_to_hdf_attrs_fn = HDF_Converter._convert_dict_to_hdf_attrs_fn\n",
    "\n",
    "\n",
    "@define(slots=False, eq=False, repr=False)\n",
    "class epoch_estimation_parameters_Parameters(HDF_SerializationMixin, AttrsBasedClassHelperMixin, BaseConfig):\n",
    "    \"\"\" Docstring for epoch_estimation_parameters_Parameters. \n",
    "    \"\"\"\n",
    "    # laps: DynamicContainer = serialized_field(default=DynamicContainer.init_from_dict({'N': 20, 'should_backup_extant_laps_obj': True, 'use_direction_dependent_laps': True}), serialization_fn=(lambda f, k, v: HDF_Converter._convert_dict_to_hdf_attrs_fn(f, k, v.to_dict())))\n",
    "    # PBEs: DynamicContainer = serialized_field(default=DynamicContainer.init_from_dict({'thresh': (0, 1.5), 'min_dur': 0.03, 'merge_dur': 0.1, 'max_dur': 0.6}), serialization_fn=(lambda f, k, v: HDF_Converter._convert_dict_to_hdf_attrs_fn(f, k, v.to_dict())))\n",
    "    # replays: DynamicContainer = serialized_field(default=DynamicContainer.init_from_dict({\n",
    "    #     'require_intersecting_epoch':  np.array([[-np.inf, 3.05477], [4.72322, 6.35677], [8.92686, 45.196], [50.87, 65.785], [72.223, 84.3034], [91.0442, 106.426], [112.965, 244.13], [250.671, 260.414], [281.269, 281.302], [290.778, 308.896], [313.935, 357.845], [365.118, 370.624], [375.996, 399.654], [406.36, 415.704], [422.177, 428.285], [435.323, 441.164], [445.835, 462.884], [470.192, 480.803], [489.746, 491.313], [496.985, 503.559], [509.798, 514.037], [520.176, 528.45], [533.022, 536.058], [541.798, 546.403], [551.775, 555.412], [563.486, 565.554], [570.76, 574.063], [580.235, 584.542], [591.581, 644.3], [652.009, 684.407], [688.613, 841.1], [847.706, 855.014], [858.417, 872.731], [878.203, 890.015], [895.488, 905.498], [910.802, 918.243], [922.282, 922.314], [930.589, 936.163], [940.765, 944.403], [949.976, 957.918], [962.622, 964.591], [972.998, 982.841], [992.619, 995.821], [1001.56, 1014.04], [1018.24, 1039.93], [1047.01, 1049.14], [1054.25, 1064.03], [1068.2, 1070.1], [1074.94, 1087.55], [1091.32, 1095.52], [1100.69, 1108.04], [1115.64, 1120.98], [1124.42, 1151.44], [1157.08, 1172.9], [1177.44, 1182.98], [1186.65, 1201.46], [1204.86, 1334.76], [1339.94, 1354.25], [1358.02, 1380.84], [1385.25, 1391.02], [1393.99, 1395.92], [1400.33, 1402.37], [1410.74, 1472.74], [1476.84, 1478.24], [1482.68, 1487.89], [1492.96, 1493.02], [1498.76, 1500.5], [1504.47, 1509.34], [1512.48, 1513.54], [1520.75, 1522.62], [1526.56, 1528.96], [1532.8, 1535.26], [1539.5, 1540.5], [1546.71, 1557.66], [1561.23, 1561.26], [1567.33, 1579.44], [1583.55, 1584.05], [1592.99, 1593.02], [1598.46, 1604.67], [1607.8, 1608.84], [1614.15, 1620.12], [1625.62, 1628.79], [1632.6, 1645.81], [1652.72, 1652.75], [1658.09, np.inf]]),\n",
    "    #     'min_epoch_included_duration': 0.06, 'max_epoch_included_duration': 0.6, 'maximum_speed_thresh': None, 'min_inclusion_fr_active_thresh': 1.0, 'min_num_unique_aclu_inclusions': 5\n",
    "    #     }), serialization_fn=(lambda f, k, v: HDF_Converter._convert_dict_to_hdf_attrs_fn(f, k, v.to_dict())))\n",
    "    \n",
    "    laps: Dict = serialized_field(default={'N': 20, 'should_backup_extant_laps_obj': True, 'use_direction_dependent_laps': True}, serialization_fn=(lambda f, k, v: override_convert_dict_to_hdf_attrs_fn(f, k, v)))\n",
    "    PBEs: Dict = serialized_field(default={'thresh': (0, 1.5), 'min_dur': 0.03, 'merge_dur': 0.1, 'max_dur': 0.6}, serialization_fn=(lambda f, k, v: override_convert_dict_to_hdf_attrs_fn(f, k, v)))\n",
    "    replays: Dict = serialized_field(default={\n",
    "        'require_intersecting_epoch':  np.array([[-np.inf, 3.05477], [4.72322, 6.35677], [8.92686, 45.196], [50.87, 65.785], [72.223, 84.3034], [91.0442, 106.426], [112.965, 244.13], [250.671, 260.414], [281.269, 281.302], [290.778, 308.896], [313.935, 357.845], [365.118, 370.624], [375.996, 399.654], [406.36, 415.704], [422.177, 428.285], [435.323, 441.164], [445.835, 462.884], [470.192, 480.803], [489.746, 491.313], [496.985, 503.559], [509.798, 514.037], [520.176, 528.45], [533.022, 536.058], [541.798, 546.403], [551.775, 555.412], [563.486, 565.554], [570.76, 574.063], [580.235, 584.542], [591.581, 644.3], [652.009, 684.407], [688.613, 841.1], [847.706, 855.014], [858.417, 872.731], [878.203, 890.015], [895.488, 905.498], [910.802, 918.243], [922.282, 922.314], [930.589, 936.163], [940.765, 944.403], [949.976, 957.918], [962.622, 964.591], [972.998, 982.841], [992.619, 995.821], [1001.56, 1014.04], [1018.24, 1039.93], [1047.01, 1049.14], [1054.25, 1064.03], [1068.2, 1070.1], [1074.94, 1087.55], [1091.32, 1095.52], [1100.69, 1108.04], [1115.64, 1120.98], [1124.42, 1151.44], [1157.08, 1172.9], [1177.44, 1182.98], [1186.65, 1201.46], [1204.86, 1334.76], [1339.94, 1354.25], [1358.02, 1380.84], [1385.25, 1391.02], [1393.99, 1395.92], [1400.33, 1402.37], [1410.74, 1472.74], [1476.84, 1478.24], [1482.68, 1487.89], [1492.96, 1493.02], [1498.76, 1500.5], [1504.47, 1509.34], [1512.48, 1513.54], [1520.75, 1522.62], [1526.56, 1528.96], [1532.8, 1535.26], [1539.5, 1540.5], [1546.71, 1557.66], [1561.23, 1561.26], [1567.33, 1579.44], [1583.55, 1584.05], [1592.99, 1593.02], [1598.46, 1604.67], [1607.8, 1608.84], [1614.15, 1620.12], [1625.62, 1628.79], [1632.6, 1645.81], [1652.72, 1652.75], [1658.09, np.inf]]),\n",
    "        'min_epoch_included_duration': 0.06, 'max_epoch_included_duration': 0.6, 'maximum_speed_thresh': None, 'min_inclusion_fr_active_thresh': 1.0, 'min_num_unique_aclu_inclusions': 5\n",
    "        }, serialization_fn=(lambda f, k, v: override_convert_dict_to_hdf_attrs_fn(f, k, v)))\n",
    "    \n",
    "    # HDFMixin Conformances ______________________________________________________________________________________________ #\n",
    "    def to_hdf(self, file_path, key: str, **kwargs):\n",
    "        \"\"\" Saves the object to key in the hdf5 file specified by file_path\"\"\"\n",
    "        # super().to_hdf(file_path, key=key, **kwargs)\n",
    "        with h5py.File(file_path, 'a') as f:\n",
    "            group = f.require_group(key)\n",
    "            # Serialize each field\n",
    "            for field_name, field_value in self.__dict__.items():\n",
    "                if hasattr(field_value, 'to_hdf'):\n",
    "                    # If the field has its own serialization method\n",
    "                    field_value.to_hdf(group, field_name, **kwargs)\n",
    "                else:\n",
    "                    # Use the serialization function specified\n",
    "                    # serialization_fn = getattr(self.__class__, field_name).metadata['serialization_fn']\n",
    "                    # serialization_fn = getattr(self.__class__, field_name).metadata['serialization_fn']\n",
    "                    override_convert_dict_to_hdf_attrs_fn(f, f\"{group}/{field_name}\", field_value)\n",
    "\n",
    "                    # serialization_fn(group, field_name, field_value)        \n",
    "\n",
    "\n",
    "\n",
    "# import epoch_estimation_parameters\n",
    "\n",
    "a_epoch_estimation_parameters_Parameters_obj: epoch_estimation_parameters_Parameters = epoch_estimation_parameters_Parameters(**{k:v.to_dict() for k, v in preprocessing_parameters.epoch_estimation_parameters.to_dict().items()}) # raw dict fields\n",
    "# a_epoch_estimation_parameters_Parameters_obj = epoch_estimation_parameters_Parameters(**{k:v for k, v in preprocessing_parameters.epoch_estimation_parameters.to_dict().items()}) # DynamicContainer fields\n",
    "a_epoch_estimation_parameters_Parameters_obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ae51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a_epoch_estimation_parameters_Parameters_obj.get_serialized_attribute_fields('hdf')\n",
    "\n",
    "# params = epoch_estimation_parameters_Parameters()\n",
    "a_epoch_estimation_parameters_Parameters_obj.to_hdf('parameters.hdf5', key='epoch_estimation_parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "import h5py\n",
    "\n",
    "# test_out_hdf5_file_path = Path('data/test_pipeline_params_ComputationKWargParameters.h5').resolve()\n",
    "test_out_hdf5_file_path = Path('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/data/').resolve()\n",
    "Assert.path_exists(test_out_hdf5_file_path)\n",
    "assert test_out_hdf5_file_path.is_dir()\n",
    "test_hdf5_file_name: str = 'test_pipeline_params_AllComputationKWargParameters.h5'\n",
    "test_out_hdf5_file_path = test_out_hdf5_file_path.joinpath(test_hdf5_file_name).resolve()\n",
    "test_out_hdf5_file_path\n",
    "\n",
    "with tb.open_file(test_out_hdf5_file_path, mode='w') as f:\n",
    "    a_global_computations_group = f.create_group('/', 'test', title='the result of computations that operate over many or all of the filters in the session.', createparents=True)\n",
    "    \n",
    "# ratemap_peaks_prominence2d_Parameters.to_hdf(test_out_hdf5_file_path, key='/test/ratemap_peaks_prominence2d_Parameters')\n",
    "test_out_hdf5_file_path\n",
    "# curr_global_param_typed_parameters.merged_directional_placefields.to_hdf(test_out_hdf5_file_path, key='/test/merged_directional_placefields')\n",
    "# curr_global_param_typed_parameters.rank_order_shuffle_analysis.to_hdf(test_out_hdf5_file_path, key='/test/rank_order_shuffle_analysis')\n",
    "# curr_global_param_typed_parameters.ratemap_peaks_prominence2d.to_hdf(test_out_hdf5_file_path, key='/test/ratemap_peaks_prominence2d_Parameters')\n",
    "# curr_global_param_typed_parameters.long_short_decoding_analyses.to_hdf(test_out_hdf5_file_path, key='/test/long_short_decoding_analyses')\n",
    "# curr_global_param_typed_parameters.perform_specific_epochs_decoding.to_hdf(test_out_hdf5_file_path, key='/test/perform_specific_epochs_decoding')\n",
    "# curr_global_param_typed_parameters.directional_train_test_split.to_hdf(test_out_hdf5_file_path, key='/test/directional_train_test_split')\n",
    "\n",
    "# preprocessing_parameters.to_hdf(test_out_hdf5_file_path, key=f'/test/preprocessing_parameters') # , enable_hdf_testing_mode=True\n",
    "\n",
    "a_epoch_estimation_parameters_Parameters_obj.to_hdf(test_out_hdf5_file_path, key=f'/test/preprocessing_parameters') # , OVERRIDE_ALLOW_GLOBAL_NESTED_EXPANSION=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c1453",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.PipelineParameterClassTemplating import GlobalComputationParametersAttrsClassTemplating\n",
    "\n",
    "_master_params_dict = {}\n",
    "\n",
    "preprocessing_parameters: ParametersContainer = deepcopy(curr_active_pipeline.active_sess_config)\n",
    "_master_params_dict['preprocessing'] = preprocessing_parameters.to_dict()\n",
    "\n",
    "# _master_params_dict\n",
    "# {'merged_directional_placefields': {'laps_decoding_time_bin_size': 0.25, 'ripple_decoding_time_bin_size': 0.025, 'should_validate_lap_decoding_performance': False},\n",
    "#  'rank_order_shuffle_analysis': {'num_shuffles': 500, 'minimum_inclusion_fr_Hz': 5.0, 'included_qclu_values': [1, 2], 'skip_laps': False},\n",
    "#  'directional_decoders_decode_continuous': {'time_bin_size': None},\n",
    "#  'directional_decoders_evaluate_epochs': {'should_skip_radon_transform': False},\n",
    "#  'directional_train_test_split': {'training_data_portion': 0.8333333333333334, 'debug_output_hdf5_file_path': None},\n",
    "#  'long_short_decoding_analyses': {'decoding_time_bin_size': None, 'perform_cache_load': False, 'always_recompute_replays': False, 'override_long_epoch_name': None, 'override_short_epoch_name': None},\n",
    "#  'long_short_rate_remapping': {'decoding_time_bin_size': None, 'perform_cache_load': False, 'always_recompute_replays': False},\n",
    "#  'long_short_inst_spike_rate_groups': {'instantaneous_time_bin_size_seconds': 0.01},\n",
    "#  'wcorr_shuffle_analysis': {'num_shuffles': 1024, 'drop_previous_result_and_compute_fresh': False},\n",
    "#  '_perform_specific_epochs_decoding': {'decoder_ndim': 2, 'filter_epochs': 'ripple', 'decoding_time_bin_size': 0.02},\n",
    "#  '_DEP_ratemap_peaks': {'peak_score_inclusion_percent_threshold': 0.25},\n",
    "#  'ratemap_peaks_prominence2d': {'step': 0.01, 'peak_height_multiplier_probe_levels': (0.5, 0.9), 'minimum_included_peak_height': 0.2, 'uniform_blur_size': 3, 'gaussian_blur_sigma': 3}}\n",
    "\n",
    "if curr_active_pipeline.global_computation_results.computation_config is not None:\n",
    "    curr_global_param_typed_parameters: ComputationKWargParameters = deepcopy(curr_active_pipeline.global_computation_results.computation_config)\n",
    "    _master_params_dict.update(curr_global_param_typed_parameters.to_dict())\n",
    "    ## TODO: are we sure we have all the parameters just from a global config? do we need to capture the default kwarg values that haven't been assigned or something?\n",
    "else:\n",
    "    print(f'WARNING: no global config so using kwarg defaults...')\n",
    "    ## only the default kwarg values:\n",
    "    registered_merged_computation_function_default_kwargs_dict, code_str, nested_classes_dict, (imports_dict, imports_list, imports_string) = GlobalComputationParametersAttrsClassTemplating.main_generate_params_classes(curr_active_pipeline=curr_active_pipeline)\n",
    "    # registered_merged_computation_function_default_kwargs_dict\n",
    "    _master_params_dict.update(registered_merged_computation_function_default_kwargs_dict)\n",
    "\n",
    "_master_params_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benedict import benedict\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import BaseGlobalComputationParameters, ComputationKWargParameters\n",
    "\n",
    "all_params_dict = curr_active_pipeline.get_all_parameters()\n",
    "type(all_params_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming 'params' is an instance of ComputationKWargParameters\n",
    "params = ComputationKWargParameters.init_from_pipeline(curr_active_pipeline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd02e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params_dict = benedict(all_params_dict)\n",
    "all_params_dict.keypaths()\n",
    "all_params_dict['rank_order_shuffle_analysis.included_qclu_values']\n",
    "all_params_dict['rank_order_shuffle_analysis.minimum_inclusion_fr_Hz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c936f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params_dict = deepcopy(all_params_dict)\n",
    "new_params_dict['rank_order_shuffle_analysis.included_qclu_values'] = [1, 2]\n",
    "new_params_dict['rank_order_shuffle_analysis.minimum_inclusion_fr_Hz'] = 5.0\n",
    "new_params_dict\n",
    "\n",
    "new_params_dict['rank_order_shuffle_analysis.included_qclu_values']\n",
    "new_params_dict['rank_order_shuffle_analysis.minimum_inclusion_fr_Hz']\n",
    "# all_params_dict.keypaths() # preprocessing.basepath\n",
    "\n",
    "# all_params_dict.rank_order_shuffle_analysis.minimum_inclusion_fr_Hz #  find('minimum_inclusion_fr_Hz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6daedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override_parameters_dict = {'rank_order_shuffle_analysis': {'minimum_inclusion_fr_Hz':2.0, 'included_qclu_values':[1,2]},\n",
    "# }\n",
    "\n",
    "\n",
    "params: ComputationKWargParameters = curr_active_pipeline.global_computation_results.computation_config\n",
    "params\n",
    "\n",
    "# Get a value using keypath\n",
    "value = params.get_by_keypath('directional_train_test_split.training_data_portion')\n",
    "print(value)  # Output: 0.8333333333333334\n",
    "\n",
    "# Set a value using keypath\n",
    "params.set_by_keypath('directional_train_test_split.training_data_portion', 0.9)\n",
    "\n",
    "# Verify the change\n",
    "new_value = params.get_by_keypath('directional_train_test_split.training_data_portion')\n",
    "print(new_value)  # Output: 0.9\n",
    "\n",
    "# Get all keypaths\n",
    "all_keypaths = params.keypaths()\n",
    "print(all_keypaths)\n",
    "# Output:\n",
    "# ['merged_directional_placefields',\n",
    "#  'merged_directional_placefields.laps_decoding_time_bin_size',\n",
    "#  'merged_directional_placefields.ripple_decoding_time_bin_size',\n",
    "#  ... (rest of the keypaths)]\n",
    "\n",
    "\n",
    "# Set a value using keypath\n",
    "params.set_by_keypath('directional_train_test_split.training_data_portion', 0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ed05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# curr_active_pipeline.global_computation_results.computation_config.update(\n",
    "\n",
    "\n",
    "override_parameters_flat_keypaths_dict = {'rank_order_shuffle_analysis.included_qclu_values': [1, 2], 'rank_order_shuffle_analysis.minimum_inclusion_fr_Hz': 5.0,}\n",
    "\n",
    "for k, v in override_parameters_flat_keypaths_dict.items():\n",
    "    # Set a value using keypath\n",
    "    curr_active_pipeline.global_computation_results.computation_config.set_by_keypath('directional_train_test_split.training_data_portion', v)\n",
    "\n",
    "\n",
    "curr_active_pipeline.update_parameters(override_parameters_flat_keypaths_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline.perform_specific_computation(computation_functions_name_includelist=['rank_order_shuffle_analysis'], computation_kwargs_list=[{'num_shuffles': 5, 'skip_laps': False, 'minimum_inclusion_fr_Hz':2.0, 'included_qclu_values':[1,2,4,5,6,7]}], \n",
    "#                                                   enabled_filter_names=None, fail_on_exception=True, debug_print=False)\n",
    "\n",
    "curr_session_context: IdentifyingContext = deepcopy(curr_active_pipeline.get_session_context())\n",
    "\n",
    "replay_suffix = '_withNormalComputedReplays'\n",
    "minimum_inclusion_fr_Hz = 5.0\n",
    "included_qclu_values = [1, 2]\n",
    "\n",
    "custom_context: IdentifyingContext = IdentifyingContext(replay_suffix=replay_suffix, minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values)\n",
    "curr_session_context = curr_session_context.overwriting_context(**custom_context.to_dict()) # session loader won't yet use these context fields though\n",
    "# Context(format_name= 'kdiba', animal= 'gor01', exper_name= 'one', session_name= '2006-6-12_15-55-31', included_qclu_values= [1, 2], minimum_inclusion_fr_Hz= 5.0)\n",
    "curr_session_context\n",
    "\n",
    "# can pass active_pickle_filename='loadedSessPickle.pkl' to batch_load_session\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import _get_custom_filenames_from_computation_metadata\n",
    "\n",
    "custom_save_filepaths, custom_save_filenames, custom_suffix = _get_custom_filenames_from_computation_metadata(replay_suffix=replay_suffix, minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, included_qclu_values=included_qclu_values)\n",
    "print(f'custom_save_filenames: {custom_save_filenames}')\n",
    "print(f'custom_suffix: \"{custom_suffix}\"')\n",
    "active_pickle_filename: str = custom_save_filenames['pipeline_pkl'] # 'loadedSessPickle_withNormalComputedReplays-frateThresh_5.0-qclu_[1, 2].pkl'\n",
    "print(f'active_pickle_filename: \"{active_pickle_filename}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdf9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "import h5py\n",
    "\n",
    "curr_global_param_typed_parameters: ComputationKWargParameters = curr_active_pipeline.global_computation_results.computation_config\n",
    "ratemap_peaks_prominence2d_Parameters = curr_global_param_typed_parameters.ratemap_peaks_prominence2d\n",
    "ratemap_peaks_prominence2d_Parameters\n",
    "\n",
    "# ratemap_peaks_prominence2d_Parameters.to_hdf(test_out_hdf5_file_path, key='/test/ratemap_peaks_prominence2d_Parameters')\n",
    "\n",
    "# test_out_hdf5_file_path = Path('data/test_pipeline_params_ComputationKWargParameters.h5').resolve()\n",
    "test_out_hdf5_file_path = Path('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/data/').resolve()\n",
    "Assert.path_exists(test_out_hdf5_file_path)\n",
    "assert test_out_hdf5_file_path.is_dir()\n",
    "test_hdf5_file_name: str = 'test_pipeline_params_AllComputationKWargParameters.h5'\n",
    "test_out_hdf5_file_path = test_out_hdf5_file_path.joinpath(test_hdf5_file_name).resolve()\n",
    "test_out_hdf5_file_path\n",
    "\n",
    "with tb.open_file(test_out_hdf5_file_path, mode='w') as f:\n",
    "    a_global_computations_group = f.create_group('/', 'test', title='the result of computations that operate over many or all of the filters in the session.', createparents=True)\n",
    "    \n",
    "# ratemap_peaks_prominence2d_Parameters.to_hdf(test_out_hdf5_file_path, key='/test/ratemap_peaks_prominence2d_Parameters')\n",
    "test_out_hdf5_file_path\n",
    "# curr_global_param_typed_parameters.merged_directional_placefields.to_hdf(test_out_hdf5_file_path, key='/test/merged_directional_placefields')\n",
    "# curr_global_param_typed_parameters.rank_order_shuffle_analysis.to_hdf(test_out_hdf5_file_path, key='/test/rank_order_shuffle_analysis')\n",
    "# curr_global_param_typed_parameters.ratemap_peaks_prominence2d.to_hdf(test_out_hdf5_file_path, key='/test/ratemap_peaks_prominence2d_Parameters')\n",
    "# curr_global_param_typed_parameters.long_short_decoding_analyses.to_hdf(test_out_hdf5_file_path, key='/test/long_short_decoding_analyses')\n",
    "# curr_global_param_typed_parameters.perform_specific_epochs_decoding.to_hdf(test_out_hdf5_file_path, key='/test/perform_specific_epochs_decoding')\n",
    "# curr_global_param_typed_parameters.directional_train_test_split.to_hdf(test_out_hdf5_file_path, key='/test/directional_train_test_split')\n",
    "\n",
    "curr_global_param_typed_parameters.to_hdf(test_out_hdf5_file_path, key=f'/test') # , enable_hdf_testing_mode=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3425100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# curr_global_param_typed_parameters.to_hdf(file_path=test_out_hdf5_file_path, key='global_param_typed_parameters')\n",
    "\n",
    "# filter_context_key:str = \"/\" + an_epoch_context.get_description(separator=\"/\", include_property_names=False) # '/kdiba/gor01/one/2006-6-08_14-26-15/maze1'\n",
    "# print(f'\\tfilter_context_key: {filter_context_key}')\n",
    "with tb.open_file(test_out_hdf5_file_path, mode='w') as f:\n",
    "    a_filter_group = f.create_group('test', 'root', title='the result of a filter function applied to the session.', createparents=True)\n",
    "    \n",
    "\n",
    "# curr_active_pipeline.global_computation_results.to_hdf('output/test_pipeline_params_ComputationKWargParameters.h5', f'test/test_pipeline_params_ComputationKWargParameters')\n",
    "curr_global_param_typed_parameters.to_hdf(test_out_hdf5_file_path, f'/test/test_pipeline_params_ComputationKWargParameters')\n",
    "curr_global_param_typed_parameters.to_hdf(test_out_hdf5_file_path, f'/test/global_param_typed_parameters')\n",
    "\n",
    "# long_short_fr_indicies_analysis_results_h5_df.to_hdf(file_path, key=f'{a_global_computations_group_key}/long_short_fr_indicies_analysis', format='table', data_columns=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_params_dict\n",
    "\n",
    "preprocessing_parameters = all_params_dict['preprocessing_parameters']\n",
    "curr_global_param_typed_parameters: ComputationKWargParameters = all_params_dict['curr_global_param_typed_parameters']\n",
    "param_typed_parameters: ComputationKWargParameters = all_params_dict['param_typed_parameters']\n",
    "\n",
    "preprocessing_parameters\n",
    "curr_global_param_typed_parameters\n",
    "param_typed_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_str: str = param_typed_parameters.values_only_repr(attr_separator_str=\",\\n\", sub_attr_additive_seperator_str='\\t')\n",
    "print(_out_str)\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# Serializable HDF5 Attributes Fields (metadata set on the HDF5 Group corresponding to this object):                   #\n",
    "# ==================================================================================================================== #\n",
    "## Get attributes fields as well\n",
    "hdf_attr_fields, hdf_attr_fields_filter_fn = self.get_serialized_attribute_fields('hdf')\n",
    "active_hdf_attributes_fields_dict = {a_field.name:a_field for a_field in hdf_attr_fields} # a dict that allows accessing the actual attr by its name\n",
    "_active_obj_attributes_values_dict = asdict(self, filter=hdf_attr_fields_filter_fn, recurse=False) # want recurse=True for this one?\n",
    "\n",
    "# Actually assign the attributes to the group:\n",
    "if len(_active_obj_attributes_values_dict) > 0: # don't open the file for no reason\n",
    "    # Open the file with h5py to add attributes to the group. The pandas.HDFStore object doesn't provide a direct way to manipulate groups as objects, as it is primarily intended to work with datasets (i.e., pandas DataFrames)\n",
    "    with h5py.File(file_path, 'r+') as f:\n",
    "        group = f[key]\n",
    "        for a_field_name, a_value in _active_obj_attributes_values_dict.items():\n",
    "            a_field_attr = active_hdf_attributes_fields_dict[a_field_name]\n",
    "            if debug_print:\n",
    "                print(f'an_attribute_field: {a_field_attr.name}')\n",
    "\n",
    "            custom_serialization_fn = a_field_attr.metadata.get('custom_serialization_fn', None) # (lambda f, k, v: a_value)\n",
    "            if custom_serialization_fn is not None:\n",
    "                # use the custom serialization function:\n",
    "                custom_serialization_fn(group.attrs, a_field_attr.name, a_value)\n",
    "            else:\n",
    "                # set that group attribute to a_value\n",
    "                group.attrs[a_field_attr.name] = a_value #TODO 2023-07-31 05:50: - [ ] Assumes that the value is valid to be used as an HDF5 attribute without conversion.\n",
    "\n",
    "\n",
    "\n",
    "# merged_directional_placefields: laps_decoding_time_bin_size: 0.25,\n",
    "# \tripple_decoding_time_bin_size: 0.025,\n",
    "# \tshould_validate_lap_decoding_performance: False,\n",
    "# rank_order_shuffle_analysis: num_shuffles: 500,\n",
    "# \tminimum_inclusion_fr_Hz: 5.0,\n",
    "# \tincluded_qclu_values: [1, 2],\n",
    "# \tskip_laps: False,\n",
    "# directional_decoders_decode_continuous: time_bin_size: None,\n",
    "# directional_decoders_evaluate_epochs: should_skip_radon_transform: False,\n",
    "# directional_train_test_split: training_data_portion: 0.8333333333333334,\n",
    "# \tdebug_output_hdf5_file_path: None,\n",
    "# long_short_decoding_analyses: decoding_time_bin_size: None,\n",
    "# \tperform_cache_load: False,\n",
    "# \talways_recompute_replays: False,\n",
    "# \toverride_long_epoch_name: None,\n",
    "# \toverride_short_epoch_name: None,\n",
    "# long_short_rate_remapping: decoding_time_bin_size: None,\n",
    "# \tperform_cache_load: False,\n",
    "# \talways_recompute_replays: False,\n",
    "# long_short_inst_spike_rate_groups: instantaneous_time_bin_size_seconds: 0.01,\n",
    "# wcorr_shuffle_analysis: num_shuffles: 1024,\n",
    "# \tdrop_previous_result_and_compute_fresh: False,\n",
    "# perform_specific_epochs_decoding: decoder_ndim: 2,\n",
    "# \tfilter_epochs: ripple,\n",
    "# \tdecoding_time_bin_size: 0.02,\n",
    "# DEP_ratemap_peaks: peak_score_inclusion_percent_threshold: 0.25,\n",
    "# ratemap_peaks_prominence2d: step: 0.01,\n",
    "# \tpeak_height_multiplier_probe_levels: (0.5, 0.9),\n",
    "# \tminimum_included_peak_height: 0.2,\n",
    "# \tuniform_blur_size: 3,\n",
    "# \tgaussian_blur_sigma: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters\n",
    "\n",
    "## Add `curr_active_pipeline.global_computation_results.computation_config` as needed:\n",
    "if curr_active_pipeline.global_computation_results.computation_config is None:\n",
    "    print('global_computation_results.computation_config is None! Making new one!')\n",
    "    curr_active_pipeline.global_computation_results.computation_config = ComputationKWargParameters.init_from_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "    print(f'\\tdone. Pipeline needs resave!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_kwargs_with_defaults(example_function))\n",
    "from pyphocorehelpers.print_helpers import ANSI_COLOR_STRINGS, ANSI_Coloring, partial\n",
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import BaseGlobalComputationParameters\n",
    "\n",
    "param_typed_parameters: ComputationKWargParameters = ComputationKWargParameters.init_from_pipeline(curr_active_pipeline=curr_active_pipeline)\n",
    "# param_typed_parameters\n",
    "# {'computation_params': registered_merged_computation_function_default_kwargs_dict}\n",
    "\n",
    "\n",
    "# _base_variable_name_only_values_dict, _base_variable_name_only_types_dict\n",
    "\n",
    "# WARNING: variable \"decoding_time_bin_size\" already exists. original value: None, new_value: None \n",
    "# WARNING: variable \"perform_cache_load\" already exists. original value: False, new_value: False \n",
    "# WARNING: variable \"always_recompute_replays\" already exists. original value: False, new_value: False \n",
    "# WARNING: variable \"num_shuffles\" already exists. original value: 500, new_value: 1024 \n",
    "# WARNING: variable \"decoding_time_bin_size\" already exists. original value: None, new_value: 0.02 \n",
    "# merged_directional_placefields/laps_decoding_time_bin_size: float = 0.25\n",
    "# merged_directional_placefields/ripple_decoding_time_bin_size: float = 0.025\n",
    "# merged_directional_placefields/should_validate_lap_decoding_performance: bool = False\n",
    "# rank_order_shuffle_analysis/num_shuffles: int = 500\n",
    "# rank_order_shuffle_analysis/minimum_inclusion_fr_Hz: float = 5.0\n",
    "# rank_order_shuffle_analysis/included_qclu_values: list = [1, 2]\n",
    "# rank_order_shuffle_analysis/skip_laps: bool = False\n",
    "# directional_decoders_decode_continuous/time_bin_size: Optional[float] = None\n",
    "# directional_decoders_evaluate_epochs/should_skip_radon_transform: bool = False\n",
    "# directional_train_test_split/training_data_portion: float = 0.8333333333333334\n",
    "# directional_train_test_split/debug_output_hdf5_file_path: Optional[pathlib.Path] = None\n",
    "# long_short_decoding_analyses/decoding_time_bin_size: Optional[float] = None\n",
    "# long_short_decoding_analyses/perform_cache_load: bool = False\n",
    "# long_short_decoding_analyses/always_recompute_replays: bool = False\n",
    "# long_short_decoding_analyses/override_long_epoch_name: Optional[str] = None\n",
    "# long_short_decoding_analyses/override_short_epoch_name: Optional[str] = None\n",
    "# long_short_rate_remapping/decoding_time_bin_size: Optional[float] = None\n",
    "# long_short_rate_remapping/perform_cache_load: bool = False\n",
    "# long_short_rate_remapping/always_recompute_replays: bool = False\n",
    "# long_short_inst_spike_rate_groups/instantaneous_time_bin_size_seconds: Optional[float] = None\n",
    "# wcorr_shuffle_analysis/num_shuffles: int = 1024\n",
    "# wcorr_shuffle_analysis/drop_previous_result_and_compute_fresh: bool = False\n",
    "# _perform_specific_epochs_decoding/decoder_ndim: int = 2\n",
    "# _perform_specific_epochs_decoding/filter_epochs: str = 'ripple'\n",
    "# _perform_specific_epochs_decoding/decoding_time_bin_size: Optional[float] = 0.02\n",
    "# _DEP_ratemap_peaks/peak_score_inclusion_percent_threshold: float = 0.25\n",
    "# ratemap_peaks_prominence2d/step: float = 0.01\n",
    "# ratemap_peaks_prominence2d/peak_height_multiplier_probe_levels: tuple = (0.5, 0.9)\n",
    "# ratemap_peaks_prominence2d/minimum_included_peak_height: float = 0.2\n",
    "# ratemap_peaks_prominence2d/uniform_blur_size: int = 3\n",
    "# ratemap_peaks_prominence2d/gaussian_blur_sigma: int = 3\n",
    "\n",
    "## OUTPUTS: _defn_lines, _flat_fields_tuples_list, _base_variable_name_only_values_dict, _base_variable_name_only_types_dict\n",
    "def _format_curr_value(depth_string, curr_key, curr_value, type_string, type_name, is_omitted_from_expansion:bool=False):\n",
    "    return f\"{depth_string}['{curr_key}']: {curr_value}\"  \n",
    "    # return f\"{depth_string}['{curr_key}']: {type_name}\"             \n",
    "\n",
    "\n",
    "def custom_string_rep_if_short_enough(value: Any, max_length:int=280, max_num_lines:int=1, allow_reformatting:bool=True, allow_ellipsis_fill_too_long_regions:bool=True, debug_print:bool=False):\n",
    "    \"\"\" returns the formatted str-rep of the value if it meets the criteria, otherwise nothing. An example `value_formatting_fn` \n",
    "    \n",
    "    allow_reformatting: if True, allows removing lines to meet max_num_lines requirements so long as max_length is short enough\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "        from functools import partial\n",
    "        from pyphocorehelpers.print_helpers import DocumentationFilePrinter\n",
    "\n",
    "        custom_value_formatting_fn = partial(DocumentationFilePrinter.string_rep_if_short_enough, max_length=280, max_num_lines=1)\n",
    "        new_custom_item_formatter = partial(DocumentationFilePrinter._default_rich_text_formatter, value_formatting_fn=custom_value_formatting_fn)\n",
    "        print_keys_if_possible('context', context, max_depth=4, custom_item_formatter=new_custom_item_formatter)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "        \n",
    "    reformatting_line_replacement_str: str = '<br>'\n",
    "    # reformatting_line_replacement_str: str = '\\t'\n",
    "    \n",
    "    \n",
    "    ellipsis_join_chars: str = '...'\n",
    "    \n",
    "    \n",
    "\n",
    "    does_repr_have_too_many_lines: bool = (len(value.splitlines()) > max_num_lines)\n",
    "    \n",
    "    if (does_repr_have_too_many_lines and allow_reformatting):\n",
    "        _val_arr = value.splitlines()\n",
    "        _original_num_lines: int = len(_val_arr)\n",
    "        _num_lines_to_combine: int = _original_num_lines - max_num_lines\n",
    "            \n",
    "        if (max_num_lines == 1) and (_original_num_lines > 1):\n",
    "            if allow_ellipsis_fill_too_long_regions:\n",
    "                _reformatted_value = reformatting_line_replacement_str.join(_val_arr) ## join all lines\n",
    "            else:\n",
    "                _reformatted_value = reformatting_line_replacement_str.join(_val_arr)\n",
    "\n",
    "        elif (max_num_lines >= 3) and (_original_num_lines >= 3):\n",
    "            _first_line = _val_arr.pop(0)\n",
    "            _last_line = _val_arr.pop(-1) # remove the last line\n",
    "            _middle_lines = _val_arr # remaining lines\n",
    "            if allow_ellipsis_fill_too_long_regions:\n",
    "                _reformatted_value = '\\n'.join((_first_line, ellipsis_join_chars, _last_line))  ## join extra lines after that                    \n",
    "            else:\n",
    "                _reformatted_value = '\\n'.join((_first_line, reformatting_line_replacement_str.join(_middle_lines), _last_line))  ## join extra lines after that\n",
    "        else:\n",
    "            raise NotImplementedError(f\"_original_num_lines: {_original_num_lines}, max_num_lines: {max_num_lines}, _val_arr: {_val_arr}\")\n",
    "\n",
    "        does_repr_have_too_many_lines: bool = (len(_reformatted_value.splitlines()) > max_num_lines)\n",
    "        assert (not does_repr_have_too_many_lines), f\"string_rep_if_short_enough(...): ERROR:\\n even after reformatting the _reformatted_value has too many lines! max_num_lines: {max_num_lines}\\n len(_reformatted_value.splitlines()): {len(_reformatted_value.splitlines())}\\n _reformatted_value: {_reformatted_value}\\n value: {value}\"\n",
    "        ## update value if needed\n",
    "        if value != _reformatted_value:\n",
    "            if debug_print:\n",
    "                print(f'string_rep_if_short_enough(...): valueChanged:\\n value: {value}\\n\\n _reformatted_value: {_reformatted_value}')\n",
    "        value = _reformatted_value\n",
    "\n",
    "    is_repr_too_long: bool = (len(value) > max_length)\n",
    "    if is_repr_too_long and allow_ellipsis_fill_too_long_regions:\n",
    "        # replaces all characters following the allowed max_length with ellipses\n",
    "        characters_to_ellipses: int = (max_length-len(ellipsis_join_chars)) - len(value) # characters needed to be replaced by elipses\n",
    "        _value_start = value[:(max_length-len(ellipsis_join_chars))]\n",
    "        _reformatted_value = f\"{_value_start}{ellipsis_join_chars}\"\n",
    "        is_repr_too_long: bool = (len(_reformatted_value) > max_length)\n",
    "        \n",
    "        assert (not is_repr_too_long), f\"string_rep_if_short_enough(...): ERROR:\\n even after replacing with ellipses the _reformatted_value has too many chars! max_length: {max_length}\\n len(_reformatted_value): {len(_reformatted_value)}\\n _reformatted_value: {_reformatted_value}\\n value: {value}\"\n",
    "        ## update value if needed\n",
    "        if (value != _reformatted_value):\n",
    "            if debug_print:\n",
    "                print(f'string_rep_if_short_enough(...): valueChanged:\\n value: {value}\\n\\n _reformatted_value: {_reformatted_value}')\n",
    "        value = _reformatted_value\n",
    "\n",
    "    is_repr_good_as_is: bool = (not does_repr_have_too_many_lines) and (not is_repr_too_long)\n",
    "    if is_repr_good_as_is:\n",
    "        # valid rep, include the value\n",
    "        return f' = {value}'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _custom_rich_text_formatter(depth_string, curr_key, curr_value, type_string, type_name, is_omitted_from_expansion=False, value_formatting_fn=None):\n",
    "    \"\"\" formats using ANSI_Coloring for rich colored output \"\"\"\n",
    "    if value_formatting_fn is None:\n",
    "        # value_string_rep_fn = cls.never_string_rep\n",
    "        value_formatting_fn = DocumentationFilePrinter.string_rep_if_short_enough\n",
    "        \n",
    "    key_color = ANSI_COLOR_STRINGS.OKBLUE\n",
    "    variable_type_color = ANSI_COLOR_STRINGS.LIGHTGREEN # looks better on screen\n",
    "    # variable_type_color = ANSI_COLOR_STRINGS.LIGHTMAGENTA # converts to greyscale for printing better\n",
    "    if is_omitted_from_expansion:\n",
    "        value_str = f\"{(ANSI_COLOR_STRINGS.WARNING + ' (children omitted)' + ANSI_COLOR_STRINGS.ENDC)}\"\n",
    "    else:\n",
    "        ## try to get the value:\n",
    "        value_str = curr_value.values_only_repr()\n",
    "        value_str = value_formatting_fn(value_str)\n",
    "        # value_str = value_formatting_fn(curr_value)\n",
    "        if (value_str is not None) and (len(value_str) > 0):\n",
    "            value_str = f\"{(ANSI_COLOR_STRINGS.WARNING + value_str + ANSI_COLOR_STRINGS.ENDC)}\"\n",
    "        else:\n",
    "            value_str = \"\"\n",
    "\n",
    "    return f\"{depth_string}- {key_color}{curr_key}{ANSI_COLOR_STRINGS.ENDC}: {value_str}\"\n",
    "\n",
    "\n",
    "# Plaintext:\n",
    "# custom_value_formatting_fn = partial(DocumentationFilePrinter.string_rep_if_short_enough, max_length=280, max_num_lines=1)\n",
    "# custom_item_formatter = partial(DocumentationFilePrinter._default_plain_text_formatter, value_formatting_fn=custom_value_formatting_fn)\n",
    "\n",
    "## Rich:\n",
    "# custom_value_formatting_fn = partial(DocumentationFilePrinter.string_rep_if_short_enough, max_length=280, max_num_lines=1)\n",
    "custom_value_formatting_fn = partial(custom_string_rep_if_short_enough, max_length=280, max_num_lines=1)\n",
    "\n",
    "# custom_item_formatter = partial(DocumentationFilePrinter._default_rich_text_formatter, value_formatting_fn=custom_value_formatting_fn)\n",
    "# custom_item_formatter = partial(_custom_rich_text_formatter, value_formatting_fn=custom_value_formatting_fn)\n",
    "custom_item_formatter = partial(_custom_rich_text_formatter, value_formatting_fn=custom_value_formatting_fn)\n",
    "\n",
    "\n",
    "\n",
    "# print_keys_if_possible('param_typed_parameters', param_typed_parameters, max_depth=3, custom_item_formatter=_format_curr_value)\n",
    "print_keys_if_possible('param_typed_parameters', param_typed_parameters, max_depth=3, custom_item_formatter=custom_item_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_class_type_dict = deepcopy(ComputationKWargParameters.__annotations__)\n",
    "params_class_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_str, nested_classes_dict, imports_dict = GlobalComputationParametersAttrsClassTemplating._subfn_build_attrs_parameters_classes(registered_merged_computation_function_default_kwargs_dict=registered_merged_computation_function_default_kwargs_dict, \n",
    "                                                                                                         params_defn_save_path=None, should_build_hdf_class=True, print_defns=False)\n",
    "nested_classes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports_dict\n",
    "\n",
    "imports_list = list(imports_dict.keys())\n",
    "print(imports_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035583d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.SpecificComputationParameterTypes import ComputationKWargParameters, merged_directional_placefields_Parameters, rank_order_shuffle_analysis_Parameters, directional_decoders_decode_continuous_Parameters, directional_decoders_evaluate_epochs_Parameters, directional_train_test_split_Parameters, long_short_decoding_analyses_Parameters, long_short_rate_remapping_Parameters, long_short_inst_spike_rate_groups_Parameters, wcorr_shuffle_analysis_Parameters, perform_specific_epochs_decoding_Parameters, DEP_ratemap_peaks_Parameters, ratemap_peaks_prominence2d_Parameters\n",
    "\n",
    "params_class_type_list = [merged_directional_placefields_Parameters, rank_order_shuffle_analysis_Parameters, directional_decoders_decode_continuous_Parameters, directional_decoders_evaluate_epochs_Parameters, directional_train_test_split_Parameters, long_short_decoding_analyses_Parameters, long_short_rate_remapping_Parameters, long_short_inst_spike_rate_groups_Parameters, wcorr_shuffle_analysis_Parameters, perform_specific_epochs_decoding_Parameters, DEP_ratemap_peaks_Parameters, ratemap_peaks_prominence2d_Parameters]\n",
    "# params_class_type_dict = dict(zip({k.removeprefix('_') for k in imports_dict.keys()}, params_class_type_list))\n",
    "# params_class_type_dict = dict(zip({k for k in imports_dict.keys()}, params_class_type_list))\n",
    "params_class_type_dict = dict(zip(imports_list, params_class_type_list))\n",
    "# params_class_type_dict\n",
    "\n",
    "## Convert to the new native types\n",
    "## INPUTS: registered_merged_computation_function_default_kwargs_dict, params_class_type_dict\n",
    "_out_param_typed_parameters_dict = {}\n",
    "for k, v_dict in registered_merged_computation_function_default_kwargs_dict.items():\n",
    "    a_type = params_class_type_dict[k]\n",
    "    _out_param_typed_parameters_dict[k.removeprefix('_')] = a_type(**v_dict)\n",
    "# _out_param_typed_parameters_dict\n",
    "\n",
    "## OUTPUTS: _out_param_typed_parameters_dict\n",
    "# param_typed_parameters: ComputationKWargParameters = ComputationKWargParameters(**_out_param_typed_parameters_dict)\n",
    "param_typed_parameters: ComputationKWargParameters = ComputationKWargParameters(**_out_param_typed_parameters_dict)\n",
    "param_typed_parameters\n",
    "\n",
    "## OUTPUTS: param_typed_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056721eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config = param_typed_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11167bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData\n",
    "\n",
    "save_root_path = Path(r\"C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\").resolve()\n",
    "Assert.path_exists(save_root_path)\n",
    "\n",
    "\n",
    "non_split_out_path = Path(r\"C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\param_typed_parameters.pkl\").resolve()\n",
    "curr_item_type = type(param_typed_parameters)\n",
    "saveData(non_split_out_path, (param_typed_parameters, str(curr_item_type.__module__), str(curr_item_type.__name__)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.assertion_helpers import Assert\n",
    "from pyphocorehelpers.Filesystem.pickling_helpers import save_split_pickled_obj\n",
    "\n",
    "## INPUTS: param_typed_parameters\n",
    "save_root_path = Path(r\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output\").resolve()\n",
    "Assert.path_exists(save_root_path)\n",
    "split_save_folder, (split_save_paths, split_save_output_types), (succeeded_keys, failed_keys, skipped_keys) = save_split_pickled_obj(param_typed_parameters, save_root_path=save_root_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956eb391",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config = deepcopy(param_typed_parameters)\n",
    "\n",
    "# computation_kwargs_parameters_dict = {'merged_directional_placefields': {'laps_decoding_time_bin_size': 0.25, 'ripple_decoding_time_bin_size': 0.025, 'should_validate_lap_decoding_performance': False},\n",
    "#  'rank_order_shuffle_analysis': {'num_shuffles': 500, 'minimum_inclusion_fr_Hz': 5.0, 'included_qclu_values': [1, 2], 'skip_laps': False},\n",
    "#  'directional_decoders_decode_continuous': {'time_bin_size': None},\n",
    "#  'directional_decoders_evaluate_epochs': {'should_skip_radon_transform': False},\n",
    "#  'directional_train_test_split': {'training_data_portion': 0.8333333333333334, 'debug_output_hdf5_file_path': None},\n",
    "#  'long_short_decoding_analyses': {'decoding_time_bin_size': None, 'perform_cache_load': False, 'always_recompute_replays': False, 'override_long_epoch_name': None, 'override_short_epoch_name': None},\n",
    "#  'long_short_rate_remapping': {'decoding_time_bin_size': None, 'perform_cache_load': False, 'always_recompute_replays': False},\n",
    "#  'long_short_inst_spike_rate_groups': {'instantaneous_time_bin_size_seconds': None},\n",
    "#  'wcorr_shuffle_analysis': {'num_shuffles': 1024, 'drop_previous_result_and_compute_fresh': False},\n",
    "#  '_perform_specific_epochs_decoding': {'decoder_ndim': 2, 'filter_epochs': 'ripple', 'decoding_time_bin_size': 0.02},\n",
    "#  '_DEP_ratemap_peaks': {'peak_score_inclusion_percent_threshold': 0.25},\n",
    "#  'ratemap_peaks_prominence2d': {'step': 0.01, 'peak_height_multiplier_probe_levels': (0.5, 0.9), 'minimum_included_peak_height': 0.2, 'uniform_blur_size': 3, 'gaussian_blur_sigma': 3}}\n",
    "\n",
    "# CodeConversion.convert_dictionary_to_class_defn(computation_kwargs_parameters_dict, 'ComputationKWargParameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3d07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_typed_parameters = deepcopy(curr_active_pipeline.global_computation_results.computation_config)\n",
    "param_typed_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7123fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions, validate_has_rank_order_results\n",
    "\n",
    "validate_has_rank_order_results(curr_active_pipeline=curr_active_pipeline)\n",
    "# param_typed_parameters = curr_active_pipeline.global_computation_results.computation_config\n",
    "# if param_typed_parameters is not None:\n",
    "#     rank_order_shuffle_analysis = param_typed_parameters.get('rank_order_shuffle_analysis', None)\n",
    "#     if rank_order_shuffle_analysis is not None:\n",
    "#         ## has valid rank_order_shuffle_analysis config:\n",
    "#         if (rank_order_shuffle_analysis.minimum_inclusion_fr_Hz != results_minimum_inclusion_fr_Hz):\n",
    "#             print(f'minimum_inclusion_fr_Hz differs! results_value: {results_minimum_inclusion_fr_Hz}, params_val: {rank_order_shuffle_analysis.minimum_inclusion_fr_Hz}')\n",
    "#             return False\n",
    "        \n",
    "#         # if (rank_order_shuffle_analysis.num_shuffles != rank_order_results.num_shuffles):\n",
    "#         #     print(f'num_shuffles differs! results_value: {rank_order_results.num_shuffles}, params_val: {rank_order_shuffle_analysis.num_shuffles}')\n",
    "#         #     return False\n",
    "        \n",
    "#         if (set(rank_order_shuffle_analysis.included_qclu_values) != set(results_included_qclu_values)):\n",
    "#             print(f'included_qclu_values differs! results_value: {results_included_qclu_values}, params_val: {rank_order_shuffle_analysis.included_qclu_values}')\n",
    "#             return False\n",
    "        \n",
    "\n",
    "# perform_rank_order_shuffle_analysis, valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56920dd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "included_qclu_values: float = rank_order_results.included_qclu_values\n",
    "print(f'minimum_inclusion_fr_Hz: {minimum_inclusion_fr_Hz}')\n",
    "print(f'included_qclu_values: {included_qclu_values}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b530290",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "{k:fn_best_name(v) for k, v in curr_active_pipeline.registered_merged_computation_function_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34a8c7",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cda25",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.active_configs # InteractivePlaceCellConfig\n",
    "curr_active_pipeline.active_sess_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a530ff",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "registered_merged_computation_function_default_kwargs_dict['computation_params._build_merged_directional_placefields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ec4d3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "curr_active_pipeline.global_computation_results.computation_config # = DynamicContainer("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becec465",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from benedict import benedict\n",
    "\n",
    "_curr_epoch_config_dict: Dict[types.DecoderName, Dict] = {k:benedict(deepcopy(v.computation_config).to_dict()) for k, v in curr_active_pipeline.computation_results.items()}\n",
    "_curr_epoch_config_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9683fd",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2f826f",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "source": [
    "# New Firing Rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173eecf8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# long_spikes_df\n",
    "# curr_active_pipeline\n",
    "epoch_spikes_df = deepcopy(long_one_step_decoder_1D.spikes_df)\n",
    "# filter_epoch_spikes_df_L\n",
    "# filter_epoch_spikes_df_S\n",
    "epoch_spikes_df\n",
    "\n",
    "epochs_df_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5aec3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "unit_specific_binned_spike_rate_df, unit_specific_binned_spike_counts_df, time_window_edges, time_window_edges_binning_info = SpikeRateTrends.compute_simple_time_binned_firing_rates_df(epoch_spikes_df, time_bin_size_seconds=0.005, debug_print=False)\n",
    "# unit_specific_binned_spike_rate_df.to_numpy() # (160580, 45)\n",
    "\n",
    "# Compute average firing rate for each neuron\n",
    "unit_avg_firing_rates = np.nanmean(unit_specific_binned_spike_rate_df.to_numpy(), axis=0) # (n_neurons, )\n",
    "unit_avg_firing_rates = np.nanmax(unit_specific_binned_spike_rate_df.to_numpy(), axis=0) # (n_neurons, )\n",
    "unit_avg_firing_rates            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5a533",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import get_proper_global_spikes_df\n",
    "\n",
    "def _compute_epochs_cell_firing_rates_metastats(epoch_inst_fr_df_list, minimal_active_firing_rate_Hz = 1e-3):\n",
    "    # epoch_inst_fr_df_list # List[pd.DataFrame] - where each df is of shape: (n_epoch_time_bins[i], n_cells) -- list of length n_epochs\n",
    "    # len(epoch_inst_fr_df_list) # n_epochs\n",
    "    # an_epoch = epoch_inst_fr_df_list[0] ## df has aclus as columns\n",
    "    n_active_aclus_per_epoch = [(an_epoch > minimal_active_firing_rate_Hz).sum(axis=1).values for an_epoch in epoch_inst_fr_df_list] # (n_epochs, ) # (n_epoch_time_bins[i], )\n",
    "    n_active_aclus_avg_per_epoch_time_bin = np.array([np.mean((an_epoch > minimal_active_firing_rate_Hz).sum(axis=1).values) for an_epoch in epoch_inst_fr_df_list]) # (n_epochs, )\n",
    "    \n",
    "    ## OUTPUTS: n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin\n",
    "    \n",
    "    \n",
    "    return n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin\n",
    "\n",
    "\n",
    "\n",
    "# instantaneous_time_bin_size_seconds = 0.005\n",
    "instantaneous_time_bin_size_seconds = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9c1c4",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "replay_epochs_df = deepcopy(active_replay_epochs_df)\n",
    "replay_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "epoch_inst_fr_df_list, epoch_inst_fr_signal_list, epoch_avg_firing_rates_list = SpikeRateTrends.compute_epochs_unit_avg_inst_firing_rates(spikes_df=replay_spikes_df, filter_epochs=replay_epochs_df, included_neuron_ids=EITHER_subset.track_exclusive_aclus, instantaneous_time_bin_size_seconds=instantaneous_time_bin_size_seconds, use_instantaneous_firing_rate=True, debug_print=True)\n",
    "# epoch_inst_fr_df_list, epoch_inst_fr_signal_list, epoch_avg_firing_rates_list = SpikeRateTrends.compute_epochs_unit_avg_inst_firing_rates(spikes_df=filter_epoch_spikes_df_L, filter_epochs=epochs_df_L, included_neuron_ids=EITHER_subset.track_exclusive_aclus, instantaneous_time_bin_size_seconds=instantaneous_time_bin_size_seconds, use_instantaneous_firing_rate=False, debug_print=False)\n",
    "# epoch_avg_firing_rates_list # (294, 42), (n_filter_epochs, n_neurons)\n",
    "# epoch_avg_firing_rates_list\n",
    "\n",
    "# epoch_avg_firing_rates_list\n",
    "# laps_all_epoch_bins_marginals_df\n",
    "n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin = _compute_epochs_cell_firing_rates_metastats(epoch_inst_fr_df_list=epoch_inst_fr_df_list)\n",
    "# n_active_aclus_avg_per_epoch_time_bin # (n_epochs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e4fd6",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "across_epoch_avg_firing_rates = np.mean(epoch_avg_firing_rates_list, 0) # (42,)\n",
    "across_epoch_avg_firing_rates\n",
    "# unit_specific_binned_spike_rate_df\n",
    "# unit_specific_binned_spike_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb50822",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## Laps\n",
    "# laps_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "laps_spikes_df = get_proper_global_spikes_df(curr_active_pipeline, minimum_inclusion_fr_Hz=5)\n",
    "# laps_filter_epochs = ensure_dataframe(deepcopy(decoder_laps_filter_epochs_decoder_result_dict['long_LR'].filter_epochs)) \n",
    "epoch_inst_fr_df_list, epoch_inst_fr_signal_list, epoch_avg_firing_rates_list = SpikeRateTrends.compute_epochs_unit_avg_inst_firing_rates(spikes_df=laps_spikes_df, filter_epochs=ensure_dataframe(global_any_laps_epochs_obj),\n",
    "                                                                                                                                           included_neuron_ids=EITHER_subset.track_exclusive_aclus, instantaneous_time_bin_size_seconds=instantaneous_time_bin_size_seconds, use_instantaneous_firing_rate=True, debug_print=False)\n",
    "# epoch_avg_firing_rates_list\n",
    "# laps_all_epoch_bins_marginals_df\n",
    "n_active_aclus_per_epoch, n_active_aclus_avg_per_epoch_time_bin = _compute_epochs_cell_firing_rates_metastats(epoch_inst_fr_df_list=epoch_inst_fr_df_list)\n",
    "n_active_aclus_avg_per_epoch_time_bin # (n_epochs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af983f8",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "# epochs_df_S\n",
    "epochs_df_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa2ee3",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "print(n_active_aclus_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed45c2",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "num_cells_active_per_epoch = np.sum((epoch_avg_firing_rates_list > 0.1), axis=1) # find the number of neurons active in each time bin. (n_filter_epochs, )\n",
    "num_cells_active_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93712b0",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "epoch_avg_firing_rates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a17e5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "len(epoch_inst_fr_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1126d3e",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "len(epoch_inst_fr_df_list) # (n_epoch_time_bins[i], n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25c0d5",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import LongShortDisplayConfigManager, long_short_display_config_manager\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColorFormatConverter, debug_print_color, build_adjusted_color\n",
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import apply_LR_to_RL_adjustment\n",
    "from pyphocorehelpers.gui.Qt.color_helpers import ColormapHelpers\n",
    "\n",
    "\n",
    "additional_cmap_names = dict(zip(TrackTemplates.get_decoder_names(), ['red', 'purple', 'green', 'orange'])) # {'long_LR': 'red', 'long_RL': 'purple', 'short_LR': 'green', 'short_RL': 'orange'}\n",
    "\n",
    "long_epoch_config = long_short_display_config_manager.long_epoch_config.as_pyqtgraph_kwargs()\n",
    "short_epoch_config = long_short_display_config_manager.short_epoch_config.as_pyqtgraph_kwargs()\n",
    "\n",
    "color_dict = {'long_LR': long_epoch_config['brush'].color(), 'long_RL': apply_LR_to_RL_adjustment(long_epoch_config['brush'].color()),\n",
    "                'short_LR': short_epoch_config['brush'].color(), 'short_RL': apply_LR_to_RL_adjustment(short_epoch_config['brush'].color())}\n",
    "additional_cmap_names = {k: ColorFormatConverter.qColor_to_hexstring(v) for k, v in color_dict.items()}\n",
    "\n",
    "additional_cmaps = {k: ColormapHelpers.create_transparent_colormap(color_literal_name=v, lower_bound_alpha=0.1) for k, v in additional_cmap_names.items()}\n",
    "additional_cmaps['long_LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853a391",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "decoder_laps_filter_epochs_decoder_result_dict['long_LR'].num_filter_epochs ## 84 laps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22203632",
   "metadata": {
    "tags": [
     "all"
    ]
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Pho2D.data_exporting import HeatmapExportConfig\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import SingleEpochDecodedResult\n",
    "from pyphoplacecellanalysis.Pho2D.data_exporting import PosteriorExporting\n",
    "\n",
    "# custom_export_formats: Dict[str, HeatmapExportConfig] = None\n",
    "# custom_export_formats: Dict[str, HeatmapExportConfig] = {\n",
    "# \t# 'greyscale': HeatmapExportConfig.init_greyscale(),\n",
    "#     'color': HeatmapExportConfig(colormap='Oranges', desired_height=400),\n",
    "#     # 'color': HeatmapExportConfig(colormap=additional_cmaps['long_LR']),\n",
    "# \t# 'color': HeatmapExportConfig(colormap=cmap1, desired_height=200),\n",
    "# }\n",
    "\n",
    "# custom_exports_dict['color'].to_dict()\n",
    "\n",
    "curr_active_pipeline.reload_default_display_functions()\n",
    "_out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_stacked_epoch_slices')\n",
    "# _out = curr_active_pipeline.display('_display_directional_merged_pf_decoded_stacked_epoch_slices', custom_export_formats=custom_export_formats) # directional_decoded_stacked_epoch_slices\n",
    "_out\n",
    "# {'export_paths': {'laps': {'long_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/long_LR'),\n",
    "#    'long_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/long_RL'),\n",
    "#    'short_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/short_LR'),\n",
    "#    'short_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/laps/short_RL')},\n",
    "#   'ripple': {'long_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/long_LR'),\n",
    "#    'long_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/long_RL'),\n",
    "#    'short_LR': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/short_LR'),\n",
    "#    'short_RL': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43/ripple/short_RL')}},\n",
    "#  'parent_output_folder': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors'),\n",
    "#  'parent_specific_session_output_folder': WindowsPath('K:/scratch/collected_outputs/figures/_temp_individual_posteriors/2024-09-30/gor01_one_2006-6-09_1-22-43')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5d208",
   "metadata": {},
   "source": [
    "# ✅ `batch_user_completion_helpers` Batch Computation Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49f666",
   "metadata": {},
   "source": [
    "### Call `compute_and_export_session_trial_by_trial_performance_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_session_trial_by_trial_performance_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "return_full_decoding_results: bool = True\n",
    "save_hdf: bool = True\n",
    "save_csvs:bool = True\n",
    "_across_session_results_extended_dict = {}\n",
    "\n",
    "additional_session_context = None\n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        additional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "        print(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "except NameError as err:\n",
    "    additional_session_context = None\n",
    "    print(f'NO CUSTOM SUFFIX.')    \n",
    "    \n",
    "active_laps_decoding_time_bin_size: float = 0.25\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_session_trial_by_trial_performance_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict, active_laps_decoding_time_bin_size=active_laps_decoding_time_bin_size,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                )\n",
    "\n",
    "\n",
    "callback_outputs = _across_session_results_extended_dict['compute_and_export_session_trial_by_trial_performance_completion_function']\n",
    "a_trial_by_trial_result = callback_outputs['a_trial_by_trial_result']\n",
    "subset_neuron_IDs_dict = callback_outputs['subset_neuron_IDs_dict']\n",
    "subset_decode_results_dict = callback_outputs['subset_decode_results_dict']\n",
    "subset_decode_results_track_id_correct_performance_dict = callback_outputs['subset_decode_results_track_id_correct_performance_dict']\n",
    "subset_neuron_IDs_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b8fed",
   "metadata": {},
   "source": [
    "### Call `export_rank_order_results_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import export_rank_order_results_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "# _across_session_results_extended_dict = {}\n",
    "\n",
    "# additional_session_context = None\n",
    "# try:\n",
    "#     if custom_suffix is not None:\n",
    "#         additional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "#         print(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "# except NameError as err:\n",
    "#     additional_session_context = None\n",
    "#     print(f'NO CUSTOM SUFFIX.')    \n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | export_rank_order_results_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                should_save_pkl=False, should_save_CSV=True,\n",
    "                                                )\n",
    "\n",
    "\n",
    "callback_outputs = _across_session_results_extended_dict['export_rank_order_results_completion_function']\n",
    "merged_complete_ripple_epoch_stats_df_output_path = callback_outputs['merged_complete_ripple_epoch_stats_df_output_path']\n",
    "minimum_inclusion_fr_Hz = callback_outputs['minimum_inclusion_fr_Hz']\n",
    "included_qclu_values = callback_outputs['included_qclu_values']\n",
    "print(f'merged_complete_ripple_epoch_stats_df_output_path: {merged_complete_ripple_epoch_stats_df_output_path}') # \"2024-11-15_Lab-2006-6-09_1-22-43_merged_complete_epoch_stats_df.csv\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd5369",
   "metadata": {},
   "source": [
    "### Call `compute_and_export_session_wcorr_shuffles_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_session_wcorr_shuffles_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "_across_session_results_extended_dict = {}\n",
    "\n",
    "# additional_session_context = None\n",
    "# try:\n",
    "#     if custom_suffix is not None:\n",
    "#         additional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "#         print(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "# except NameError as err:\n",
    "#     additional_session_context = None\n",
    "#     print(f'NO CUSTOM SUFFIX.')    \n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_session_wcorr_shuffles_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                )\n",
    "\n",
    "\n",
    "callback_outputs = _across_session_results_extended_dict['compute_and_export_session_wcorr_shuffles_completion_function']\n",
    "wcorr_shuffles_data_output_filepath = callback_outputs['wcorr_shuffles_data_output_filepath']\n",
    "standalone_MAT_filepath = callback_outputs['standalone_MAT_filepath']\n",
    "ripple_WCorrShuffle_df_export_CSV_path = callback_outputs['ripple_WCorrShuffle_df_export_CSV_path']\n",
    "print(f'wcorr_shuffles_data_output_filepath: {wcorr_shuffles_data_output_filepath}') # \"2024-11-15_Lab-2006-6-09_1-22-43_merged_complete_epoch_stats_df.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2ec9a",
   "metadata": {},
   "source": [
    "#### #TODO 2024-11-15 14:27: - [ ] Fix the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_custom_suffix_for_replay_filename(new_replay_epochs: Epoch, *extras_strings) -> str:\n",
    "    \"\"\" Uses metadata stored in the replays dataframe to determine an appropriate filename\n",
    "    \n",
    "    \n",
    "    from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _get_custom_suffix_for_replay_filename\n",
    "    custom_suffix = _get_custom_suffix_for_replay_filename(new_replay_epochs=new_replay_epochs)\n",
    "\n",
    "    print(f'custom_suffix: \"{custom_suffix}\"')\n",
    "\n",
    "    \"\"\"\n",
    "    assert new_replay_epochs.metadata is not None\n",
    "    metadata = deepcopy(new_replay_epochs.metadata)\n",
    "    extras_strings = []\n",
    "\n",
    "    epochs_source = metadata.get('epochs_source', None)\n",
    "    assert epochs_source is not None\n",
    "    # print(f'epochs_source: {epochs_source}')\n",
    "\n",
    "    valid_epochs_source_values = ['compute_diba_quiescent_style_replay_events', 'diba_evt_file', 'initial_loaded', 'normal_computed']\n",
    "    assert epochs_source in valid_epochs_source_values, f\"epochs_source: '{epochs_source}' is not in valid_epochs_source_values: {valid_epochs_source_values}\"\n",
    "\n",
    "    custom_suffix: str = _get_custom_suffix_replay_epoch_source_name(epochs_source=epochs_source)\n",
    "    \n",
    "    if epochs_source == 'compute_diba_quiescent_style_replay_events':\n",
    "        # qclu = new_replay_epochs.metadata.get('qclu', \"[1,2]\")\n",
    "        custom_suffix = '-'.join([custom_suffix, f\"qclu_{metadata.get('included_qclu_values', '[1,2]')}\", f\"frateThresh_{metadata['minimum_inclusion_fr_Hz']:.1f}\", *extras_strings])\n",
    "\n",
    "    elif epochs_source == 'diba_evt_file':\n",
    "        custom_suffix = '-'.join([custom_suffix, f\"qclu_{metadata.get('included_qclu_values', '[1,2]')}\", f\"frateThresh_{metadata.get('minimum_inclusion_fr_Hz', 5.0):.1f}\", *extras_strings])\n",
    "        # qclu = new_replay_epochs.metadata.get('qclu', \"[1,2]\") # Diba export files are always qclus [1, 2]\n",
    "    elif epochs_source == 'initial_loaded':\n",
    "        custom_suffix = '-'.join([custom_suffix, f\"qclu_{metadata.get('included_qclu_values', 'XX')}\", f\"frateThresh_{metadata.get('minimum_inclusion_fr_Hz', 0.1):.1f}\", *extras_strings])\n",
    "\n",
    "    elif epochs_source == 'normal_computed':\n",
    "        custom_suffix = '-'.join([custom_suffix, f\"qclu_{metadata.get('included_qclu_values', '[1,2]')}\", f\"frateThresh_{metadata['minimum_inclusion_fr_Hz']:.1f}\", *extras_strings])\n",
    "    else:\n",
    "        raise NotImplementedError(f'epochs_source: {epochs_source} is of unknown type or is missing metadata.')    \n",
    "        \n",
    "    return custom_suffix\n",
    "\n",
    "\n",
    "\n",
    "custom_suffix: str = _get_custom_suffix_for_replay_filename(new_replay_epochs=a_replay_epochs) # looks right\n",
    "print(f'\\treplay_epochs_key: {replay_epochs_key}: custom_suffix: \"{custom_suffix}\"')\n",
    "\n",
    "## Modify .BATCH_DATE_TO_USE to include the custom suffix\n",
    "# curr_BATCH_DATE_TO_USE: str = f\"{base_BATCH_DATE_TO_USE}{custom_suffix}\"\n",
    "\n",
    "curr_BATCH_DATE_TO_USE: str = f\"{base_BATCH_DATE_TO_USE}\"\n",
    "print(f'\\tcurr_BATCH_DATE_TO_USE: \"{curr_BATCH_DATE_TO_USE}\"')\n",
    "self.BATCH_DATE_TO_USE = curr_BATCH_DATE_TO_USE # set the internal BATCH_DATE_TO_USE which is used to determine the .csv and .h5 export names\n",
    "# self.BATCH_DATE_TO_USE = '2024-11-01_Apogee'\n",
    "\n",
    "\n",
    "# standalone save\n",
    "standalone_pkl_filename: str = f'{get_now_rounded_time_str()}{custom_suffix}_standalone_wcorr_ripple_shuffle_data_only_{wcorr_shuffles.n_completed_shuffles}.pkl' \n",
    "standalone_pkl_filepath = a_curr_active_pipeline.get_output_path().joinpath(standalone_pkl_filename).resolve() # Path(\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-08_14-26-15\\output\\2024-05-30_0925AM_standalone_wcorr_ripple_shuffle_data_only_1100.pkl\")\n",
    "print(f'saving to \"{standalone_pkl_filepath}\"...')\n",
    "wcorr_shuffles.save_data(standalone_pkl_filepath)\n",
    "## INPUTS: wcorr_ripple_shuffle\n",
    "standalone_mat_filename: str = f'{get_now_rounded_time_str()}{custom_suffix}_standalone_all_shuffles_wcorr_array.mat' \n",
    "standalone_mat_filepath = a_curr_active_pipeline.get_output_path().joinpath(standalone_mat_filename).resolve() # r\"W:\\Data\\KDIBA\\gor01\\one\\2006-6-09_1-22-43\\output\\2024-06-03_0400PM_standalone_all_shuffles_wcorr_array.mat\"\n",
    "wcorr_shuffles.save_data_mat(filepath=standalone_mat_filepath, **{'session': a_curr_active_pipeline.get_session_context().to_dict()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a406c9",
   "metadata": {},
   "source": [
    "### Call `compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _subfn_compute_complete_df_metrics\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "try:\n",
    "    if _across_session_results_extended_dict is not None:\n",
    "        pass\n",
    "    else:\n",
    "        _across_session_results_extended_dict = {}\n",
    "except NameError as err:\n",
    "    _across_session_results_extended_dict = {}\n",
    "    \n",
    "additional_session_context = None\n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        additional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "        print(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "except NameError as err:\n",
    "    additional_session_context = None\n",
    "    print(f'NO CUSTOM SUFFIX.')    \n",
    "\n",
    "ripple_decoding_time_bin_size_override = 0.058\n",
    "# ripple_decoding_time_bin_size_override = 0.025\n",
    "needs_recompute_heuristics = True\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict,\n",
    "                                                ripple_decoding_time_bin_size_override=ripple_decoding_time_bin_size_override,\n",
    "                                                laps_decoding_time_bin_size_override=None,\n",
    "                                                needs_recompute_heuristics=needs_recompute_heuristics, allow_append_to_session_h5_file=False, save_hdf=True, force_recompute_all_decoding=True, \n",
    "                                                )\n",
    "\n",
    "callback_outputs = _across_session_results_extended_dict['compute_and_export_decoders_epochs_decoding_and_evaluation_dfs_completion_function']\n",
    "ripple_decoding_time_bin_size_override = callback_outputs['ripple_decoding_time_bin_size_override']\n",
    "print(f'ripple_decoding_time_bin_size_override: {ripple_decoding_time_bin_size_override}')\n",
    "output_csv_paths = callback_outputs['output_csv_paths']\n",
    "print(f'output_csv_paths: {output_csv_paths}')\n",
    "output_hdf_paths = callback_outputs['output_hdf_paths']\n",
    "print(f'output_hdf_paths: {output_hdf_paths}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c82f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908055ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = '/home/halechr/FastData/collected_outputs/2024-11-22_Lab-kdiba_gor01_one_2006-6-12_15-55-31__withNormalComputedReplays-qclu_[1, 2]-frateThresh_5.0-(ripple_all_scores_merged_df)_tbin-0.025.csv'\n",
    "csv_path = '/home/halechr/FastData/collected_outputs/2024-11-22_Lab-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0-(ripple_all_scores_merged_df)_tbin-0.016.csv'\n",
    "test_df = pd.read_csv(csv_path)\n",
    "test_df\n",
    "\n",
    "\n",
    "test_df[np.logical_not(test_df['short_best_jump'].isnull())]['short_best_jump']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9868d14",
   "metadata": {},
   "source": [
    "### Call `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import _subfn_compute_complete_df_metrics\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "return_full_decoding_results: bool = True\n",
    "save_hdf: bool = False\n",
    "save_csvs:bool = True\n",
    "# _across_session_results_extended_dict = {}\n",
    "\n",
    "try:\n",
    "    if _across_session_results_extended_dict is not None:\n",
    "        pass\n",
    "    else:\n",
    "        _across_session_results_extended_dict = {}\n",
    "except NameError as err:\n",
    "    _across_session_results_extended_dict = {}\n",
    "    \n",
    "additional_session_context = None\n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        additional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "        print(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "except NameError as err:\n",
    "    additional_session_context = None\n",
    "    print(f'NO CUSTOM SUFFIX.')    \n",
    "\n",
    "# %pdb on\n",
    "## Combine the output of `perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function` into two dataframes for the laps, one per-epoch and one per-time-bin\n",
    "# desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.5, num=10)\n",
    "# desired_shared_decoding_time_bin_sizes = np.linspace(start=0.005, stop=0.03, num=10)\n",
    "# _across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(a_dummy, None,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\tcurr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\tacross_session_results_extended_dict=_across_session_results_extended_dict, save_hdf=save_hdf, return_full_decoding_results=return_full_decoding_results,\n",
    "#                                                 desired_shared_decoding_time_bin_sizes=desired_shared_decoding_time_bin_sizes,\n",
    "#                                                 )\n",
    "\n",
    "\n",
    "# desired_laps_decoding_time_bin_size = [None] # doesn't work\n",
    "# desired_laps_decoding_time_bin_size = [1.5] # large so it doesn't take long\n",
    "# desired_ripple_decoding_time_bin_size = [0.010, 0.020]\n",
    "# desired_ripple_decoding_time_bin_size = [0.010, 0.020, 0.025]\n",
    "\n",
    "# desired_shared_decoding_time_bin_sizes = np.array([0.025, 0.030, 0.044, 0.050, 0.058, 0.072, 0.086, 0.100])\n",
    "# desired_shared_decoding_time_bin_sizes = np.array([0.025, 0.030, 0.044, 0.050, 0.058,])\n",
    "# desired_shared_decoding_time_bin_sizes = np.array([0.010, 0.025, 0.058,])\n",
    "# desired_shared_decoding_time_bin_sizes = np.array([0.025, 0.058,])\n",
    "desired_shared_decoding_time_bin_sizes = np.array([0.058,])\n",
    "# custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_laps_decoding_time_bin_size=desired_laps_decoding_time_bin_size,\n",
    "#                                                                                 desired_ripple_decoding_time_bin_size=desired_ripple_decoding_time_bin_size,\n",
    "#                                                                         use_single_time_bin_per_epoch=[False],\n",
    "#                                                                         minimum_event_duration=[desired_ripple_decoding_time_bin_size[-1]])\n",
    "\n",
    "# Shared time bin sizes\n",
    "custom_all_param_sweep_options, param_sweep_option_n_values = parameter_sweeps(desired_shared_decoding_time_bin_size=desired_shared_decoding_time_bin_sizes, use_single_time_bin_per_epoch=[False], minimum_event_duration=[desired_shared_decoding_time_bin_sizes[-1]]) # with Ripples\n",
    "\n",
    "\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict, save_hdf=save_hdf, save_csvs=save_csvs, return_full_decoding_results=return_full_decoding_results,\n",
    "                                                # desired_shared_decoding_time_bin_sizes = np.linspace(start=0.030, stop=0.5, num=4),\n",
    "                                                custom_all_param_sweep_options=custom_all_param_sweep_options, # directly provide the parameter sweeps\n",
    "                                                # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                additional_session_context = None,\n",
    "                                                )\n",
    "\n",
    "\n",
    "if return_full_decoding_results:\n",
    "    # with `return_full_decoding_results == True`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_full_directional_merged_decoders_result, output_directional_decoders_epochs_decode_results_dict, output_saved_individual_sweep_files_dict = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    # validate the result:\n",
    "    {k:v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size for k,v in output_full_directional_merged_decoders_result.items()}\n",
    "    # assert np.all([np.isclose(dict(k)['desired_shared_decoding_time_bin_size'], v.all_directional_laps_filter_epochs_decoder_result.decoding_time_bin_size) for k,v in output_full_directional_merged_decoders_result.items()]), f\"the desired time_bin_size in the parameters should match the one used that will appear in the decoded result\"\n",
    "\n",
    "else:\n",
    "    # with `return_full_decoding_results == False`\n",
    "    out_path, output_laps_decoding_accuracy_results_df, output_extracted_result_tuples, combined_multi_timebin_outputs_tuple, output_saved_individual_sweep_files_dict = _across_session_results_extended_dict['perform_sweep_decoding_time_bin_sizes_marginals_dfs_completion_function']\n",
    "    output_full_directional_merged_decoders_result = None\n",
    "\n",
    "\n",
    "(several_time_bin_sizes_laps_df, laps_out_path, several_time_bin_sizes_time_bin_laps_df, laps_time_bin_marginals_out_path), (several_time_bin_sizes_ripple_df, ripple_out_path, several_time_bin_sizes_time_bin_ripple_df, ripple_time_bin_marginals_out_path) = combined_multi_timebin_outputs_tuple\n",
    "\n",
    "#  exported files: {'laps_out_path': WindowsPath('K:/scratch/collected_outputs/2024-09-27-kdiba_gor01_two_2006-6-07_16-40-19_None-(laps_marginals_df).csv'), 'laps_time_bin_marginals_out_path': WindowsPath('K:/scratch/collected_outputs/2024-09-27-kdiba_gor01_two_2006-6-07_16-40-19_None-(laps_time_bin_marginals_df).csv'), 'ripple_out_path': WindowsPath('K:/scratch/collected_outputs/2024-09-27-kdiba_gor01_two_2006-6-07_16-40-19_None-(ripple_marginals_df).csv'), 'ripple_time_bin_marginals_out_path': WindowsPath('K:/scratch/collected_outputs/2024-09-27-kdiba_gor01_two_2006-6-07_16-40-19_None-(ripple_time_bin_marginals_df).csv')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc64a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_saved_individual_sweep_files_dict\n",
    "# combined_multi_timebin_outputs_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.result_context import DisplaySpecifyingIdentifyingContext, CollisionOutcome\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Computation import PipelineWithComputedPipelineStageMixin\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Computation import session_context_filename_formatting_fn\n",
    "from pyphocorehelpers.print_helpers import get_now_day_str, get_now_rounded_time_str\n",
    "\n",
    "curr_active_pipeline.session_name\n",
    "# complete_session_context, (curr_session_context,  additional_session_context) = curr_active_pipeline.get_complete_session_context(parts_separator='_')\n",
    "\n",
    "# complete_session_context.get_description(separator='|') # 'kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays_qclu_[1, 2, 4, 6, 7, 9]_frateThresh_5.0'\n",
    "# complete_session_context.get_specific_purpose_description(specific_purpose='filename_formatting') # 'kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays_qclu_[1, 2, 4, 6, 7, 9]_frateThresh_5.0'\n",
    "# curr_active_pipeline.get_complete_session_identifier_string(parts_separator='_', sub_parts_separator='|') # 'kdiba-gor01-one-2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0'\n",
    "curr_active_pipeline.get_complete_session_identifier_string(parts_separator='_', custom_parameter_keyvalue_parts_separator='-', session_identity_parts_separator='_')\n",
    "\n",
    "# \"kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_1.0\"\n",
    "\n",
    "out_path, out_filename, out_basename = curr_active_pipeline.build_complete_session_identifier_filename_string(output_date_str=None, data_identifier_str=\"(ripple_WCorrShuffle_df)\", parent_output_path=None, out_extension='.csv', extra_parts=None, ensure_no_duplicate_parts=False)\n",
    "out_filename # '2024-11-19_0148AM-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0-(ripple_WCorrShuffle_df).csv'\n",
    "# out_path, out_filename, out_basename = curr_active_pipeline.build_complete_session_identifier_filename_string(output_date_str=get_now_rounded_time_str(), data_identifier_str=\"(ripple_WCorrShuffle_df)\", parent_output_path=None, out_extension='.csv', extra_parts=None, ensure_no_duplicate_parts=False)\n",
    "# out_path, out_filename, out_basename = curr_active_pipeline.build_complete_session_identifier_filename_string(data_identifier_str=\"(ripple_WCorrShuffle_df)\", parent_output_path=None, out_extension='.csv', extra_parts=None, ensure_no_duplicate_parts=True)\n",
    "out_path, out_filename, out_basename = curr_active_pipeline.build_complete_session_identifier_filename_string(data_identifier_str=\"(ripple_WCorrShuffle_df)\", parent_output_path=None, out_extension='.csv', suffix_string='_tbin-0.025')\n",
    "out_filename  # '2024-11-19_0148AM-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0-(ripple_WCorrShuffle_df)_tbin-0.025.csv'\n",
    "\n",
    "# \"2024-11-18_1020PM-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_1.0-(ripple_WCorrShuffle_df)_tbin-0.025.csv\"\n",
    "# \"2024-11-19_0125AM-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_5.0-(ripple_WCorrShuffle_df).csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_session_context.get_raw_identifying_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_session_context\n",
    "\n",
    "# ['format_name', 'animal', 'exper_name', 'session_name']\n",
    "curr_session_context.get_description()\n",
    "curr_session_context.get_specific_purpose_description(specific_purpose='filename_formatting')\n",
    "\n",
    "curr_session_context.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ac92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_test_complete_session_context: DisplaySpecifyingIdentifyingContext = DisplaySpecifyingIdentifyingContext.init_from_context(a_context=curr_active_pipeline.get_session_context(), \n",
    "        specific_purpose_display_dict={'filename_formatting': session_context_filename_formatting_fn,},\n",
    "        #  display_dict={'epochs_source': lambda k, v: to_filename_conversion_dict[v],\n",
    "        #         'included_qclu_values': lambda k, v: f\"qclu_{v}\",\n",
    "        #         'minimum_inclusion_fr_Hz': lambda k, v: f\"frateThresh_{v:.1f}\",\n",
    "        # },\n",
    "    )\n",
    "_test_complete_session_context\n",
    "_test_complete_session_context.get_description()\n",
    "_test_complete_session_context.get_specific_purpose_description(specific_purpose='filename_formatting')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "additional_session_context.get_description()\n",
    "additional_session_context.get_specific_purpose_description(specific_purpose='filename_formatting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _test_complete_session_context: DisplaySpecifyingIdentifyingContext = curr_session_context.adding_context(collision_prefix='_additional', strategy=CollisionOutcome.FAIL_IF_DIFFERENT, **additional_session_context.to_dict())\n",
    "_test_complete_session_context: DisplaySpecifyingIdentifyingContext = curr_session_context.adding_context(collision_prefix='_additional', strategy=CollisionOutcome.FAIL_IF_DIFFERENT, **additional_session_context.get_raw_identifying_context().to_dict())\n",
    "\n",
    "\n",
    "_test_complete_session_context\n",
    "_test_complete_session_context.get_description()\n",
    "_test_complete_session_context.get_specific_purpose_description(specific_purpose='filename_formatting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_complete_session_identifier_string(parts_separator='_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500bfd4",
   "metadata": {},
   "source": [
    "### Call `compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb60e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.result_context import DisplaySpecifyingIdentifyingContext, IdentifyingContext\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.Computation import PipelineWithComputedPipelineStageMixin\n",
    "\n",
    "complete_session_context, (session_context, additional_session_context) = curr_active_pipeline.get_complete_session_context()\n",
    "session_context\n",
    "additional_session_context\n",
    "complete_session_context\n",
    "\n",
    "\n",
    "session_context.get_description()\n",
    "additional_session_context.get_description()\n",
    "complete_session_context.get_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_session_context.get_specific_purpose_description(specific_purpose='filename_formatting') # additional_session_context.get_specific_purpose_description(specific_purpose='filename_formatting')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76164cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_session_context.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e58214",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_session_context.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c303453",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_session_context.get_specific_purpose_description(specific_purpose='filename_formatting') # '-_withNormalComputedReplays-frateThresh_5.0-qclu_[1, 2, 4, 6, 7, 9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9185d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_session_context.get_description() # 'kdiba_gor01_two_2006-6-12_16-53-46__withNormalComputedReplays_qclu_[1, 2, 4, 6, 7, 9]_frateThresh_5.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_context = complete_session_context\n",
    "active_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_context.get_description()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497465d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "curr_active_pipeline.reload_default_computation_functions()\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "# return_full_decoding_results: bool = True\n",
    "# save_hdf: bool = True\n",
    "# save_csvs:bool = True\n",
    "\n",
    "try:\n",
    "    _across_session_results_extended_dict\n",
    "except NameError as e:\n",
    "    _across_session_results_extended_dict = {} # initialize\n",
    "\n",
    "additional_session_context = None\n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        additional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "        print(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "except NameError as err:\n",
    "    additional_session_context = None\n",
    "    print(f'NO CUSTOM SUFFIX.')    \n",
    "    \n",
    "rank_order_results = curr_active_pipeline.global_computation_results.computed_data.get('RankOrder', None)\n",
    "if rank_order_results is not None:\n",
    "    minimum_inclusion_fr_Hz: float = rank_order_results.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = rank_order_results.included_qclu_values\n",
    "else:        \n",
    "    ## get from parameters:\n",
    "    minimum_inclusion_fr_Hz: float = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.minimum_inclusion_fr_Hz\n",
    "    included_qclu_values: List[int] = curr_active_pipeline.global_computation_results.computation_config.rank_order_shuffle_analysis.included_qclu_values\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict, included_qclu_values=included_qclu_values, minimum_inclusion_fr_Hz=minimum_inclusion_fr_Hz, drop_previous_result_and_compute_fresh=True, num_wcorr_shuffles=1024,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb49fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function' in _across_session_results_extended_dict\n",
    "compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function_output = deepcopy(_across_session_results_extended_dict['compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function'])\n",
    "# compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function_output\n",
    "callback_outputs = deepcopy(_across_session_results_extended_dict['compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function'])\n",
    "\n",
    "custom_suffix = callback_outputs['custom_suffix']\n",
    "replay_epoch_variations = callback_outputs['replay_epoch_variations']\n",
    "replay_epoch_outputs = callback_outputs['replay_epoch_outputs']\n",
    "\n",
    "replay_epoch_name = 'normal_computed'\n",
    "a_replay_epoch_variation: Epoch = replay_epoch_variations[replay_epoch_name]\n",
    "a_replay_epoch_outputs = replay_epoch_outputs[replay_epoch_name]\n",
    "\n",
    "## Unpack `a_replay_epoch_outputs`\n",
    "exported_evt_file_path = a_replay_epoch_outputs['exported_evt_file_path']\n",
    "did_change = a_replay_epoch_outputs['did_change']\n",
    "custom_save_filenames = a_replay_epoch_outputs['custom_save_filenames']\n",
    "custom_save_filepaths = a_replay_epoch_outputs['custom_save_filepaths']\n",
    "custom_suffix = a_replay_epoch_outputs['custom_suffix']\n",
    "wcorr_ripple_shuffle_all_df = a_replay_epoch_outputs['wcorr_ripple_shuffle_all_df']\n",
    "all_shuffles_only_best_decoder_wcorr_df = a_replay_epoch_outputs['all_shuffles_only_best_decoder_wcorr_df']\n",
    "standalone_pkl_filepath = a_replay_epoch_outputs['standalone_pkl_filepath']\n",
    "standalone_mat_filepath = a_replay_epoch_outputs['standalone_mat_filepath']\n",
    "active_context = a_replay_epoch_outputs['active_context']\n",
    "export_files_dict = a_replay_epoch_outputs['export_files_dict']\n",
    "# params_description_str = a_replay_epoch_outputs['params_description_str']\n",
    "# footer_annotation_text = a_replay_epoch_outputs['footer_annotation_text']\n",
    "# out_hist_fig_result = a_replay_epoch_outputs['out_hist_fig_result']\n",
    "\n",
    "\n",
    "custom_save_filenames\n",
    "custom_save_filepaths\n",
    "export_files_dict\n",
    "# print_keys_if_possible('callback_outputs', compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function_output, max_depth=3)\n",
    "# a_replay_epoch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strs = []\n",
    "for k, v in a_replay_epoch_outputs.items():\n",
    "    code_strs.append(f\"{k} = a_replay_epoch_outputs['{k}']\")\n",
    "\n",
    "print('\\n'.join(code_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function_output['export_files_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8beeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_epoch_outputs = deepcopy(compute_and_export_session_alternative_replay_wcorr_shuffles_completion_function_output['replay_epoch_outputs'])\n",
    "# replay_epoch_outputs\n",
    "\n",
    "normal_computed_replay_epoch_outputs = replay_epoch_outputs['normal_computed']\n",
    "normal_computed_replay_epoch_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(replay_epoch_outputs.keys())\n",
    "\n",
    "export_files_dict = normal_computed_replay_epoch_outputs['export_files_dict']\n",
    "export_files_dict\n",
    "\n",
    "export_file_path_ripple_WCorrShuffle_df = export_files_dict['ripple_WCorrShuffle_df'] # 'W:/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/2024-11-18_1020PM-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_1.0-(ripple_WCorrShuffle_df)_tbin-0.025.csv'\n",
    "export_file_path_ripple_WCorrShuffle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df882f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_save_filepaths = normal_computed_replay_epoch_outputs['custom_save_filepaths'] ## these seem to be misnamed AND redundant\n",
    "custom_save_filepaths\n",
    "\n",
    "ripple_csv_out_path = custom_save_filepaths['ripple_csv_out_path'] # 'K:/scratch/collected_outputs/2024-11-18-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_1.0_withNormalComputedReplays-frateThresh_1.0-qclu_[1, 2, 4, 6, 7, 9]-(ripple_marginals_df).csv'\n",
    "ripple_csv_out_path\n",
    "\n",
    "ripple_csv_time_bin_marginals = custom_save_filepaths['ripple_csv_time_bin_marginals'] # 'K:/scratch/collected_outputs/2024-11-18-kdiba_gor01_one_2006-6-09_1-22-43__withNormalComputedReplays-qclu_[1, 2, 4, 6, 7, 9]-frateThresh_1.0_withNormalComputedReplays-frateThresh_1.0-qclu_[1, 2, 4, 6, 7, 9]-(ripple_time_bin_marginals_df).csv'\n",
    "ripple_csv_time_bin_marginals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020b4fd",
   "metadata": {},
   "source": [
    "### Call `compute_and_export_session_trial_by_trial_performance_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import DecodedFilterEpochsResult\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_session_trial_by_trial_performance_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "## Settings:\n",
    "return_full_decoding_results: bool = True\n",
    "save_hdf: bool = True\n",
    "save_csvs:bool = True\n",
    "_across_session_results_extended_dict = {}\n",
    "\n",
    "additional_session_context = None\n",
    "try:\n",
    "    if custom_suffix is not None:\n",
    "        additional_session_context = IdentifyingContext(custom_suffix=custom_suffix)\n",
    "        print(f'Using custom suffix: \"{custom_suffix}\" - additional_session_context: \"{additional_session_context}\"')\n",
    "except NameError as err:\n",
    "    additional_session_context = None\n",
    "    print(f'NO CUSTOM SUFFIX.')    \n",
    "    \n",
    "active_laps_decoding_time_bin_size: float = 0.025\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_session_trial_by_trial_performance_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict, active_laps_decoding_time_bin_size=active_laps_decoding_time_bin_size,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364eca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_outputs = _across_session_results_extended_dict['compute_and_export_session_trial_by_trial_performance_completion_function']\n",
    "a_trial_by_trial_result: TrialByTrialActivityResult = callback_outputs['a_trial_by_trial_result']\n",
    "subset_neuron_IDs_dict = callback_outputs['subset_neuron_IDs_dict']\n",
    "subset_decode_results_dict = callback_outputs['subset_decode_results_dict']\n",
    "subset_decode_results_track_id_correct_performance_dict = callback_outputs['subset_decode_results_track_id_correct_performance_dict']\n",
    "directional_active_lap_pf_results_dicts: Dict[types.DecoderName, TrialByTrialActivity] = a_trial_by_trial_result.directional_active_lap_pf_results_dicts\n",
    "_out_subset_decode_results_track_id_correct_performance_dict = callback_outputs['subset_decode_results_track_id_correct_performance_dict']\n",
    "_out_subset_decode_results_dict = callback_outputs['subset_decode_results_dict']\n",
    "(complete_decoded_context_correctness_tuple, laps_marginals_df, all_directional_pf1D_Decoder, all_test_epochs_df, all_directional_laps_filter_epochs_decoder_result, _out_separate_decoder_results)  = _out_subset_decode_results_dict['any_decoder'] ## get the result for all cells\n",
    "filtered_laps_time_bin_marginals_df: pd.DataFrame = callback_outputs['subset_decode_results_time_bin_marginals_df_dict']['filtered_laps_time_bin_marginals_df']\n",
    "# active_results: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy({k:v.decoder_result for k, v in _out_separate_decoder_results[0].items()})\n",
    "active_results: Dict[types.DecoderName, DecodedFilterEpochsResult] = deepcopy({k:v for k, v in _out_separate_decoder_results[1].items()})\n",
    "filtered_laps_time_bin_marginals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07515974",
   "metadata": {},
   "source": [
    "### Call `compute_and_export_cell_first_spikes_characteristics_completion_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a69538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_cell_first_spikes_characteristics_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "try:\n",
    "    _across_session_results_extended_dict\n",
    "except NameError as e:\n",
    "    _across_session_results_extended_dict = {} # initialize\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_cell_first_spikes_characteristics_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import figures_plot_cell_first_spikes_characteristics_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "try:\n",
    "    _across_session_results_extended_dict\n",
    "except NameError as e:\n",
    "    _across_session_results_extended_dict = {} # initialize\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | figures_plot_cell_first_spikes_characteristics_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None),\n",
    "                                                later_appearing_cell_lap_start_id=4,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5028b3f",
   "metadata": {},
   "source": [
    "# 2024-10-08 - Older Cell Inclusion/Exclusion Properties:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d76ae0",
   "metadata": {},
   "source": [
    "## 2023-12-18 - Simpily detect bimodal cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.mixins.peak_location_representing import ContinuousPeakLocationRepresentingMixin\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from scipy.signal import find_peaks\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative\n",
    "\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', sortby=None)\n",
    "\n",
    "# active_ratemap = deepcopy(long_pf1D.ratemap)\n",
    "active_ratemap: Ratemap = deepcopy(long_LR_pf1D.ratemap)\n",
    "peaks_dict, aclu_n_peaks_dict, peaks_results_df = active_ratemap.compute_tuning_curve_modes(height=0.2, width=None)\n",
    "\n",
    "## INPUTS: track_templates\n",
    "included_columns = ['pos', 'peak_heights'] # the columns of interest that you want in the final dataframe.\n",
    "included_columns_renamed = dict(zip(included_columns, ['peak', 'peak_height']))\n",
    "decoder_peaks_results_dfs = [a_decoder.pf.ratemap.get_tuning_curve_peak_df(height=0.2, width=None) for a_decoder in (track_templates.long_LR_decoder, track_templates.long_RL_decoder, track_templates.short_LR_decoder, track_templates.short_RL_decoder)]\n",
    "prefix_names = [f'{a_decoder_name}_' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "all_included_columns = ['aclu', 'series_idx', 'subpeak_idx'] + included_columns # Used to filter out the unwanted columns from the output\n",
    "\n",
    "# [['aclu', 'series_idx', 'subpeak_idx', 'pos']]\n",
    "\n",
    "# rename_list_fn = lambda a_prefix: {'pos': f\"{a_prefix}pos\"}\n",
    "rename_list_fn = lambda a_prefix: {a_col_name:f\"{a_prefix}{included_columns_renamed[a_col_name]}\" for a_col_name in included_columns}\n",
    "\n",
    "# column_names = [f'{a_decoder_name}_peak' for a_decoder_name in track_templates.get_decoder_names()]\n",
    "\n",
    "# dataFrames = decoder_peaks_results_dfs\n",
    "# names = self.get_decoder_names()\n",
    "\n",
    "# rename 'pos' column in each dataframe and then reduce to perform cumulative outer merge\n",
    "result_df = decoder_peaks_results_dfs[0][all_included_columns].rename(columns=rename_list_fn(prefix_names[0]))\n",
    "for df, a_prefix in zip(decoder_peaks_results_dfs[1:], prefix_names[1:]):\n",
    "    result_df = pd.merge(result_df, df[all_included_columns].rename(columns=rename_list_fn(a_prefix)), on=['aclu', 'series_idx', 'subpeak_idx'], how='outer')\n",
    "\n",
    "# result = reorder_columns(result, column_name_desired_index_dict=dict(zip(['Long_LR_evidence', 'Long_RL_evidence', 'Short_LR_evidence', 'Short_RL_evidence'], np.arange(4)+4)))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "# list(filter(lambda column: column.endswith('_peak_heights'), result.columns))\n",
    "# result_df = reorder_columns(result_df, column_name_desired_index_dict=dict(zip(list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), np.arange(len(result_df.columns)-4, len(result_df.columns)))))\n",
    "# result_df\n",
    "\n",
    "# print(list(result.columns))\n",
    "\n",
    "## Move the \"height\" columns to the end\n",
    "result_df: pd.DataFrame = reorder_columns_relative(result_df, column_names=list(filter(lambda column: column.endswith('_peak_heights'), result_df.columns)), relative_mode='end').sort_values(['aclu', 'series_idx', 'subpeak_idx']).reset_index(drop=True)\n",
    "result_df\n",
    "\n",
    "decoder_peaks_dict_dict, decoder_aclu_n_peaks_dict_dict, decoder_peaks_results_df_dict = track_templates.get_decoders_tuning_curve_modes()\n",
    "\n",
    "# decoder_peaks_results_df_dict\n",
    "# decoder_peaks_dict_dict\n",
    "## OUTPUTS: result_df, decoder_peaks_dict_dict, decoder_aclu_n_peaks_dict_dict, decoder_peaks_results_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ad574",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aclu = 51\n",
    "\n",
    "{k:v[test_aclu] for k, v in decoder_aclu_n_peaks_dict_dict.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0496848",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclu_n_peaks_dict: Dict = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index().set_index('aclu').to_dict()['subpeak_idx_count'] # number of peaks (\"models\" for each aclu)\n",
    "aclu_n_peaks_dict\n",
    "\n",
    "# peaks_results_df = peaks_results_df.groupby(['aclu']).agg(subpeak_idx_count=('subpeak_idx', 'count')).reset_index()\n",
    "\n",
    "# peaks_results_df[peaks_results_df.aclu == 5]\n",
    "# peaks_results_df.aclu.value_counts()\n",
    "\n",
    "aclu_n_peaks_dict[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ratemap.n_neurons\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=active_ratemap.neuron_ids, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aclu_n_peaks_dict\n",
    "unimodal_only_aclus = np.array(list(unimodal_peaks_dict.keys()))\n",
    "unimodal_only_aclus\n",
    "curr_active_pipeline.display('_display_1d_placefields', 'maze1_any', included_unit_neuron_IDs=unimodal_only_aclus, sortby=np.arange(active_ratemap.n_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e8df7",
   "metadata": {},
   "source": [
    "## 2024-01-31 - Reinvestigation regarding remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## long_short_endcap_analysis:\n",
    "truncation_checking_result: TruncationCheckingResults = curr_active_pipeline.global_computation_results.computed_data.long_short_endcap\n",
    "truncation_checking_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577587f3",
   "metadata": {},
   "source": [
    "### From Jonathan Long/Short Peaks\n",
    "\n",
    "adds `active_peak_prominence_2d_results` to existing `neuron_replay_stats_df` from `jonathan_firing_rate_analysis_result`, adding the `['long_pf2D_peak_x', 'long_pf2D_peak_y'] + ['short_pf2D_peak_x', 'short_pf2D_peak_y']` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f145b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "jonathan_firing_rate_analysis_result: JonathanFiringRateAnalysisResult = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "neuron_replay_stats_df: pd.DataFrame = deepcopy(jonathan_firing_rate_analysis_result.neuron_replay_stats_df)\n",
    "neuron_replay_stats_df, all_modified_columns = jonathan_firing_rate_analysis_result.add_peak_promenance_pf_peaks(curr_active_pipeline=curr_active_pipeline, track_templates=track_templates)\n",
    "neuron_replay_stats_df, all_modified_columns = jonathan_firing_rate_analysis_result.add_directional_pf_maximum_peaks(track_templates=track_templates)\n",
    "both_included_neuron_stats_df = deepcopy(neuron_replay_stats_df[neuron_replay_stats_df['LS_pf_peak_x_diff'].notnull()]).drop(columns=['track_membership', 'neuron_type'])\n",
    "neuron_replay_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4860cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jonathan_firing_rate_analysis_result) # pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations.JonathanFiringRateAnalysisResult\n",
    "\n",
    "rdf_df: pd.DataFrame = deepcopy(jonathan_firing_rate_analysis_result.rdf.rdf)\n",
    "rdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b198216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to JSON\n",
    "output_path = Path(f'output/{get_now_day_str()}_rdf_df.json').resolve()\n",
    "rdf_df.to_json(output_path, orient='records', lines=True) ## This actually looks pretty good!\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to JSON\n",
    "output_path = Path(f'output/{get_now_day_str()}_neuron_replay_stats_df.json').resolve()\n",
    "neuron_replay_stats_df.to_json(output_path, orient='records', lines=True) ## This actually looks pretty good!\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c12f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_columns = ['start', 'end']\n",
    "invalid_columns = ['active_aclus', 'is_neuron_active', 'firing_rates']\n",
    "invalid_df_subset = rdf_df[join_columns + invalid_columns]\n",
    "invalid_df_subset\n",
    "\n",
    "# Reload DataFrame from JSON\n",
    "df_read: pd.DataFrame = pd.read_json(output_path, orient='records', lines=True)\n",
    "df_read\n",
    "\n",
    "# rdf_df.convert_dtypes().dtypes\n",
    "# rdf_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3233f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_long_pf].to_numpy()\n",
    "short_pf_aclus = both_included_neuron_stats_df.aclu[both_included_neuron_stats_df.has_short_pf].to_numpy()\n",
    "\n",
    "long_pf_aclus, short_pf_aclus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a35fdc",
   "metadata": {},
   "source": [
    "## 2024-04-09 - Maximum peaks only for each template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import NumpyHelpers\n",
    "from neuropy.utils.indexing_helpers import intersection_of_arrays, union_of_arrays\n",
    "from neuropy.utils.indexing_helpers import unwrap_single_item\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing import NewType\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "DecoderName = NewType('DecoderName', str)\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "\n",
    "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _get_directional_pf_peaks_dfs\n",
    "\n",
    "# (LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = _get_directional_pf_peaks_dfs(track_templates, drop_aclu_if_missing_long_or_short=False)\n",
    "\n",
    "(LR_only_decoder_aclu_MAX_peak_maps_df, RL_only_decoder_aclu_MAX_peak_maps_df), AnyDir_decoder_aclu_MAX_peak_maps_df = track_templates.get_directional_pf_maximum_peaks_dfs(drop_aclu_if_missing_long_or_short=False)\n",
    "\n",
    "\n",
    "AnyDir_decoder_aclu_MAX_peak_maps_df\n",
    "# LR_only_decoder_aclu_MAX_peak_maps_df\n",
    "# RL_only_decoder_aclu_MAX_peak_maps_df\n",
    "\n",
    "long_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['long_LR'].to_numpy()\n",
    "short_peak_x = LR_only_decoder_aclu_MAX_peak_maps_df['short_LR'].to_numpy()\n",
    "peak_x_diff = LR_only_decoder_aclu_MAX_peak_maps_df['peak_diff'].to_numpy()\n",
    "# decoder_aclu_peak_maps_dict\n",
    "\n",
    "## OUTPUTS: AnyDir_decoder_aclu_MAX_peak_maps_df,\n",
    "## OUTPUTS: LR_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n",
    "## OUTPUTS: RL_only_decoder_aclu_MAX_peak_maps_df, long_peak_x, long_peak_x, peak_x_diff\n",
    "\n",
    "AnyDir_decoder_aclu_MAX_peak_maps_df\n",
    "LR_only_decoder_aclu_MAX_peak_maps_df\n",
    "RL_only_decoder_aclu_MAX_peak_maps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_filtered_flat_peaks_df: pd.DataFrame = deepcopy(AnyDir_decoder_aclu_MAX_peak_maps_df).reset_index(drop=False, names=['aclu'])\n",
    "a_filtered_flat_peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ed3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_peak_prominence_2d_results.filtered_flat_peaks_df\n",
    "\n",
    "binned_peak_columns = ['peak_center_binned_x', 'peak_center_binned_y']\n",
    "continuous_peak_columns = ['peak_center_x', 'peak_center_y']\n",
    "\n",
    "['peak_prominence', 'peak_relative_height', 'slice_level_multiplier']\n",
    "\n",
    "['neuron_id', 'neuron_peak_firing_rate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cd12d",
   "metadata": {},
   "source": [
    "## 2024-02-08 - Filter to find only the clear remap examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.reliability import TrialByTrialActivity\n",
    "from pyphocorehelpers.indexing_helpers import dict_to_full_array\n",
    "\n",
    "any_decoder_neuron_IDs = deepcopy(track_templates.any_decoder_neuron_IDs)\n",
    "any_decoder_neuron_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e4d30",
   "metadata": {},
   "source": [
    "### Get num peaks exclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8125da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS: `directional_active_lap_pf_results_dicts`, not sure why\n",
    "\n",
    "neuron_ids_dict = {k:v.neuron_ids for k,v in directional_active_lap_pf_results_dicts.items()}\n",
    "neuron_ids_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7a4ee",
   "metadata": {},
   "source": [
    "# 2024-10-09 - Testing getting number of cells per time bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo2D_decoder_continuously_decoded_result.laps_time_bin_marginals_df\n",
    "\n",
    "# pseudo2D_decoder.laps_time_bin_marginals_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f54cdc",
   "metadata": {},
   "source": [
    "# 2024-10-16 - Step-by-step neural decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cddf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BayesianPlacemapPositionDecoder\n",
    "\n",
    "# all_directional_pf1D_Decoder\n",
    "a_pf1D_decoder: BayesianPlacemapPositionDecoder = deepcopy(all_directional_pf1D_Decoder_dict['long_LR'])\n",
    "a_pf1D: PfND = a_pf1D_decoder.pf\n",
    "a_pf1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75777da",
   "metadata": {},
   "source": [
    "# 2024-11-01 - Cell's first firing -- during PBE, theta, or resting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52559185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import CellsFirstSpikeTimes\n",
    "from pyphocorehelpers.indexing_helpers import partition_df_dict\n",
    "\n",
    "cells_first_spike_times: CellsFirstSpikeTimes = CellsFirstSpikeTimes.init_from_pipeline(curr_active_pipeline, hdf_save_parent_path=None, should_include_only_spikes_after_initial_laps=False)\n",
    "# cells_first_spike_times: CellsFirstSpikeTimes = CellsFirstSpikeTimes.init_from_pipeline(curr_active_pipeline, hdf_save_parent_path=None, should_include_only_spikes_after_initial_laps=True)\n",
    "cells_first_spike_times.all_cells_first_spike_time_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(cells_first_spike_times.all_cells_first_spike_time_df) == len(cells_first_spike_times.all_cells_first_spike_time_df['aclu'].unique()), f\"end result must have one entry for every unique aclu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells_first_spike_time_df: pd.DataFrame = deepcopy(cells_first_spike_times.all_cells_first_spike_time_df)\n",
    "all_cells_first_spike_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce61b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import add_time_indicator_lines\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import BatchPhoJonathanFiguresHelper\n",
    "\n",
    "# included_neuron_ids = cells_first_spike_times.all_cells_first_spike_time_df.aclu.unique()\n",
    "\n",
    "included_neuron_ids = [5, 6, 93]\n",
    "time_point_formatting_kwargs_dict = {'lap': dict(color='orange', alpha=0.8), 'PBE': dict(color='purple', alpha=0.8)}\n",
    "\n",
    "filtered_cells_first_spike_times: CellsFirstSpikeTimes = cells_first_spike_times.sliced_by_neuron_id(included_neuron_ids)\n",
    "later_lap_appearing_aclus_df = filtered_cells_first_spike_times.all_cells_first_spike_time_df ## find ones that appear only on later laps\n",
    "included_neuron_ids = later_lap_appearing_aclus_df['aclu'].to_numpy() ## get the aclus that only appear on later laps\n",
    "\n",
    "## plot each aclu in a separate figures\n",
    "active_out_figure_container_dict: Dict[IdentifyingContext, MatplotlibRenderPlots] = BatchPhoJonathanFiguresHelper.perform_run(curr_active_pipeline, shared_aclus=included_neuron_ids, n_max_page_rows=10, disable_top_row=True,\n",
    "                                                                                                                                progress_print=False, write_png=False, write_vector_format=False, # explicitly don't save here, because we need to add the indicator lines\n",
    "                                                                                                                            )\n",
    "## Inputs: later_lap_appearing_aclus_df\n",
    "later_lap_appearing_aclus_times_dict: Dict[types.aclu_index, Dict[str, float]] = {aclu_tuple.aclu:{'lap': aclu_tuple.first_spike_lap, 'PBE': aclu_tuple.first_spike_PBE} for aclu_tuple in later_lap_appearing_aclus_df.itertuples(index=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cells_first_spike_times.all_cells_first_spike_time_df[cells_first_spike_times.all_cells_first_spike_time_df['lap_spike_lap'] > 3]\n",
    "\n",
    "# cells_first_spike_times.all_cells_first_spike_time_df[cells_first_spike_times.all_cells_first_spike_time_df['lap_spike_lap'] > 3].aclu.unique()\n",
    "\n",
    "# cells_first_spike_times.all_cells_first_spike_time_df[cells_first_spike_times.all_cells_first_spike_time_df['lap_spike_lap'] > 4].aclu.unique()\n",
    "# cells_first_spike_times.all_cells_first_spike_time_df[cells_first_spike_times.all_cells_first_spike_time_df['lap_spike_lap'] > 2].aclu.unique()\n",
    "cells_first_spike_times.all_cells_first_spike_time_df[cells_first_spike_times.all_cells_first_spike_time_df['lap_spike_lap'] > 4].aclu.unique()\n",
    "\n",
    "\n",
    "# later_lap_appearing_aclus = [32, 33,34, 35, 62, 67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import CellsFirstSpikeTimes\n",
    "from pyphocorehelpers.DataStructure.RenderPlots.MatplotLibRenderPlots import MatplotlibRenderPlots\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import BatchPhoJonathanFiguresHelper\n",
    "\n",
    "later_lap_appearing_aclus = cells_first_spike_times.all_cells_first_spike_time_df[cells_first_spike_times.all_cells_first_spike_time_df['lap_spike_lap'] > 4]['aclu'].unique()\n",
    "# later_lap_appearing_aclus = [32, 33,34, 35, 62, 67]\n",
    "# later_lap_appearing_aclus = [62]\n",
    "filtered_cells_first_spike_times: CellsFirstSpikeTimes = cells_first_spike_times.sliced_by_neuron_id(later_lap_appearing_aclus)\n",
    "\n",
    "later_lap_appearing_aclus_df = filtered_cells_first_spike_times.all_cells_first_spike_time_df ## find ones that appear only on later laps\n",
    "# later_lap_appearing_aclus_df\n",
    "later_lap_appearing_aclus = later_lap_appearing_aclus_df['aclu'].to_numpy() ## get the aclus that only appear on later laps\n",
    "later_lap_appearing_aclus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(later_lap_appearing_aclus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03084d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "later_lap_appearing_figures_dict = filtered_cells_first_spike_times.plot_PhoJonathan_plots_with_time_indicator_lines(curr_active_pipeline, included_neuron_ids=later_lap_appearing_aclus, write_vector_format=True, write_png=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import compute_and_export_cell_first_spikes_characteristics_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "try:\n",
    "    _across_session_results_extended_dict\n",
    "except NameError as e:\n",
    "    _across_session_results_extended_dict = {} # initialize\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | compute_and_export_cell_first_spikes_characteristics_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None)\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc28a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import figures_plot_cell_first_spikes_characteristics_completion_function\n",
    "from pyphoplacecellanalysis.General.Batch.BatchJobCompletion.UserCompletionHelpers.batch_user_completion_helpers import SimpleBatchComputationDummy\n",
    "\n",
    "a_dummy = SimpleBatchComputationDummy(BATCH_DATE_TO_USE, collected_outputs_path, True)\n",
    "\n",
    "try:\n",
    "    _across_session_results_extended_dict\n",
    "except NameError as e:\n",
    "    _across_session_results_extended_dict = {} # initialize\n",
    "\n",
    "_across_session_results_extended_dict = _across_session_results_extended_dict | figures_plot_cell_first_spikes_characteristics_completion_function(a_dummy, None,\n",
    "                                                curr_session_context=curr_active_pipeline.get_session_context(), curr_session_basedir=curr_active_pipeline.sess.basepath.resolve(), curr_active_pipeline=curr_active_pipeline,\n",
    "                                                across_session_results_extended_dict=_across_session_results_extended_dict,\n",
    "                                                # # additional_session_context=additional_session_context,\n",
    "                                                # additional_session_context=IdentifyingContext(custom_suffix=None),\n",
    "                                                later_appearing_cell_lap_start_id=4,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5507122",
   "metadata": {},
   "outputs": [],
   "source": [
    "_across_session_results_extended_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_output_manager().figure_output_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b32bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_active_pipeline.get_complete_session_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "later_lap_appearing_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ee337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the PhoJonathanPlots\n",
    "separate_single_aclu_phojonathan_figures_dict: Dict[IdentifyingContext, MatplotlibRenderPlots] = BatchPhoJonathanFiguresHelper.perform_run(curr_active_pipeline, shared_aclus=track_templates.any_decoder_neuron_IDs, n_max_page_rows=1, disable_top_row=True,\n",
    "                                                                                                                               write_png=True, write_vector_format=True, # progress_print=True, \n",
    "                                                                                                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6253be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot each aclu in a separate figures\n",
    "later_lap_appearing_figures_dict: Dict[IdentifyingContext, MatplotlibRenderPlots] = BatchPhoJonathanFiguresHelper.perform_run(curr_active_pipeline, shared_aclus=later_lap_appearing_aclus, n_max_page_rows=1, disable_top_row=True,\n",
    "                                                                                                                            #    progress_print=True, write_png=True, write_vector_format=True,\n",
    "                                                                                                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the PhoJonathanPlots\n",
    "later_lap_appearing_figures_dict_single_fig: Dict[IdentifyingContext, MatplotlibRenderPlots] = BatchPhoJonathanFiguresHelper.perform_run(curr_active_pipeline, shared_aclus=later_lap_appearing_aclus, n_max_page_rows=10, disable_top_row=True)\n",
    "\n",
    "# later_lap_appearing_figures_dict: Dict[IdentifyingContext, MatplotlibRenderPlots] = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df, n_max_page_rows=10, included_unit_neuron_IDs=later_lap_appearing_aclus, disable_top_row=True, split_by_short_long_shared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_first_spike_times.global_spikes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91149c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import add_time_indicator_lines\n",
    "\n",
    "## Inputs: later_lap_appearing_aclus_df\n",
    "time_point_formatting_kwargs_dict = {'lap': dict(color='orange', alpha=0.8), 'PBE': dict(color='purple', alpha=0.8)}\n",
    "later_lap_appearing_aclus_times_dict: Dict[types.aclu_index, Dict[str, float]] = {aclu_tuple.aclu:{'lap': aclu_tuple.first_spike_lap, 'PBE': aclu_tuple.first_spike_PBE} for aclu_tuple in later_lap_appearing_aclus_df.itertuples(index=False)}\n",
    "\n",
    "# ## add the lines:\n",
    "add_time_indicator_lines(later_lap_appearing_figures_dict, later_lap_appearing_aclus_times_dict=later_lap_appearing_aclus_times_dict, time_point_formatting_kwargs_dict=time_point_formatting_kwargs_dict, defer_draw=False)\n",
    "# add_time_indicator_lines(later_lap_appearing_figures_dict_single_fig, later_lap_appearing_aclus_times_dict=later_lap_appearing_aclus_times_dict, time_point_formatting_kwargs_dict=time_point_formatting_kwargs_dict, defer_draw=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8361fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_temp_df = later_lap_appearing_aclus_df[later_lap_appearing_aclus_df['aclu'] == aclu][['first_spike_lap', 'first_spike_PBE']]\n",
    "lap_time_point = _temp_df['first_spike_lap'].to_numpy()[0]\n",
    "pbe_time_point = _temp_df['first_spike_PBE'].to_numpy()[0]\n",
    "time_point_dict = {'lap': lap_time_point, 'pbe': pbe_time_point}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cells_first_spike_times.plot_first_lap_spike_relative_first_PBE_spike_scatter_figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_spikes_aclu_ax_dict[aclu].remove(lap_first_spike_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724f19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361251b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b3d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cells_first_spike_times: CellsFirstSpikeTimes = CellsFirstSpikeTimes.init_from_pipeline(curr_active_pipeline, hdf_save_parent_path=collected_outputs_path, should_include_only_spikes_after_initial_laps=True)\n",
    "filtered_cells_first_spike_times.all_cells_first_spike_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cells_first_spike_times.global_spikes_df\n",
    "print(list(cells_first_spike_times.global_spikes_df.columns))\n",
    "\n",
    "['aclu', 'shank', 'cluster', 'qclu']\n",
    "['x', 'y', 'lin_pos', 'speed', 'traj', 'lap', 'theta_phase_radians', 'maze_relative_lap', 'maze_id'] ## lap properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c58918",
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_spikes_df = deepcopy(cells_first_spike_times.global_spikes_dict['lap'])\n",
    "\n",
    "\n",
    "laps_first_spikes_df = cells_first_spike_times._subfn_get_first_spikes(laps_spikes_df)\n",
    "laps_first_spikes_df\n",
    "later_lap_appearing_aclus_df = laps_first_spikes_df[laps_first_spikes_df['lap'] > 2] ## find ones that appear only on later laps\n",
    "later_lap_appearing_aclus = later_lap_appearing_aclus_df['aclu'].to_numpy() ## get the aclus that only appear on later laps\n",
    "later_lap_appearing_aclus # add this info to the first_spikes_df\n",
    "\n",
    "cells_first_spike_times\n",
    "\n",
    "\n",
    "## Find first lap spike time for each cell\n",
    "\n",
    "## See which lap that time occured on\n",
    "laps_spikes_df.all_cells_first_spike_time_df['first_appearing_lap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e424574",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_first_spike_times.all_cells_first_spike_time_df[''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get cell colors\n",
    "# active_config = deepcopy(curr_active_pipeline.config\n",
    "active_config.plotting_config.pf_neuron_identities, active_config.plotting_config.pf_sort_ind, active_config.plotting_config.pf_colors, active_config.plotting_config.pf_colormap, active_config.plotting_config.pf_listed_colormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12725565",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_config.plotting_config.pf_neuron_identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0db60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_config = deepcopy(curr_active_pipeline.active_configs[global_epoch_name])\n",
    "# active_pf1D = deepcopy(global_pf1D)\n",
    "\n",
    "## INPUTS: active_config\n",
    "# type(active_config.plotting_config.pf_colormap)\n",
    "aclu_to_color_map = {v.cell_uid:v.color.tolist() for v in active_config.plotting_config.pf_neuron_identities}\n",
    "cells_first_spike_times.all_cells_first_spike_time_df['color'] = cells_first_spike_times.all_cells_first_spike_time_df['aclu'].map(lambda x: aclu_to_color_map.get(x, [1.0, 1.0, 0.0, 1.0]))\n",
    "column_names = ['first_spike_any', 'first_spike_theta', 'first_spike_lap', 'first_spike_PBE']\n",
    "interpolated_position_column_names = []\n",
    "for a_col in column_names:\t\n",
    "    ## interpolate positions for each of these spike times\n",
    "    cells_first_spike_times.all_cells_first_spike_time_df[f'interp_pos_{a_col}'] = np.interp(cells_first_spike_times.all_cells_first_spike_time_df[a_col], cells_first_spike_times.global_position_df.t, cells_first_spike_times.global_position_df.x)\n",
    "    interpolated_position_column_names.append(f'interp_pos_{a_col}')\n",
    "\n",
    "column_to_interpolated_position_column_name_dict = dict(zip(column_names, interpolated_position_column_names))\n",
    "cells_first_spike_times.all_cells_first_spike_time_df\n",
    "# global_session.config.plotting_config\n",
    "active_config = deepcopy(curr_active_pipeline.active_configs[global_epoch_name])\n",
    "active_pf1D = deepcopy(global_pf1D)\n",
    "aclu_to_color_map = {v.cell_uid:v.color.tolist() for v in active_config.plotting_config.pf_neuron_identities}\n",
    "fig, ax = cells_first_spike_times.plot_first_spike_scatter_figure(aclu_to_color_map=aclu_to_color_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "app, win, plots, plots_data = filtered_cells_first_spike_times.plot_first_lap_spike_relative_first_PBE_spike_scatter_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529052f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition_df(global_spikes_df, 'is_theta')\n",
    "first_spikes_dict\n",
    "\n",
    "\n",
    "neuron_ids = {k:v.aclu.unique() for k, v in global_spikes_dict.items()}\n",
    "at_least_one_decoder_neuron_ids = union_of_arrays(*list(neuron_ids.values()))\n",
    "at_least_one_decoder_neuron_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a787cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Assuming first_spikes_dict is already defined as per your data\n",
    "\n",
    "# List to hold the dataframes for each category\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each category and its dataframe\n",
    "for category, df in first_spikes_dict.items():\n",
    "    # Group by 'aclu' and get the minimum 't' (first spike time)\n",
    "    df_grouped = df.groupby('aclu')['t_rel_seconds'].min().reset_index()\n",
    "    # Rename the 't' column to include the category\n",
    "    df_grouped.rename(columns={'t_rel_seconds': f'first_spike_{category}'}, inplace=True)\n",
    "    # Append the grouped dataframe to the list\n",
    "    dfs.append(df_grouped)\n",
    "\n",
    "# Merge all dataframes on 'aclu'\n",
    "df_final = reduce(lambda left, right: pd.merge(left, right, on='aclu', how='outer'), dfs)\n",
    "\n",
    "# Display the final dataframe\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e898c2",
   "metadata": {},
   "source": [
    "# Stitch Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c444b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pyphocorehelpers.plotting.media_output_helpers import vertical_image_stack, horizontal_image_stack, image_grid\n",
    "\n",
    "\n",
    "figure_paths = [\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_pin01_one_fet11-01_12-58-54_BatchPhoJonathanReplayFRC_aclu_28_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_one_2006-6-08_14-26-15_BatchPhoJonathanReplayFRC_aclu_19_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_one_2006-6-08_14-26-15_BatchPhoJonathanReplayFRC_aclu_24_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_one_2006-6-08_14-26-15_BatchPhoJonathanReplayFRC_aclu_34_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_one_2006-6-09_1-22-43_BatchPhoJonathanReplayFRC_aclu_16_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_one_2006-6-09_1-22-43_BatchPhoJonathanReplayFRC_aclu_33_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_one_2006-6-09_1-22-43_BatchPhoJonathanReplayFRC_aclu_34_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_one_2006-6-09_1-22-43_BatchPhoJonathanReplayFRC_aclu_67_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_two_2006-6-07_16-40-19_BatchPhoJonathanReplayFRC_aclu_25_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_gor01_two_2006-6-07_16-40-19_BatchPhoJonathanReplayFRC_aclu_33_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_pin01_one_fet11-01_12-58-54_BatchPhoJonathanReplayFRC_aclu_5_first_spike.png\",\n",
    "\"K:/scratch/2024-11-08 - First-spikes/individual/kdiba_pin01_one_fet11-01_12-58-54_BatchPhoJonathanReplayFRC_aclu_9_first_spike.png\",\n",
    "]\n",
    "\n",
    "figure_paths = [Path(v) for v in figure_paths]\n",
    "\n",
    "\n",
    "# Open the images\n",
    "_raster_imgs = [Image.open(i) for i in figure_paths]\n",
    "_out_vstack = vertical_image_stack(_raster_imgs, padding=5)\n",
    "_out_vstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_hstack = horizontal_image_stack(_raster_imgs, padding=5)\n",
    "_out_hstack\n",
    "\n",
    "directional_merged_decoders_result.perform_compute_marginals()\n",
    "directional_merged_decoders_result.ripple_all_epoch_bins_marginals_df\n",
    "\n",
    "# parent_array_as_image_output_folder = Path(r'E:\\Dropbox (Personal)\\Active\\Kamran Diba Lab\\Presentations\\2024-05-30 - Pho iNAV Poster\\Exports\\array_as_image').resolve()\n",
    "parent_array_as_image_output_folder = Path('output/Exports/array_as_image').resolve()\n",
    "parent_array_as_image_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "assert parent_array_as_image_output_folder.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "(parsed_h5_files_df, h5_files, h5_sessions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
