{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions failed: Traceback (most recent call last):\n",
      "  File \"/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 274, in check\n",
      "    superreload(m, reload, self.old_objects, self.shell)\n",
      "  File \"/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 323, in update_instances\n",
      "    object.__setattr__(ref, \"__class__\", new)\n",
      "TypeError: can't apply this __setattr__ to ComputationFunctionRegistryHolder object\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from attrs import define, field, Factory\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "\n",
    "# Jupyter interactivity:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pyphocorehelpers.gui.Jupyter.JupyterButtonRowWidget import JupyterButtonRowWidget\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core import Epoch\n",
    "\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SavingOptions\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import SessionBatchProgress\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionTables, AcrossSessionsVisualizations\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "from pyphocorehelpers.print_helpers import CapturedException\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import InstantaneousFiringRatesDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult, BatchSessionCompletionHandler\n",
    "\n",
    "from pyphocorehelpers.Filesystem.metadata_helpers import FilesystemMetadata, get_file_metadata\n",
    "from pyphocorehelpers.Filesystem.path_helpers import discover_data_files, generate_copydict, copy_movedict, copy_file, save_copydict_to_text_file, read_copydict_from_text_file, invert_filedict\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import get_file_str_if_file_exists\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import check_output_h5_files, copy_files_in_filelist_to_dest\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import ConcreteSessionFolder, BackupMethods\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots, BatchPhoJonathanFiguresHelper\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "\n",
    "from neuropy.core.neuron_identities import NeuronIdentityDataframeAccessor\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import build_merged_neuron_firing_rate_indicies\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsHelpers\n",
    "\n",
    "# BATCH_DATE_TO_USE = '2023-10-20' # used for filenames throught the notebook\n",
    "# BATCH_DATE_TO_USE = '2023-10-18_Apogee' # used for filenames throught the notebook\n",
    "BATCH_DATE_TO_USE = '2023-10-25' # used for filenames throught the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef5938c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/global_batch_result_2023-10-25.pkl... done.\n",
      "no difference between provided and internal paths.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-07_11-26-53</td>\n",
       "      <td>kdiba_gor01_one_2006-6-07_11-26-53</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 11:26:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.FAILED</td>\n",
       "      <td>!! 'NoneType' object is not subscriptable ::::...</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>46</td>\n",
       "      <td>235</td>\n",
       "      <td>40</td>\n",
       "      <td>180</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_3-23-37</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_3-23-37</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 03:23:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>76</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-04_21-20-3</td>\n",
       "      <td>kdiba_pin01_one_fet11-04_21-20-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-04 21:20:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>redundant</td>\n",
       "      <td>kdiba_pin01_one_redundant</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>showclus</td>\n",
       "      <td>kdiba_pin01_one_showclus</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>sleep</td>\n",
       "      <td>kdiba_pin01_one_sleep</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>tmaze</td>\n",
       "      <td>kdiba_pin01_one_tmaze</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze</td>\n",
       "      <td>SessionBatchProgress.NOT_STARTED</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "0        kdiba  gor01        one  2006-6-07_11-26-53   \n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "3        kdiba  gor01        one   2006-6-09_3-23-37   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "..         ...    ...        ...                 ...   \n",
       "67       kdiba  pin01        one    fet11-04_21-20-3   \n",
       "68       kdiba  pin01        one           redundant   \n",
       "69       kdiba  pin01        one            showclus   \n",
       "70       kdiba  pin01        one               sleep   \n",
       "71       kdiba  pin01        one               tmaze   \n",
       "\n",
       "                               context  \\\n",
       "0   kdiba_gor01_one_2006-6-07_11-26-53   \n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "3    kdiba_gor01_one_2006-6-09_3-23-37   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "..                                 ...   \n",
       "67    kdiba_pin01_one_fet11-04_21-20-3   \n",
       "68           kdiba_pin01_one_redundant   \n",
       "69            kdiba_pin01_one_showclus   \n",
       "70               kdiba_pin01_one_sleep   \n",
       "71               kdiba_pin01_one_tmaze   \n",
       "\n",
       "                                             basedirs  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/red...   \n",
       "69  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sho...   \n",
       "70   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/sleep   \n",
       "71   /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/tmaze   \n",
       "\n",
       "                              status  \\\n",
       "0   SessionBatchProgress.NOT_STARTED   \n",
       "1        SessionBatchProgress.FAILED   \n",
       "2     SessionBatchProgress.COMPLETED   \n",
       "3   SessionBatchProgress.NOT_STARTED   \n",
       "4     SessionBatchProgress.COMPLETED   \n",
       "..                               ...   \n",
       "67  SessionBatchProgress.NOT_STARTED   \n",
       "68  SessionBatchProgress.NOT_STARTED   \n",
       "69  SessionBatchProgress.NOT_STARTED   \n",
       "70  SessionBatchProgress.NOT_STARTED   \n",
       "71  SessionBatchProgress.NOT_STARTED   \n",
       "\n",
       "                                               errors    session_datetime  \\\n",
       "0                                                None 2006-06-07 11:26:53   \n",
       "1   !! 'NoneType' object is not subscriptable ::::... 2006-06-08 14:26:15   \n",
       "2                                                None 2006-06-09 01:22:43   \n",
       "3                                                None 2006-06-09 03:23:37   \n",
       "4                                                None 2006-06-12 15:55:31   \n",
       "..                                                ...                 ...   \n",
       "67                                               None 2009-11-04 21:20:03   \n",
       "68                                               None                 NaT   \n",
       "69                                               None                 NaT   \n",
       "70                                               None                 NaT   \n",
       "71                                               None                 NaT   \n",
       "\n",
       "    n_long_laps  n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "0             0               0             0                0     False   \n",
       "1             0               0             0                0     False   \n",
       "2            46             235            40              180      True   \n",
       "3             0               0             0                0     False   \n",
       "4            40              57            34               76      True   \n",
       "..          ...             ...           ...              ...       ...   \n",
       "67            0               0             0                0     False   \n",
       "68            0               0             0                0     False   \n",
       "69            0               0             0                0     False   \n",
       "70            0               0             0                0     False   \n",
       "71            0               0             0                0     False   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3                                                       \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "0   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "3   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "..                                                ...   \n",
       "67  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "0                         False                                  True  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "3                         False                                  True  \n",
       "4                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "67                        False                                  True  \n",
       "68                        False                                 False  \n",
       "69                        False                                 False  \n",
       "70                        False                                 False  \n",
       "71                        False                                 False  \n",
       "\n",
       "[72 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_global_batch_result_filename=f'global_batch_result_{BATCH_DATE_TO_USE}.pkl'\n",
    "\n",
    "debug_print = False\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/home/halechr/cloud/turbo/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/nfs/turbo/umms-kdiba/Data')] # , Path(r'/home/halechr/FastData')\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "# try to load an existing batch result:\n",
    "global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "batch_progress_df\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    # display(batch_progress_df)\n",
    "    # display(good_only_batch_progress_df)\n",
    "    display(batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "019afbbd-70d2-4e75-9548-b6f22d2e31ca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-08_14-26-15</td>\n",
       "      <td>kdiba_gor01_one_2006-6-08_14-26-15</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.FAILED</td>\n",
       "      <td>!! 'NoneType' object is not subscriptable ::::...</td>\n",
       "      <td>2006-06-08 14:26:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-09_1-22-43</td>\n",
       "      <td>kdiba_gor01_one_2006-6-09_1-22-43</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-09 01:22:43</td>\n",
       "      <td>46</td>\n",
       "      <td>235</td>\n",
       "      <td>40</td>\n",
       "      <td>180</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>one</td>\n",
       "      <td>2006-6-12_15-55-31</td>\n",
       "      <td>kdiba_gor01_one_2006-6-12_15-55-31</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-12 15:55:31</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>76</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-07_16-40-19</td>\n",
       "      <td>kdiba_gor01_two_2006-6-07_16-40-19</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-07 16:40:19</td>\n",
       "      <td>42</td>\n",
       "      <td>287</td>\n",
       "      <td>40</td>\n",
       "      <td>446</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>gor01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-6-08_21-16-25</td>\n",
       "      <td>kdiba_gor01_two_2006-6-08_21-16-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-06-08 21:16:25</td>\n",
       "      <td>40</td>\n",
       "      <td>62</td>\n",
       "      <td>40</td>\n",
       "      <td>77</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>vvp01</td>\n",
       "      <td>two</td>\n",
       "      <td>2006-4-10_12-58-3</td>\n",
       "      <td>kdiba_vvp01_two_2006-4-10_12-58-3</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-04-10 12:58:03</td>\n",
       "      <td>40</td>\n",
       "      <td>75</td>\n",
       "      <td>42</td>\n",
       "      <td>31</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_17-46-44</td>\n",
       "      <td>kdiba_pin01_one_11-02_17-46-44</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 17:46:44</td>\n",
       "      <td>54</td>\n",
       "      <td>137</td>\n",
       "      <td>66</td>\n",
       "      <td>156</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-02_19-28-0</td>\n",
       "      <td>kdiba_pin01_one_11-02_19-28-0</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-02 19:28:00</td>\n",
       "      <td>56</td>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>11-03_12-3-25</td>\n",
       "      <td>kdiba_pin01_one_11-03_12-3-25</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-03 12:03:25</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>kdiba</td>\n",
       "      <td>pin01</td>\n",
       "      <td>one</td>\n",
       "      <td>fet11-01_12-58-54</td>\n",
       "      <td>kdiba_pin01_one_fet11-01_12-58-54</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>SessionBatchProgress.COMPLETED</td>\n",
       "      <td>None</td>\n",
       "      <td>2009-11-01 12:58:54</td>\n",
       "      <td>315</td>\n",
       "      <td>584</td>\n",
       "      <td>163</td>\n",
       "      <td>278</td>\n",
       "      <td>True</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   format_name animal exper_name        session_name  \\\n",
       "1        kdiba  gor01        one  2006-6-08_14-26-15   \n",
       "2        kdiba  gor01        one   2006-6-09_1-22-43   \n",
       "4        kdiba  gor01        one  2006-6-12_15-55-31   \n",
       "6        kdiba  gor01        two  2006-6-07_16-40-19   \n",
       "8        kdiba  gor01        two  2006-6-08_21-16-25   \n",
       "..         ...    ...        ...                 ...   \n",
       "32       kdiba  vvp01        two   2006-4-10_12-58-3   \n",
       "52       kdiba  pin01        one      11-02_17-46-44   \n",
       "53       kdiba  pin01        one       11-02_19-28-0   \n",
       "54       kdiba  pin01        one       11-03_12-3-25   \n",
       "64       kdiba  pin01        one   fet11-01_12-58-54   \n",
       "\n",
       "                               context  \\\n",
       "1   kdiba_gor01_one_2006-6-08_14-26-15   \n",
       "2    kdiba_gor01_one_2006-6-09_1-22-43   \n",
       "4   kdiba_gor01_one_2006-6-12_15-55-31   \n",
       "6   kdiba_gor01_two_2006-6-07_16-40-19   \n",
       "8   kdiba_gor01_two_2006-6-08_21-16-25   \n",
       "..                                 ...   \n",
       "32   kdiba_vvp01_two_2006-4-10_12-58-3   \n",
       "52      kdiba_pin01_one_11-02_17-46-44   \n",
       "53       kdiba_pin01_one_11-02_19-28-0   \n",
       "54       kdiba_pin01_one_11-03_12-3-25   \n",
       "64   kdiba_pin01_one_fet11-01_12-58-54   \n",
       "\n",
       "                                             basedirs  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                            status  \\\n",
       "1      SessionBatchProgress.FAILED   \n",
       "2   SessionBatchProgress.COMPLETED   \n",
       "4   SessionBatchProgress.COMPLETED   \n",
       "6   SessionBatchProgress.COMPLETED   \n",
       "8   SessionBatchProgress.COMPLETED   \n",
       "..                             ...   \n",
       "32  SessionBatchProgress.COMPLETED   \n",
       "52  SessionBatchProgress.COMPLETED   \n",
       "53  SessionBatchProgress.COMPLETED   \n",
       "54  SessionBatchProgress.COMPLETED   \n",
       "64  SessionBatchProgress.COMPLETED   \n",
       "\n",
       "                                               errors    session_datetime  \\\n",
       "1   !! 'NoneType' object is not subscriptable ::::... 2006-06-08 14:26:15   \n",
       "2                                                None 2006-06-09 01:22:43   \n",
       "4                                                None 2006-06-12 15:55:31   \n",
       "6                                                None 2006-06-07 16:40:19   \n",
       "8                                                None 2006-06-08 21:16:25   \n",
       "..                                                ...                 ...   \n",
       "32                                               None 2006-04-10 12:58:03   \n",
       "52                                               None 2009-11-02 17:46:44   \n",
       "53                                               None 2009-11-02 19:28:00   \n",
       "54                                               None 2009-11-03 12:03:25   \n",
       "64                                               None 2009-11-01 12:58:54   \n",
       "\n",
       "    n_long_laps  n_long_replays  n_short_laps  n_short_replays  is_ready  \\\n",
       "1             0               0             0                0     False   \n",
       "2            46             235            40              180      True   \n",
       "4            40              57            34               76      True   \n",
       "6            42             287            40              446      True   \n",
       "8            40              62            40               77      True   \n",
       "..          ...             ...           ...              ...       ...   \n",
       "32           40              75            42               31      True   \n",
       "52           54             137            66              156      True   \n",
       "53           56             110            50               51      True   \n",
       "54           50              32            46               18      True   \n",
       "64          315             584           163              278      True   \n",
       "\n",
       "                       global_computation_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                           loaded_session_pickle_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "                                   ripple_result_file  \\\n",
       "1   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "2   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "4   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/200...   \n",
       "6   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "8   /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/200...   \n",
       "..                                                ...   \n",
       "32  /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/200...   \n",
       "52  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "53  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "54  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-...   \n",
       "64  /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet...   \n",
       "\n",
       "    has_user_replay_annotations  has_user_grid_bin_bounds_annotations  \n",
       "1                          True                                  True  \n",
       "2                          True                                  True  \n",
       "4                          True                                  True  \n",
       "6                          True                                  True  \n",
       "8                          True                                  True  \n",
       "..                          ...                                   ...  \n",
       "32                         True                                  True  \n",
       "52                         True                                  True  \n",
       "53                         True                                  True  \n",
       "54                         True                                  True  \n",
       "64                         True                                  True  \n",
       "\n",
       "[15 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hardcoded included_session_contexts:\n",
    "included_session_contexts = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "good_session_concrete_folders = ConcreteSessionFolder.build_concrete_session_folders(global_data_root_parent_path, included_session_contexts)\n",
    "good_session_concrete_folders\n",
    "\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.pythonScriptTemplating import generate_batch_single_session_scripts\n",
    "\n",
    "## Build Slurm Scripts:\n",
    "session_basedirs_dict: Dict[IdentifyingContext, Path] = {a_session_folder.context:a_session_folder.path for a_session_folder in good_session_concrete_folders}\n",
    "included_session_contexts, output_python_scripts, output_slurm_scripts = generate_batch_single_session_scripts(global_data_root_parent_path, session_batch_basedirs=session_basedirs_dict, included_session_contexts=included_session_contexts, output_directory=Path('output/generated_slurm_scripts/').resolve(), use_separate_run_directories=True)\n",
    "output_python_scripts\n",
    "\n",
    "included_session_batch_progress_df = batch_progress_df[np.isin(batch_progress_df['context'].values, included_session_contexts)]\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(included_session_batch_progress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71584edc",
   "metadata": {},
   "source": [
    "# Execute Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6ae279",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(included_session_contexts): 15\n",
      "Beginning processing with len(included_session_contexts): 15\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15\"):build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25\"):build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31\"):build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19\"):build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40\"):build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43\"):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-08_14-26-15.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-08_21-16-25.logbuild_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50\"):build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30\"):\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-07_16-40-19.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-09_22-24-40.log\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-09_1-22-43.log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.one.2006-6-12_15-55-31.log\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-10_12-25-50.log\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.one.2006-4-09_17-29-30.log\n",
      "\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.gor01.two.2006-6-12_16-53-46.log\n",
      "\n",
      "========================== runBatch STARTING ==========================\n",
      "========================== runBatch STARTING ==========================\n",
      "========================== runBatch STARTING ==========================\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\n",
      "========================== runBatch STARTING ==================================================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\tsession_context: kdiba_gor01_two_2006-6-07_16-40-19\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data========================== runBatch STARTING ==========================\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "========================== runBatch STARTING ==========================\n",
      "\n",
      "\n",
      "\n",
      "\tsession_context: kdiba_gor01_two_2006-6-09_22-24-40\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19========================== runBatch STARTING ==========================\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_gor01_one_2006-6-08_14-26-15\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data========================== runBatch STARTING ==========================\n",
      "\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "__________________________________________________________________\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_gor01_one_2006-6-12_15-55-31\n",
      "\n",
      "\tsession_context: kdiba_vvp01_one_2006-4-09_17-29-30\n",
      "\n",
      "__________________________________________________________________basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_gor01_two_2006-6-08_21-16-25\n",
      "\n",
      "\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31\n",
      "\tsession_context: kdiba_gor01_one_2006-6-09_1-22-43\n",
      "\tsession_context: kdiba_gor01_two_2006-6-12_16-53-46basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40\n",
      "active_data_mode_name: kdiba\n",
      "__________________________________________________________________\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\n",
      "\n",
      "active_data_mode_name: kdiba\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "\n",
      "__________________________________________________________________basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30\n",
      "\n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl...__________________________________________________________________\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43\tsession_context: kdiba_vvp01_one_2006-4-10_12-25-50\n",
      "\n",
      "__________________________________________________________________ \tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15\n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl...\n",
      "active_data_mode_name: kdiba\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31\n",
      "__________________________________________________________________ basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50\n",
      "\n",
      "active_data_mode_name: kdiba\n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl...active_data_mode_name: kdiba\n",
      "active_data_mode_name: kdibabasedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15__________________________________________________________________ \n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43\n",
      "\n",
      "\n",
      "\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl...active_data_mode_name: kdibaactive_data_mode_name: kdibabasedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl...\n",
      " \n",
      "\n",
      " active_data_mode_name: kdibaLoading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl...Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl... \n",
      " Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl... Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl... unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "unpickling from old NeuropyPipelineStageusing provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/loadedSessPickle.pkl.\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "unpickling from old NeuropyPipelineStage\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/loadedSessPickle.pkl.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_datadue to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "\n",
      "Recomputing active_epoch_placefields... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... \t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\n",
      "\t done.\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \n",
      "using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (285810,) should be less than time_window_edges: (1572,)!\n",
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)WARNING: PREVIOUSLY ASSERT: \n",
      "\n",
      "\t spikes_df[time_variable_name]: (14925,) should be less than time_window_edges: (1484,)!\n",
      "\t done._execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 28, 812521), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 31, 301386), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 14, 58, 53, 990741), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 14, 58, 55, 158677)}\n",
      "updating computation_results...\n",
      "\n",
      "Recomputing active_epoch_time_dependent_placefields2D... done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-08_14-26-15/loadedSessPickle.pkl.\n",
      "using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "exception occured: !! 'NoneType' object is not subscriptable ::::: (<class 'TypeError'>, TypeError(\"'NoneType' object is not subscriptable\"), <traceback object at 0x147ee13fa700>)\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-09_16-40-54.log\t done.\n",
      "\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-09_16-40-54\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds_1D: (24.481516142738176, 255.4815161427382)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((24.481516142738176, 255.4815161427382), (132.49260896751392, 155.30747604466447))\n",
      "\t done.\n",
      "unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (285810,) should be less than time_window_edges: (1572,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (14925,) should be less than time_window_edges: (1484,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 59, 545585), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 59, 2, 660001), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 14, 59, 20, 612956), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 14, 59, 21, 690561)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147f99dc8c00>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/global_computation_results.pkl... done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (345914,) should be less than time_window_edges: (2246,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11108,) should be less than time_window_edges: (2079,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 38, 156263), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 47, 248503), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 14, 59, 29, 281018), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 14, 59, 30, 450961)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (13127,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (28.300282316379977, 259.30028231638)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (9198,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((28.300282316379977, 259.30028231638), (128.30369397123394, 154.72988093974095))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (298971,) should be less than time_window_edges: (2785,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10597,) should be less than time_window_edges: (2660,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 40, 804307), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 47, 338386), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 14, 59, 40, 175474), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 14, 59, 41, 25044)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (318704,) should be less than time_window_edges: (3449,)!\n",
      "using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (22940,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8135,) should be less than time_window_edges: (2813,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 59, 10, 551613), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 59, 14, 672660), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 14, 59, 42, 619068), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 14, 59, 43, 436034)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (28.54313873072426, 255.54313873072425)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (22454,)\n",
      "using self.config.grid_bin_bounds: ((28.54313873072426, 255.54313873072425), (-55.2405385510412, -12.237798967230454))\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (248779,) should be less than time_window_edges: (2828,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: using self.config.grid_bin_bounds_1D: (29.64642522460817, 257.8732552112081)\n",
      "\n",
      "\t spikes_df[time_variable_name]: (9916,) should be less than time_window_edges: (2721,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 45, 640900), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 52, 674053), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 14, 59, 50, 683956), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 14, 59, 51, 421158)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((29.64642522460817, 257.8732552112081), (106.68603845428224, 146.71219371189815))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (618380,) should be less than time_window_edges: (2404,)!\n",
      "using self.config.grid_bin_bounds_1D: (25.5637332724328, 257.964172947664)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (22454,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f64a49840>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.150447\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46/output/pipeline_results.h5\n",
      "using self.config.grid_bin_bounds: ((25.5637332724328, 257.964172947664), (89.1844223602494, 131.92462510535915))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (24205,) should be less than time_window_edges: (2258,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 43, 13023), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 52, 453750), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 14, 59, 55, 541733), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 0, 61573)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_16-53-46'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (24.71824744583462, 248.6393456241123)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "using self.config.grid_bin_bounds: ((24.71824744583462, 248.6393456241123), (136.77104473778593, 152.85274652666337))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (868636,) should be less than time_window_edges: (3476,)!\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1175478,) should be less than time_window_edges: (5148,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (23851,) should be less than time_window_edges: (3148,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 47, 572096), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 55, 788964), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 0, 10, 609536), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 17, 67290)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (345914,) should be less than time_window_edges: (2246,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11108,) should be less than time_window_edges: (2079,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 59, 34, 65458), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 59, 37, 605160), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 0, 18, 58404), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 19, 228545)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147ee15b96c0>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/global_computation_results.pkl... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (960238,) should be less than time_window_edges: (5177,)!\n",
      "using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (318704,) should be less than time_window_edges: (3449,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21760,) should be less than time_window_edges: (4684,)!\n",
      "using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 50, 747214), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 59, 803697), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 0, 13, 510618), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 22, 669731)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8135,) should be less than time_window_edges: (2813,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 59, 48, 872279), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 59, 54, 203037), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 0, 22, 164860), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 23, 5047)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147ee13f1a00>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/global_computation_results.pkl... \t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-12_16-53-46...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16205,) should be less than time_window_edges: (4754,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 58, 49, 878986), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 58, 57, 359272), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 0, 19, 521142), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 24, 308932)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (36.58620390950715, 248.91627658974846)\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "using self.config.grid_bin_bounds: ((36.58620390950715, 248.91627658974846), (132.81136363636367, 149.2840909090909))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.397021260868584, 245.3970212608686)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds_1D: (29.088604852961407, 251.70402561515647)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (18467,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f99cc26c0>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_gor01_two_2006-6-12_16-53-46]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-12_16-53-46, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-12_16-53-46, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|gor01|two|2006-6-12_16-53-46\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f5e41a5c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.vvp01.two.2006-4-10_12-58-3.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_vvp01_two_2006-4-10_12-58-3\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl... using self.config.grid_bin_bounds: ((22.397021260868584, 245.3970212608686), (133.66465594522782, 155.97244934208123))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds: ((29.088604852961407, 251.70402561515647), (138.496638485457, 153.496638485457))\n",
      "\t done.\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (298971,) should be less than time_window_edges: (2785,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26171,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (10597,) should be less than time_window_edges: (2660,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 59, 45, 813716), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 59, 50, 674208), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 0, 42, 843953), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 43, 716286)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147ee15a77c0>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (12943,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (14863,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (248779,) should be less than time_window_edges: (2828,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (9916,) should be less than time_window_edges: (2721,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 14, 59, 55, 579463), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 14, 59, 58, 986003), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 0, 54, 823904), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 0, 55, 448153)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147ee15d7e40>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/global_computation_results.pkl... done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (25027,)\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (32287,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (43010,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (14069,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (618380,) should be less than time_window_edges: (2404,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (24205,) should be less than time_window_edges: (2258,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 0, 4, 781603), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 0, 9, 602314), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 1, 12, 383227), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 1, 16, 505511)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147ee1445e40>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/global_computation_results.pkl... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (31477,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (26312,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (42597,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (13343,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (40880,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (20270,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (31477,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f66b46080>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.006768\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31/output/pipeline_results.h5\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (12894,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-12_15-55-31'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (205356,) should be less than time_window_edges: (2919,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (868636,) should be less than time_window_edges: (3476,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7355,) should be less than time_window_edges: (2688,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 0, 51, 93195), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 0, 54, 101087), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 1, 43, 534770), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 1, 44, 142129)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (42597,)\n",
      "using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f99a15bc0>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.014470\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54/output/pipeline_results.h5\n",
      "using self.config.grid_bin_bounds_1D: (30.511181558838498, 247.5111815588389)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (23851,) should be less than time_window_edges: (3148,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 0, 22, 685856), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 0, 30, 246553), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 1, 43, 530351), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 1, 49, 909922)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147ee158fe80>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/global_computation_results.pkl... using self.config.grid_bin_bounds: ((30.511181558838498, 247.5111815588389), (106.97411662767412, 147.52430924258078))\n",
      "\t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (34680,)\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (1175478,) should be less than time_window_edges: (5148,)!\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_16-40-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (40280,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (21760,) should be less than time_window_edges: (4684,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 0, 33, 617184), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 0, 41, 857652), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 1, 51, 12289), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 1, 59, 625736)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147ee160e140>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/global_computation_results.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (42319,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-12_15-55-31...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (960238,) should be less than time_window_edges: (5177,)!\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (34184,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (16205,) should be less than time_window_edges: (4754,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 0, 33, 98072), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 0, 40, 993847), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 2, 3, 361854), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 2, 7, 936955)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147f99d9c080>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/global_computation_results.pkl... WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f605aad80>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_gor01_one_2006-6-12_15-55-31]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_one_2006-6-12_15-55-31, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-12_15-55-31, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|gor01|one|2006-6-12_15-55-31\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f5e35bd00>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-02_17-46-44.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_17-46-44\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-09_16-40-54...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (30745,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickling from old NeuropyPipelineStage\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (40280,)\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... \t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f64aa0d80>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.040830\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30/output/pipeline_results.h5\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f648e5f40>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_vvp01_two_2006-4-09_16-40-54]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_two_2006-4-09_16-40-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-09_16-40-54, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|vvp01|two|2006-4-09_16-40-54\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f627435c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0\"):========================== runBatch STARTING ==========================\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-02_19-28-0.log\n",
      "\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_pin01_one_11-02_19-28-0\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (34184,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f648ec680>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.037333\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25/output/pipeline_results.h5\n",
      "using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (18711,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n",
      "unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-09_17-29-30'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (27155,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (41198,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-08_21-16-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n",
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (50132,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (36136,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (205356,) should be less than time_window_edges: (2919,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (7355,) should be less than time_window_edges: (2688,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 1, 47, 227384), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 1, 50, 93109), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 2, 38, 60945), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 2, 38, 678874)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147e8110d900>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (49904,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-09_17-29-30...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (47668,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f5c13bfc0>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_vvp01_one_2006-4-09_17-29-30]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_one_2006-4-09_17-29-30, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-09_17-29-30, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|vvp01|one|2006-4-09_17-29-30\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f59f4da40>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25\"):\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.11-03_12-3-25.log\n",
      "========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_pin01_one_11-03_12-3-25\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl... unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (38641,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/loadedSessPickle.pkl.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (233697,) should be less than time_window_edges: (2357,)!\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11613,) should be less than time_window_edges: (2251,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 2, 26, 975784), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 2, 30, 430950), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 2, 52, 938551), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 2, 53, 521517)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (77624,)\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (41198,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f64a8f100>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.094291\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50/output/pipeline_results.h5\n",
      "using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "using self.config.grid_bin_bounds_1D: (20.551685242617875, 249.52142297024744)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (47668,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f99b37740>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.084641\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5\n",
      "using self.config.grid_bin_bounds: ((20.551685242617875, 249.52142297024744), (136.6282885482392, 154.9308054334688))\n",
      "\t done.\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (320780,) should be less than time_window_edges: (3884,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (27018,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8528,) should be less than time_window_edges: (3634,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 2, 20, 403559), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 2, 24, 462765), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 3, 3, 60592), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 3, 3, 914952)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-25-50'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... \t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-08_21-16-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "using self.config.grid_bin_bounds_1D: (26.927879930920472, 253.7869451377655)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... using self.config.grid_bin_bounds: ((26.927879930920472, 253.7869451377655), (129.2279041328145, 152.2279041328145))\n",
      "\t done.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (14198,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (134557,) should be less than time_window_edges: (2012,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8423,) should be less than time_window_edges: (1826,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 2, 56, 391925), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 3, 0, 483579), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 3, 16, 191030), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 3, 16, 555575)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (70951,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_1-22-43'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "using self.config.grid_bin_bounds_1D: (22.2851382680749, 246.39985985110218)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... \t doing specific instantaneous firing rate computation for context: kdiba_vvp01_one_2006-4-10_12-25-50...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f66ca0280>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_gor01_two_2006-6-08_21-16-25]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-08_21-16-25, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|gor01|two|2006-6-08_21-16-25\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f5c187440>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "build_batch_task_logger(module_name=\"gl0004.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54\"):\n",
      "\n",
      "\t Batch Task logger com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54 has file logging enabled and will log to EXTERNAL/TESTING/Logging/debug_com.PhoHale.PhoPy3DPositionAnalyis.Batch.runBatch.run_specific_batch.gl0004.arc-ts.umich.edu.kdiba.pin01.one.fet11-01_12-58-54.log========================== runBatch STARTING ==========================\n",
      "\tglobal_data_root_parent_path: /home/halechr/cloud/turbo/Data\n",
      "\tsession_context: kdiba_pin01_one_fet11-01_12-58-54\n",
      "\tsession_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "__________________________________________________________________\n",
      "basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl... using self.config.grid_bin_bounds: ((22.2851382680749, 246.39985985110218), (133.85711719213543, 152.81579979839964))\n",
      "\t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (77332,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (233697,) should be less than time_window_edges: (2357,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (11613,) should be less than time_window_edges: (2251,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 2, 58, 639821), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 3, 2, 783229), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 3, 26, 392849), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 3, 27, 19732)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147f5c1e9d40>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/global_computation_results.pkl... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "unpickling from old DynamicParameters-based computationResult\n",
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f5e3b9740>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_vvp01_one_2006-4-10_12-25-50]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_one_2006-4-10_12-25-50, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/one/2006-4-10_12-25-50, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|vvp01|one|2006-4-10_12-25-50\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f99c70840>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (70951,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f62796100>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.007418\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40/output/pipeline_results.h5\n",
      "unpickling from old NeuropyPipelineStage\n",
      "done.\n",
      "Loading pickled pipeline success: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/loadedSessPickle.pkl.\n",
      "properties already present in pickled version. No need to save.\n",
      "pipeline load success!\n",
      "using provided computation_functions_name_includelist: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding']\n",
      "not using direction-dependent laps.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_odd_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (22485,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (134557,) should be less than time_window_edges: (2012,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8423,) should be less than time_window_edges: (1826,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 3, 20, 538423), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 3, 24, 931635), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 3, 40, 166750), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 3, 40, 515541)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147f59ff3600>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/global_computation_results.pkl... using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (43046,)\n",
      "using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (11282,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (320780,) should be less than time_window_edges: (3884,)!\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (18921,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (8528,) should be less than time_window_edges: (3634,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 3, 7, 508714), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 3, 11, 723621), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 3, 51, 251149), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 3, 52, 130294)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147f5e499c80>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/global_computation_results.pkl... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (72013,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-09_22-24-40'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (9200,)\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (34248,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (28419,)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (34083,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (29312,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (40706,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (22102,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (27638,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (34083,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_pin01_one_11-02_19-28-0. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f605ef1c0>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.008007\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0/output/pipeline_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_one_2006-6-09_1-22-43...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (72013,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f66c402c0>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.042718\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19/output/pipeline_results.h5\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_19-28-0'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (27638,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_pin01_one_11-03_12-3-25. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f59f08a40>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.034670\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25/output/pipeline_results.h5\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-03_12-3-25'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (56677,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (40706,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147e806682c0>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.042647\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-6-07_16-40-19'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-03_12-3-25...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f59fe1a40>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_11-03_12-3-25]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_11-03_12-3-25, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-03_12-3-25, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|pin01|one|11-03_12-3-25\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f57e9c2c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_19-28-0...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '2006-4-10_12-58-3'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f5c211540>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_11-02_19-28-0]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_11-02_19-28-0, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_19-28-0, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|pin01|one|11-02_19-28-0\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f5c2617c0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f5a046200>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_gor01_one_2006-6-09_1-22-43]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_one_2006-6-09_1-22-43, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|gor01|one|2006-6-09_1-22-43\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147dfa301d80>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (182379,) should be less than time_window_edges: (6063,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6999,) should be less than time_window_edges: (5840,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 3, 41, 221336), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 3, 43, 577019), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 4, 49, 303027), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 4, 49, 731089)}\n",
      "updating computation_results...\n",
      "done.\n",
      "Performing perform_action_for_all_contexts with action EvaluationActions.EVALUATE_COMPUTATIONS on filtered_session with filter named \"maze_even_laps\"...\n",
      "due to includelist, including only 4 out of 16 registered computation functions.\n",
      "Performing _execute_computation_functions(...) with 4 registered_computation_functions...\n",
      "Recomputing active_epoch_placefields... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_placefields2D... DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields... two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (55036,)\n",
      "using self.config.grid_bin_bounds_1D: (22.403791476255435, 255.28121598502332)\n",
      "\t done.\n",
      "Recomputing active_epoch_time_dependent_placefields2D... Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using self.config.grid_bin_bounds: ((22.403791476255435, 255.28121598502332), (135.43617904962073, 153.6679723832235))\n",
      "\t done.\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_vvp01_two_2006-4-10_12-58-3...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147e807253c0>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_vvp01_two_2006-4-10_12-58-3]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_vvp01_two_2006-4-10_12-58-3, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|vvp01|two|2006-4-10_12-58-3\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147f60634ec0>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (55036,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_pin01_one_11-02_17-46-44. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147ee00ee680>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.007244\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44/output/pipeline_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-09_22-24-40...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: '11-02_17-46-44'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:267: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['firing_rates', 'is_neuron_active', 'active_aclus'], dtype='object')]\n",
      "\n",
      "  self.rdf.rdf.to_hdf(file_path, key=f'{key}/rdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n",
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/General/Pipeline/Stages/ComputationFunctions/MultiContextComputationFunctions/LongShortTrackComputations.py:273: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['firing_rates'], dtype='object')]\n",
      "\n",
      "  self.irdf.irdf.to_hdf(file_path, key=f'{key}/irdf/df') # , format='table', data_columns=True Can't do 'table' format because `TypeError: Cannot serialize the column [firing_rates] because its data contents are not [string] but [mixed] object dtype`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_short_inst_spike_rate_groups is missing and will be skipped\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f99c43e40>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_gor01_two_2006-6-09_22-24-40]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-09_22-24-40, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-09_22-24-40, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|gor01|two|2006-6-09_22-24-40\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147d97004a00>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_11-02_17-46-44...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-07_16-40-19...\n",
      "setting LxC_aclus/SxC_aclus from user annotation.\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147e38c8e5c0>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_11-02_17-46-44]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_11-02_17-46-44, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/11-02_17-46-44, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|pin01|one|11-02_17-46-44\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147ec9a17c80>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'long_short_inst_spike_rate_groups' ::::: (<class 'KeyError'>, KeyError('long_short_inst_spike_rate_groups'), <traceback object at 0x147f66c5ec40>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_gor01_two_2006-6-07_16-40-19]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/gor01/two/2006-6-07_16-40-19, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|gor01|two|2006-6-07_16-40-19\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147e25355800>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (182379,) should be less than time_window_edges: (6063,)!\n",
      "WARNING: PREVIOUSLY ASSERT: \n",
      "\t spikes_df[time_variable_name]: (6999,) should be less than time_window_edges: (5840,)!\n",
      "_execute_computation_functions(...): \n",
      "\taccumulated_errors: None\n",
      "\tcomputation_times: {<function PlacefieldComputations._perform_baseline_placefield_computation at 0x147f009f98b0>: datetime.datetime(2023, 10, 25, 15, 4, 51, 818142), <function PlacefieldComputations._perform_time_dependent_placefield_computation at 0x147f009f99d0>: datetime.datetime(2023, 10, 25, 15, 4, 53, 840478), <function DefaultComputationFunctions._perform_position_decoding_computation at 0x147f00a650d0>: datetime.datetime(2023, 10, 25, 15, 5, 59, 24465), <function SpikeAnalysisComputations._perform_firing_rate_trends_computation at 0x147f0042a280>: datetime.datetime(2023, 10, 25, 15, 5, 59, 458846)}\n",
      "updating computation_results...\n",
      "done.\n",
      ".try_compute_directional_laps_for_global_epoch(...) failed with exception: !! too many values to unpack (expected 2) ::::: (<class 'ValueError'>, ValueError('too many values to unpack (expected 2)'), <traceback object at 0x147f5e430600>)\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computed_data\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/global_computation_results.pkl... done.\n",
      "have an existing `global_computation_results.computation_config`: DynamicContainer({'instantaneous_time_bin_size_seconds': 0.01})\n",
      "included includelist is specified: ['pf_computation', 'pfdt_computation', 'firing_rate_trends', 'position_decoding', 'position_decoding_two_step', 'long_short_decoding_analyses', 'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping', 'long_short_inst_spike_rate_groups', 'long_short_endcap_analysis', 'split_to_directional_laps'], so only performing these extended computations.\n",
      "Running batch_extended_computations(...) with global_epoch_name: \"maze_even_laps\"\n",
      "pf_computation, maze_even_laps already computed.\n",
      "pfdt_computation, maze_even_laps already computed.\n",
      "position_decoding, maze_even_laps already computed.\n",
      "position_decoding_two_step missing.\n",
      "\t Recomputing position_decoding_two_step...\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze1\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (53345,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze2\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (28623,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (90848,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_odd_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (88460,)\n",
      "Performing run_specific_computations_single_context on filtered_session with filter named \"maze_even_laps\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/pyPhoPlaceCellAnalysis/src/pyphoplacecellanalysis/Analysis/Decoder/reconstruction.py:367: RuntimeWarning: divide by zero encountered in divide\n",
      "  return C * np.exp(numerator/denominator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_step_decoder_result['most_likely_position_flat_max_likelihood_values'].shape = (88460,)\n",
      "\t done.\n",
      "Exception occured while computing (`perform_specific_computation(...)`) or validating (`validate_computation_test(...)`) after recomputation:\n",
      " Inner exception: 'pf1D_TwoStepDecoder'\n",
      "ERROR perform `batch_extended_computations` or saving GLOBAL COMPUTATION RESULTS for pipeline of curr_session_context: kdiba_pin01_one_fet11-01_12-58-54. error: !! 'pf1D_TwoStepDecoder' ::::: (<class 'KeyError'>, KeyError('pf1D_TwoStepDecoder'), <traceback object at 0x147f66c0ac80>)\n",
      "no changes in global results.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "DEPRICATION WARNING: workaround to allow subscripting ComputationResult objects. Will be depricated. key: computation_times\n",
      "\t time since last computation: 0:00:00.006123\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n",
      "OVERWRITING (or writing) the file /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5!\n",
      "pipeline hdf5_output_path: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54/output/pipeline_results.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/Spike3D/.venv/lib/python3.9/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'fet11-01_12-58-54'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x147f5c1f4640>) while trying to build the session HDF output.\n",
      "ERROR: encountered exception !! 'long_short_fr_indicies_analysis' ::::: (<class 'KeyError'>, KeyError('long_short_fr_indicies_analysis'), <traceback object at 0x147f68d4a0c0>) while trying to build the session HDF output for kdiba_pin01_one_fet11-01_12-58-54\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_pin01_one_fet11-01_12-58-54...\n",
      "WARN: on_complete_success_execution_session: encountered exception !! 'jonathan_firing_rate_analysis' ::::: (<class 'KeyError'>, KeyError('jonathan_firing_rate_analysis'), <traceback object at 0x147e29263f80>) while trying to compute the instantaneous firing rates and set self.across_sessions_instantaneous_fr_dict[kdiba_pin01_one_fet11-01_12-58-54]\n",
      "\t>> calling external computation function: a_test_completion_function\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "a_test_completion_function(curr_session_context: kdiba_pin01_one_fet11-01_12-58-54, curr_session_basedir: /nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-01_12-58-54, ...,across_session_results_extended_dict: {})\n",
      "-----------------_____------______---- BEGIN Directional Laps Result: \n",
      "\toutput_file_prefix: kdiba|pin01|one|fet11-01_12-58-54\n",
      "\tcurr_active_pipeline.active_config_names: ['maze1', 'maze2', 'maze', 'maze_odd_laps', 'maze_even_laps']\n",
      "error occured in post_run_callback_fn: !! 'DirectionalLaps' ::::: (<class 'KeyError'>, KeyError('DirectionalLaps'), <traceback object at 0x147e2c1fce80>). Suppressing.\n",
      "\"========================== END BATCH ==========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "\n",
    "# multiprocessing_kwargs = dict(use_multiprocessing=False, num_processes=1)\n",
    "multiprocessing_kwargs = dict(use_multiprocessing=True, num_processes=9)\n",
    "  \n",
    "# Whether to output figures:\n",
    "should_perform_figure_generation_to_file=False\n",
    "# should_perform_figure_generation_to_file=True\n",
    "\n",
    "## Included Session Contexts:\n",
    "# included_session_contexts = batch_progress_df[np.logical_and(batch_progress_df['has_user_replay_annotations'], batch_progress_df['is_ready'])]['context'].to_numpy().tolist()\n",
    "\n",
    "# Only require sessions to have replay annotations:\n",
    "# included_session_contexts = batch_progress_df[batch_progress_df['has_user_replay_annotations']]['context'].to_numpy().tolist()\n",
    "\n",
    "# included_session_contexts = batch_progress_df['context'].to_numpy().tolist()[:4] # Only get the first 6\n",
    "# Limit the contexts to run to the last N:\n",
    "# included_session_contexts = included_session_contexts[3:5]\n",
    "\n",
    "# included_session_contexts = [included_session_contexts[3]]\n",
    "\n",
    "# ALL\n",
    "included_session_contexts = included_session_contexts\n",
    "\n",
    "# ## No filtering the sessions:\n",
    "# included_session_contexts = None\n",
    "\n",
    "if included_session_contexts is not None:\n",
    "    print(f'len(included_session_contexts): {len(included_session_contexts)}')\n",
    "else:\n",
    "    print(f'included_session_contexts is None so all session contexts will be included.')\n",
    "\n",
    "# included_session_contexts\n",
    "\n",
    "# # No recomputing at all:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER, override_file=),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=SavingOptions.NEVER),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "# No Reloading\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "                                                session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=True, should_save=SavingOptions.IF_CHANGED),\n",
    "                                                should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, should_generate_all_plots=True, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "                                                **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "# # # Forced Reloading:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=True,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=SavingOptions.ALWAYS),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE, force_global_recompute=True,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "active_post_run_callback_fn = result_handler.on_complete_success_execution_session\n",
    "# active_post_run_callback_fn = _temp_on_complete_success_execution_session\n",
    "\n",
    "\n",
    "def a_test_completion_function(self, global_data_root_parent_path, curr_session_context, curr_session_basedir, curr_active_pipeline, across_session_results_extended_dict: dict) -> dict:\n",
    "    # print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    print(f'a_test_completion_function(curr_session_context: {curr_session_context}, curr_session_basedir: {str(curr_session_basedir)}, ...,across_session_results_extended_dict: {across_session_results_extended_dict})')\n",
    "    long_epoch_name, short_epoch_name, global_epoch_name = curr_active_pipeline.find_LongShortGlobal_epoch_names()\n",
    "    # long_session, short_session, global_session = [curr_active_pipeline.filtered_sessions[an_epoch_name] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "    # long_results, short_results, global_results = [curr_active_pip eline.computation_results[an_epoch_name]['computed_data'] for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "    # Get existing laps from session:\n",
    "    # long_laps, short_laps, global_laps = [curr_active_pipeline.filtered_sessions[an_epoch_name].laps.as_epoch_obj() for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "    # long_replays, short_replays, global_replays = [Epoch(curr_active_pipeline.filtered_sessions[an_epoch_name].replay.epochs.get_valid_df()) for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "    # long_PBEs, short_PBEs, global_PBEs = [curr_active_pipeline.filtered_sessions[an_epoch_name].pbe for an_epoch_name in [long_epoch_name, short_epoch_name, global_epoch_name]]\n",
    "\n",
    "\n",
    "    output_file_prefix = curr_session_context.get_description(separator=\"|\", include_property_names=False)\n",
    "    print(f'-----------------_____------______---- BEGIN Directional Laps Result: \\n\\toutput_file_prefix: {output_file_prefix}\\n\\tcurr_active_pipeline.active_config_names: {curr_active_pipeline.active_config_names}')\n",
    "    directional_laps_result = curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']['computed_base_epoch_names']\n",
    "    print(f\"\\tsplit_directional_laps_names: {curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']['split_directional_laps_names']}\")\n",
    "    print(f\"\\tsplit_directional_laps_names: {curr_active_pipeline.global_computation_results.computed_data['DirectionalLaps']['computed_base_epoch_names']}\")\n",
    "    print(f'\\n__ End Directional Laps result')\n",
    "\n",
    "    # jonathan_firing_rate_analysis_result = curr_active_pipeline.global_computation_results.computed_data.jonathan_firing_rate_analysis\n",
    "    # (epochs_df_L, epochs_df_S), (filter_epoch_spikes_df_L, filter_epoch_spikes_df_S), (good_example_epoch_indicies_L, good_example_epoch_indicies_S), (short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset), new_all_aclus_sort_indicies, assigning_epochs_obj = PAPER_FIGURE_figure_1_add_replay_epoch_rasters(curr_active_pipeline)\n",
    "    # neuron_replay_stats_df, short_exclusive, long_exclusive, BOTH_subset, EITHER_subset, XOR_subset, NEITHER_subset = jonathan_firing_rate_analysis_result.get_cell_track_partitions(frs_index_inclusion_magnitude=0.05)\n",
    "\n",
    "    # ## Output the BatchPhoJonathanFiguresHelper\n",
    "    # fig_1c_figures_all_dict = BatchPhoJonathanFiguresHelper.run(curr_active_pipeline, neuron_replay_stats_df.sort_values('custom_frs_index', ascending=True, inplace=False), included_unit_neuron_IDs=None,\n",
    "\t# n_max_page_rows=20, write_vector_format=False, write_png=True,\n",
    "\t# show_only_refined_cells=False, disable_top_row=False, split_by_short_long_shared=False)\n",
    "\n",
    "    \n",
    "    # global_replays.filename = Path(f\"output/{output_file_prefix}_global_replays\").resolve()\n",
    "    # print(f'global_replays.filename: {global_replays.filename}')\n",
    "    # global_replays.to_neuroscope()\n",
    "\n",
    "    # global_PBEs.filename = Path(f\"output/{output_file_prefix}_global_PBEs\").resolve()\n",
    "    # print(f'global_PBEs.filename: {global_PBEs.filename}')\n",
    "    # global_PBEs.to_neuroscope('PBE')\n",
    "\n",
    "\n",
    "    # curr_active_pipeline, directional_lap_specific_configs = DirectionalLapsHelpers.split_to_directional_laps(curr_active_pipeline, add_created_configs_to_pipeline=True)\n",
    "    # curr_active_pipeline, directional_lap_specific_configs = constrain_to_laps(curr_active_pipeline)\n",
    "    # list(directional_lap_specific_configs.keys())\n",
    "\n",
    "    # joined_neruon_fri_df = build_merged_neuron_firing_rate_indicies(curr_active_pipeline, enable_display_intermediate_results=False)\n",
    "    # AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df')\n",
    "    print(f'>>\\t done with {output_file_prefix}')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "    return across_session_results_extended_dict\n",
    "\n",
    "\n",
    "result_handler.completion_functions.append(a_test_completion_function)\n",
    "\n",
    "## Specific Setup for 2023-09-28 Changes to LxC/SxC \"refinements\"\n",
    "result_handler.extended_computations_include_includelist = ['pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "                                                # 'pf_dt_sequential_surprise',\n",
    "                                                # 'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                'position_decoding_two_step', \n",
    "                                                'long_short_decoding_analyses',\n",
    "                                                'jonathan_firing_rate_analysis', 'long_short_fr_indicies_analyses', 'short_long_pf_overlap_analyses', 'long_short_post_decoding', 'long_short_rate_remapping',\n",
    "                                                'long_short_inst_spike_rate_groups',\n",
    "                                                'long_short_endcap_analysis',\n",
    "                                                'split_to_directional_laps'\n",
    "                                                ]\n",
    "\n",
    "\n",
    "basic_local_computations = ['pf_computation', 'pfdt_computation', 'firing_rate_trends',\n",
    "#                                                 'pf_dt_sequential_surprise',\n",
    "                                                # 'ratemap_peaks_prominence2d',\n",
    "                                                'position_decoding', \n",
    "                                                #'position_decoding_two_step', \n",
    "                                                ]\n",
    " \n",
    "# result_handler.extended_computations_include_includelist = ['long_short_inst_spike_rate_groups']\n",
    "\n",
    "\n",
    "result_handler.enable_hdf5_output = True # output the HDF5 when done.\n",
    "# result_handler.override_existing_frs_index_values = True\n",
    "# result_handler.frs_index_inclusion_magnitude = 0.1\n",
    "\n",
    "# result_handler.enable_hdf5_output = False\n",
    "result_handler.override_existing_frs_index_values = False\n",
    "\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "global_batch_run.execute_all(force_reload=result_handler.force_reload_all, saving_mode=result_handler.saving_mode, skip_extended_batch_computations=True, post_run_callback_fn=active_post_run_callback_fn,\n",
    "                             fail_on_exception=False, included_session_contexts=included_session_contexts,\n",
    "                                                                                        **{'computation_functions_name_includelist': basic_local_computations,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}, **multiprocessing_kwargs) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0981cde1",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_name</th>\n",
       "      <th>animal</th>\n",
       "      <th>exper_name</th>\n",
       "      <th>session_name</th>\n",
       "      <th>context</th>\n",
       "      <th>basedirs</th>\n",
       "      <th>status</th>\n",
       "      <th>errors</th>\n",
       "      <th>session_datetime</th>\n",
       "      <th>n_long_laps</th>\n",
       "      <th>n_long_replays</th>\n",
       "      <th>n_short_laps</th>\n",
       "      <th>n_short_replays</th>\n",
       "      <th>is_ready</th>\n",
       "      <th>global_computation_result_file</th>\n",
       "      <th>loaded_session_pickle_file</th>\n",
       "      <th>ripple_result_file</th>\n",
       "      <th>has_user_replay_annotations</th>\n",
       "      <th>has_user_grid_bin_bounds_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [format_name, animal, exper_name, session_name, context, basedirs, status, errors, session_datetime, n_long_laps, n_long_replays, n_short_laps, n_short_replays, is_ready, global_computation_result_file, loaded_session_pickle_file, ripple_result_file, has_user_replay_annotations, has_user_grid_bin_bounds_annotations]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f140f",
   "metadata": {},
   "source": [
    "# Across Sessions After Batching Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce885b-5b99-4a7a-9f72-2eed2e45ae18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_batch_progress_df = included_session_batch_progress_df.copy()\n",
    "\n",
    "good_session_concrete_folders = [ConcreteSessionFolder(a_context, a_basedir) for a_context, a_basedir in zip(list(a_batch_progress_df.context.values), list(a_batch_progress_df.basedirs.values))]\n",
    "\n",
    "# good_only_batch_progress_df.batch_results\n",
    "# included_h5_paths = [get_file_str_if_file_exists(v.joinpath('output','pipeline_results.h5').resolve()) for v in list(good_only_batch_progress_df.basedirs.values)]\n",
    "# included_h5_paths = [a_dir.joinpath('output','pipeline_results.h5').resolve() for a_dir in included_session_batch_progress_df['basedirs']]\n",
    "included_h5_paths = [get_file_str_if_file_exists(v.pipeline_results_h5) for v in good_session_concrete_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39738fa-f8f3-46d2-a937-0fb08c0ccb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_dir = Path('output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-09-29').resolve()\n",
    "# target_dir = Path('/home/halechr/cloud/turbo/Pho/Output/across_session_results/2023-10-03').resolve()\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, target_dir=target_dir)\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-05', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix=BATCH_DATE_TO_USE, only_include_file_types=['local_pkl', 'global_pkl'])\n",
    "# copy_dict = ConcreteSessionFolder.build_backup_copydict(good_session_concrete_folders, backup_mode=BackupMethods.RenameInSourceDirectory, rename_backup_suffix='2023-10-07', only_include_file_types=['local_pkl', 'global_pkl','h5'])\n",
    "copy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cb2d8-65b3-405b-a41b-22b2fa7cb28e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_dict_h5_files = copy_movedict(copy_dict)\n",
    "moved_files_dict_h5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e3954-15a4-449c-b927-56d5d79153c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moved_files_copydict_output_filename=f'backed_up_files_copydict_{BATCH_DATE_TO_USE}.csv'\n",
    "moved_files_copydict_file_path = Path(global_data_root_parent_path).joinpath(moved_files_copydict_output_filename).resolve() # Use Default\n",
    "print(f'moved_files_copydict_file_path: {moved_files_copydict_file_path}')\n",
    "\n",
    "_out_string, filedict_out_path = save_copydict_to_text_file(moved_files_dict_h5_files, moved_files_copydict_file_path, debug_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab869730",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_moved_files_dict_files = read_copydict_from_text_file(moved_files_copydict_file_path, debug_print=False)\n",
    "read_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_moved_files_dict_files\n",
    "restore_moved_files_dict_files = invert_filedict(read_moved_files_dict_files)\n",
    "restore_moved_files_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output_h5_files(included_h5_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0953",
   "metadata": {},
   "source": [
    "## Extract `across_sessions_instantaneous_fr_dict` from the computation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39691fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "across_sessions_recomputed_instantaneous_fr_dict = {}\n",
    "\n",
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "        a_good_recomp_result = a_result.across_session_results.get('recomputed_inst_fr_comps', None)\n",
    "        if a_good_recomp_result is not None:\n",
    "            across_sessions_recomputed_instantaneous_fr_dict[a_ctxt] = a_good_recomp_result\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_sessions_recomputed_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09136799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "# AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "#                                                  inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_recomputed_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path,\n",
    "                                                 inst_fr_output_filename=f'across_session_result_long_short_recomputed_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "# neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True, )\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_and_save_all_combined_tables(included_session_contexts, included_h5_paths, override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'{BATCH_DATE_TO_USE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a859bff-8cdb-4281-a64f-251d24db7cb9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionTables\n",
    "\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.build_all_known_tables(included_session_contexts, included_h5_paths, should_restore_native_column_types=True)\n",
    "# neuron_replay_stats_table['is_refined_LxC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b2430-c9ec-4adb-81a8-1ffbf8a0cb3c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95013ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_identities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(neuron_replay_stats_table['is_refined_LxC'])\n",
    "# np.isnan(neuron_replay_stats_table['is_refined_LxC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8615ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "session_identifier_key: str = 'session_name'\n",
    "# session_identifier_key: str = 'session_datetime'\n",
    "\n",
    "## !IMPORTANT! Count of the fields of interest using .value_counts(...) and converting to an explicit pd.DataFrame:\n",
    "# _out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "# _out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'count']\n",
    "_out_value_counts_df: pd.DataFrame = neuron_replay_stats_table.value_counts(subset=['format_name', 'animal', 'session_name', 'session_datetime','track_membership','is_refined_LxC', 'is_refined_SxC'], normalize=False, sort=False, ascending=True, dropna=True).reset_index()\n",
    "_out_value_counts_df.columns = ['format_name', 'animal', 'session_name', 'session_datetime', 'track_membership', 'is_refined_LxC', 'is_refined_SxC', 'count']\n",
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time of the first session for each animal:\n",
    "first_session_time  = _out_value_counts_df.groupby(['animal']).agg(session_datetime_first=('session_datetime', 'first')).reset_index()\n",
    "\n",
    "## Subtract this initial time from all of the 'session_datetime' entries for each animal:\n",
    "# Merge the first session time back into the original DataFrame\n",
    "merged_df = pd.merge(_out_value_counts_df, first_session_time, on='animal')\n",
    "\n",
    "# Subtract this initial time from all of the 'session_datetime' entries for each animal\n",
    "merged_df['time_since_first_session'] = merged_df['session_datetime'] - merged_df['session_datetime_first']\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point_size = 8\n",
    "df = _out_value_counts_df.copy()\n",
    "animals = df['animal'].unique()\n",
    "track_memberships = df['track_membership'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(animals), figsize=(15, 5))\n",
    "\n",
    "for i, animal in enumerate(animals):\n",
    "\tax = axes[i]\n",
    "\tsubset_df = df[df['animal'] == animal]\n",
    "\t\n",
    "\tfor track_membership in track_memberships:\n",
    "\t\ttrack_subset_df = subset_df[subset_df['track_membership'] == track_membership]\n",
    "\t\tax.plot(track_subset_df['session_datetime'], track_subset_df['count'], label=f'Track: {track_membership}')\n",
    "\t\tax.scatter(track_subset_df['session_datetime'], track_subset_df['count'], s=point_size)\n",
    "\t\t\n",
    "\tax.set_title(f'Animal: {animal}')\n",
    "\tax.set_xlabel('Session Datetime')\n",
    "\tax.set_ylabel('Count')\n",
    "\tax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## See if the number of cells decreases over re-exposures to the track\n",
    "df = _out_value_counts_df[_out_value_counts_df['animal'] == 'gor01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'pin01']\n",
    "# df = _out_value_counts_df[_out_value_counts_df['animal'] == 'vvp01']\n",
    "\n",
    "# Sort by column: 'session_datetime' (ascending)\n",
    "df = df.sort_values(['session_datetime'])\n",
    "\n",
    "'LEFT_ONLY'\n",
    "\n",
    "# df.to_clipboard(index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of cells in each session of the animal:\n",
    "num_LxCs = df[df['track_membership'] == 'LEFT_ONLY']['count'].to_numpy()\n",
    "num_Shared = df[df['track_membership'] == 'SHARED']['count'].to_numpy()\n",
    "num_SxCs = df[df['track_membership'] == 'RIGHT_ONLY']['count'].to_numpy()\n",
    "\n",
    "num_TotalCs = num_LxCs + num_Shared + num_SxCs\n",
    "num_TotalCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only safe point to align each session to is the switchpoint (the delta):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each session can be expressed in terms of time from the start of the first session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "source": [
    "## Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0cd9-3e60-4425-9351-dfc903f3f067",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import convert_filelist_to_new_parent\n",
    "\n",
    "def save_filelist_to_text_file(hdf5_output_paths, filelist_path: Path):\n",
    "    _out_string = '\\n'.join([str(a_file) for a_file in hdf5_output_paths])\n",
    "    print(f'{_out_string}')\n",
    "    print(f'saving out to \"{filelist_path}\"...')\n",
    "    with open(filelist_path, 'w') as f:\n",
    "        f.write(_out_string)\n",
    "    return _out_string, filelist_path\n",
    "\n",
    "# Save output filelist:\n",
    "\n",
    "# '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'\n",
    "\n",
    "# kdiba_vvp01_two_2006-4-10_12-58-3\n",
    "# \toutputs_local ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl')}\n",
    "# \toutputs_global ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'), 'hdf5': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5')}\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths = global_batch_run.build_output_files_lists()\n",
    "\n",
    "h5_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_HDF5_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_HDF5_savepath = save_filelist_to_text_file(hdf5_output_paths, h5_filelist_path)\n",
    "\n",
    "pkls_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_pkls_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_pkls_savepath = save_filelist_to_text_file(pkl_output_paths, pkls_filelist_path)\n",
    "\n",
    "# source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "source_parent_path = Path(r'/nfs/turbo/umms-kdiba/Data')\n",
    "dest_parent_path = Path(r'/~/W/Data/')\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_source = hdf5_output_paths\n",
    "filelist_dest_paths = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest_paths\n",
    "\n",
    "dest_Apogee_h5_filelist_path = global_data_root_parent_path.joinpath(f'dest_fileList_Apogee_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, dest_filelist_savepath = save_filelist_to_text_file(filelist_dest_paths, dest_Apogee_h5_filelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e69a3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "file_path\n",
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ce774-98fb-4e69-a7e2-e94cbff1b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "\n",
    "# list(global_batch_run.session_batch_outputs.keys())\n",
    "\n",
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "# good_session_batch_outputs = global_batch_run.session_batch_outputs\n",
    "\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d19a7-5a89-43f1-aa33-3a7450d1f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178426c-54df-47ac-8103-a66f114c77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[a_ctxt.get_initialization_code_string() for a_ctxt in sessions_with_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28828512",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d9a7",
   "metadata": {},
   "source": [
    "# 2023-10-06 - `joined_neruon_fri_df` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DATE_TO_USE = '2023-10-05_NewParameters'\n",
    "BATCH_DATE_TO_USE = '2023-10-07'\n",
    "all_sessions_joined_neruon_fri_df, out_path = build_and_merge_all_sessions_joined_neruon_fri_df(global_data_root_parent_path, BATCH_DATE_TO_USE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_neruon_fri_df_basename = f'{BATCH_DATE_TO_USE}_{output_file_prefix}_joined_neruon_fri_df'\n",
    "AcrossSessionTables.write_table_to_files(joined_neruon_fri_df, global_data_root_parent_path=global_data_root_parent_path, output_basename=joined_neruon_fri_df_basename, include_csv=False)\n",
    "print(f'>>\\t done with {output_file_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c4c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-10-04 - Load Saved across-sessions-data and testing Batch-computed inst_firing_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PaperFigureTwo, InstantaneousSpikeRateGroupsComputation\n",
    "# from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "# from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import list_of_dicts_to_dict_of_lists\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34549c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved across-session results:\n",
    "# inst_fr_output_filename = 'long_short_inst_firing_rate_result_handlers_2023-07-12.pkl'\n",
    "# inst_fr_output_filename = 'across_session_result_long_short_inst_firing_rate.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-07-21.pkl'\n",
    "# inst_fr_output_filename=f'across_session_result_handler_{BATCH_DATE_TO_USE}.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-08-09_Test.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-10-04-GL.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-04-GL-Recomp.pkl'\n",
    "inst_fr_output_filename='across_session_result_long_short_recomputed_inst_firing_rate_2023-10-07.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abea5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionTables\n",
    " \n",
    "## Load all across-session tables from the pickles:\n",
    "neuron_identities_table, long_short_fr_indicies_analysis_table, neuron_replay_stats_table = AcrossSessionTables.load_all_combined_tables(override_output_parent_path=global_data_root_parent_path, output_path_suffix=f'2023-10-07') # output_path_suffix=f'2023-10-04-GL-Recomp'\n",
    "num_sessions = len(neuron_replay_stats_table.session_uid.unique().to_numpy())\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_replay_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.core.user_annotations import UserAnnotationsManager, SessionCellExclusivityRecord\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "# for a_ctx, a_val in annotation_man.get_hardcoded_specific_session_override_dict().items():\n",
    "# \tannotation_man.annotations[a_ctx] = a_val\n",
    "\n",
    "# for a_ctx, a_val in UserAnnotationsManager.get_user_annotations().items():\n",
    "# \tannotation_man.annotations[a_ctx] = a_val\n",
    "\n",
    "# for a_ctx, a_val in session_cell_exclusivity_annotations.items():\n",
    "# \t# Not ideal. Adds a key 'session_cell_exclusivity' to the extant session context instead of being indexable by an entirely new context\n",
    "# \tannotation_man.annotations[a_ctx] = annotation_man.annotations.get(a_ctx, {}) | dict(session_cell_exclusivity=a_val)\n",
    "# \t# annotation_man.annotations[a_ctx.overwriting_context(user_annotation='session_cell_exclusivity')] = a_val\n",
    "\n",
    "annotation_man = UserAnnotationsManager()\n",
    "\n",
    "LxC_uids = []\n",
    "SxC_uids = []\n",
    "\n",
    "for a_ctxt in included_session_contexts:\n",
    "\tsession_uid = a_ctxt.get_description(separator=\"|\", include_property_names=False)\n",
    "\tsession_uid\n",
    "\tsession_cell_exclusivity: SessionCellExclusivityRecord = annotation_man.annotations[a_ctxt].get('session_cell_exclusivity', None)\n",
    "\tLxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.LxC])\n",
    "\tSxC_uids.extend([f\"{session_uid}|{aclu}\" for aclu in session_cell_exclusivity.SxC])\n",
    "\t\n",
    "# [a_ctxt.get_description(separator=\"|\", include_property_names=False) for a_ctxt in included_session_contexts]\n",
    "\n",
    "long_short_fr_indicies_analysis_table['XxC_status'] = 'Shared'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, LxC_uids), 'XxC_status'] = 'LxC'\n",
    "long_short_fr_indicies_analysis_table.loc[np.isin(long_short_fr_indicies_analysis_table.neuron_uid, SxC_uids), 'XxC_status'] = 'SxC'\n",
    "\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Get the long peak location\n",
    "\n",
    "long_short_fr_indicies_analysis_table['long_pf_peak_x'] = neuron_replay_stats_table['long_pf_peak_x']\n",
    "long_short_fr_indicies_analysis_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1641aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='x_frs_index', title='Pf Peak position vs. LapsFRI', ylabel='Lap FRI')\n",
    "\n",
    "long_short_fr_indicies_analysis_table.plot.scatter(x='long_pf_peak_x', y='y_frs_index', title='Pf Peak position vs. ReplayFRI', ylabel='Replay FRI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9987dc",
   "metadata": {},
   "source": [
    " #TODO 2023-10-05 11:40: - [ ] Extract the \"contrarian cells\", the ones that have a strong exclusivity on the laps but the opposite tendency on the replays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_short_fr_indicies_analysis_table.to_csv('output/2023-10-07_long_short_fr_indicies_analysis_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427597f",
   "metadata": {},
   "source": [
    "# 2023-10-10 - Statistics for `across_sessions_bar_graphs`, analysing `across_session_inst_fr_computation` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33422d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import pho_stats_perform_diagonal_line_binomial_test, pho_stats_bar_graph_t_tests\n",
    "\n",
    "binom_test_chance_result = pho_stats_perform_diagonal_line_binomial_test(long_short_fr_indicies_analysis_table)\n",
    "print(f'binom_test_chance_result: {binom_test_chance_result}')\n",
    "\n",
    "LxC_Laps_T_result, SxC_Laps_T_result, LxC_Replay_T_result, SxC_Replay_T_result = pho_stats_bar_graph_t_tests(across_session_inst_fr_computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea5a5",
   "metadata": {},
   "source": [
    "## 2023-10-04 - Run `AcrossSessionsVisualizations` corresponding to the PhoDibaPaper2023 figures for all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1152c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \n",
    "global_multi_session_context, _out_aggregate_fig_2 = AcrossSessionsVisualizations.across_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels=False, enable_hover_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsVisualizations\n",
    "\n",
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_firing_rate_index_figure(long_short_fr_indicies_analysis_results=long_short_fr_indicies_analysis_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "graphics_output_dict = AcrossSessionsVisualizations.across_sessions_long_and_short_firing_rate_replays_v_laps_figure(neuron_replay_stats_table=neuron_replay_stats_table, num_sessions=num_sessions, save_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_man = UserAnnotationsManager()\n",
    "included_annotations = {ctxt:ann_man.annotations[ctxt].get('session_cell_exclusivity', None) for ctxt in included_session_contexts}\n",
    "\n",
    "all_LxCs = []\n",
    "all_SxCs = []\n",
    "\n",
    "for ctxt, an_ann in included_annotations.items():\n",
    "\tsession_ctxt_key:str = ctxt.get_description(separator='|', subset_includelist=IdentifyingContext._get_session_context_keys())\n",
    "\tall_LxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.LxC])\n",
    "\tall_SxCs.extend([f\"{session_ctxt_key}|{aclu}\" for aclu in an_ann.SxC])\n",
    "\t\n",
    "all_LxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1222a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_SxCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1381fcf5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'across_session_inst_fr_computation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/halechr/repos/Spike3D/BatchGenerateOutputs_2023-10-25.ipynb Cell 66\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchGenerateOutputs_2023-10-25.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m across_session_inst_fr_computation\u001b[39m.\u001b[39mLxC_scatter_props\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/halechr/repos/Spike3D/BatchGenerateOutputs_2023-10-25.ipynb#Y122sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m across_session_inst_fr_computation\u001b[39m.\u001b[39mSxC_scatter_props\n",
      "\u001b[0;31mNameError\u001b[0m: name 'across_session_inst_fr_computation' is not defined"
     ]
    }
   ],
   "source": [
    "across_session_inst_fr_computation.LxC_scatter_props\n",
    "across_session_inst_fr_computation.SxC_scatter_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate across all of the sessions to build a new combined `InstantaneousSpikeRateGroupsComputation`, which can be used to plot the \"PaperFigureTwo\", bar plots for many sessions.\n",
    "global_multi_session_context = IdentifyingContext(format_name='kdiba', num_sessions=num_sessions) # some global context across all of the sessions, not sure what to put here.\n",
    "\n",
    "# To correctly aggregate results across sessions, it only makes sense to combine entries at the `.cell_agg_inst_fr_list` variable and lower (as the number of cells can be added across sessions, treated as unique for each session).\n",
    "\n",
    "## Display the aggregate across sessions:\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # WARNING: we didn't save this info\n",
    "_out_fig_2.computation_result = across_session_inst_fr_computation # the result loaded from the file\n",
    "_out_fig_2.active_identifying_session_ctx = across_session_inst_fr_computation.active_identifying_session_ctx\n",
    "# Set callback, the only self-specific property\n",
    "# _out_fig_2._pipeline_file_callback_fn = curr_active_pipeline.output_figure # lambda args, kwargs: self.write_to_file(args, kwargs, curr_active_pipeline)\n",
    "_out_fig_2.scatter_props_fn = _return_scatter_props_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LxC_aclus = _out_fig_2.computation_result.LxC_aclus\n",
    "SxC_aclus = _out_fig_2.computation_result.SxC_aclus\n",
    "\n",
    "LxC_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureOutputManager, FigureOutputLocation, ContextToPathMode\n",
    "\n",
    "registered_output_files = {}\n",
    "\n",
    "def output_figure(final_context: IdentifyingContext, fig, write_vector_format:bool=False, write_png:bool=True, debug_print=True):\n",
    "    \"\"\" outputs the figure using the provided context. \"\"\"\n",
    "    from pyphoplacecellanalysis.General.Mixins.ExportHelpers import build_and_write_to_file\n",
    "    def register_output_file(output_path, output_metadata=None):\n",
    "        \"\"\" registers a new output file for the pipeline \"\"\"\n",
    "        print(f'register_output_file(output_path: {output_path}, ...)')\n",
    "        registered_output_files[output_path] = output_metadata or {}\n",
    "\n",
    "    fig_out_man = FigureOutputManager(figure_output_location=FigureOutputLocation.DAILY_PROGRAMMATIC_OUTPUT_FOLDER, context_to_path_mode=ContextToPathMode.HIERARCHY_UNIQUE)\n",
    "    active_out_figure_paths = build_and_write_to_file(fig, final_context, fig_out_man, write_vector_format=write_vector_format, write_png=write_png, register_output_file_fn=register_output_file)\n",
    "    return active_out_figure_paths, final_context\n",
    "\n",
    "\n",
    "# Set callback, the only self-specific property\n",
    "_out_fig_2._pipeline_file_callback_fn = output_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing\n",
    "restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# Perform interactive Matplotlib operations with 'Qt5Agg' backend\n",
    "_fig_2_theta_out, _fig_2_replay_out = _out_fig_2.display(active_context=global_multi_session_context, title_modifier_fn=lambda original_title: f\"{original_title} ({num_sessions} sessions)\", save_figure=True)\n",
    "\t\n",
    "_out_fig_2.perform_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2023-10-11 - Surprise Shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d829b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
