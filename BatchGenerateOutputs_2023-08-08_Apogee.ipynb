{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "is_executing": true,
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n",
      "active_global_batch_result_suffix: 2023-08-08_Apogee\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultDataframeAccessor, run_diba_batch, BatchComputationProcessOptions, BatchSessionCompletionHandler, SessionBatchProgress, main\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows\n",
    "\n",
    "BATCH_DATE_TO_USE = '2023-08-08' # used for filenames throught the notebook\n",
    "active_global_batch_result_suffix:str = f\"{BATCH_DATE_TO_USE}_Apogee\"\n",
    "print(f'active_global_batch_result_suffix: {active_global_batch_result_suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019afbbd-70d2-4e75-9548-b6f22d2e31ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hardcoded included_session_contexts:\n",
    "\n",
    "\n",
    "\n",
    "included_session_contexts: Optional[List[IdentifyingContext]] = [\n",
    "    # IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'),\n",
    "    # IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "    # IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54')]\n",
    "\n",
    "## Included Session Contexts:\n",
    "# included_session_contexts = batch_progress_df[np.logical_and(batch_progress_df['has_user_replay_annotations'], batch_progress_df['is_ready'])]['context'].to_numpy().tolist()\n",
    "\n",
    "# Only require sessions to have replay annotations:\n",
    "# included_session_contexts = batch_progress_df[batch_progress_df['has_user_replay_annotations']]['context'].to_numpy().tolist()\n",
    "\n",
    "# included_session_contexts = batch_progress_df['context'].to_numpy().tolist()[:4] # Only get the first 6\n",
    "## Limit the contexts to run to the last N:\n",
    "# included_session_contexts = included_session_contexts[:2]\n",
    "\n",
    "# ## No filtering the sessions:\n",
    "# included_session_contexts = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d382a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting runBatch.main(active_result_suffix: \"2023-08-08_Apogee\", ...) ...\n",
      "finalized_loaded_global_batch_result_pickle_path: W:\\Data\\global_batch_result_2023-08-08_Apogee.pkl\n",
      "finalized_loaded_global_batch_result_pickle_path: W:\\Data\\global_batch_result_2023-08-08_Apogee.pkl\n",
      "Loading loaded session pickle file results : W:\\Data\\global_batch_result_2023-08-08_Apogee.pkl... encountered exception 'BatchRun' object is not iterable while printing. Turning into a warning and continuing.\n",
      "done.\n",
      "no difference between provided and internal paths.\n",
      "len(included_session_contexts): 12\n",
      "Beginning processing with len(included_session_contexts): 12\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\loadedSessPickle.pkl.\n",
      "property already present in pickled version. No need to save.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-07_16-40-19, curr_session_basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19, ...)\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\output\\global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 3:35:28.502903\n",
      "pipeline hdf5_output_path: W:\\Data\\KDIBA\\gor01\\two\\2006-6-07_16-40-19\\output\\pipeline_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-07_16-40-19...\n",
      "\t\t Now have 1 entries in self.across_sessions_instantaneous_fr_dict!\n",
      "\t\t done (success).\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-08_21-16-25\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-08_21-16-25\\loadedSessPickle.pkl... done.\n",
      "Loading pickled pipeline success: W:\\Data\\KDIBA\\gor01\\two\\2006-6-08_21-16-25\\loadedSessPickle.pkl.\n",
      "property already present in pickled version. No need to save.\n",
      "using provided computation_functions_name_includelist: ['_perform_baseline_placefield_computation', '_perform_extended_statistics_computation', '_perform_position_decoding_computation', '_perform_firing_rate_trends_computation', '_perform_pf_find_ratemap_peaks_computation', '_perform_two_step_position_decoding_computation']\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze1] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze2] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: skipping computation because overwrite_extant_results=False and active_computation_results[maze] already exists and is non-None\n",
      "\t TODO: this will prevent recomputation even when the excludelist/includelist or computation function definitions change. Rework so that this is smarter.\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "saving_mode.shouldSave == False, so not saving at the end of batch_load_session\n",
      "on_complete_success_execution_session(curr_session_context: kdiba_gor01_two_2006-6-08_21-16-25, curr_session_basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-08_21-16-25, ...)\n",
      "were pipeline preprocessing parameters missing and updated?: False\n",
      "WARNING: saving_mode is SKIP_SAVING so pipeline will not be saved despite calling .save_pipeline(...).\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-08_21-16-25\\output\\global_computation_results.pkl... done.\n",
      "skipping figure generation because should_perform_figure_generation_to_file == False\n",
      "\t time since last computation: 3:27:27.817629\n",
      "pipeline hdf5_output_path: W:\\Data\\KDIBA\\gor01\\two\\2006-6-08_21-16-25\\output\\pipeline_results.h5\n",
      "\t doing specific instantaneous firing rate computation for context: kdiba_gor01_two_2006-6-08_21-16-25...\n",
      "\t\t Now have 2 entries in self.across_sessions_instantaneous_fr_dict!\n",
      "\t\t done (success).\n",
      "basedir: W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\n",
      "active_data_mode_name: kdiba\n",
      "Loading loaded session pickle file results : W:\\Data\\KDIBA\\gor01\\two\\2006-6-09_22-24-40\\loadedSessPickle.pkl... done.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m global_batch_run, result_handler, across_sessions_instantaneous_fr_dict, output_filenames_tuple \u001b[39m=\u001b[39m main(active_result_suffix\u001b[39m=\u001b[39;49mactive_global_batch_result_suffix, included_session_contexts\u001b[39m=\u001b[39;49mincluded_session_contexts,\n\u001b[0;32m      2\u001b[0m                                                                                                        num_processes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, should_force_reload_all\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, should_perform_figure_generation_to_file\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, debug_print\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:1449\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(active_result_suffix, included_session_contexts, num_processes, should_force_reload_all, should_perform_figure_generation_to_file, debug_print)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[39m## Execute with the custom arguments.\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m active_computation_functions_name_includelist\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_perform_baseline_placefield_computation\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1439\u001b[0m                                         \u001b[39m# '_perform_time_dependent_placefield_computation',\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m                                         \u001b[39m'\u001b[39m\u001b[39m_perform_extended_statistics_computation\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1446\u001b[0m                                         \u001b[39m# '_perform_recursive_latent_placefield_decoding'\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m                                     ]\n\u001b[1;32m-> 1449\u001b[0m global_batch_run\u001b[39m.\u001b[39mexecute_all(force_reload\u001b[39m=\u001b[39mresult_handler\u001b[39m.\u001b[39mforce_reload_all, saving_mode\u001b[39m=\u001b[39mresult_handler\u001b[39m.\u001b[39msaving_mode, skip_extended_batch_computations\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, post_run_callback_fn\u001b[39m=\u001b[39mresult_handler\u001b[39m.\u001b[39mon_complete_success_execution_session,\n\u001b[0;32m   1450\u001b[0m                          fail_on_exception\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, included_session_contexts\u001b[39m=\u001b[39mincluded_session_contexts,\n\u001b[0;32m   1451\u001b[0m                                                                                     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mcomputation_functions_name_includelist\u001b[39m\u001b[39m'\u001b[39m: active_computation_functions_name_includelist,\n\u001b[0;32m   1452\u001b[0m                                                                                         \u001b[39m'\u001b[39m\u001b[39mactive_session_computation_configs\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1453\u001b[0m                                                                                         \u001b[39m'\u001b[39m\u001b[39mallow_processing_previously_completed\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m}, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmultiprocessing_kwargs) \u001b[39m# can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[39m# Save to pickle:\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m saveData(finalized_loaded_global_batch_result_pickle_path, global_batch_run) \u001b[39m# Update the global batch run dictionary\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:318\u001b[0m, in \u001b[0;36mBatchRun.execute_all\u001b[1;34m(self, use_multiprocessing, num_processes, included_session_contexts, session_inclusion_filter, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m curr_session_status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_batch_status[curr_session_context]\n\u001b[0;32m    317\u001b[0m curr_session_basedir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_batch_basedirs[curr_session_context]\n\u001b[1;32m--> 318\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_batch_status[curr_session_context], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_batch_errors[curr_session_context], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_batch_outputs[curr_session_context] \u001b[39m=\u001b[39m run_specific_batch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_data_root_parent_path, curr_session_context, curr_session_basedir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:1346\u001b[0m, in \u001b[0;36mrun_specific_batch\u001b[1;34m(global_data_root_parent_path, curr_session_context, curr_session_basedir, force_reload, post_run_callback_fn, **kwargs)\u001b[0m\n\u001b[0;32m   1343\u001b[0m debug_print \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mdebug_print\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1346\u001b[0m     curr_active_pipeline \u001b[39m=\u001b[39m batch_load_session(global_data_root_parent_path, active_data_mode_name, basedir, epoch_name_includelist\u001b[39m=\u001b[39mepoch_name_includelist,\n\u001b[0;32m   1347\u001b[0m                                     computation_functions_name_includelist\u001b[39m=\u001b[39mactive_computation_functions_name_includelist,\n\u001b[0;32m   1348\u001b[0m                                     saving_mode\u001b[39m=\u001b[39msaving_mode, force_reload\u001b[39m=\u001b[39mforce_reload, skip_extended_batch_computations\u001b[39m=\u001b[39mskip_extended_batch_computations, debug_print\u001b[39m=\u001b[39mdebug_print, fail_on_exception\u001b[39m=\u001b[39mfail_on_exception, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1351\u001b[0m     \u001b[39m## can fail here before callback function is even called.\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m     \u001b[39mreturn\u001b[39;00m (SessionBatchProgress\u001b[39m.\u001b[39mFAILED, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m# return the Failed status and the exception that occured.\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\NonInteractiveProcessing.py:126\u001b[0m, in \u001b[0;36mbatch_load_session\u001b[1;34m(global_data_root_parent_path, active_data_mode_name, basedir, force_reload, saving_mode, fail_on_exception, skip_extended_batch_computations, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m active_data_mode_type_properties \u001b[39m=\u001b[39m known_data_session_type_properties_dict[active_data_mode_name]\n\u001b[0;32m    125\u001b[0m \u001b[39m## Begin main run of the pipeline (load or execute):\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m curr_active_pipeline \u001b[39m=\u001b[39m NeuropyPipeline\u001b[39m.\u001b[39;49mtry_init_from_saved_pickle_or_reload_if_needed(active_data_mode_name, active_data_mode_type_properties,\n\u001b[0;32m    127\u001b[0m     override_basepath\u001b[39m=\u001b[39;49mPath(basedir), force_reload\u001b[39m=\u001b[39;49mforce_reload, active_pickle_filename\u001b[39m=\u001b[39;49mactive_pickle_filename, skip_save_on_initial_load\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    129\u001b[0m was_loaded_from_file: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m  curr_active_pipeline\u001b[39m.\u001b[39mhas_associated_pickle \u001b[39m# True if pipeline was loaded from an existing file, False if it was created fresh\u001b[39;00m\n\u001b[0;32m    132\u001b[0m active_session_filter_configurations \u001b[39m=\u001b[39m active_data_mode_registered_class\u001b[39m.\u001b[39mbuild_default_filter_functions(sess\u001b[39m=\u001b[39mcurr_active_pipeline\u001b[39m.\u001b[39msess, epoch_name_includelist\u001b[39m=\u001b[39mepoch_name_includelist) \u001b[39m# build_filters_pyramidal_epochs(sess=curr_kdiba_pipeline.sess)\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\NeuropyPipeline.py:237\u001b[0m, in \u001b[0;36mNeuropyPipeline.try_init_from_saved_pickle_or_reload_if_needed\u001b[1;34m(cls, type_name, known_type_properties, override_basepath, override_post_load_functions, force_reload, active_pickle_filename, skip_save_on_initial_load, progress_print, debug_print)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfinalized_loaded_sess_pickle_path: \u001b[39m\u001b[39m{\u001b[39;00mfinalized_loaded_sess_pickle_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     loaded_pipeline \u001b[39m=\u001b[39m loadData(finalized_loaded_sess_pickle_path, debug_print\u001b[39m=\u001b[39;49mdebug_print)\n\u001b[0;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m    240\u001b[0m     \u001b[39m# loading failed\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailure loading \u001b[39m\u001b[39m{\u001b[39;00mfinalized_loaded_sess_pickle_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Pipeline\\Stages\\Loading.py:44\u001b[0m, in \u001b[0;36mloadData\u001b[1;34m(pkl_path, debug_print, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(pkl_path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m dbfile:\n\u001b[0;32m     43\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         db \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(dbfile, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     47\u001b[0m         error_message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(err)\n",
      "File \u001b[1;32mc:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\.venv\\lib\\site-packages\\dill\\_dill.py:373\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, ignore, **kwds)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(file, ignore\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m    368\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39m    Unpickle an object from a file.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[39m    See :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mreturn\u001b[39;00m Unpickler(file, ignore\u001b[39m=\u001b[39;49mignore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[1;32mc:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\.venv\\lib\\site-packages\\dill\\_dill.py:646\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m--> 646\u001b[0m     obj \u001b[39m=\u001b[39m StockUnpickler\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    647\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39mgetattr\u001b[39m(_main_module, \u001b[39m'\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    648\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore:\n\u001b[0;32m    649\u001b[0m             \u001b[39m# point obj class to main\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\NeuroPy\\neuropy\\utils\\dynamic_container.py:289\u001b[0m, in \u001b[0;36mDynamicContainer.__setstate__\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Remove the unpicklable entries.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# del state['file']\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m state\n\u001b[1;32m--> 289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setstate__\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m    290\u001b[0m     \u001b[39m# Restore instance attributes (i.e., _mapping and _keys_at_init).\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mupdate(state)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_batch_run, result_handler, across_sessions_instantaneous_fr_dict, output_filenames_tuple = main(active_result_suffix=active_global_batch_result_suffix, included_session_contexts=included_session_contexts,\n",
    "                                                                                                       num_processes=4, should_force_reload_all=False, should_perform_figure_generation_to_file=False, debug_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ced46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_active_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun\n",
    "\n",
    "batch_slurm_scripts_directory = global_batch_run.global_data_root_parent_path.joinpath('slurm_scripts')\n",
    "batch_slurm_scripts_directory.mkdir(mode=755, parents=False, exist_ok=True)\n",
    "print(f'batch_slurm_scripts_directory: {batch_slurm_scripts_directory}')\n",
    "included_session_contexts, output_python_scripts, output_slurm_scripts = global_batch_run.generate_batch_slurm_jobs(included_session_contexts, batch_slurm_scripts_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ca2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_python_scripts\n",
    "# output_slurm_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_slurm_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "# good_only_batch_progress_df\n",
    "batch_progress_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dddddb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Old (pre 2023-08-08) Run Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to load an existing batch result:\n",
    "# # with set_posix_windows():\n",
    "# global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "# \t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "# # batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "# # good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "# # batch_progress_df.batch_results.build_all_columns()\n",
    "# # good_only_batch_progress_df.batch_results.build_all_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd22042-113c-486f-8264-8ddfa6bb9474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multiprocessing_kwargs = dict(use_multiprocessing=False, num_processes=1)\n",
    "multiprocessing_kwargs = dict(use_multiprocessing=True, num_processes=8)\n",
    "\n",
    "# Whether to output figures:\n",
    "# should_perform_figure_generation_to_file=False\n",
    "should_perform_figure_generation_to_file=True\n",
    "\n",
    "## Included Session Contexts:\n",
    "# included_session_contexts = batch_progress_df[np.logical_and(batch_progress_df['has_user_replay_annotations'], batch_progress_df['is_ready'])]['context'].to_numpy().tolist()\n",
    "\n",
    "# Only require sessions to have replay annotations:\n",
    "# included_session_contexts = batch_progress_df[batch_progress_df['has_user_replay_annotations']]['context'].to_numpy().tolist()\n",
    "\n",
    "# included_session_contexts = batch_progress_df['context'].to_numpy().tolist()[:4] # Only get the first 6\n",
    "## Limit the contexts to run to the last N:\n",
    "# included_session_contexts = included_session_contexts[:2]\n",
    "\n",
    "# ## No filtering the sessions:\n",
    "# included_session_contexts = None\n",
    "\n",
    "if included_session_contexts is not None:\n",
    "    print(f'len(included_session_contexts): {len(included_session_contexts)}')\n",
    "else:\n",
    "    print(f'included_session_contexts is None so all session contexts will be included.')\n",
    "\n",
    "# included_session_contexts\n",
    "\n",
    "\n",
    "# No Reloading\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=False,\n",
    "                                                session_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=False),\n",
    "                                                global_computations_options=BatchComputationProcessOptions(should_load=True, should_compute=False, should_save=False),\n",
    "                                                should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False,\n",
    "                                                **multiprocessing_kwargs)\n",
    "\n",
    "# # Forced Reloading:\n",
    "# result_handler = BatchSessionCompletionHandler(force_reload_all=True,\n",
    "#                                                 session_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=True),\n",
    "#                                                 global_computations_options=BatchComputationProcessOptions(should_load=False, should_compute=True, should_save=True),\n",
    "#                                                 should_perform_figure_generation_to_file=should_perform_figure_generation_to_file, saving_mode=PipelineSavingScheme.OVERWRITE_IN_PLACE, force_global_recompute=True,\n",
    "#                                                 **multiprocessing_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "active_computation_functions_name_includelist=['_perform_baseline_placefield_computation',\n",
    "                                        # '_perform_time_dependent_placefield_computation',\n",
    "                                        '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "# active_computation_functions_name_includelist=['_perform_baseline_placefield_computation']\n",
    "global_batch_run.execute_all(force_reload=result_handler.force_reload_all, saving_mode=result_handler.saving_mode, skip_extended_batch_computations=True, post_run_callback_fn=result_handler.on_complete_success_execution_session,\n",
    "                             fail_on_exception=False, included_session_contexts=included_session_contexts,\n",
    "                                                                                        **{'computation_functions_name_includelist': active_computation_functions_name_includelist,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}, **multiprocessing_kwargs) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23326ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "sessions_with_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data_root_parent_path = global_batch_run.global_data_root_parent_path.resolve()\n",
    "global_data_root_parent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae798c-78db-4cd4-8eb5-276447dce1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to pickle:\n",
    "global_batch_result_file_path = global_data_root_parent_path.joinpath(f'global_batch_run_test_{active_global_batch_result_suffix}')\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b42a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21482b-23ac-44e6-bfb2-8e3d480d74bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine across .h5 files:\n",
    "import tables as tb\n",
    "import h5py\n",
    "\n",
    "# f'/kdiba/gor01/one/2006-6-12_15-55-31/neuron_identities/table'\n",
    "\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths\n",
    "\n",
    "file_path = f'output/test_external_links.h5'\n",
    "a_global_computations_group_key: str = f\"{session_group_key}/global_computations\"\n",
    "with tb.open_file(file_path, mode='w') as f: # this mode='w' is correct because it should overwrite the previous file and not append to it.\n",
    "    a_global_computations_group = f.create_group(session_group_key, 'global_computations', title='the result of computations that operate over many or all of the filters in the session.', createparents=True)\n",
    "    an_external_link = f.create_external_link(f'', n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb43f9-214a-401b-8ae9-7dbe9af40180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import List\n",
    "from attrs import define, field, Factory\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "\n",
    "@define(slots=False)\n",
    "class H5Loader:\n",
    "    \"\"\"\n",
    "    H5Loader class for loading and consolidating .h5 files\n",
    "    Usage:\n",
    "        loader = H5Loader(file_list)\n",
    "        master_table = loader.load_and_consolidate()\n",
    "    \"\"\"\n",
    "    file_list: List[str] = field(default=Factory(list))\n",
    "    table_key_list: List[str] = field(default=Factory(list))\n",
    "    \n",
    "    def load_and_consolidate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads .h5 files and consolidates into a master table\n",
    "        \"\"\"\n",
    "        data_frames = []\n",
    "        for file, table_key in zip(self.file_list, self.table_key_list):\n",
    "            with tb.open_file(file) as h5_file:\n",
    "                a_table = h5_file.get_node(table_key)\n",
    "                print(f'a_table: {a_table}')\n",
    "                # for a_record in a_table\n",
    "                \n",
    "                # data_frames.append(a_table)\n",
    "#                 for table in h5_file.get_node(table_key):\n",
    "#                 # for table in h5_file.root:\n",
    "                # df = pd.DataFrame.from_records(a_table[:]) # .read()\n",
    "                df = pd.DataFrame.from_records(a_table.read()) \n",
    "                data_frames.append(df)\n",
    "\n",
    "        master_table = pd.concat(data_frames, ignore_index=True)\n",
    "        return master_table\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "session_group_keys: List[str] = [(\"/\" + a_ctxt.get_description(separator=\"/\", include_property_names=False)) for a_ctxt in session_identifiers] # 'kdiba/gor01/one/2006-6-08_14-26-15'\n",
    "# session_uid: str = session_context.get_description(separator=\"|\", include_property_names=False)\n",
    "neuron_identities_table_keys = [f\"{session_group_key}/neuron_identities/table\" for session_group_key in session_group_keys]\n",
    "\n",
    "\n",
    "a_loader = H5Loader(file_list=hdf5_output_paths, table_key_list=neuron_identities_table_keys)\n",
    "a_loader\n",
    "\n",
    "_out_table = a_loader.load_and_consolidate()\n",
    "_out_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc1ac7-5771-4cd5-94c6-7a6244eb8217",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract output files from all completed sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749849c-9f57-4d2c-aab8-5ee9353f6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/nfs/turbo/umms-kdiba/Data/KDIBA/gor01/one/2006-6-09_1-22-43/output/pipeline_results.h5'\n",
    "\n",
    "# kdiba_vvp01_two_2006-4-10_12-58-3\n",
    "# \toutputs_local ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/loadedSessPickle.pkl')}\n",
    "# \toutputs_global ={'pkl': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/global_computation_results.pkl'), 'hdf5': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/vvp01/two/2006-4-10_12-58-3/output/pipeline_results.h5')}\n",
    "\n",
    "session_identifiers, pkl_output_paths, hdf5_output_paths = global_batch_run.build_output_files_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361385b-385d-4e12-997b-e2a68404858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want .values from SingleBarResult \n",
    "session_identifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8288dee-a41a-4640-a139-cb0a64f01814",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0cd9-3e60-4425-9351-dfc903f3f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output filelist:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import save_filelist_to_text_file, convert_filelist_to_new_parent\n",
    "\n",
    "h5_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_HDF5_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_HDF5_savepath = save_filelist_to_text_file(hdf5_output_paths, h5_filelist_path)\n",
    "\n",
    "pkls_filelist_path = global_data_root_parent_path.joinpath(f'fileList_Greatlakes_pkls_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, src_filelist_pkls_savepath = save_filelist_to_text_file(pkl_output_paths, pkls_filelist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ab67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.Filesystem.path_helpers import convert_filelist_to_new_parent\n",
    "# source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "source_parent_path = Path(r'/nfs/turbo/umms-kdiba/Data')\n",
    "dest_parent_path = Path(r'/~/W/Data/')\n",
    "\n",
    "\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_source = hdf5_output_paths\n",
    "filelist_dest_paths = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest_paths\n",
    "\n",
    "dest_Apogee_h5_filelist_path = global_data_root_parent_path.joinpath(f'dest_fileList_Apogee_{BATCH_DATE_TO_USE}.txt').resolve()\n",
    "_out_string, dest_filelist_savepath = save_filelist_to_text_file(filelist_dest_paths, dest_Apogee_h5_filelist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412260ec-ddbd-4727-8795-438717dd9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_filelist_savepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e69a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.runBatch import PipelineCompletionResult\n",
    "from neuropy.core.epoch import Epoch\n",
    "\n",
    "# Save to HDF5\n",
    "suffix = f'{BATCH_DATE_TO_USE}'\n",
    "## Build Pickle Path:\n",
    "file_path = global_data_root_parent_path.joinpath(f'global_batch_output_{suffix}.h5').resolve()\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0d19b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.to_hdf(file_path,'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ce774-98fb-4e69-a7e2-e94cbff1b0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the sessions with non-None results\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "\n",
    "# list(global_batch_run.session_batch_outputs.keys())\n",
    "\n",
    "# Somewhere in there there are `InstantaneousSpikeRateGroupsComputation` results to extract\n",
    "across_sessions_instantaneous_fr_dict = {} # InstantaneousSpikeRateGroupsComputation\n",
    "\n",
    "# good_session_batch_outputs = global_batch_run.session_batch_outputs\n",
    "\n",
    "sessions_with_results = [a_ctxt for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None]\n",
    "good_session_batch_outputs = {a_ctxt:a_result for a_ctxt, a_result in global_batch_run.session_batch_outputs.items() if a_result is not None}\n",
    "\n",
    "for a_ctxt, a_result in good_session_batch_outputs.items():\n",
    "    if a_result is not None:\n",
    "        # a_good_result = a_result.__dict__.get('across_sessions_batch_results', {}).get('inst_fr_comps', None)\n",
    "        a_good_result = a_result.across_session_results.get('inst_fr_comps', None)\n",
    "        if a_good_result is not None:\n",
    "            across_sessions_instantaneous_fr_dict[a_ctxt] = a_good_result\n",
    "            # print(a_result['across_sessions_batch_results']['inst_fr_comps'])\n",
    "            \n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n",
    "\n",
    "# When done, `result_handler.across_sessions_instantaneous_fr_dict` is now equivalent to what it would have been before. It can be saved using the normal `.save_across_sessions_data(...)`\n",
    "\n",
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "AcrossSessionsResults.save_across_sessions_data(across_sessions_instantaneous_fr_dict=across_sessions_instantaneous_fr_dict, global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl')\n",
    "\n",
    "# ## Save pickle:\n",
    "# inst_fr_output_filename=f'across_session_result_long_short_inst_firing_rate_{BATCH_DATE_TO_USE}.pkl'\n",
    "# global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "# print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# # Save the all sessions instantaneous firing rate dict to the path:\n",
    "# saveData(global_batch_result_inst_fr_file_path, across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d19a7-5a89-43f1-aa33-3a7450d1f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "across_sessions_instantaneous_fr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178426c-54df-47ac-8103-a66f114c77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[a_ctxt.get_initialization_code_string() for a_ctxt in sessions_with_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd124683-bfc2-49bf-9531-583e19db5a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "included_contexts = [IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'),\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'),\n",
    "IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be651cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2023-07-14 - Load Saved across-sessions-data and testing Batch-computed inst_firing_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.matplotlib_helpers import matplotlib_configuration_update\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import PaperFigureTwo, InstantaneousSpikeRateGroupsComputation\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import list_of_dicts_to_dict_of_lists\n",
    "from pyphoplacecellanalysis.General.Batch.AcrossSessionResults import AcrossSessionsResults, AcrossSessionsVisualizations\n",
    "\n",
    "## Load the saved across-session results:\n",
    "# inst_fr_output_filename = 'long_short_inst_firing_rate_result_handlers_2023-07-12.pkl'\n",
    "# inst_fr_output_filename = 'across_session_result_long_short_inst_firing_rate.pkl'\n",
    "# inst_fr_output_filename='across_session_result_long_short_inst_firing_rate_2023-07-21.pkl'\n",
    "inst_fr_output_filename=f'across_session_result_handler_{BATCH_DATE_TO_USE}.pkl'\n",
    "across_session_inst_fr_computation, across_sessions_instantaneous_fr_dict, across_sessions_instantaneous_frs_list = AcrossSessionsResults.load_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename=inst_fr_output_filename)\n",
    "# across_sessions_instantaneous_fr_dict = loadData(global_batch_result_inst_fr_file_path)\n",
    "num_sessions = len(across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hacks the `PaperFigureTwo` and `InstantaneousSpikeRateGroupsComputation` \n",
    "global_multi_session_context, _out_aggregate_fig_2 = AcrossSessionsVisualizations.across_sessions_bar_graphs(across_session_inst_fr_computation, num_sessions, enable_tiny_point_labels=False, enable_hover_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381fcf5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "across_session_inst_fr_computation.LxC_scatter_props\n",
    "across_session_inst_fr_computation.SxC_scatter_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db1ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45d728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the unique scatter plot dictionaries:\n",
    "across_session_contexts = list(across_sessions_instantaneous_fr_dict.keys())\n",
    "unique_animals = IdentifyingContext.find_unique_values(across_session_contexts)['animal'] # {'gor01', 'pin01', 'vvp01'}\n",
    "# Get number of animals to plot\n",
    "marker_list = [(5, i) for i in np.arange(len(unique_animals))] # [(5, 0), (5, 1), (5, 2)]\n",
    "scatter_props = [{'marker': mkr} for mkr in marker_list]  # Example, you should provide your own scatter properties\n",
    "scatter_props_dict = dict(zip(unique_animals, scatter_props))\n",
    "# {'pin01': {'marker': (5, 0)},\n",
    "#  'gor01': {'marker': (5, 1)},\n",
    "#  'vvp01': {'marker': (5, 2)}}\n",
    "scatter_props_dict\n",
    "\n",
    "# Pass a function that will return a set of kwargs for a given context\n",
    "def _return_scatter_props_fn(ctxt: IdentifyingContext):\n",
    "\t\"\"\" captures `scatter_props_dict` \"\"\"\n",
    "\tanimal_id = str(ctxt.animal)\n",
    "\treturn scatter_props_dict[animal_id]\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate across all of the sessions to build a new combined `InstantaneousSpikeRateGroupsComputation`, which can be used to plot the \"PaperFigureTwo\", bar plots for many sessions.\n",
    "global_multi_session_context = IdentifyingContext(format_name='kdiba', num_sessions=num_sessions) # some global context across all of the sessions, not sure what to put here.\n",
    "\n",
    "# To correctly aggregate results across sessions, it only makes sense to combine entries at the `.cell_agg_inst_fr_list` variable and lower (as the number of cells can be added across sessions, treated as unique for each session).\n",
    "\n",
    "## Display the aggregate across sessions:\n",
    "_out_fig_2 = PaperFigureTwo(instantaneous_time_bin_size_seconds=0.01) # WARNING: we didn't save this info\n",
    "_out_fig_2.computation_result = across_session_inst_fr_computation # the result loaded from the file\n",
    "_out_fig_2.active_identifying_session_ctx = across_session_inst_fr_computation.active_identifying_session_ctx\n",
    "# Set callback, the only self-specific property\n",
    "# _out_fig_2._pipeline_file_callback_fn = curr_active_pipeline.output_figure # lambda args, kwargs: self.write_to_file(args, kwargs, curr_active_pipeline)\n",
    "_out_fig_2.scatter_props_fn = _return_scatter_props_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LxC_aclus = _out_fig_2.computation_result.LxC_aclus\n",
    "SxC_aclus = _out_fig_2.computation_result.SxC_aclus\n",
    "\n",
    "LxC_aclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import FigureOutputManager, FigureOutputLocation, ContextToPathMode\n",
    "\n",
    "registered_output_files = {}\n",
    "\n",
    "def output_figure(final_context: IdentifyingContext, fig, write_vector_format:bool=False, write_png:bool=True, debug_print=True):\n",
    "    \"\"\" outputs the figure using the provided context. \"\"\"\n",
    "    from pyphoplacecellanalysis.General.Mixins.ExportHelpers import build_and_write_to_file\n",
    "    def register_output_file(output_path, output_metadata=None):\n",
    "        \"\"\" registers a new output file for the pipeline \"\"\"\n",
    "        print(f'register_output_file(output_path: {output_path}, ...)')\n",
    "        registered_output_files[output_path] = output_metadata or {}\n",
    "\n",
    "    fig_out_man = FigureOutputManager(figure_output_location=FigureOutputLocation.DAILY_PROGRAMMATIC_OUTPUT_FOLDER, context_to_path_mode=ContextToPathMode.HIERARCHY_UNIQUE)\n",
    "    active_out_figure_paths = build_and_write_to_file(fig, final_context, fig_out_man, write_vector_format=write_vector_format, write_png=write_png, register_output_file_fn=register_output_file)\n",
    "    return active_out_figure_paths, final_context\n",
    "\n",
    "\n",
    "# Set callback, the only self-specific property\n",
    "_out_fig_2._pipeline_file_callback_fn = output_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17b920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_fig_2.computation_result.Fig2_Laps_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing\n",
    "restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# Perform interactive Matplotlib operations with 'Qt5Agg' backend\n",
    "_fig_2_theta_out, _fig_2_replay_out = _out_fig_2.display(active_context=global_multi_session_context, title_modifier_fn=lambda original_title: f\"{original_title} ({num_sessions} sessions)\", save_figure=True)\n",
    "\t\n",
    "_out_fig_2.perform_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32781c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Single Session testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_out = global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, computation_functions_name_includelist =['_perform_baseline_placefield_computation'], active_session_computation_configs=None) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "_test_out\n",
    "\n",
    "# global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, **{'computation_functions_name_includelist': ['_perform_baseline_placefield_computation'], 'active_session_computation_configs': None}) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 23.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2bb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "full_good_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is None]\n",
    "bad_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is not None]\n",
    "full_good_dirs\n",
    "bad_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f73f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status\n",
    "global_batch_run.session_batch_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faf2bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed516134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf02efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the list of sessions that are completely ready to process:\n",
    "full_good_ready_to_process_sessions = list(good_only_batch_progress_df['context'].to_numpy())\n",
    "full_good_ready_to_process_sessions\n",
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ad809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run[\"good_sessions_list\"].extend(full_good_ready_to_process_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636e3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.stop()\n",
    "project.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\",\\n\".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # List definitions\n",
    "\n",
    "# [IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a4bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\ncurr_context = \".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # Line definitions\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5689d1d-6400-4f87-be0e-a184a1d5bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82aedf-b024-4f07-b1a9-350730b4db8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "save_time = datetime.now()\n",
    " \n",
    "print(\"save_time =\", save_time)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = save_time.strftime(\"%Y-%m-%d_%I-%M%p\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc6bd-5b34-41d0-b906-b0ad9927b08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get output file paths:\n",
    "completed_pipeline_filename = 'loadedSessPickle.pkl'\n",
    "completed_global_computations_filename = 'outputs/global_computation_results.pkl'\n",
    "\n",
    "full_good_ready_to_process_session_paths = list(good_only_batch_progress_df['basedirs'].to_numpy())\n",
    "session_paths_output_folders = [sess_path.joinpath('outputs').resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "\n",
    "completed_pipeline_file_paths = [sess_path.joinpath(completed_pipeline_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths = [sess_path.joinpath(completed_global_computations_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab75122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countable Additivity \n",
    "# Any countable collections of points is size 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
