{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows collecting results across multiple pipeline runs. Concatenating across sessions and bin sizes. \n",
    "It takes CSVs, then determines the most recent one from the filename. \n",
    "\n",
    "There are two major sets of decoded epochs - Laps and Ripples/Replays\n",
    "There are two sets of marginals for the decoded epochs - the \"by epoch\" and \"by time bin\" marginals.\n",
    "The \"by time bin\" epochs are a larger granulation, with each epoch consisting of one or more time bin.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# 2024-01-23 - \n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-fet11-01_12-58-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-03_12-3-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_17-46-44_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_19-28-0_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-58-3_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-25-50_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_16-40-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_22-24-40_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_16-53-46_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_17-29-30_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_14-26-15_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_1-22-43_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-07_16-40-19_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_21-16-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_15-55-31_time_bin_size_sweep_results.h5\n",
    "\n",
    "\n",
    "\n",
    "# found_session_export_paths = [Path(v).resolve() for v in  [\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# ]]\n",
    "\n",
    "[\n",
    "    \"2024-02-19_0430PM-kdiba_vvp01_one_2006-4-10_12-25-50-(ripple_simple_pf_pearson_merged_df)_tbin-0.025.csv\",\n",
    "    \"2024-02-17_0210AM-kdiba_vvp01_one_2006-4-09_17-29-30-(laps_simple_pf_pearson_merged_df)_tbin-0.025.csv\",\n",
    "    \"2024-02-19_0430PM-kdiba_vvp01_one_2006-4-10_12-25-50-(laps_weighted_corr_merged_df)_tbin-0.025.csv\",\n",
    "    \"2024-02-19_0430PM-kdiba_vvp01_one_2006-4-10_12-25-50-(ripple_weighted_corr_merged_df)_tbin-0.025.csv\",\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "TODAY_DAY_DATE: 2024-03-14_Apogee\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# Switch to the desired interactivity mode\n",
    "plt.interactive(True)\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "template: str = 'plotly_dark' # set plotl template\n",
    "pio.templates.default = template\n",
    "from pyphocorehelpers.plotting.media_output_helpers import fig_to_clipboard\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _helper_build_figure, plotly_pre_post_delta_scatter, plotly_pre_post_delta_scatter, plot_across_sessions_scatter_results\n",
    "\n",
    "# from ..PendingNotebookCode import plot_across_sessions_scatter_results, plot_histograms, plot_stacked_histograms\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import find_csv_files, find_HDF5_files, find_most_recent_files, process_csv_file, plot_across_sessions_scatter_results, plot_histograms, plot_stacked_histograms\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DecoderDecodedEpochsResult\n",
    "\n",
    "debug_print: bool = False\n",
    "\n",
    "TODAY_DAY_DATE: str = f\"2024-03-14_Apogee\"\n",
    "\n",
    "print(f'TODAY_DAY_DATE: {TODAY_DAY_DATE}')\n",
    "\n",
    "types.session_str: TypeAlias = str # a unique session identifier\n",
    "\n",
    "\n",
    "def process_and_load_file(session_dict, df_file_name_key, loaded_dict, session_name, curr_session_t_delta, time_key):\n",
    "    try:\n",
    "        file_path = session_dict[df_file_name_key]\n",
    "        loaded_dict[session_name] = process_csv_file(file_path, session_name, curr_session_t_delta, time_key)\n",
    "    except BaseException as e:\n",
    "        print(f'session {session_name} did not fully work. (error {e}. Skipping.')\n",
    "\n",
    "\n",
    "# def prototype_flexible_process_and_load_file(session_dict, df_file_name_key, loaded_dict, session_name, curr_session_t_delta, time_key):\n",
    "#     \"\"\" supposed to not require a bunch of manually created input variable names like:\n",
    "\n",
    "#     final_sessions_loaded_laps_dict = {}\n",
    "#     final_sessions_loaded_ripple_dict = {}\n",
    "#     final_sessions_loaded_laps_time_bin_dict = {}\n",
    "#     final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "#     and then processing them separately\n",
    "\n",
    "#     Instead it splits the df_file_name_key like 'ripple_weighted_corr_merged_df' -> ('ripple', 'weighted_corr_merged_df')\n",
    "#         and assigns to a nested dictionary\n",
    "    \n",
    "#     \"\"\"\n",
    "#     (an_epochs_source_name, a_df_variable_name) = df_file_name_key.split(sep='_', maxsplit=1) # get the first part of the variable names that indicates whether it's for \"laps\" or \"ripple\"\n",
    "#     # like 'ripple_weighted_corr_merged_df' -> ('ripple', 'weighted_corr_merged_df')\n",
    "#     try:\n",
    "#         file_path = session_dict[df_file_name_key]\n",
    "#         if an_epochs_source_name not in loaded_dict[session_name]:\n",
    "#             loaded_dict[session_name][an_epochs_source_name] = {}\n",
    "#         loaded_dict[session_name][an_epochs_source_name][a_df_variable_name] = process_csv_file(file_path, session_name, curr_session_t_delta, time_key)\n",
    "#     except BaseException as e:\n",
    "#         print(f'session {session_name} did not fully work. (error {e}. Skipping.')\n",
    "\n",
    "\n",
    "def _common_cleanup_operations(a_df):\n",
    "    \"\"\" post loading and concatenation across sessions dataframe cleanup \"\"\"\n",
    "    if a_df is None:\n",
    "        return None\n",
    "    ## Drop the weird 'Unnamed: 0' column:\n",
    "    # Rename column 'Unnamed: 0' to 'abs_time_bin_index'\n",
    "    a_df = a_df.rename(columns={'Unnamed: 0': 'abs_time_bin_index'})\n",
    "    # Drop column: 'abs_time_bin_index'\n",
    "    a_df = a_df.drop(columns=['abs_time_bin_index'])\n",
    "    # Add additional 'epoch_idx' column for compatibility:\n",
    "    if 'epoch_idx' not in a_df:\n",
    "        if 'lap_idx' in a_df:\n",
    "            a_df['epoch_idx'] = a_df['lap_idx']\n",
    "        if 'ripple_idx' in a_df:\n",
    "            a_df['epoch_idx'] = a_df['ripple_idx']\n",
    "    return a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "\"_new\" (as in `all_sessions_new_ripple_df`) adds simple_pearson and a few other measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected_outputs_directory: C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs\n",
      "ERR: Could not parse filename: \"2024-01-18_GL_t_split_df\"\n",
      "earliest_delta_aligned_t_start: -2057.2259484970764, latest_delta_aligned_t_end: 1661.8560019930592\n",
      "session kdiba_gor01_two_2006-6-07_16-40-19 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_gor01_one_2006-6-09_1-22-43 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_gor01_one_2006-6-08_14-26-15 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_pin01_one_11-02_19-28-0 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_gor01_one_2006-6-12_15-55-31 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_pin01_one_fet11-01_12-58-54 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-09_22-24-40 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-08_21-16-25 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_vvp01_one_2006-4-09_17-29-30 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_pin01_one_11-03_12-3-25 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-09_16-40-54 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_vvp01_one_2006-4-10_12-25-50 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-10_12-58-3 did not fully work. (error 'laps_simple_pf_pearson_merged_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-10_12-58-3 did not fully work. (error 'ripple_simple_pf_pearson_merged_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-10_12-58-3 did not fully work. (error 'laps_weighted_corr_merged_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-10_12-58-3 did not fully work. (error 'ripple_weighted_corr_merged_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-10_12-58-3 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-12_16-53-46 did not fully work. (error 'laps_simple_pf_pearson_merged_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-12_16-53-46 did not fully work. (error 'ripple_simple_pf_pearson_merged_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-12_16-53-46 did not fully work. (error 'laps_weighted_corr_merged_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-12_16-53-46 did not fully work. (error 'ripple_weighted_corr_merged_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-12_16-53-46 did not fully work. (error 'ripple_all_scores_merged_df'. Skipping.\n"
     ]
    }
   ],
   "source": [
    "## Load across session t_delta CSV, which contains the t_delta for each session:\n",
    "\n",
    "# t_delta_csv_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs\\2024-01-18_GL_t_split_df.csv').resolve() # Apogee\n",
    "# t_delta_csv_path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-01-18_GL_t_split_df.csv').resolve() # GL\n",
    "\n",
    "# collected_outputs_directory = '/home/halechr/FastData/collected_outputs/'\n",
    "# collected_outputs_directory = r'C:\\Users\\pho\\Desktop\\collected_outputs'\n",
    "# collected_outputs_directory = r'C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs' # APOGEE\n",
    "# collected_outputs_directory = '/home/halechr/cloud/turbo/Data/Output/collected_outputs' # GL\n",
    "\n",
    "known_collected_outputs_paths = [Path(v).resolve() for v in [r'C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs', '/home/halechr/cloud/turbo/Data/Output/collected_outputs', '/home/halechr/FastData/collected_outputs/']]\n",
    "collected_outputs_directory = find_first_extant_path(known_collected_outputs_paths)\n",
    "assert collected_outputs_directory.exists(), f\"collected_outputs_directory: {collected_outputs_directory} does not exist! Is the right computer's config commented out above?\"\n",
    "# fullwidth_path_widget(scripts_output_path, file_name_label='Scripts Output Path:')\n",
    "print(f'collected_outputs_directory: {collected_outputs_directory}')\n",
    "\n",
    "t_delta_csv_path = collected_outputs_directory.joinpath('2024-01-18_GL_t_split_df.csv').resolve() # GL\n",
    "assert t_delta_csv_path.exists()\n",
    "\n",
    "## Find the files:\n",
    "csv_files = find_csv_files(collected_outputs_directory)\n",
    "h5_files = find_HDF5_files(collected_outputs_directory)\n",
    "\n",
    "csv_sessions = find_most_recent_files(found_session_export_paths=csv_files)\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "\n",
    "## The CSV containing the session delta time:\n",
    "t_delta_df = pd.read_csv(t_delta_csv_path, index_col=0) # Assuming that your CSV file has an index column\n",
    "# adds `delta_aligned_t_start`, `delta_aligned_t_end` columns\n",
    "t_delta_df['delta_aligned_t_start'] = t_delta_df['t_start'] - t_delta_df['t_delta']\n",
    "t_delta_df['delta_aligned_t_end'] = t_delta_df['t_end'] - t_delta_df['t_delta']\n",
    "\n",
    "# computes `earliest_delta_aligned_t_start`, latest_delta_aligned_t_end\n",
    "earliest_delta_aligned_t_start: float = np.nanmin(t_delta_df['delta_aligned_t_start'])\n",
    "latest_delta_aligned_t_end: float = np.nanmax(t_delta_df['delta_aligned_t_end'])\n",
    "print(f'earliest_delta_aligned_t_start: {earliest_delta_aligned_t_start}, latest_delta_aligned_t_end: {latest_delta_aligned_t_end}')\n",
    "t_delta_dict = t_delta_df.to_dict(orient='index')\n",
    "# t_delta_df\n",
    "\n",
    "\n",
    "# #TODO 2024-03-02 12:12: - [ ] Could add weighted correlation if there is a dataframe for that and it's computed:\n",
    "_df_raw_variable_names = ['simple_pf_pearson_merged_df', 'weighted_corr_merged_df']\n",
    "_df_variables_names = ['laps_weighted_corr_merged_df', 'ripple_weighted_corr_merged_df', 'laps_simple_pf_pearson_merged_df', 'ripple_simple_pf_pearson_merged_df']\n",
    "\n",
    "# # tbin_values_dict = {'laps': self.laps_decoding_time_bin_size, 'ripple': self.ripple_decoding_time_bin_size}\n",
    "time_col_name_dict = {'laps': 'lap_start_t', 'ripple': 'ripple_start_t'} ## default should be 't_bin_center'\n",
    "\n",
    "# fold older files:\n",
    "# {'laps_marginals_df': 'lap_start_t', 'ripple_marginals_df': 'ripple_start_t', 'laps_time_bin_marginals_df':'t_bin_center', 'ripple_time_bin_marginals_df':'t_bin_center'}\n",
    "    \n",
    "\n",
    "# csv_sessions\n",
    "# Extract each of the separate files from the sessions:\n",
    "\n",
    "# final_sessions: Dict[types.session_str, Dict[str, Path]] = {}\n",
    "\n",
    "final_sessions_loaded_laps_dict = {}\n",
    "final_sessions_loaded_ripple_dict = {}\n",
    "final_sessions_loaded_laps_time_bin_dict = {}\n",
    "final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "final_sessions_loaded_simple_pearson_laps_dict = {}\n",
    "final_sessions_loaded_simple_pearson_ripple_dict = {}\n",
    "\n",
    "# stupid reudndant but compatible method\n",
    "final_sessions_loaded_laps_wcorr_dict = {}\n",
    "final_sessions_loaded_ripple_wcorr_dict = {}\n",
    "\n",
    "final_sessions_loaded_laps_all_scores_dict = {}\n",
    "final_sessions_loaded_ripple_all_scores_dict = {}\n",
    "\n",
    "\n",
    "# final_sessions_loaded_nested_dict = {} # to be filled like\n",
    "# {\n",
    "#     'laps': {},\n",
    "#     'ripple': {},\n",
    "# }\n",
    "\n",
    "final_sessions: Dict[types.session_str, Dict[str, Path]] = {session_str:{file_type:a_path for file_type, (an_decoding_time_bin_size_str, a_path, an_export_datetime) in session_dict.items()}\n",
    "                                                                                          for session_str, session_dict in csv_sessions.items()}\n",
    "\n",
    "for session_str, session_dict in final_sessions.items():\n",
    "    session_name = str(session_str)  # Extract session name from the filename\n",
    "    if debug_print:\n",
    "        print(f'processing session_name: {session_name}')\n",
    "    curr_session_t_delta = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "    if curr_session_t_delta is None:\n",
    "        print(f'WARN: curr_session_t_delta is None for session_str = \"{session_str}\"')\n",
    "\n",
    "    # Process each file type with its corresponding details\n",
    "    process_and_load_file(session_dict, 'laps_marginals_df', final_sessions_loaded_laps_dict, session_str, curr_session_t_delta, 'lap_start_t')\n",
    "    process_and_load_file(session_dict, 'ripple_marginals_df', final_sessions_loaded_ripple_dict, session_str, curr_session_t_delta, 'ripple_start_t')\n",
    "    process_and_load_file(session_dict, 'laps_time_bin_marginals_df', final_sessions_loaded_laps_time_bin_dict, session_str, curr_session_t_delta, 't_bin_center')\n",
    "    process_and_load_file(session_dict, 'ripple_time_bin_marginals_df', final_sessions_loaded_ripple_time_bin_dict, session_str, curr_session_t_delta, 't_bin_center')\n",
    "    process_and_load_file(session_dict, 'laps_simple_pf_pearson_merged_df', final_sessions_loaded_simple_pearson_laps_dict, session_str, curr_session_t_delta, 'lap_start_t')\n",
    "    process_and_load_file(session_dict, 'ripple_simple_pf_pearson_merged_df', final_sessions_loaded_simple_pearson_ripple_dict, session_str, curr_session_t_delta, 'ripple_start_t')\n",
    "    process_and_load_file(session_dict, 'laps_weighted_corr_merged_df', final_sessions_loaded_laps_wcorr_dict, session_str, curr_session_t_delta, 'lap_start_t')\n",
    "    process_and_load_file(session_dict, 'ripple_weighted_corr_merged_df', final_sessions_loaded_ripple_wcorr_dict, session_str, curr_session_t_delta, 'ripple_start_t')\n",
    "    \n",
    "\n",
    "\n",
    "    # process_and_load_file(session_dict, 'laps_all_scores_merged_df', final_sessions_loaded_laps_all_scores_dict, session_str, curr_session_t_delta, 'lap_start_t')\n",
    "    process_and_load_file(session_dict, 'ripple_all_scores_merged_df', final_sessions_loaded_ripple_all_scores_dict, session_str, curr_session_t_delta, 'ripple_start_t')\n",
    "\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'laps_marginals_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 'lap_start_t')\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'ripple_marginals_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 'ripple_start_t')\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'laps_time_bin_marginals_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 't_bin_center')\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'ripple_time_bin_marginals_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 't_bin_center')\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'laps_simple_pf_pearson_merged_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 'lap_start_t')\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'ripple_simple_pf_pearson_merged_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 'ripple_start_t')\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'laps_weighted_corr_merged_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 'lap_start_t')\n",
    "    # prototype_flexible_process_and_load_file(session_dict, 'ripple_weighted_corr_merged_df', final_sessions_loaded_nested_dict, session_str, curr_session_t_delta, 'ripple_start_t')\n",
    "\n",
    "\n",
    "## Build across_sessions join dataframes:\n",
    "all_sessions_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_dict.values()), axis='index', ignore_index=True)\n",
    "# Add 'epoch_idx' column for compatibility:\n",
    "all_sessions_laps_df['epoch_idx'] = all_sessions_laps_df['lap_idx']\n",
    "all_sessions_ripple_df['epoch_idx'] = all_sessions_ripple_df['ripple_idx']\n",
    "\n",
    "# *_time_bin marginals:\n",
    "all_sessions_laps_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# NEW ________________________________________________________________________________________________________________ #\n",
    "all_sessions_simple_pearson_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_simple_pearson_laps_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_simple_pearson_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_simple_pearson_ripple_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "if len(final_sessions_loaded_laps_wcorr_dict) > 0:\n",
    "    all_sessions_wcorr_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_wcorr_dict.values()), axis='index', ignore_index=True)\n",
    "else:\n",
    "    all_sessions_wcorr_laps_df = None # empty df would be better\n",
    "\n",
    "all_sessions_wcorr_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_wcorr_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# `*_all_scores_*`: __________________________________________________________________________________________________ #\n",
    "# all_sessions_all_score_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_all_scores_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_all_scores_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_all_scores_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "\n",
    "if 'time_bin_size' not in all_sessions_laps_df:\n",
    "    print('Uh-oh! time_bin_size is missing! This must be old exports!')\n",
    "    print(f'\\tTry to determine the time_bin_size from the filenames: {csv_sessions}')\n",
    "    ## manual correction UwU\n",
    "    time_bin_size: float = 0.025\n",
    "    print(f'WARNING! MANUAL OVERRIDE TIME BIN SIZE SET: time_bin_size = {time_bin_size}. Assigning to dataframes....')\n",
    "    all_sessions_laps_df['time_bin_size'] = time_bin_size\n",
    "    all_sessions_ripple_df['time_bin_size'] = time_bin_size\n",
    "    all_sessions_laps_time_bin_df['time_bin_size'] = time_bin_size\n",
    "    all_sessions_ripple_time_bin_df['time_bin_size'] = time_bin_size\n",
    "    print(f'\\tdone.')\n",
    "else:\n",
    "    # Filter rows based on column: 'time_bin_size'\n",
    "    all_sessions_laps_df = all_sessions_laps_df[all_sessions_laps_df['time_bin_size'].notna()]\n",
    "    all_sessions_ripple_df = all_sessions_ripple_df[all_sessions_ripple_df['time_bin_size'].notna()]\n",
    "    all_sessions_laps_time_bin_df = all_sessions_laps_time_bin_df[all_sessions_laps_time_bin_df['time_bin_size'].notna()]\n",
    "    all_sessions_ripple_time_bin_df = all_sessions_ripple_time_bin_df[all_sessions_ripple_time_bin_df['time_bin_size'].notna()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_sessions_laps_df, all_sessions_ripple_df, all_sessions_laps_time_bin_df, all_sessions_ripple_time_bin_df = [_common_cleanup_operations(a_df) for a_df in (all_sessions_laps_df, all_sessions_ripple_df, all_sessions_laps_time_bin_df, all_sessions_ripple_time_bin_df)]\n",
    "all_sessions_simple_pearson_laps_df, all_sessions_simple_pearson_ripple_df, all_sessions_wcorr_laps_df, all_sessions_wcorr_ripple_df = [_common_cleanup_operations(a_df) for a_df in (all_sessions_simple_pearson_laps_df, all_sessions_simple_pearson_ripple_df, all_sessions_wcorr_laps_df, all_sessions_wcorr_ripple_df)]\n",
    "\n",
    "# all_sessions_all_score_laps_df, all_sessions_all_scores_ripple_df = [_common_cleanup_operations(a_df) for a_df in (all_sessions_all_score_laps_df, all_sessions_all_scores_ripple_df)]\n",
    "# all_sessions_all_score_laps_df = _common_cleanup_operations(all_sessions_all_score_laps_df)\n",
    "all_sessions_all_scores_ripple_df = _common_cleanup_operations(all_sessions_all_scores_ripple_df)\n",
    "\n",
    "all_sessions_simple_pearson_laps_df: pd.DataFrame = DecoderDecodedEpochsResult.merge_decoded_epochs_result_dfs(all_sessions_simple_pearson_laps_df, all_sessions_wcorr_laps_df, should_drop_directional_columns=False, start_t_idx_name='delta_aligned_start_t')\n",
    "all_sessions_simple_pearson_ripple_df: pd.DataFrame = DecoderDecodedEpochsResult.merge_decoded_epochs_result_dfs(all_sessions_simple_pearson_ripple_df, all_sessions_wcorr_ripple_df, should_drop_directional_columns=False, start_t_idx_name='ripple_start_t')\n",
    "\n",
    "# all_sessions_laps_time_bin_df # 601845 rows × 9 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_all_scores_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_new_ripple_df # 3138 rows × 24 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_new_laps_df # 931 rows × 25 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_ripple_df # 17592 rows × 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_sessions_new_laps_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m final_output_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../output/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# final_sessions\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# {'kdiba_gor01_one_2006-6-08_14-26-15': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_marginals_df).csv'),\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_marginals_df).csv')},\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Save out the four dataframes to CSVs:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m final_dfs_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcrossSession_Laps_per-Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_sessions_laps_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcrossSession_Ripple_per-Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_sessions_ripple_df,\n\u001b[0;32m     13\u001b[0m \t\t\t\t   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcrossSession_Laps_per-TimeBin\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_sessions_laps_time_bin_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcrossSession_Ripple_per-TimeBin\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_sessions_ripple_time_bin_df,\n\u001b[1;32m---> 14\u001b[0m \t\t\t\t   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcrossSession_NEW_Laps_per-Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mall_sessions_new_laps_df\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcrossSession_NEW_Ripple_per-Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_sessions_new_ripple_df,\n\u001b[0;32m     15\u001b[0m \t\t\t\t   }\n\u001b[0;32m     17\u001b[0m final_csv_export_paths \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a_name, a_final_df \u001b[38;5;129;01min\u001b[39;00m final_dfs_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     19\u001b[0m \t\u001b[38;5;66;03m# save out one final DF to csv.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_sessions_new_laps_df' is not defined"
     ]
    }
   ],
   "source": [
    "final_output_path = Path(\"../output/\").resolve()\n",
    "\n",
    "# final_sessions\n",
    "# {'kdiba_gor01_one_2006-6-08_14-26-15': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_marginals_df).csv')},\n",
    "#  'kdiba_gor01_one_2006-6-09_1-22-43': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(laps_marginals_df).csv')},\n",
    "#  'kdiba_pin01_one_fet11-01_12-58-54': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv')}}\n",
    "\n",
    "# Save out the four dataframes to CSVs:\n",
    "final_dfs_dict = {\"AcrossSession_Laps_per-Epoch\": all_sessions_laps_df, \"AcrossSession_Ripple_per-Epoch\": all_sessions_ripple_df,\n",
    "\t\t\t\t   \"AcrossSession_Laps_per-TimeBin\": all_sessions_laps_time_bin_df, \"AcrossSession_Ripple_per-TimeBin\": all_sessions_ripple_time_bin_df,\n",
    "\t\t\t\t   \"AcrossSession_NEW_Laps_per-Epoch\": all_sessions_new_laps_df, \"AcrossSession_NEW_Ripple_per-Epoch\": all_sessions_new_ripple_df,\n",
    "\t\t\t\t   }\n",
    "\n",
    "final_csv_export_paths = {}\n",
    "for a_name, a_final_df in final_dfs_dict.items():\n",
    "\t# save out one final DF to csv.\n",
    "\tout_csv_filename: str = f\"{TODAY_DAY_DATE}_{a_name}.csv\"\n",
    "\ta_final_csv_export_path = final_output_path.joinpath(out_csv_filename).resolve()\n",
    "\ta_final_df.to_csv(a_final_csv_export_path) # save to CSV.\n",
    "\tfinal_csv_export_paths[a_name] = a_final_csv_export_path\n",
    "\t\n",
    "final_csv_export_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_new_laps_df, all_sessions_new_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_new_ripple_df.columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024-03-02 - Get only the user-annotated ripples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import partition_df\n",
    "\n",
    "all_unique_session_names = all_sessions_new_ripple_df['session_name'].unique()\n",
    "# all_unique_session_names\n",
    "\n",
    "# all_sessions_new_ripple_df['is_user_annotated_epoch'].notna()\n",
    "# all_sessions_user_annotated_laps_df = all_sessions_new_laps_df.dropna(axis='index', subset=['is_user_annotated_epoch'], inplace=False) # user doesn't annotate laps\n",
    "all_sessions_user_annotated_ripple_df: pd.DataFrame = all_sessions_new_ripple_df.dropna(axis='index', subset=['is_user_annotated_epoch', 'is_valid_epoch'], inplace=False)\n",
    "user_annotated_epoch_unique_session_names = all_sessions_user_annotated_ripple_df['session_name'].unique()\n",
    "print(f'user_annotated_epoch_unique_session_names: {user_annotated_epoch_unique_session_names}')\n",
    "\n",
    "unannotated_session_names = set(all_unique_session_names) - set(user_annotated_epoch_unique_session_names)\n",
    "print(f'unannotated_session_names: {unannotated_session_names}')\n",
    "\n",
    "\n",
    "_validity_partitioned_dfs = dict(zip(*partition_df(all_sessions_user_annotated_ripple_df, partitionColumn='is_valid_epoch')))\n",
    "valid_ripple_df: pd.DataFrame = _validity_partitioned_dfs[True].drop(columns=['is_valid_epoch']).reset_index(drop=True)\n",
    "invalid_ripple_df: pd.DataFrame = _validity_partitioned_dfs[False].drop(columns=['is_valid_epoch']).reset_index(drop=True)\n",
    "\n",
    "_partitioned_dfs = dict(zip(*partition_df(valid_ripple_df, partitionColumn='is_user_annotated_epoch'))) # use `valid_ripple_df` instead of the original dataframe to only get those which are valid.\n",
    "user_approved_ripple_df: pd.DataFrame = _partitioned_dfs[True].drop(columns=['is_user_annotated_epoch']).reset_index(drop=True)\n",
    "user_rejected_ripple_df: pd.DataFrame = _partitioned_dfs[False].drop(columns=['is_user_annotated_epoch']).reset_index(drop=True)\n",
    "\n",
    "## 2024-03-14 - 'is_valid_epoch' column\n",
    "# 'is_valid_epoch'\n",
    "user_approved_ripple_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## want to compare difference between user approved and user rejected ripples\n",
    "\n",
    "user_approved_ripple_df\n",
    "user_rejected_ripple_df\n",
    "# OUTPUTS: all_sessions_user_annotated_ripple_df, user_approved_ripple_df, user_rejected_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Want to test for differences between long/short among only user approved ripples:\n",
    "\n",
    "0.025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = deepcopy(all_sessions_user_annotated_ripple_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2024-02-29 - 4pm - Filter the events for those meeting wcorr criteria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df: pd.DataFrame = deepcopy(all_sessions_user_annotated_ripple_df)\n",
    "df: pd.DataFrame = deepcopy(valid_ripple_df) # valid epochs, but not just those that the user approved\n",
    "# df: pd.DataFrame = deepcopy(user_approved_ripple_df)\n",
    "\n",
    "## INPUTS: df\n",
    "\n",
    "min_wcorr_threshold: float = 0.33\n",
    "min_wcorr_diff_threshold: float = 0.2\n",
    "\n",
    "is_included_large_wcorr_diff = np.any((df[['wcorr_abs_diff']].abs() > min_wcorr_diff_threshold), axis=1)\n",
    "is_included_high_wcorr = np.any((df[['long_best_wcorr', 'short_best_wcorr']].abs() > min_wcorr_threshold), axis=1)\n",
    "\n",
    "df = df[is_included_high_wcorr]\n",
    "df\n",
    "\n",
    "# delta_aligned_start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "template: str = 'plotly_dark' # set plotl template\n",
    "pio.templates.default = template\n",
    "from PIL import Image\n",
    "\n",
    "from pyphocorehelpers.programming_helpers import copy_image_to_clipboard\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _helper_build_figure, plotly_pre_post_delta_scatter, plot_across_sessions_scatter_results\n",
    "\n",
    "# fig_to_clipboard(fig)\n",
    "\n",
    "# ## Test collapsed histograms-only results:\n",
    "# histograms_only_all_time_bin_session_figures = plot_across_sessions_scatter_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_time_bin_df, concatenated_ripple_df=all_sessions_ripple_time_bin_df,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t#    enabled_time_bin_sizes=[0.03, 0.10],\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# enabled_time_bin_sizes=[0.03, 0.058, 0.10], # [0.03 , 0.044, 0.058, 0.072, 0.086, 0.1]\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t   earliest_delta_aligned_t_start=earliest_delta_aligned_t_start, latest_delta_aligned_t_end=latest_delta_aligned_t_end,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t   main_plot_mode='default',\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t   laps_title_prefix=f\"Laps_per_time_bin\", ripple_title_prefix=f\"Ripples_per_time_bin\", save_figures=False, figure_save_extension=['.html','.png'])\n",
    "# histograms_only_fig_time_bin_laps, histograms_only_fig_time_bin_ripples = histograms_only_all_time_bin_session_figures[0]\n",
    "# # histograms_only_fig_time_bin_laps.show()\n",
    "# histograms_only_fig_time_bin_ripples.show()\n",
    "\n",
    "all_sessions_user_annotated_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_ripple_df = deepcopy(all_sessions_user_annotated_ripple_df).infer_objects()\n",
    "ripple_title_prefix = \"User Annotated Sessions Ripples\"\n",
    "\n",
    "# concatenated_ripple_df = deepcopy(user_approved_ripple_df)\n",
    "# ripple_title_prefix = \"User Selected Ripples Only\"\n",
    "\n",
    "# concatenated_ripple_df = deepcopy(user_rejected_ripple_df)\n",
    "# ripple_title_prefix = \"User Rejected Ripples Only\"\n",
    "\n",
    "# concatenated_ripple_df = deepcopy(df)\n",
    "\n",
    "\n",
    "# Create a bubble chart for ripples\n",
    "enabled_time_bin_sizes = None\n",
    "ripple_num_unique_sessions: int = concatenated_ripple_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "ripple_num_unique_time_bins: int = concatenated_ripple_df.time_bin_size.nunique(dropna=True)\n",
    "ripple_title_string_suffix: str = f'{ripple_num_unique_sessions} Sessions'\n",
    "ripple_title: str = f\"{ripple_title_prefix} - {ripple_title_string_suffix}\"\n",
    "fig_ripples = _helper_build_figure(data_results_df=concatenated_ripple_df, histogram_bins=25, earliest_delta_aligned_t_start=earliest_delta_aligned_t_start, latest_delta_aligned_t_end=latest_delta_aligned_t_end, enabled_time_bin_sizes=enabled_time_bin_sizes, main_plot_mode='default', title=ripple_title)\n",
    "fig_ripples.show()\n",
    "\n",
    "# fig_to_clipboard(fig_ripples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_bins = 25\n",
    "# concatenated_ripple_df = deepcopy(df)\n",
    "\n",
    "\n",
    "# ripple_title_prefix = 'test'\n",
    "# # Create a bubble chart for ripples\n",
    "# ripple_num_unique_sessions: int = concatenated_ripple_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "# # ripple_num_unique_time_bins: int = concatenated_ripple_df.time_bin_size.nunique(dropna=True)\n",
    "# ripple_title_string_suffix: str = f'{ripple_num_unique_sessions} Sessions'\n",
    "# ripple_title: str = f\"{ripple_title_prefix} - {ripple_title_string_suffix}\"\n",
    "# fig_ripples = _helper_build_figure(data_results_df=concatenated_ripple_df, histogram_bins=histogram_bins, earliest_delta_aligned_t_start=earliest_delta_aligned_t_start, latest_delta_aligned_t_end=latest_delta_aligned_t_end, enabled_time_bin_sizes=None,\n",
    "#                                     main_plot_mode='separate_row_per_session', title=ripple_title)\n",
    "\n",
    "\n",
    "scatter_title = 'Sevveral Sessions'\n",
    "# variable_name = 'P_Long'\n",
    "# variable_name = 'wcorr_abs_diff'\n",
    "variable_name = 'pearsonr_abs_diff'\n",
    "# 'color':'is_user_annotated_epoch'\n",
    "# 'color': 'is_user_annotated_epoch', \n",
    "px_scatter_kwargs = {'x': 'delta_aligned_start_t', 'y': variable_name, 'color':\"is_user_annotated_epoch\", 'title': scatter_title, 'range_y': [0.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size', 'is_user_annotated_epoch':'user_sel'}} # , 'color': 'time_bin_size'\n",
    "# hist_kwargs = dict(color=\"time_bin_size\")\n",
    "hist_kwargs = dict(color=\"is_user_annotated_epoch\", histnorm='probability density')\n",
    "new_fig_ripples = plotly_pre_post_delta_scatter(data_results_df=deepcopy(concatenated_ripple_df), out_scatter_fig=None, histogram_bins=histogram_bins,\n",
    "                        px_scatter_kwargs=px_scatter_kwargs, histogram_variable_name=variable_name, hist_kwargs=hist_kwargs, forced_range_y=None,\n",
    "                        time_delta_tuple=(earliest_delta_aligned_t_start, 0.0, latest_delta_aligned_t_end))\n",
    "new_fig_ripples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(new_fig_ripples) # plotly.graph_objs._figure.Figure\n",
    "\n",
    "from plotly.graph_objects import Figure as PlotlyFigure\n",
    "\n",
    "isinstance(new_fig_ripples, PlotlyFigure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## User non-selected:\n",
    "scatter_title = f'user_approved_ripple_df Sevveral Sessions {variable_name}'\n",
    "# variable_name = 'wcorr_abs_diff'\n",
    "px_scatter_kwargs = {'x': 'delta_aligned_start_t', 'y': variable_name, 'title': scatter_title, 'range_y': [0.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size'}} # , 'color': 'time_bin_size'\n",
    "new_fig_ripples = plotly_pre_post_delta_scatter(data_results_df=deepcopy(user_approved_ripple_df), out_scatter_fig=None, histogram_bins=histogram_bins,\n",
    "                        px_scatter_kwargs=px_scatter_kwargs, histogram_variable_name=variable_name, forced_range_y=None,\n",
    "                        time_delta_tuple=(earliest_delta_aligned_t_start, 0.0, latest_delta_aligned_t_end))\n",
    "new_fig_ripples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA: The ones with clear replays (diagonal sequences in the decoded posteriors) are by definiition ambiguous, because there's not much difference between the long/short decoders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User non-selected:\n",
    "scatter_title = f'Non-selected Sevveral Sessions {variable_name}'\n",
    "# variable_name = 'wcorr_abs_diff'\n",
    "px_scatter_kwargs = {'x': 'delta_aligned_start_t', 'y': variable_name, 'title': scatter_title, 'range_y': [0.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size'}} # , 'color': 'time_bin_size'\n",
    "new_fig_ripples = plotly_pre_post_delta_scatter(data_results_df=deepcopy(user_rejected_ripple_df), out_scatter_fig=None, histogram_bins=histogram_bins,\n",
    "                        px_scatter_kwargs=px_scatter_kwargs, histogram_variable_name=variable_name, forced_range_y=None,\n",
    "                        time_delta_tuple=(earliest_delta_aligned_t_start, 0.0, latest_delta_aligned_t_end))\n",
    "new_fig_ripples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laps test\n",
    "concatenated_ripple_df = deepcopy(all_sessions_new_laps_df)\n",
    "\n",
    "scatter_title = 'Sevveral Sessions'\n",
    "variable_name = 'wcorr_abs_diff'\n",
    "px_scatter_kwargs = {'x': 'delta_aligned_start_t', 'y': variable_name, 'title': scatter_title, 'range_y': [0.0, 1.0], 'labels': {'session_name': 'Session', 'time_bin_size': 'tbin_size'}} \n",
    "new_fig_ripples = plotly_pre_post_delta_scatter(data_results_df=deepcopy(concatenated_ripple_df), out_scatter_fig=None, histogram_bins=histogram_bins,\n",
    "                        px_scatter_kwargs=px_scatter_kwargs, histogram_variable_name=variable_name, forced_range_y=None,\n",
    "                        time_delta_tuple=(earliest_delta_aligned_t_start, 0.0, latest_delta_aligned_t_end))\n",
    "new_fig_ripples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_across_sessions_scatter_results\n",
    "\n",
    "# Example usage:\n",
    "all_session_figures = plot_across_sessions_scatter_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_df, concatenated_ripple_df=all_sessions_ripple_df,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t#    enabled_time_bin_sizes=[0.03, 0.10],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   earliest_delta_aligned_t_start=earliest_delta_aligned_t_start, latest_delta_aligned_t_end=latest_delta_aligned_t_end,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   laps_title_prefix=f\"Laps\", ripple_title_prefix=f\"Ripples\", save_figures=True, figure_save_extension=['.html','.png'])\n",
    "fig_laps, fig_ripples = all_session_figures[0]\n",
    "\n",
    "\n",
    "# fig_laps.show()\n",
    "fig_ripples.show()\n",
    "# fig_laps.write_html(f\"../output/{TODAY_DAY_DATE}_AcrossSession_fig_laps.html\")\n",
    "# fig_ripples.write_html(f\"../output/{TODAY_DAY_DATE}_AcrossSession_fig_ripples.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_to_clipboard(fig_ripples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "## time_bin version:\n",
    "all_time_bin_session_figures = plot_across_sessions_scatter_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_time_bin_df, concatenated_ripple_df=all_sessions_ripple_time_bin_df,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t#    enabled_time_bin_sizes=[0.03, 0.10],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   earliest_delta_aligned_t_start=earliest_delta_aligned_t_start, latest_delta_aligned_t_end=latest_delta_aligned_t_end,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   main_plot_mode='separate_row_per_session',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   laps_title_prefix=f\"Laps_per_time_bin\", ripple_title_prefix=f\"Ripples_per_time_bin\", save_figures=True, figure_save_extension=['.html','.png'])\n",
    "fig_time_bin_laps, fig_time_bin_ripples = all_time_bin_session_figures[0]\n",
    "# fig_time_bin_laps.show()\n",
    "fig_time_bin_ripples.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_across_sessions_scatter_results, _helper_build_figure\n",
    "\n",
    "## Test collapsed histograms-only results:\n",
    "histograms_only_all_time_bin_session_figures = plot_across_sessions_scatter_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_time_bin_df, concatenated_ripple_df=all_sessions_ripple_time_bin_df,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t#    enabled_time_bin_sizes=[0.03, 0.10],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# enabled_time_bin_sizes=[0.03, 0.058, 0.10], # [0.03 , 0.044, 0.058, 0.072, 0.086, 0.1]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   earliest_delta_aligned_t_start=earliest_delta_aligned_t_start, latest_delta_aligned_t_end=latest_delta_aligned_t_end,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   main_plot_mode='default',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   laps_title_prefix=f\"Laps_per_time_bin\", ripple_title_prefix=f\"Ripples_per_time_bin\", save_figures=False, figure_save_extension=['.html','.png'])\n",
    "histograms_only_fig_time_bin_laps, histograms_only_fig_time_bin_ripples = histograms_only_all_time_bin_session_figures[0]\n",
    "# histograms_only_fig_time_bin_laps.show()\n",
    "histograms_only_fig_time_bin_ripples.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plotly_pre_post_delta_scatter\n",
    "# data_results_df: pd.DataFrame = deepcopy(all_sessions_laps_df) # all_sessions_laps_time_bin_df\n",
    "# histogram_bins = 'auto'\n",
    "histogram_bins: int = 25\n",
    "\n",
    "new_laps_fig = plotly_pre_post_delta_scatter(data_results_df=deepcopy(all_sessions_laps_df), out_scatter_fig=fig_laps, histogram_bins=histogram_bins)\n",
    "new_laps_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dash-based Interactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Dash",
     "interactive"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "from dash import Dash, html, dash_table, dcc, callback, Output, Input, State\n",
    "from dash.dash_table import DataTable, FormatTemplate\n",
    "from dash.dash_table.Format import Format, Padding\n",
    "\n",
    "import dash_bootstrap_components as dbc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# import plotly.express as px\n",
    "import plotly.io as pio\n",
    "template: str = 'plotly_dark' # set plotl template\n",
    "pio.templates.default = template\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_across_sessions_scatter_results, _helper_build_figure\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_blue_yellow_points\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _build_dash_app\n",
    "\n",
    "# Incorporate data\n",
    "# final_csv_export_paths = {'AcrossSession_Laps_per-Epoch': Path('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/2024-01-30_AcrossSession_Laps_per-Epoch.csv'),\n",
    "#  'AcrossSession_Ripple_per-Epoch': Path('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/2024-01-30_AcrossSession_Ripple_per-Epoch.csv'),\n",
    "#  'AcrossSession_Laps_per-TimeBin': Path('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/2024-01-30_AcrossSession_Laps_per-TimeBin.csv'),\n",
    "#  'AcrossSession_Ripple_per-TimeBin': Path('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/2024-01-30_AcrossSession_Ripple_per-TimeBin.csv')}\n",
    "\n",
    "final_csv_export_paths = {'AcrossSession_Laps_per-Epoch': Path('/home/halechr/repos/Spike3D/output/2024-03-01_AcrossSession_Laps_per-Epoch.csv'),\n",
    " 'AcrossSession_Ripple_per-Epoch': Path('/home/halechr/repos/Spike3D/output/2024-03-01_AcrossSession_Ripple_per-Epoch.csv'),\n",
    " 'AcrossSession_Laps_per-TimeBin': Path('/home/halechr/repos/Spike3D/output/2024-03-01_AcrossSession_Laps_per-TimeBin.csv'),\n",
    " 'AcrossSession_Ripple_per-TimeBin': Path('/home/halechr/repos/Spike3D/output/2024-03-01_AcrossSession_Ripple_per-TimeBin.csv'),\n",
    " 'AcrossSession_NEW_Laps_per-Epoch': Path('/home/halechr/repos/Spike3D/output/2024-03-01_AcrossSession_NEW_Laps_per-Epoch.csv'),\n",
    " 'AcrossSession_NEW_Ripple_per-Epoch': Path('/home/halechr/repos/Spike3D/output/2024-03-01_AcrossSession_NEW_Ripple_per-Epoch.csv')}\n",
    "\n",
    "final_dfs_dict = {a_name:pd.read_csv(a_path.resolve()) for a_name, a_path in final_csv_export_paths.items()}\n",
    "app = _build_dash_app(final_dfs_dict, earliest_delta_aligned_t_start=earliest_delta_aligned_t_start, latest_delta_aligned_t_end=latest_delta_aligned_t_end)\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting the app.\")  # Add this line to check if it's executed\n",
    "    app.run(port=\"8054\", debug=True, use_reloader=False)  # Turn off reloader if inside Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = deepcopy(final_dfs_dict['AcrossSession_Laps_per-TimeBin'])\n",
    "a_df\n",
    "# print(list(a_df.columns))\n",
    "# ['Unnamed: 0.1', 'Unnamed: 0', 'P_LR', 'P_RL', 'P_Long', 'P_Short', 'epoch_idx', 't_bin_center', 'session_name', 'delta_aligned_start_t', 'time_bin_size']\n",
    "\n",
    "yellow_blue_relevent_columns = ['epoch_idx', 'session_name', 'time_bin_size', 'delta_aligned_start_t', 'P_Long', 'P_Short', 'P_LR', 'P_RL']\n",
    "a_df = a_df[yellow_blue_relevent_columns]\n",
    "\n",
    "# filter by time_bin_size first:\n",
    "a_single_selected_time_bin_size: float = 0.1 # time_bin_size\n",
    "a_df = a_df[a_df.time_bin_size.astype(float) == a_single_selected_time_bin_size]\n",
    "\n",
    "## Filter by selected epoch index:\n",
    "selected_epoch_idxs = [5, 6, 9]\n",
    "a_df = a_df[a_df.epoch_idx.isin(selected_epoch_idxs)]\n",
    "\n",
    "# filter by selected epoch index:\n",
    "a_single_epoch_row_idx: int = 0\n",
    "a_single_epoch_idx = selected_epoch_idxs[a_single_epoch_row_idx]\n",
    "a_single_epoch_df = a_df[a_df.epoch_idx == a_single_epoch_idx]\n",
    "a_single_epoch_df\n",
    "\n",
    "## filter by session:\n",
    "a_single_session_name: str = 'kdiba_vvp01_one_2006-4-10_12-25-50'\n",
    "a_single_epoch_df = a_single_epoch_df[a_single_epoch_df.session_name == a_single_session_name]\n",
    "a_single_epoch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plotly_plot_1D_most_likely_position_comparsions\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_blue_yellow_points\n",
    "\n",
    "# def plot_blue_yellow(a_df, selected_epoch_idxs, a_single_session_name: str):\n",
    "\n",
    "specific_point_list = [{'session_name': 'kdiba_vvp01_one_2006-4-10_12-25-50', 'time_bin_size': 0.03, 'epoch_idx': 0, 'delta_aligned_start_t': -713.908702568122}]\n",
    "\n",
    "# def plot_blue_yellow(a_df, selected_epoch_idxs, a_single_session_name: str):\n",
    "\n",
    "## Takes: a_df, selected_epoch_idxs, a_single_session_name\n",
    "a_df\n",
    "a_single_session_name: str = 'kdiba_vvp01_one_2006-4-10_12-25-50' # #TODO 2024-01-30 16:01: - [ ] Hardcoded session:\n",
    "selected_epoch_idxs = [5, 6, 9]\n",
    "fig = plot_blue_yellow_points(a_df, selected_epoch_idxs=selected_epoch_idxs, a_single_session_name=a_single_session_name)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib-based versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_histograms\n",
    "\n",
    "matplotlib.use('Qt5Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Spike3D.PendingNotebookCode import plot_stacked_histograms\n",
    "\n",
    "# You can use it like this:'long_best_dir_simple_pearsonr'\n",
    "_out0: \"MatplotlibRenderPlots\" = plot_histograms(all_sessions_laps_time_bin_df, 'Laps', 'All Sessions', \"75 ms\")\n",
    "_out1: \"MatplotlibRenderPlots\" = plot_histograms(all_sessions_ripple_time_bin_df, 'Ripples', 'All Sessions', \"75 ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig_to_clipboard(_out0.figures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all_sessions_new_laps_df, all_sessions_new_ripple_df\n",
    "plot_histograms(all_sessions_new_laps_df, 'New Laps', 'All Sessions', \"25 ms\")\n",
    "plot_histograms(all_sessions_new_ripple_df, 'New Ripples', 'All Sessions', \"25 ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_new_laps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sessions_new_laps_df['session_name'].unique()) # 10 sessions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_stacked_histograms\n",
    "\n",
    "# You can use it like this:\n",
    "num_unique_sessions: int = all_sessions_laps_time_bin_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "num_unique_time_bins: int = all_sessions_laps_time_bin_df.time_bin_size.nunique(dropna=True)\n",
    "plot_stacked_histograms(all_sessions_laps_time_bin_df, 'Laps', f'{num_unique_sessions} Sessions', f\"{num_unique_time_bins} tbin sizes\")\n",
    "\n",
    "num_unique_sessions: int = all_sessions_ripple_time_bin_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "num_unique_time_bins: int = all_sessions_ripple_time_bin_df.time_bin_size.nunique(dropna=True)\n",
    "plot_stacked_histograms(all_sessions_ripple_time_bin_df, 'Ripples', f'{num_unique_sessions} Sessions', f\"{num_unique_time_bins} tbin sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024-02-20 - New Pearsonr analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = ['long_LR_pf_peak_x_pearsonr', 'long_RL_pf_peak_x_pearsonr', 'short_LR_pf_peak_x_pearsonr', 'short_RL_pf_peak_x_pearsonr']\n",
    "\n",
    "\n",
    "def _add_best_direction_columns(_temp_all_sessions_new_ripple_df):\n",
    "    \"\"\" Get the most-likely direction to determine which correlation to use\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Get the most-likely direction to determine which correlation to use\n",
    "    # _temp_all_sessions_new_ripple_df['best_dir_simple_pearsonr'] = pd.NA\n",
    "\n",
    "    # _temp_all_sessions_new_ripple_df[_temp_all_sessions_new_ripple_df['P_LR'] > 0.5] # LR-likely plots\n",
    "\n",
    "    ## Adds two new columns using the best direction as determined by the decoder: ['long_best_dir_simple_pearsonr', 'short_best_dir_simple_pearsonr']\n",
    "    # Using numpy where\n",
    "    _temp_all_sessions_new_ripple_df['long_best_dir_simple_pearsonr'] = np.where((_temp_all_sessions_new_ripple_df['P_LR'] > 0.5), _temp_all_sessions_new_ripple_df['long_LR_pf_peak_x_pearsonr'], _temp_all_sessions_new_ripple_df['long_RL_pf_peak_x_pearsonr'])\n",
    "    _temp_all_sessions_new_ripple_df['short_best_dir_simple_pearsonr'] = np.where((_temp_all_sessions_new_ripple_df['P_LR'] > 0.5), _temp_all_sessions_new_ripple_df['short_LR_pf_peak_x_pearsonr'], _temp_all_sessions_new_ripple_df['short_RL_pf_peak_x_pearsonr'])\n",
    "    ## Define a difference of the absolutes\n",
    "    _temp_all_sessions_new_ripple_df['long_short_diff_best_dir_simple_pearsonr'] = (_temp_all_sessions_new_ripple_df['long_best_dir_simple_pearsonr'].abs() - _temp_all_sessions_new_ripple_df['short_best_dir_simple_pearsonr'].abs())\n",
    "    return _temp_all_sessions_new_ripple_df\n",
    "\n",
    "\n",
    "## Drop the dang NaN columns:\n",
    "_temp_all_sessions_new_ripple_df= _add_best_direction_columns(all_sessions_new_ripple_df.dropna(subset=new_column_names))\n",
    "_temp_all_sessions_new_laps_df= _add_best_direction_columns(all_sessions_new_laps_df.dropna(subset=new_column_names))\n",
    "_temp_all_sessions_new_laps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['long_best_dir_simple_pearsonr_abs']\n",
    "\n",
    "plot_histograms(_temp_all_sessions_new_laps_df, 'Laps', 'Some Sessions', \"25 ms\", column='long_best_dir_simple_pearsonr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get only high-confidence replays:\n",
    "_temp_high_confidence_ripple_df = _temp_all_sessions_new_ripple_df[_temp_all_sessions_new_ripple_df['P_Long'] > 0.85]\n",
    "# plot_histograms(_temp_high_confidence_ripple_df, 'Ripple', 'Some Sessions', \"25 ms\", column='long_best_dir_simple_pearsonr')\n",
    "plot_histograms(_temp_high_confidence_ripple_df, 'Ripple', 'Some Sessions', \"25 ms\", column='long_best_dir_simple_pearsonr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _temp_all_sessions_new_ripple_df\n",
    "\n",
    "plt.scatter(x='delta_aligned_start_t', y='long_best_dir_simple_pearsonr', data=_temp_all_sessions_new_ripple_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x='delta_aligned_start_t', y='long_best_dir_simple_pearsonr', data=_temp_all_sessions_new_ripple_df)\n",
    "\n",
    "# pile\n",
    "plt.figure()\n",
    "plt.ion()\n",
    "# plt.scatter(x=_temp_all_sessions_new_ripple_df['delta_aligned_start_t'], y=_temp_all_sessions_new_ripple_df['long_best_dir_simple_pearsonr'])\n",
    "plt.scatter(x=_temp_all_sessions_new_ripple_df['delta_aligned_start_t'], y=_temp_all_sessions_new_ripple_df['long_short_diff_best_dir_simple_pearsonr'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_histograms(_temp_all_sessions_new_ripple_df, 'Ripple', 'Some Sessions', \"25 ms\", column='long_short_diff_best_dir_simple_pearsonr')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "data_results_df: pd.DataFrame = deepcopy(_temp_all_sessions_new_ripple_df)\n",
    "column = 'long_short_diff_best_dir_simple_pearsonr' # REJECT (t-statistic: 2.0498849677801947, p-value: 0.04048207344769608)\n",
    "# column = 'short_best_dir_simple_pearsonr' # FAIL\n",
    "# column = 'long_best_dir_simple_pearsonr' # FAIL (t-statistic: 0.5238142457664882, p-value: 0.600455893577746)\n",
    "\n",
    "# get the pre-delta epochs\n",
    "pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "# Perform the t-test for two independent samples\n",
    "# T_result = stats.ttest_ind(pre_delta_df[column].to_numpy(), post_delta_df[column].to_numpy(), equal_var=False)\n",
    "T_result = stats.ttest_ind(pre_delta_df[column].to_numpy(), post_delta_df[column].to_numpy(), equal_var=False) # Welch's t-test\n",
    "t_statistic, p_value = T_result\n",
    "\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Decision making\n",
    "alpha = 0.05  # commonly used significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference between the two group means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference between the two group means.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_sessions_new_ripple_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin_sizes: int = all_sessions_laps_time_bin_df['time_bin_size'].unique()\n",
    "time_bin_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_time_bin_df.hist(column='P_Long')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "from attrs import define, field, Factory\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# Switch to the desired interactivity mode\n",
    "plt.interactive(True)\n",
    "\n",
    "\n",
    "@define(slots=False)\n",
    "class TwoCSV:\n",
    "\t\"\"\" simple class wrapper to emulate the object that holds the other dfs \"\"\"\n",
    "\tlaps_all_epoch_bins_marginals_df = field()\n",
    "\tripple_all_epoch_bins_marginals_df = field()\n",
    "\n",
    "save_figure = True\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\tprint(f'final_context: {final_context}')\n",
    "\t# if save_figure:\n",
    "\t# \tfig.save_fig(\n",
    "\t# \t# return owning_pipeline_reference.output_figure(final_context, fig)\n",
    "\t# else:\n",
    "\t# \tpass # do nothing, don't save\n",
    "\t\n",
    "# all_sessions_laps_time_bin_df\n",
    "\n",
    "collector = plot_all_epoch_bins_marginal_predictions(TwoCSV(laps_all_epoch_bins_marginals_df=all_sessions_laps_df, ripple_all_epoch_bins_marginals_df=all_sessions_ripple_df), t_start=None, t_split=0.0, t_end=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=IdentifyingContext(), perform_write_to_file_callback=_perform_write_to_file_callback)\n",
    "collector\n",
    "\n",
    "\n",
    "result = tuple(collector.figures)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_df, all_sessions_ripple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAD 2024-02-08 - Temporal Timecourse of Long/Short Replays after Delta using cross-correlations and auto-correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphocorehelpers.indexing_helpers import partition_df\n",
    "\n",
    "def _split_is_post_delta(df):\n",
    "    df['is_post_delta'] = (df['delta_aligned_start_t'] > 0.0)\n",
    "    return partition_df(df, partitionColumn='is_post_delta')[-1] # [-1] gets the actual dataframe list\n",
    "\n",
    "def _split_long_likely(df):\n",
    "    df['is_long_likely'] = (df['P_Long'] > 0.5)\n",
    "    return partition_df(df, partitionColumn='is_long_likely')[-1] # [-1] gets the actual dataframe list\n",
    "\n",
    "def _split_on_pre_post_delta_and_likely_track(df):\n",
    "    pre_delta_ripple_df, post_delta_ripple_df = _split_is_post_delta(df)\n",
    "    pre_delta_long_likely_ripple_df, pre_delta_short_likely_ripple_df = _split_long_likely(pre_delta_ripple_df)\n",
    "    post_delta_long_likely_ripple_df, post_delta_short_likely_ripple_df = _split_long_likely(post_delta_ripple_df)\n",
    "    return (pre_delta_ripple_df, post_delta_ripple_df), (pre_delta_long_likely_ripple_df, pre_delta_short_likely_ripple_df), (post_delta_long_likely_ripple_df, post_delta_short_likely_ripple_df)\n",
    "\n",
    "\n",
    "# only allow one time bin size\n",
    "# active_time_bin_size = 0.10\n",
    "\n",
    "# active_time_bin_size = 0.025\n",
    "active_time_bin_size = 0.03\n",
    "\n",
    "## Get only those after the delta, which occurs at t=0 relative to delta_aligned_start_t:\n",
    "filtered_all_sessions_laps_df = all_sessions_laps_df[all_sessions_laps_df['time_bin_size'] == active_time_bin_size].copy()\n",
    "filtered_all_sessions_laps_df['is_long_likely'] = (filtered_all_sessions_laps_df['P_Long'] > 0.5)\n",
    "(pre_delta_laps_df, post_delta_laps_df), (pre_delta_long_likely_laps_df, pre_delta_short_likely_laps_df), (post_delta_long_likely_laps_df, post_delta_short_likely_laps_df) = _split_on_pre_post_delta_and_likely_track(filtered_all_sessions_laps_df)\n",
    "\n",
    "filtered_all_sessions_ripple_df = all_sessions_ripple_df[all_sessions_ripple_df['time_bin_size'] == active_time_bin_size].copy().drop(columns=['time_bin_size'])\n",
    "filtered_all_sessions_ripple_df['is_long_likely'] = (filtered_all_sessions_ripple_df['P_Long'] > 0.5)\n",
    "(pre_delta_ripple_df, post_delta_ripple_df), (pre_delta_long_likely_ripple_df, pre_delta_short_likely_ripple_df), (post_delta_long_likely_ripple_df, post_delta_short_likely_ripple_df) = _split_on_pre_post_delta_and_likely_track(filtered_all_sessions_ripple_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laps_df_dict = {\n",
    "\t\"all_post_delta_laps\": filtered_all_sessions_laps_df,\n",
    "\t\"long_likely_laps_df\": long_likely_laps_df,\n",
    "\t\"short_likely_laps_df\": short_likely_laps_df,\n",
    "}\n",
    "\t\n",
    "ripples_df_dict = {\n",
    "\t\"all_post_delta_ripple\": filtered_all_sessions_ripple_df,\n",
    "\t\"long_likely_ripple_df\": long_likely_ripple_df,\n",
    "\t\"short_likely_ripple_df\": short_likely_ripple_df,\n",
    "}\n",
    "\n",
    "all_sessions_laps_df, all_sessions_ripple_df\n",
    "all_sessions_laps_time_bin_df, all_sessions_ripple_time_bin_df\n",
    "earliest_delta_aligned_t_start, latest_delta_aligned_t_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "# import proplot as pplt\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_laps(laps_df_dict):\n",
    "    \"\"\"\n",
    "    Consolidates the laps plots into a single figure\n",
    "    Args:\n",
    "        laps_df_dict (dict): A dictionary with keys as plot titles and values as DataFrame with 'delta_aligned_start_t' column\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(3, figsize=(10, 15))\n",
    "    fig.suptitle('Laps Plots')\n",
    "    \n",
    "    for i, (title, df) in enumerate(laps_df_dict.items()):\n",
    "        pd.plotting.autocorrelation_plot(df.delta_aligned_start_t, ax=axs[i])\n",
    "        axs[i].set_title(title)\n",
    "        axs[i].set_xlabel('Lag (seconds)')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "def plot_ripples(ripples_df_dict):\n",
    "    \"\"\"\n",
    "    Consolidates the ripple plots into a single figure\n",
    "    Args:\n",
    "        ripples_df_dict (dict): A dictionary with keys as plot titles and values as DataFrame with 'delta_aligned_start_t' column\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(3, figsize=(10, 15))\n",
    "    fig.suptitle('Ripples Plots')\n",
    "    \n",
    "    for i, (title, df) in enumerate(ripples_df_dict.items()):\n",
    "        pd.plotting.autocorrelation_plot(df.delta_aligned_start_t, ax=axs[i])\n",
    "        axs[i].set_title(title)\n",
    "        axs[i].set_xlabel('Lag (seconds)')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "## BODY\n",
    "def _plot_all_signal_correlations(filtered_all_sessions_laps_df, long_likely_laps_df, short_likely_laps_df, filtered_all_sessions_ripple_df, long_likely_ripple_df, short_likely_ripple_df):\n",
    "    laps_df_dict = {\n",
    "        \"all_post_delta_laps\": filtered_all_sessions_laps_df,\n",
    "        \"long_likely_laps_df\": long_likely_laps_df,\n",
    "        \"short_likely_laps_df\": short_likely_laps_df,\n",
    "    }\n",
    "    plot_laps(laps_df_dict)\n",
    "        \n",
    "    ripples_df_dict = {\n",
    "        \"all_post_delta_ripple\": filtered_all_sessions_ripple_df,\n",
    "        \"long_likely_ripple_df\": long_likely_ripple_df,\n",
    "        \"short_likely_ripple_df\": short_likely_ripple_df,\n",
    "    }\n",
    "    plot_ripples(ripples_df_dict)\n",
    "\n",
    "\n",
    "\n",
    "## Inputs: pre_delta_long_likely_laps_df, pre_delta_short_likely_laps_df, pre_delta_long_likely_ripple_df, pre_delta_short_likely_ripple_df\n",
    "# long_likely_laps_df = pre_delta_long_likely_laps_df.copy()\n",
    "# short_likely_laps_df = pre_delta_short_likely_laps_df.copy()\n",
    "\n",
    "# long_likely_ripple_df = pre_delta_long_likely_ripple_df.copy()\n",
    "# short_likely_ripple_df = pre_delta_short_likely_ripple_df.copy()\n",
    "\n",
    "## Inputs: pre_delta_long_likely_laps_df, pre_delta_short_likely_laps_df, pre_delta_long_likely_ripple_df, pre_delta_short_likely_ripple_df\n",
    "\n",
    "# _plot_all_signal_correlations(filtered_all_sessions_laps_df, long_likely_laps_df, short_likely_laps_df, filtered_all_sessions_ripple_df, long_likely_ripple_df, short_likely_ripple_df)\n",
    "# plt.suptitle('Pre-$\\Delta$ Time Correlations')\n",
    "\n",
    "# _plot_all_signal_correlations(filtered_all_sessions_laps_df, pre_delta_long_likely_laps_df.copy(), pre_delta_short_likely_laps_df.copy(), filtered_all_sessions_ripple_df, pre_delta_long_likely_ripple_df.copy(), pre_delta_short_likely_ripple_df.copy())\n",
    "_plot_all_signal_correlations(filtered_all_sessions_laps_df, post_delta_long_likely_laps_df.copy(), post_delta_short_likely_laps_df.copy(), filtered_all_sessions_ripple_df, post_delta_long_likely_ripple_df.copy(), post_delta_short_likely_ripple_df.copy())\n",
    "plt.suptitle('Post-$\\Delta$ Time Correlations - All Sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.kdeplot(filtered_all_sessions_ripple_df['delta_aligned_start_t'])\n",
    "# sns.kdeplot(filtered_all_sessions_ripple_df['delta_aligned_start_t'])\n",
    "# sns.kdeplot(filtered_all_sessions_ripple_df, x='delta_aligned_start_t', hue='is_post_delta', y='is_long_likely')\n",
    "sns.kdeplot(filtered_all_sessions_ripple_df, x='delta_aligned_start_t', hue='is_long_likely')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(long_likely_ripple_df, x='delta_aligned_start_t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(short_likely_ripple_df['delta_aligned_start_t'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(post_delta_long_likely_ripple_df['delta_aligned_start_t'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(post_delta_short_likely_ripple_df['delta_aligned_start_t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming series1 and series2 are your two pandas Series\n",
    "correlation = long_likely_ripple_df.delta_aligned_start_t.corr(short_likely_ripple_df.delta_aligned_start_t)\n",
    "print('Correlation: ', correlation)\n",
    "\n",
    "# # Scatter plot\n",
    "# plt.scatter(long_likely_ripple_df.delta_aligned_start_t, short_likely_ripple_df.delta_aligned_start_t)\n",
    "# plt.title('Scatter Plot')\n",
    "# plt.xlabel('Series 1')\n",
    "# plt.ylabel('Series 2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # delta_aligned_start_t\n",
    "\n",
    "# filtered_all_sessions_laps_df.delta_aligned_start_t.autocorr()\n",
    "\n",
    "# plot_acf(filtered_all_sessions_laps_df.delta_aligned_start_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycorrelate as pyc\n",
    "\n",
    "# compute lags in timestamp units (not in seconds!)\n",
    "# to avoid floating point inacuracies\n",
    "# bins_per_dec = 10\n",
    "# unit = np.timedelta64()\n",
    "\n",
    "# bins = pyc.make_loglags(1, 8, bins_per_dec)[bins_per_dec//2:]\n",
    "# Convert the float array to timedelta64 by multiplying with '1s'\n",
    "# bins = (bins * 1e9).astype('timedelta64[ns]')\n",
    "\n",
    "unit = 1e9\n",
    "# convert_to_time_units = lambda t: (t * 1e9).astype('timedelta64[ns]')\n",
    "convert_to_time_units = lambda t: (t) #.astype('timedelta64[ns]')\n",
    "\n",
    "# compute lags in sec. then convert to timestamp units\n",
    "# bins = pyc.make_loglags(-1, 5, 20)\n",
    "# bins = pyc.make_loglags(-1, 5, 5)\n",
    "bins = pyc.make_loglags(-1, 2, 5)\n",
    "\n",
    "# Convert the float array to timedelta64 by multiplying with '1s'\n",
    "# bins = (bins * 1e9).astype('timedelta64[ns]')\n",
    "bins = convert_to_time_units(bins)\n",
    "\n",
    "long_likely_epoch_times = convert_to_time_units(deepcopy(long_likely_laps_df['delta_aligned_start_t'].to_numpy()))\n",
    "short_likely_epoch_times = convert_to_time_units(deepcopy(short_likely_laps_df['delta_aligned_start_t'].to_numpy()))\n",
    "\n",
    "# long_likely_epoch_times = convert_to_time_units(deepcopy(long_likely_ripples_df['delta_aligned_start_t'].to_numpy()))\n",
    "# short_likely_epoch_times = convert_to_time_units(deepcopy(short_likely_ripples_df['delta_aligned_start_t'].to_numpy()))\n",
    "\n",
    "long_likely_autocorrelation = pyc.pcorrelate(long_likely_epoch_times, long_likely_epoch_times, bins)\n",
    "short_likely_autocorrelation = pyc.pcorrelate(short_likely_epoch_times, short_likely_epoch_times, bins)\n",
    "long_short_likely_cross_correlation = pyc.pcorrelate(long_likely_epoch_times, short_likely_epoch_times, bins)\n",
    "\n",
    "long_likely_autocorrelation\n",
    "short_likely_autocorrelation\n",
    "long_short_likely_cross_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Convert data to DataFrame\n",
    "t_bins = deepcopy(bins.astype('float'))  # convert timedelta64[ns] to float\n",
    "t_bins = (t_bins[1:] + t_bins[:-1]) / 2\n",
    "\n",
    "df1 = pd.DataFrame({'Bins': t_bins, 'Correlation': long_likely_autocorrelation, 'Type': 'Long Autocorrelation'})\n",
    "df2 = pd.DataFrame({'Bins': t_bins, 'Correlation': short_likely_autocorrelation, 'Type': 'Short Autocorrelation'})\n",
    "df3 = pd.DataFrame({'Bins': t_bins, 'Correlation': long_short_likely_cross_correlation, 'Type': 'Cross Correlation'})\n",
    "\n",
    "df = pd.concat([df1, df2, df3])\n",
    "\n",
    "# Plot using Plotly Express\n",
    "fig = px.line(df, x=\"Bins\", y=\"Correlation\", color=\"Type\", facet_row=\"Type\", \n",
    "              labels={'Bins': 'Bins (seconds)', 'Correlation': 'Correlation'},\n",
    "              title='Correlation vs Bins')\n",
    "# fig.update_xaxes(type=\"log\")\n",
    "fig.update_yaxes(type=\"log\")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = long_short_likely_cross_correlation.copy()\n",
    "# bins_unit = bins*unit\n",
    "bins_unit = bins.copy()\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.plot(bins_unit, np.hstack((G[:1], G)), drawstyle='steps-pre')\n",
    "plt.xlabel('Time (s)')\n",
    "#for x in bins[1:]: plt.axvline(x*unit, lw=0.2)  # to mark bins\n",
    "plt.grid(True); plt.grid(True, which='minor', lw=0.3)\n",
    "plt.xscale('log')\n",
    "# plt.xlim(30e-9, 2)\n",
    "plt.title('Cross-correlation of Long and Short')\n",
    "\n",
    "long_short_likely_cross_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak here matplotlib style\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'].insert(0, 'Arial')\n",
    "mpl.rcParams['font.size'] = 10\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# %matplotlib qt\n",
    "# matplotlib.use('Qt5Agg')\n",
    "\n",
    "def plot_correlate(G):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.plot(bins, np.hstack((G[:1], G)), drawstyle='steps-pre')\n",
    "    plt.xlabel('Time (s)')\n",
    "    #for x in bins[1:]: plt.axvline(x*unit, lw=0.2)  # to mark bins\n",
    "    plt.grid(True); plt.grid(True, which='minor', lw=0.3)\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(30e-9, 2)\n",
    "    return fig, ax\n",
    "\n",
    "# fig, ax = plot_correlate(long_likely_autocorrelation)\n",
    "fig, ax = plot_correlate(long_short_likely_cross_correlation)\n",
    "# fig.show()\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_yellow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
