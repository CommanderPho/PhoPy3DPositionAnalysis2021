{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows collecting results across multiple pipeline runs. Concatenating across sessions and bin sizes. \n",
    "It takes CSVs, then determines the most recent one from the filename. \n",
    "\n",
    "There are two major sets of decoded epochs - Laps and Ripples/Replays\n",
    "There are two sets of marginals for the decoded epochs - the \"by epoch\" and \"by time bin\" marginals.\n",
    "The \"by time bin\" epochs are a larger granulation, with each epoch consisting of one or more time bin.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# 2024-01-23 - \n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-fet11-01_12-58-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-03_12-3-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_17-46-44_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_19-28-0_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-58-3_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-25-50_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_16-40-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_22-24-40_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_16-53-46_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_17-29-30_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_14-26-15_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_1-22-43_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-07_16-40-19_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_21-16-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_15-55-31_time_bin_size_sweep_results.h5\n",
    "\n",
    "\n",
    "\n",
    "# found_session_export_paths = [Path(v).resolve() for v in  [\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Optional, Union, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "# matplotlib.use('Qt5Agg')\n",
    "%matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "import seaborn as sns\n",
    "\n",
    "# from ..PendingNotebookCode import plot_across_sessions_scatter_results, plot_histograms, plot_stacked_histograms\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import find_csv_files, find_HDF5_files, find_most_recent_files, process_csv_file\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_across_sessions_scatter_results, plot_histograms, plot_stacked_histograms\n",
    "\n",
    "debug_print: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## Load across session t_delta CSV, which contains the t_delta for each session:\n",
    "t_delta_csv_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs\\2024-01-18_GL_t_split_df.csv').resolve() # Apogee\n",
    "# t_delta_csv_path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-01-18_GL_t_split_df.csv').resolve() # GL\n",
    "\n",
    "# collected_outputs_directory = '/home/halechr/FastData/collected_outputs/'\n",
    "# collected_outputs_directory = r'C:\\Users\\pho\\Desktop\\collected_outputs'\n",
    "collected_outputs_directory = r'C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs' # APOGEE\n",
    "# collected_outputs_directory = '/home/halechr/cloud/turbo/Data/Output/collected_outputs' # GL\n",
    "\n",
    "## Find the files:\n",
    "csv_files = find_csv_files(collected_outputs_directory)\n",
    "h5_files = find_HDF5_files(collected_outputs_directory)\n",
    "\n",
    "csv_sessions = find_most_recent_files(found_session_export_paths=csv_files)\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "\n",
    "\n",
    "## The CSV containing the session delta time:\n",
    "t_delta_df = pd.read_csv(t_delta_csv_path, index_col=0) # Assuming that your CSV file has an index column\n",
    "# adds `delta_aligned_t_start`, `delta_aligned_t_end` columns\n",
    "t_delta_df['delta_aligned_t_start'] = t_delta_df['t_start'] - t_delta_df['t_delta']\n",
    "t_delta_df['delta_aligned_t_end'] = t_delta_df['t_end'] - t_delta_df['t_delta']\n",
    "\n",
    "# computes `earliest_delta_aligned_t_start`, latest_delta_aligned_t_end\n",
    "earliest_delta_aligned_t_start: float = np.nanmin(t_delta_df['delta_aligned_t_start'])\n",
    "latest_delta_aligned_t_end: float = np.nanmax(t_delta_df['delta_aligned_t_end'])\n",
    "print(f'earliest_delta_aligned_t_start: {earliest_delta_aligned_t_start}, latest_delta_aligned_t_end: {latest_delta_aligned_t_end}')\n",
    "t_delta_dict = t_delta_df.to_dict(orient='index')\n",
    "# t_delta_df\n",
    "\n",
    "# csv_sessions\n",
    "# Extract each of the separate files from the sessions:\n",
    "    \n",
    "final_sessions = {}\n",
    "final_sessions_loaded_laps_dict = {}\n",
    "final_sessions_loaded_ripple_dict = {}\n",
    "final_sessions_loaded_laps_time_bin_dict = {}\n",
    "final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in csv_sessions.items():\n",
    "    # try:\n",
    "        final_sessions[session_str] = {}\n",
    "        for file_type, (an_decoding_time_bin_size_str, a_path, an_export_datetime) in session_dict.items():\n",
    "            final_sessions[session_str][file_type] = a_path\n",
    "            \n",
    "        session_name: str = str(session_str)  # Extract session name from the filename\n",
    "        if debug_print:\n",
    "            print(f'processing session_name: {session_name}')\n",
    "        curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "        if curr_session_t_delta is None:\n",
    "            print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "        # finds each of the four exports:\n",
    "        try:\n",
    "            laps_file = final_sessions[session_str]['laps_marginals_df']\n",
    "            final_sessions_loaded_laps_dict[session_str] = process_csv_file(laps_file, session_name, curr_session_t_delta, 'lap_start_t')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "                    \n",
    "        try:\n",
    "            ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "            final_sessions_loaded_ripple_dict[session_str] = process_csv_file(ripple_file, session_name, curr_session_t_delta, 'ripple_start_t')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "            \n",
    "\n",
    "        try:\n",
    "            laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "            final_sessions_loaded_laps_time_bin_dict[session_str] = process_csv_file(laps_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "\n",
    "\n",
    "        try:\n",
    "            ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "            final_sessions_loaded_ripple_time_bin_dict[session_str] = process_csv_file(ripple_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "\n",
    "\n",
    "## Build across_sessions join dataframes:\n",
    "all_sessions_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# *_time_bin marginals:\n",
    "all_sessions_laps_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# Filter rows based on column: 'time_bin_size'\n",
    "all_sessions_laps_df = all_sessions_laps_df[all_sessions_laps_df['time_bin_size'].notna()]\n",
    "all_sessions_ripple_df = all_sessions_ripple_df[all_sessions_ripple_df['time_bin_size'].notna()]\n",
    "all_sessions_laps_time_bin_df = all_sessions_laps_time_bin_df[all_sessions_laps_time_bin_df['time_bin_size'].notna()]\n",
    "all_sessions_ripple_time_bin_df = all_sessions_ripple_time_bin_df[all_sessions_ripple_time_bin_df['time_bin_size'].notna()]\n",
    "\n",
    "all_sessions_laps_time_bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# final_sessions\n",
    "# {'kdiba_gor01_one_2006-6-08_14-26-15': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_marginals_df).csv')},\n",
    "#  'kdiba_gor01_one_2006-6-09_1-22-43': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(laps_marginals_df).csv')},\n",
    "#  'kdiba_pin01_one_fet11-01_12-58-54': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv')}}\n",
    "\n",
    "\n",
    "# Save out the four dataframes to CSVs:\n",
    "final_dfs_dict = {\"AcrossSession_Laps_per-Epoch\": all_sessions_laps_df, \"AcrossSession_Ripple_per-Epoch\": all_sessions_ripple_df,\n",
    "\t\t\t\t   \"AcrossSession_Laps_per-TimeBin\": all_sessions_laps_time_bin_df, \"AcrossSession_Ripple_per-TimeBin\": all_sessions_ripple_time_bin_df}\n",
    "\n",
    "final_output_path = Path(\"../output/\").resolve()\n",
    "TODAY_DAY_DATE: str = f\"2024-01-29\"\n",
    "\n",
    "final_csv_export_paths = {}\n",
    "for a_name, a_final_df in final_dfs_dict.items():\n",
    "\t# save out one final DF to csv.\n",
    "\tout_csv_filename: str = f\"{TODAY_DAY_DATE}_{a_name}.csv\"\n",
    "\ta_final_csv_export_path = final_output_path.joinpath(out_csv_filename).resolve()\n",
    "\ta_final_df.to_csv(a_final_csv_export_path) # save to CSV.\n",
    "\tfinal_csv_export_paths[a_name] = a_final_csv_export_path\n",
    "\t\n",
    "\n",
    "final_csv_export_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Model.Configs.LongShortDisplayConfig import PlottingHelpers\n",
    "from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import plot_across_sessions_scatter_results\n",
    "\n",
    "# Example usage:\n",
    "all_session_figures = plot_across_sessions_scatter_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_df, concatenated_ripple_df=all_sessions_ripple_df, save_figures=True)\n",
    "fig_laps, fig_ripples = all_session_figures[0]\n",
    "\n",
    "# Show figures for all sessions\n",
    "# for fig_laps, fig_ripples in all_session_figures:\n",
    "# # Create two shapes for the pre/post-delta periods. Requires earliest_delta_aligned_t_start, earliest_delta_aligned_t_start\n",
    "t_split: float = 0.0\n",
    "_laps_extras_output_dict = PlottingHelpers.helper_plotly_add_long_short_epoch_indicator_regions(fig_laps, t_split=t_split, t_start=earliest_delta_aligned_t_start, t_end=latest_delta_aligned_t_end)\n",
    "_ripple_extras_output_dict = PlottingHelpers.helper_plotly_add_long_short_epoch_indicator_regions(fig_ripples, t_split=t_split, t_start=earliest_delta_aligned_t_start, t_end=latest_delta_aligned_t_end)\n",
    "\n",
    "\n",
    "# fig_laps.update_layout(shapes=[blue_shape, red_shape], xaxis=dict(range=[earliest_delta_aligned_t_start, latest_delta_aligned_t_end]), yaxis=dict(range=[0.0, 1.0]))\n",
    "# fig_ripples.update_layout(shapes=[blue_shape, red_shape], xaxis=dict(range=[earliest_delta_aligned_t_start, latest_delta_aligned_t_end]), yaxis=dict(range=[0.0, 1.0]))\n",
    "\n",
    "fig_laps.show()\n",
    "fig_ripples.show()\n",
    "fig_laps.write_html(f\"../output/{TODAY_DAY_DATE}_AcrossSession_fig_laps.html\")\n",
    "fig_ripples.write_html(f\"../output/{TODAY_DAY_DATE}_AcrossSession_fig_ripples.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_laps.data\n",
    "fig_laps.add_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fig_laps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_laps.layout \n",
    "\n",
    "# Layout({\n",
    "#     'legend': {'itemsizing': 'constant', 'title': {'text': 'session_name'}, 'tracegroupgap': 0},\n",
    "#     'template': '...',\n",
    "#     'title': {'text': 'Laps - 13 Sessions'},\n",
    "#     'xaxis': {'anchor': 'y', 'domain': [0.0, 1.0], 'title': {'text': 'delta_aligned_start_t'}},\n",
    "#     'yaxis': {'anchor': 'x', 'domain': [0.0, 1.0], 'range': [0.0, 1.0], 'title': {'text': 'P_Long'}}\n",
    "# })\n",
    "\n",
    "fig_laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_laps.layout.xaxis\n",
    "fig_laps.update_yaxes(range = [0,1])\n",
    "fig_laps.layout.xaxis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "active_plot_fn = plot_across_sessions_scatter_results\n",
    "# active_plot_fn = plot_across_sessions_results_with_histogram_gpt3\n",
    "# active_plot_fn = plot_across_sessions_results_with_histogram_new\n",
    "all_time_bin_session_figures = active_plot_fn(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_time_bin_df, concatenated_ripple_df=all_sessions_ripple_time_bin_df, save_figures=False)\n",
    "all_time_bin_session_figures\n",
    "\n",
    "for fig_time_bin_laps, fig_time_bin_ripples in all_time_bin_session_figures:\n",
    "    fig_time_bin_laps.show()\n",
    "    fig_time_bin_ripples.show()\n",
    "    # fig_laps.write_html(\"../output/2024-01-18_AcrossSession_fig_laps.html\")\n",
    "    # fig_ripples.write_html(\"../output/2024-01-18_AcrossSession_fig_ripples.html\")\n",
    "    fig_time_bin_laps.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_laps.html\")\n",
    "    fig_time_bin_ripples.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_ripples.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_bin_session_figures[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from Spike3D.PendingNotebookCode import plot_stacked_histograms\n",
    "\n",
    "# You can use it like this:\n",
    "plot_histograms(all_sessions_laps_time_bin_df, 'Laps', 'All Sessions', \"75 ms\")\n",
    "plot_histograms(all_sessions_ripple_time_bin_df, 'Ripples', 'All Sessions', \"75 ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use it like this:\n",
    "num_unique_sessions: int = all_sessions_laps_time_bin_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "num_unique_time_bins: int = all_sessions_laps_time_bin_df.time_bin_size.nunique(dropna=True)\n",
    "plot_stacked_histograms(all_sessions_laps_time_bin_df, 'Laps', f'{num_unique_sessions} Sessions', f\"{num_unique_time_bins} tbin sizes\")\n",
    "\n",
    "num_unique_sessions: int = all_sessions_ripple_time_bin_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "num_unique_time_bins: int = all_sessions_ripple_time_bin_df.time_bin_size.nunique(dropna=True)\n",
    "plot_stacked_histograms(all_sessions_ripple_time_bin_df, 'Ripples', f'{num_unique_sessions} Sessions', f\"{num_unique_time_bins} tbin sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_across_sessions_scatter_results(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    Unknowingly captured: session_name\n",
    "    \n",
    "    \"\"\"\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    laps_num_unique_sessions: int = concatenated_laps_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "    laps_num_unique_time_bins: int = concatenated_laps_df.time_bin_size.nunique(dropna=True)\n",
    "    laps_title_string_suffix: str = f'{laps_num_unique_sessions} Sessions'\n",
    "    fig_laps = go.Figure(px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - {laps_title_string_suffix}\", color='session_name', size='time_bin_size'), layout_yaxis_range=[0.0, 1.0])\n",
    "\n",
    "    # Create a bubble chart for ripples\n",
    "    ripple_num_unique_sessions: int = concatenated_ripple_df.session_name.nunique(dropna=True) # number of unique sessions, ignoring the NA entries\n",
    "    ripple_num_unique_time_bins: int = concatenated_ripple_df.time_bin_size.nunique(dropna=True)\n",
    "    ripple_title_string_suffix: str = f'{ripple_num_unique_sessions} Sessions'\n",
    "    fig_ripples = go.Figure(px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - {ripple_title_string_suffix}\", color='session_name', size='time_bin_size'), layout_yaxis_range=[0.0, 1.0])\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figures to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_laps_name = Path(figures_folder, f\"{laps_title_string_suffix.replace(' ', '-')}_laps_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_laps_name}\"...')\n",
    "        fig_laps.write_image(fig_laps_name)\n",
    "        fig_ripple_name = Path(figures_folder, f\"{ripple_title_string_suffix.replace(' ', '-')}_ripples_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_ripple_name}\"...')\n",
    "        fig_ripples.write_image(fig_ripple_name)\n",
    "    \n",
    "    # Append both figures to the list\n",
    "    all_figures.append((fig_laps, fig_ripples))\n",
    "    \n",
    "    return all_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_time_bin_df.hist(column='P_Long')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "from attrs import define, field, Factory\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "@define(slots=False)\n",
    "class TwoCSV:\n",
    "\t\"\"\" simple class wrapper to emulate the object that holds the other dfs \"\"\"\n",
    "\tlaps_all_epoch_bins_marginals_df = field()\n",
    "\tripple_all_epoch_bins_marginals_df = field()\n",
    "\n",
    "save_figure = True\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\tprint(f'final_context: {final_context}')\n",
    "\t# if save_figure:\n",
    "\t# \tfig.save_fig(\n",
    "\t# \t# return owning_pipeline_reference.output_figure(final_context, fig)\n",
    "\t# else:\n",
    "\t# \tpass # do nothing, don't save\n",
    "\t\n",
    "# all_sessions_laps_time_bin_df\n",
    "\n",
    "collector = plot_all_epoch_bins_marginal_predictions(TwoCSV(laps_all_epoch_bins_marginals_df=all_sessions_laps_df, ripple_all_epoch_bins_marginals_df=all_sessions_ripple_df), t_start=None, t_split=0.0, t_end=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=IdentifyingContext(), perform_write_to_file_callback=_perform_write_to_file_callback)\n",
    "collector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tuple(collector.figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import H5FileAggregator\n",
    "\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "h5_sessions\n",
    "\n",
    "final_sessions = {}\n",
    "time_bin_size_sweep_results_files = {}\n",
    "\n",
    "# final_sessions_loaded_laps_dict = {}\n",
    "# final_sessions_loaded_ripple_dict = {}\n",
    "# final_sessions_loaded_laps_time_bin_dict = {}\n",
    "# final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in h5_sessions.items():\n",
    "    final_sessions[session_str] = {}\n",
    "    for file_type, (a_path, an_export_datetime) in session_dict.items():\n",
    "        final_sessions[session_str][file_type] = a_path\n",
    "        \n",
    "    session_name: str = str(session_str)  # Extract session name from the filename\n",
    "    if debug_print:\n",
    "        print(f'processing session_name: {session_name}')\n",
    "    curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "    if curr_session_t_delta is None:\n",
    "        print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "    # finds each of the four exports:\n",
    "    time_bin_size_sweep_results_file = final_sessions[session_str]['time_bin_size_sweep_results']\n",
    "    time_bin_size_sweep_results_files[session_str] = Path(time_bin_size_sweep_results_file).resolve()\n",
    "    # ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "    # laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "    # ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "    \n",
    "\n",
    "\n",
    "session_identifiers = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "# Keys are like: \"/kdiba|gor01|two|2006-6-12_16-53-46\"\n",
    "included_h5_paths = list(time_bin_size_sweep_results_files.values())\n",
    "session_group_keys: List[str] = [(\"/\" + a_ctxt.get_description(separator=\"|\", include_property_names=False)) for a_ctxt in session_identifiers] # 'kdiba/gor01/one/2006-6-08_14-26-15'\n",
    "laps_decoding_accuracy_results_table_keys = [f\"{session_group_key}/laps_decoding_accuracy_results/table\" for session_group_key in session_group_keys]\n",
    "a_loader = H5FileAggregator.init_from_file_lists(file_list=included_h5_paths, table_key_list=laps_decoding_accuracy_results_table_keys)\n",
    "_out_table = a_loader.load_and_consolidate()\n",
    "_out_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_yellow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
