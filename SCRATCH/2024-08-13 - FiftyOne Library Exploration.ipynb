{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "# %xmode Verbose\n",
    "# %xmode context\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "from copy import deepcopy\n",
    "# from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# pd.options.mode.dtype_backend = 'pyarrow' # use new pyarrow backend instead of numpy\n",
    "from attrs import define, field, fields, Factory\n",
    "import tables as tb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pho's Formatting Preferences\n",
    "import builtins\n",
    "\n",
    "import IPython\n",
    "from IPython.core.formatters import PlainTextFormatter\n",
    "from IPython import get_ipython\n",
    "\n",
    "from pyphocorehelpers.preferences_helpers import set_pho_preferences, set_pho_preferences_concise, set_pho_preferences_verbose\n",
    "set_pho_preferences_concise()\n",
    "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# BEGIN PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "\n",
    "## IPython pprint\n",
    "from pyphocorehelpers.pprint import wide_pprint, wide_pprint_ipython, wide_pprint_jupyter, MAX_LINE_LENGTH\n",
    "\n",
    "# Override default pprint\n",
    "builtins.pprint = wide_pprint\n",
    "\n",
    "text_formatter: PlainTextFormatter = IPython.get_ipython().display_formatter.formatters['text/plain']\n",
    "text_formatter.max_width = MAX_LINE_LENGTH\n",
    "text_formatter.for_type(object, wide_pprint_jupyter)\n",
    "\n",
    "\n",
    "# END PPRINT CUSTOMIZATION ___________________________________________________________________________________________ #\n",
    "\n",
    "from pyphocorehelpers.print_helpers import get_now_time_str, get_now_day_str\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path, file_uri_from_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "# from neuropy import core\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
    "from typing_extensions import TypeAlias\n",
    "from nptyping import NDArray\n",
    "import neuropy.utils.type_aliases as types\n",
    "\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.core.epoch import NamedTimerange, Epoch\n",
    "from neuropy.core.ratemap import Ratemap\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.utils.matplotlib_helpers import matplotlib_file_only, matplotlib_configuration, matplotlib_configuration_update\n",
    "from neuropy.core.neuron_identities import NeuronIdentityTable, neuronTypesList, neuronTypesEnum\n",
    "from neuropy.utils.mixins.AttrsClassHelpers import AttrsBasedClassHelperMixin, serialized_field, serialized_attribute_field, non_serialized_field, custom_define\n",
    "from neuropy.utils.mixins.HDF5_representable import HDF_DeserializationMixin, post_deserialize, HDF_SerializationMixin, HDFMixin, HDF_Converter\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.analyses.placefields import PlacefieldComputationParameters\n",
    "from neuropy.utils.dynamic_container import DynamicContainer\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "from neuropy.core.neurons import NeuronType\n",
    "from neuropy.core.user_annotations import UserAnnotationsManager\n",
    "from neuropy.core.position import Position\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.analyses.time_dependent_placefields import PfND_TimeDependent, PlacefieldSnapshot\n",
    "from neuropy.utils.debug_helpers import debug_print_placefield, debug_print_subsession_neuron_differences, debug_print_ratemap, debug_print_spike_counts, debug_plot_2d_binning, print_aligned_columns\n",
    "from neuropy.utils.debug_helpers import parameter_sweeps, _plot_parameter_sweep, compare_placefields_info\n",
    "from neuropy.utils.indexing_helpers import NumpyHelpers, union_of_arrays, intersection_of_arrays, find_desired_sort_indicies, paired_incremental_sorting\n",
    "from pyphocorehelpers.print_helpers import print_object_memory_usage, print_dataframe_memory_usage, print_value_overview_only, DocumentationFilePrinter, print_keys_if_possible, generate_html_string, document_active_variables\n",
    "\n",
    "## Pho Programming Helpers:\n",
    "import inspect\n",
    "from pyphocorehelpers.print_helpers import DocumentationFilePrinter, TypePrintMode, print_keys_if_possible, debug_dump_object_member_shapes, print_value_overview_only, document_active_variables\n",
    "from pyphocorehelpers.programming_helpers import IPythonHelpers, PythonDictionaryDefinitionFormat, MemoryManagement, inspect_callable_arguments, get_arguments_as_optional_dict, GeneratedClassDefinitionType, CodeConversion\n",
    "from pyphocorehelpers.gui.Qt.TopLevelWindowHelper import TopLevelWindowHelper, print_widget_hierarchy\n",
    "from pyphocorehelpers.indexing_helpers import reorder_columns, reorder_columns_relative, dict_to_full_array\n",
    "from pyphocorehelpers.programming_helpers import CodeConversion, SourceCodeParsing, GeneratedClassDefinitionType\n",
    "\n",
    "# doc_output_parent_folder: Path = Path('EXTERNAL/DEVELOPER_NOTES/DataStructureDocumentation').resolve() # ../.\n",
    "# print(f\"doc_output_parent_folder: {doc_output_parent_folder}\")\n",
    "# assert doc_output_parent_folder.exists()\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import NeuropyPipeline # get_neuron_identities\n",
    "from pyphoplacecellanalysis.General.Mixins.ExportHelpers import export_pyqtgraph_plot\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, batch_extended_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "import pyphoplacecellanalysis.External.pyqtgraph as pg\n",
    "\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_perform_all_plots\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import JonathanFiringRateAnalysisResult\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import _find_any_context_neurons\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchSessionCompletionHandler # for `post_compute_validate(...)`\n",
    "from pyphoplacecellanalysis.Analysis.Decoder.reconstruction import BasePositionDecoder\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import AcrossSessionsResults\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends # for `_perform_long_short_instantaneous_spike_rate_groups_analysis`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.LongShortTrackComputations import SingleBarResult, InstantaneousSpikeRateGroupsComputation, TruncationCheckingResults # for `BatchSessionCompletionHandler`, `AcrossSessionsAggregator`\n",
    "from pyphoplacecellanalysis.General.Mixins.CrossComputationComparisonHelpers import SplitPartitionMembership\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import DirectionalPlacefieldGlobalComputationFunctions, DirectionalLapsResult, TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderGlobalComputationFunctions\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import TrackTemplates\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderComputationsContainer, RankOrderResult\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.RankOrderComputations import RankOrderAnalyses\n",
    "\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "# %matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "_restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "\n",
    "from silx.gui import qt\n",
    "from silx.gui.plot import Plot2D, Plot1D\n",
    "from silx.gui.colors import Colormap\n",
    "from silx.gui.plot.items import ImageBase\n",
    "\n",
    "# import pylustrator # call `pylustrator.start()` before creating your first figure in code.\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap\n",
    "from pyphoplacecellanalysis.Pho2D.matplotlib.visualize_heatmap import visualize_heatmap_pyqtgraph # used in `plot_kourosh_activity_style_figure`\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import plot_multiple_raster_plot, plot_raster_plot\n",
    "from pyphoplacecellanalysis.General.Mixins.DataSeriesColorHelpers import UnitColoringMode, DataSeriesColorHelpers\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.DisplayFunctions.SpikeRasters import _build_default_tick, build_scatter_plot_kwargs\n",
    "from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import Render2DScrollWindowPlotMixin, ScatterItemData\n",
    "from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_extended_programmatic_figures, batch_programmatic_figures\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.SpikeAnalysis import SpikeRateTrends\n",
    "from pyphoplacecellanalysis.General.Mixins.SpikesRenderingBaseMixin import SpikeEmphasisState\n",
    "\n",
    "# from pyphoplacecellanalysis.SpecificResults.PhoDiba2023Paper import PAPER_FIGURE_figure_1_add_replay_epoch_rasters, PAPER_FIGURE_figure_1_full, PAPER_FIGURE_figure_3, main_complete_figure_generations\n",
    "# from pyphoplacecellanalysis.SpecificResults.fourthYearPresentation import *\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# api_key = os.getenv('MY_API_KEY')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "\n",
    "posterior_images_path = Path(r\"C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\_temp_individual_posteriors\\2024-08-13\").resolve()\n",
    "assert posterior_images_path.exists()\n",
    "assert posterior_images_path.is_dir()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "## INPUTS: posterior_images_path\n",
    "sample_dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "sample_session = fo.launch_app(sample_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "session = fo.launch_app(desktop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = \"test-exported-posteriors-dataset\"\n",
    "dataset_dir = posterior_images_path.resolve().as_posix()\n",
    "debug_print = False\n",
    "# dataset_dir = posterior_images_path.resolve().as_posix()\n",
    "processed_images_folder = posterior_images_path.joinpath('processed').resolve()\n",
    "\n",
    "\n",
    "# Check if the dataset already exists\n",
    "if name in fo.list_datasets():\n",
    "    dataset = fo.load_dataset(name)\n",
    "else:\n",
    "    # Create the dataset\n",
    "    dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=dataset_dir,\n",
    "        dataset_type=fo.types.ImageDirectory,\n",
    "        name=name,\n",
    "    )    \n",
    "\n",
    "\n",
    "\n",
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic screenshots as you work\n",
    "\n",
    "Notebooks are great for many reasons, one of which is the ability to share your work with others. FiftyOne is designed to help you write notebooks that capture your work on visual datasets, using a feature we call **automatic screenshotting**.\n",
    "\n",
    "Whenever you open a new App instance in a notebook cell, e.g., by updating your [Session](https://voxel51.com/docs/fiftyone/api/fiftyone.core.session.html#fiftyone.core.session.Session) object, any previous App instances will be automatically replaced with a static screenshot. In fact, that's what you're seeing below; screenshots of the Apps we opened when we created this notebook!\n",
    "\n",
    "The cell below issues a [session.show()](https://voxel51.com/docs/fiftyone/api/fiftyone.core.session.html#fiftyone.core.session.Session.show) command, which opens a new App instance in the cell's output. When you run the cell for yourself, notice that the App instance in the previous cell is automatically replaced with a screenshot of its current state. You can reactivate old App instances by hovering over them and clicking anywhere.\n",
    "\n",
    "After running the cell below, try double-clicking on an image in the grid to expand the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "## INPUTS: dataset\n",
    "\n",
    "def apply_laplacian(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply the Laplacian edge detector\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    abs_laplacian = np.uint8(np.absolute(laplacian))\n",
    "    \n",
    "    return abs_laplacian\n",
    "\n",
    "\n",
    "\n",
    "dataset2 = dataset.clone()\n",
    "dataset2.add_sample_field(\"processed_img_path\", fo.StringField)\n",
    "\n",
    "# Apply the Laplacian edge detector to each image in the dataset\n",
    "for sample in dataset2:\n",
    "    # Get the file path of the image\n",
    "    image_path = sample.filepath\n",
    "    \n",
    "    # Apply the Laplacian edge detector\n",
    "    laplacian_image = apply_laplacian(image_path)\n",
    "    \n",
    "    raw_img_path = Path(image_path).resolve()\n",
    "    # processed_img_path = processed_images_folder.joinpath(raw_img_path.name)\n",
    "    processed_img_path = processed_images_folder.joinpath(f\"{raw_img_path.stem}_laplacian\").with_suffix(raw_img_path.suffix)\n",
    "    if debug_print:\n",
    "        print(f'image_path: {image_path}')\n",
    "        print(f'processed_img_path: {processed_img_path}')\n",
    "        \n",
    "    # processed_img_path.with_suffix(\"_laplacian\")\n",
    "    # raw_img_path\n",
    "    \n",
    "    # # Save the result as a new image\n",
    "    # output_path = image_path.replace(\".jpg\", \"_laplacian.jpg\")\n",
    "    output_path = processed_img_path.as_posix()\n",
    "    _is_success = cv2.imwrite(output_path, laplacian_image)\n",
    "    \n",
    "    # print(sample)\n",
    "    sample[\"processed_img_path\"] = output_path\n",
    "    # Add the new image to the dataset\n",
    "    sample[\"laplacian\"] = output_path # fo.Image(filepath=output_path)\n",
    "    sample.save()\n",
    "    \n",
    "\n",
    "session2 = fo.launch_app(dataset2)\n",
    "session2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_laplacian_edge_detection(dataset, processed_images_folder, debug_print=False):\n",
    "    _computation_id_key: str = \"laplacian\"\n",
    "    \n",
    "    def apply_laplacian(image_path):\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply the Laplacian edge detector\n",
    "        laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "        \n",
    "        # Convert back to uint8\n",
    "        abs_laplacian = np.uint8(np.absolute(laplacian))\n",
    "        \n",
    "        return abs_laplacian\n",
    "\n",
    "\n",
    "    dataset2 = dataset.clone()\n",
    "    dataset2.add_sample_field(\"processed_img_path\", fo.StringField)\n",
    "\n",
    "    # Apply the Laplacian edge detector to each image in the dataset\n",
    "    for sample in dataset2:\n",
    "        # Get the file path of the image\n",
    "        image_path = sample.filepath\n",
    "        \n",
    "        # Apply the Laplacian edge detector\n",
    "        laplacian_image = apply_laplacian(image_path)\n",
    "        \n",
    "        raw_img_path = Path(image_path).resolve()\n",
    "        # processed_img_path = processed_images_folder.joinpath(raw_img_path.name)\n",
    "        processed_img_path = processed_images_folder.joinpath(f\"{raw_img_path.stem}_{_computation_id_key}\").with_suffix(raw_img_path.suffix)\n",
    "        if debug_print:\n",
    "            print(f'image_path: {image_path}')\n",
    "            print(f'processed_img_path: {processed_img_path}')\n",
    "            \n",
    "        # processed_img_path.with_suffix(\"_laplacian\")\n",
    "        # raw_img_path\n",
    "        \n",
    "        # # Save the result as a new image\n",
    "        # output_path = image_path.replace(\".jpg\", \"_laplacian.jpg\")\n",
    "        output_path = processed_img_path.as_posix()\n",
    "        _is_success = cv2.imwrite(output_path, laplacian_image)\n",
    "        \n",
    "        # print(sample)\n",
    "        sample[\"processed_img_path\"] = output_path\n",
    "        # Add the new image to the dataset\n",
    "        sample[_computation_id_key] = output_path # fo.Image(filepath=output_path)\n",
    "        sample.save()\n",
    "        \n",
    "    return dataset2\n",
    "\n",
    "dataset2 = perform_laplacian_edge_detection(dataset, processed_images_folder=processed_images_folder, debug_print=False)\n",
    "# session2 = fo.launch_app(dataset2)\n",
    "# session2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ridge_detection(dataset, processed_images_folder, debug_print=False):\n",
    "    _computation_id_key: str = \"ridges\"\n",
    "    \n",
    "    def apply_ridge_detection(image_path):\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply the ridge detection filter\n",
    "        ridge_filter = cv2.ximgproc.RidgeDetectionFilter_create()\n",
    "        ridges = ridge_filter.getRidgeFilteredImage(gray)\n",
    "        \n",
    "        # Convert back to uint8\n",
    "        abs_ridges = np.uint8(np.absolute(ridges))\n",
    "        \n",
    "        return abs_ridges\n",
    "\n",
    "\n",
    "    dataset2 = dataset.clone()\n",
    "\n",
    "    # Apply the Ridge Detection Filter to each image in the dataset\n",
    "    for sample in dataset2:\n",
    "        # Get the file path of the image\n",
    "        image_path = sample.filepath\n",
    "        \n",
    "        # Apply the ridge detection\n",
    "        ridges_image = apply_ridge_detection(image_path)\n",
    "        \n",
    "        raw_img_path = Path(image_path).resolve()\n",
    "        # processed_img_path = processed_images_folder.joinpath(raw_img_path.name)\n",
    "        processed_img_path = processed_images_folder.joinpath(f\"{raw_img_path.stem}_{_computation_id_key}\").with_suffix(raw_img_path.suffix)\n",
    "        if debug_print:\n",
    "            print(f'image_path: {image_path}')\n",
    "            print(f'processed_img_path: {processed_img_path}')\n",
    "            \n",
    "        # processed_img_path.with_suffix(\"_laplacian\")\n",
    "        # raw_img_path\n",
    "        \n",
    "        # # Save the result as a new image\n",
    "        # output_path = image_path.replace(\".jpg\", \"_laplacian.jpg\")\n",
    "        output_path = processed_img_path.as_posix()\n",
    "        _is_success = cv2.imwrite(output_path, ridges_image)\n",
    "        \n",
    "        # print(sample)\n",
    "        sample[\"processed_img_path\"] = output_path\n",
    "        # Add the new image to the dataset\n",
    "        sample[_computation_id_key] = output_path # fo.Image(filepath=output_path)\n",
    "        sample.save()\n",
    "\n",
    "    return dataset2\n",
    "\n",
    "dataset3 = perform_ridge_detection(dataset, processed_images_folder=processed_images_folder, debug_print=False)\n",
    "# session3 = fo.launch_app(dataset3)\n",
    "# session3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "api_key = 'swBzQ0x.72AzLyKaC3DNvFSbrDHN4FwOaB1PGU7P'\n",
    "\n",
    "json = {\"name\": \"My Dataset\"}\n",
    "headers = {\"Authorization\": f\"ApiKey {api_key}\"}\n",
    "url = \"https://darwin.v7labs.com/api/datasets\"\n",
    "\n",
    "result = requests.post(url, json=json, headers=headers)\n",
    "print(result.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_ds_name = \"test-exported-posteriors-processed-dataset\"\n",
    "processed_dataset_dir = processed_images_folder.resolve().as_posix()\n",
    "\n",
    "# Check if the dataset already exists\n",
    "if processed_ds_name in fo.list_datasets():\n",
    "    processed_dataset = fo.load_dataset(processed_ds_name)\n",
    "else:\n",
    "    # Create the dataset\n",
    "    processed_dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=processed_dataset_dir,\n",
    "        dataset_type=fo.types.ImageDirectory,\n",
    "        name=processed_ds_name,\n",
    "    )    \n",
    "\n",
    "# View summary info about the dataset\n",
    "print(processed_dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(processed_dataset.head())\n",
    "\n",
    "session2 = fo.launch_app(processed_dataset)\n",
    "# session2.show()\n",
    "session2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Views\n",
    "\n",
    "The power of FiftyOne truly comes alive when using [dataset views](https://voxel51.com/docs/fiftyone/user_guide/using_views.html).\n",
    "\n",
    "Think of a [Dataset](https://voxel51.com/docs/fiftyone/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset) as the root view into your all of your data. Creating a [DatasetView](https://voxel51.com/docs/fiftyone/api/fiftyone.core.view.html#fiftyone.core.view.DatasetView) allows you to study a specific subset of the samples and/or fields of your dataset.\n",
    "\n",
    "Dataset views can be created and modified both in Python and in the App. The active view in the App is always available via the [Session.view](https://voxel51.com/docs/fiftyone/api/fiftyone.core.session.html#fiftyone.core.session.Session.view) property of your session. This means that if you update your view in the App, its state will be captured by [Session.view](https://voxel51.com/docs/fiftyone/api/fiftyone.core.session.html#fiftyone.core.session.Session.view). Or, you can create a view programmatically in Python and open it in the App by setting the [Session.view](https://voxel51.com/docs/fiftyone/api/fiftyone.core.session.html#fiftyone.core.session.Session.view) property.\n",
    "\n",
    "Let's start by creating a view into our dataset via the App. We'll sort the dataset by the `uniqueness` field to show the most unique images first. To do this, we will click `+ add stage` in the View Bar and add a `SortBy` stage with `uniqueness` as the field, and `reverse` equal to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then access the view in Python and, for example, print the most unique sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.view.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex views in Python\n",
    "\n",
    "Sometimes you may be interested in creating a [complex view](https://voxel51.com/docs/fiftyone/user_guide/using_views.html#view-stages) into a dataset that is specified by a series of conditions or complex filtering operations.\n",
    "\n",
    "You can achieve this in FiftyOne by [chaining view stages](https://voxel51.com/docs/fiftyone/user_guide/using_views.html#tips-tricks) together to define the view you want.\n",
    "\n",
    "As an example, let's create a view that contains only the 25 most unique samples in the dataset, and only predictions on those samples with confidence > 0.5.\n",
    "\n",
    "Remember that, because we are working in a notebook, any time we change our [Session](https://voxel51.com/docs/fiftyone/api/fiftyone.core.session.html#fiftyone.core.session.Session) object, a new App will be displayed in the cell's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "session.view = (\n",
    "    dataset\n",
    "    .sort_by(\"uniqueness\", reverse=True)\n",
    "    .limit(25)\n",
    "    .filter_labels(\"predictions\", F(\"confidence\") > 0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the objects in the `predictions` field with respect to the\n",
    "# objects in the `ground_truth` field\n",
    "results = sample_dataset.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"eval_predictions\",\n",
    ")\n",
    "sample_session.show()\n",
    "\n",
    "# # Detection evaluation\n",
    "# dataset.evaluate_detections(\n",
    "#     \"predictions\",\n",
    "#     gt_field=\"ground_truth\",\n",
    "#     eval_key=eval_key,\n",
    "#     ...,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "This quickstart touched on only a few of the possibilities of using FiftyOne. If you'd like to learn more, check out these [tutorials](https://voxel51.com/docs/fiftyone/tutorials/index.html) and [recipes](https://voxel51.com/docs/fiftyone/recipes/index.html) to see more concrete use cases and best practices.\n",
    "\n",
    "And did we mention that FiftyOne is open source? Check out the project [on GitHub](https://github.com/voxel51/fiftyone) and [leave an issue](https://github.com/voxel51/fiftyone/issues/new/choose) if you think something is missing.\n",
    "\n",
    "Thanks for tuning in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/evaluation\n",
    "fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/annotation\n",
    "fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/io\n",
    "\n",
    "## Community\n",
    "# fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/evaluation @voxel51/annotation @voxel51/io @danielgural/outlier_detection @jacobmarks/clustering @danielgural/clustering_algorithms\n",
    "\n",
    "fiftyone plugins download https://github.com/danielgural/outlier_detection\n",
    "fiftyone plugins download https://github.com/danielgural/clustering_algorithms\n",
    "fiftyone plugins download https://github.com/jacobmarks/clustering-plugin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2429300835.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    fiftyone plugins list\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# List all plugins you've downloaded\n",
    "fiftyone plugins list\n",
    "\n",
    "# List the available operators\n",
    "fiftyone operators list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "# Testing Annotations with Darwin V7\n",
    "https://darwin.v7labs.com/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "view = dataset.take(1)\n",
    "\n",
    "anno_key = \"v7_new_field\"\n",
    "\n",
    "view.annotate(\n",
    "    anno_key,\n",
    "    backend=\"darwin\",\n",
    "    label_field=\"new_classifications\",\n",
    "    label_type=\"classifications\",\n",
    "    classes=[\"dog\", \"cat\", \"person\"],\n",
    "    dataset_slug=anno_key,\n",
    "    launch_editor=True,\n",
    ")\n",
    "print(dataset.get_annotation_info(anno_key))\n",
    "\n",
    "# Create annotations in V7\n",
    "\n",
    "dataset.load_annotations(anno_key, cleanup=True)\n",
    "dataset.delete_annotation_run(anno_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "view = dataset.take(1)\n",
    "\n",
    "anno_key = \"v7_new_field\"\n",
    "\n",
    "label_schema = {\n",
    "    \"new_classifications\": {\n",
    "        \"type\": \"classifications\",\n",
    "        \"classes\": [\"dog\", \"cat\", \"person\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "view.annotate(\n",
    "    anno_key,\n",
    "    backend=\"darwin\",\n",
    "    label_schema=label_schema,\n",
    "    dataset_slug=anno_key,\n",
    "    launch_editor=True,\n",
    ")\n",
    "print(dataset.get_annotation_info(anno_key))\n",
    "\n",
    "# Create annotations in V7\n",
    "\n",
    "dataset.load_annotations(anno_key, cleanup=True)\n",
    "dataset.delete_annotation_run(anno_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
