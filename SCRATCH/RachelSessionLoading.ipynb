{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatBaseRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.dataSession import DataSession\n",
    "from neuropy.core.session.Formats.SessionSpecifications import SessionFolderSpec, SessionFileSpec\n",
    "\n",
    "# For specific load functions:\n",
    "from neuropy.core import DataWriter, NeuronType, Neurons, BinnedSpiketrain, Mua, ProbeGroup, Position, Epoch, Signal, Laps, FlattenedSpiketrains, Shank, Probe, ProbeGroup\n",
    "from neuropy.io import OptitrackIO, PhyIO\n",
    "from neuropy.utils.mixins.print_helpers import ProgressMessagePrinter, SimplePrintable, OrderedMeta\n",
    "\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder, DataSessionFormatBaseRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.Filesystem.open_in_system_file_manager import reveal_in_system_file_manager\n",
    "\n",
    "global_data_root_parent_path = find_first_extant_path([Path(r'W:\\Data'), Path(r'/home/halechr/FastData'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/home/halechr/cloud/turbo/Data')])\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\n",
    "\n",
    "## Rachel:\n",
    "active_data_mode_name = 'rachel'\n",
    "local_session_root_parent_context = IdentifyingContext(format_name=active_data_mode_name) # , animal_name='', configuration_name='one', session_name=a_sess.session_name\n",
    "local_session_root_parent_path = global_data_root_parent_path.joinpath('Rachel')\n",
    "\n",
    "# basedir: Path = Path(r'W:\\Data\\Rachel\\20230614_Rachel').resolve()\n",
    "\n",
    "# basedir: Path = Path('/home/halechr/FastData/Rachel/20230614_Rachel/merged_20230614_2crs.GUI').resolve()\n",
    "\n",
    "basedir: Path = Path('/home/halechr/FastData/Rachel/20230614_Rachel').resolve()\n",
    "assert basedir.exists()\n",
    "\n",
    "# filename: str = '20230614_Rachel'\n",
    "# filename: str = '20230614_Rachel_2'\n",
    "filename: str = 'merged_20230614_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id column does not exist in cluster_info.tsv. Using cluster_id column instead.\n",
      "neuronIDs:      cluster_id  q\n",
      "0             0  0\n",
      "1             3  0\n",
      "2             5  0\n",
      "3             6  0\n",
      "4            12  0\n",
      "..          ... ..\n",
      "284        1381  0\n",
      "285        1382  0\n",
      "286        1388  0\n",
      "287        1389  1\n",
      "288        1390  1\n",
      "\n",
      "[289 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RachelDataSessionFormat.initialize_data_directory(basedir)\n",
    "## Builds the .neurons.npy:\n",
    "# folder = Path('/home/wahlberg/Exp_Data/M1_Nov2021/20211123/merged_M1_20211123_raw/merged_M1_20211123_raw_phy')\n",
    "# folder = Path(r'W:\\Data\\Rachel\\20230614_Rachel')\n",
    "folder = basedir.resolve()\n",
    "phydata = PhyIO(folder)\n",
    "# /home/halechr/FastData/Rachel/20230614_Rachel/params.py\n",
    "\n",
    "# phydata.\n",
    "# neuronIDs = pd.read_csv(r'W:\\Data\\Rachel\\20230614_Rachel\\cluster_q.tsv');\n",
    "phydata.shank_ids\n",
    "neuronIDs = pd.read_csv(basedir.joinpath('cluster_q.tsv'), sep='\\t')\n",
    "print(f'neuronIDs: {neuronIDs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neuronIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuronIDs.cluster_id.to_numpy()\n",
    "neuronIDs.q.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NeuronType.from_qclu_series(a_q) for a_q in neuronIDs.q.to_numpy()]\n",
    "\n",
    "NeuronType.from_qclu_series(neuronIDs.q.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neurons\n",
       " n_neurons: 63\n",
       " n_total_spikes: 1166460\n",
       " t_start: 0.0\n",
       " t_stop: 53602.047"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons = Neurons(spiketrains=phydata.spiketrains, t_stop=53602.047, sampling_rate=30000, neuron_ids=neuronIDs.cluster_id.to_numpy(), neuron_type=NeuronType.from_qclu_series(neuronIDs.q.to_numpy()), shank_ids=np.array([0]*len(neuronIDs))) # np.arange(len(neuron_IDs))\n",
    "\t\t\t\t#    neuron_type=neuronIDs)\n",
    "\n",
    "\t\n",
    "# {1:'pyr1',2:'pyr2',3:'pyr3',4:'int1',5:'int2',6:'int3',7:\"mua1\",8:'mua2',9:'mua3'}\n",
    "# neurons = Neurons(spiketrains=phydata.spiketrains, t_stop=2*3600, sampling_rate=30000, neuron_ids = {1:'pyr1',2:'pyr2',3:'pyr3',4:'int1',5:'int2',6:'int3',7:\"mua1\",8:'mua2',9:'mua3'})\n",
    "\n",
    "# neuronIDs\n",
    "neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_20230614_2.neurons.npy saved\n"
     ]
    }
   ],
   "source": [
    "neurons.filename = folder.joinpath(f'{filename}.neurons.npy')\n",
    "neurons.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_t: 43339.095\n",
      "brelative.shape: (221455, 4)\n",
      "behaviordf.shape: (221455, 3)\n",
      "merged_20230614_2.position.npy saved\n",
      "merged_20230614_2.paradigm.npy saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Probe Groups file\n",
    "# TODO: Probe group generation\n",
    "# shanks = []\n",
    "# # channel_groups = sess.recinfo.channel_groups\n",
    "# for i in range(8):\n",
    "#     shank = Shank.auto_generate(\n",
    "#         columns=1,\n",
    "#         contacts_per_column=128,\n",
    "#         xpitch=90,\n",
    "#         ypitch=0,\n",
    "#         y_shift_per_column=[0, 0],\n",
    "#         channel_id=np.arange(0,128,1)\n",
    "#         ),\n",
    "\t\n",
    "# elec_IDs = np.arange(0,128,1)\n",
    "# shanks = Shank.auto_generate(channels=1, contacts_per_column = 128)\n",
    "# shanks = pd.read_csv('/home/wahlberg/Exp_Data/M1_Nov2021/20211123/merged_M1_20211123_raw/Probe.csv',delimiter=',',usecols=[\"ShankNumber\"])\n",
    "# shanks = pd.read_csv(folder.joinpath(f'{filename}.Probe.csv'),delimiter=',',usecols=[\"ShankNumber\"])\n",
    "# prb = Probe(shanks)\n",
    "# prbgroup = ProbeGroup()\n",
    "# prbgroup.add_probe(prb)\n",
    "\n",
    "\n",
    "## Builds the .position.npy:\n",
    "# opti_folder = Path(r'W:\\Data\\Rachel\\20230614_Rachel')\n",
    "# opti_folder = basedir.resolve()\n",
    "# opti_data = OptitrackIO(opti_folder)\n",
    "# brelative = pd.read_csv(r'W:\\Data\\Rachel\\20230614_Rachel\\merged_M1_20211123_raw_behavior_relativetoLFP.csv',header = None)\n",
    "\n",
    "csv_path = basedir.joinpath(f'20230614_positionData.csv')\n",
    "# brelative = pd.read_csv(csv_path, header = None)\n",
    "brelative = pd.read_csv(csv_path)\n",
    "brelative\n",
    "# Change column type to timedelta64[ns] for column: 'AbsoluteTime'\n",
    "brelative = brelative.astype({'AbsoluteTime': 'timedelta64[ns]'})\n",
    "\n",
    "brelative_seconds = brelative.AbsoluteTime.dt.total_seconds().to_numpy()\n",
    "start_t = np.min(brelative_seconds)\n",
    "# start_t = np.min(brelative.AbsoluteTime.to_numpy())\n",
    "print(f'start_t: {start_t}')\n",
    "\n",
    "t_relative = brelative_seconds - start_t\n",
    "t_relative\n",
    "\n",
    "print(f'brelative.shape: {brelative.shape}')\n",
    "# d = {'t':brelative[0],'x':opti_data.z,'y':opti_data.x} \n",
    "d = {'t':brelative_seconds,'x':brelative.Z.to_numpy(),'y':brelative.X.to_numpy()} \n",
    "\n",
    "behaviordf = pd.DataFrame(data=d)\n",
    "print(f'behaviordf.shape: {behaviordf.shape}')\n",
    "position = Position(behaviordf)\n",
    "# position.filename = Path(f'W:\\Data\\Rachel\\20230614_Rachel\\{filename}.position.npy')\n",
    "position.filename = basedir.joinpath(f'{filename}.position.npy')\n",
    "position.save()\n",
    "\n",
    "## Builds the .paradigm.npy file from scratch:\n",
    "# starts = [0, 5*60]\n",
    "# stops = [5*60-1, 3.8398632e+03]\n",
    "# labels = ['pre','maze']\n",
    "\n",
    "paradigm_df = pd.DataFrame(dict(label=['pre','maze','post'],\n",
    "\tstart = ['11:15:49.000','12:02:19.095','13:08:37.999'],\n",
    "\tstops = ['11:53:19.384','13:04:54.815','14:53:22.047'],\n",
    "))\n",
    "# Change column type to timedelta64[ns] for columns: 'Starts', 'Stops'\n",
    "paradigm_df = paradigm_df.astype({'start': 'timedelta64[ns]', 'stops': 'timedelta64[ns]'})\n",
    "\n",
    "\n",
    "## Build and save the paradigm.npy\n",
    "d = {'start':paradigm_df.start.dt.total_seconds().to_numpy(),\n",
    "\t 'stop':paradigm_df.stops.dt.total_seconds().to_numpy(),\n",
    "\t 'label':paradigm_df.label.to_list()} \n",
    "paradigmdf = pd.DataFrame(data=d)\n",
    "paradigm = Epoch(paradigmdf)\n",
    "# paradigm.filename = Path('/home/wahlberg/Exp_Data/M1_Nov2021/20211123/merged_M1_20211123_raw/merged_M1_20211123_raw_phy/merged_M1_20211123_raw.paradigm.npy')\n",
    "paradigm.filename = basedir.joinpath(f'{filename}.paradigm.npy')\n",
    "paradigm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neuropy.core.probe.Shank at 0x7f7273ba3df0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shanks = pd.read_csv(folder.joinpath(f'{filename}.Probe.csv'),delimiter=',',usecols=[\"ShankNumber\"])\n",
    "# shanks = pd.read_csv(folder.joinpath(f'{filename}.Probe.csv'))\n",
    "# Change column type to float64 for column: 'ShankNumber'\n",
    "# shanks = shanks.astype({'ShankNumber': 'float64'})\n",
    "\n",
    "# elec_IDs = np.arange(0,128,1)\n",
    "# shanks = Shank.auto_generate(channels=1, contacts_per_column = 128)\n",
    "\n",
    "shanks = Shank.auto_generate(columns=1, contacts_per_column = 128, xpitch=0, ypitch=0, channel_id = np.arange(127,-1,-1))\n",
    "\n",
    "# shanks_list = [Shank.auto_generate(columns=1, contacts_per_column = 32, xpitch=0, ypitch=0, channel_id = np.arange(31,-1,-1)) for i in np.arange(4)]\n",
    "shanks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_20230614_2.probegroup.npy saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/probe.py:149: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self._data = self._data.append(shank_df)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/probe.py:341: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self._data = self._data.append(probe_df)\n",
      "/home/halechr/repos/NeuroPy/neuropy/core/probe.py:341: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  self._data = self._data.append(probe_df)\n"
     ]
    }
   ],
   "source": [
    "prb = Probe(shanks)\n",
    "# prb = Probe(list(np.arange(128)))\n",
    "prbgroup = ProbeGroup()\n",
    "prbgroup.add_probe(prb)\n",
    "prbgroup.filename = basedir.joinpath(f'{filename}.probegroup.npy')\n",
    "prbgroup.save(prbgroup.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/FastData/Rachel/20230614_Rachel/merged_20230614_2.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSession(config: rachel_20230614_Rachel_sess): Not yet configured."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "_test_session = RachelDataSessionFormat.build_session(basedir)\n",
    "_test_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_session.neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure loading .flattened.spikes.npy. Must recompute.\n",
      "\n",
      "Computing flattened_spike_identities results : build_spike_dataframe(...)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halechr/repos/NeuroPy/neuropy/core/session/Formats/SessionSpecifications.py:123: UserWarning: WARNING: Optional File: /home/halechr/FastData/Rachel/20230614_Rachel/merged_20230614_2.dat does not exist. Continuing without it.\n",
      "  warnings.warn(f'WARNING: Optional File: {an_optional_filepath} does not exist. Continuing without it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Computing flattened_spike_types results : build_spike_dataframe(...)... done.\n",
      "Computing flattened_spike_shank_identities results : build_spike_dataframe(...)... done.\n",
      "Computing flattened_spike_linear_unit_spike_idx results : build_spike_dataframe(...)... done.\n",
      "Computing flattened_spike_times results : build_spike_dataframe(...)... done.\n",
      "Sorting flattened_sort_indicies results : build_spike_dataframe(...)... done.\n",
      "Building final dataframe (containing 1166460 spikes) spikes_df results : build_spike_dataframe(...)... done.\n",
      "spikes_df.columns: Index(['flat_spike_idx', 't_seconds', 'aclu', 'unit_id', 'shank',\n",
      "       'intra_unit_spike_idx', 'neuron_type'],\n",
      "      dtype='object')\n",
      "\t Done!\n",
      "\t force_recompute is True! Forcing recomputation of .interpolated_spike_positions.npy\n",
      "\n",
      "Computing interpolate_spike_positions columns results : spikes_df... done.\n"
     ]
    }
   ],
   "source": [
    "_test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)\n",
    "_test_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: shape mismatch: value array of shape (2,221425) could not be broadcast to indexing result of shape (221425,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
