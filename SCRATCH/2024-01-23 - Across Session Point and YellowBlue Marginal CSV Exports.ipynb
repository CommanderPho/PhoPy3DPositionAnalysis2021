{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Optional, Union, Callable\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "# matplotlib.use('Qt5Agg')\n",
    "%matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def find_csv_files(directory: str, recurrsive: bool=False):\n",
    "    directory_path = Path(directory) # Convert string path to a Path object\n",
    "    if recurrsive:\n",
    "        return list(directory_path.glob('**/*.csv')) # Return a list of all .csv files in the directory and its subdirectories\n",
    "    else:\n",
    "        return list(directory_path.glob('*.csv')) # Return a list of all .csv files in the directory and its subdirectories\n",
    "    \n",
    "\n",
    "def find_HDF5_files(directory: str):\n",
    "    directory_path = Path(directory) # Convert string path to a Path object\n",
    "    return list(directory_path.glob('**/*.h5')) # Return a list of all .h5 files in the directory and its subdirectories\n",
    "\n",
    "\n",
    "# 2024-01-23 - \n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-fet11-01_12-58-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-03_12-3-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_17-46-44_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_19-28-0_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-58-3_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-25-50_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_16-40-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_22-24-40_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_16-53-46_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_17-29-30_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_14-26-15_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_1-22-43_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-07_16-40-19_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_21-16-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_15-55-31_time_bin_size_sweep_results.h5\n",
    "\n",
    "\n",
    "\n",
    "# found_session_export_paths = [Path(v).resolve() for v in  [\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# ]]\n",
    "\n",
    "def parse_filename(path: Path, debug_print:bool=False) -> Tuple[datetime, str, str]:\n",
    "    \"\"\" \n",
    "    # from the found_session_export_paths, get the most recently exported laps_csv, ripple_csv (by comparing `export_datetime`) for each session (`session_str`)\n",
    "    a_export_filename: str = \"2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\"\n",
    "    export_datetime = \"2024-01-12_0420PM\"\n",
    "    session_str = \"kdiba_pin01_one_fet11-01_12-58-54\"\n",
    "    export_file_type = \"(laps_marginals_df)\" # .csv\n",
    "\n",
    "    # return laps_csv, ripple_csv\n",
    "    laps_csv = Path(\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\").resolve()\n",
    "    ripple_csv = Path(\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\").resolve()\n",
    "\n",
    "    \"\"\"\n",
    "    filename = path.stem   # Get filename without extension\n",
    "    decoding_time_bin_size_str = None\n",
    "    \n",
    "    pattern = r\"(?P<export_datetime_str>.*_\\d{2}\\d{2}[APMF]{2})-(?P<session_str>.*)-(?P<export_file_type>\\(?.+\\)?)(?:_tbin-(?P<decoding_time_bin_size_str>[^)]+))\"\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match is not None:\n",
    "        # export_datetime_str, session_str, export_file_type = match.groups()\n",
    "        export_datetime_str, session_str, export_file_type, decoding_time_bin_size_str = match.group('export_datetime_str'), match.group('session_str'), match.group('export_file_type'), match.group('decoding_time_bin_size_str')\n",
    "    \n",
    "        # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "        export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d_%I%M%p\")\n",
    "\n",
    "    else:\n",
    "        if debug_print:\n",
    "            print(f'did not match pattern with time.')\n",
    "        # day_date_only_pattern = r\"(.*(?:_\\d{2}\\d{2}[APMF]{2})?)-(.*)-(\\(.+\\))\"\n",
    "        day_date_only_pattern = r\"(\\d{4}-\\d{2}-\\d{2})-(.*)-(\\(?.+\\)?)\" # \n",
    "        day_date_only_match = re.match(day_date_only_pattern, filename) # '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'        \n",
    "        if day_date_only_match is not None:\n",
    "            export_datetime_str, session_str, export_file_type = day_date_only_match.groups()\n",
    "            # print(export_datetime_str, session_str, export_file_type)\n",
    "            # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "            export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d\")\n",
    "        \n",
    "        else:\n",
    "            # Try H5 pattern:\n",
    "            # matches '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'\n",
    "            day_date_with_variant_suffix_pattern = r\"(?P<export_datetime_str>\\d{4}-\\d{2}-\\d{2})_?(?P<variant_suffix>[^-_]*)-(?P<session_str>.+?)_(?P<export_file_type>[A-Za-z_]+)\"\n",
    "            day_date_with_variant_suffix_match = re.match(day_date_with_variant_suffix_pattern, filename) # '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'\n",
    "            if day_date_with_variant_suffix_match is not None:\n",
    "                export_datetime_str, session_str, export_file_type = day_date_with_variant_suffix_match.group('export_datetime_str'), day_date_with_variant_suffix_match.group('session_str'), day_date_with_variant_suffix_match.group('export_file_type')\n",
    "                # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "                export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d\")\n",
    "        \n",
    "            else:\n",
    "                print(f'ERR: Could not parse filename: \"{filename}\"') # 2024-01-18_GL_t_split_df\n",
    "                return None, None, None # used to return ValueError when it couldn't parse, but we'd rather skip unparsable files\n",
    "\n",
    "        \n",
    "    if export_file_type[0] == '(' and export_file_type[-1] == ')':\n",
    "        # Trim the brackets from the file type if they're present:\n",
    "        export_file_type = export_file_type[1:-1]\n",
    "\n",
    "    return export_datetime, session_str, export_file_type, decoding_time_bin_size_str\n",
    "\n",
    "\n",
    "def find_most_recent_files(found_session_export_paths: List[Path], debug_print: bool = False) -> Dict[str, Dict[str, Tuple[Path, datetime]]]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary representing the most recent files for each session type among a list of provided file paths.\n",
    "\n",
    "    Parameters:\n",
    "    found_session_export_paths (List[Path]): A list of Paths representing files to be checked.\n",
    "    debug_print (bool): A flag to trigger debugging print statements within the function. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Dict[str, Tuple[Path, datetime]]]: A nested dictionary where the main keys represent \n",
    "    different session types. The inner dictionary's keys represent file types and values are the most recent \n",
    "    Path and datetime for this combination of session and file type.\n",
    "    \n",
    "    # now sessions is a dictionary where the key is the session_str and the value is another dictionary.\n",
    "    # This inner dictionary's key is the file type and the value is the most recent path for this combination of session and file type\n",
    "    # Thus, laps_csv and ripple_csv can be obtained from the dictionary for each session\n",
    "\n",
    "    \"\"\"\n",
    "    # Function 'parse_filename' should be defined in the global scope\n",
    "    parsed_paths = [(*parse_filename(p), p) for p in found_session_export_paths if (parse_filename(p)[0] is not None)]\n",
    "    parsed_paths.sort(reverse=True)\n",
    "\n",
    "    if debug_print:\n",
    "        print(f'parsed_paths: {parsed_paths}')\n",
    "\n",
    "    sessions = {}\n",
    "    for export_datetime, session_str, file_type, path, decoding_time_bin_size_str in parsed_paths:\n",
    "        if session_str not in sessions:\n",
    "            sessions[session_str] = {}\n",
    "\n",
    "        if (file_type not in sessions[session_str]) or (sessions[session_str][file_type][-1] < export_datetime):\n",
    "            sessions[session_str][file_type] = (path, decoding_time_bin_size_str, export_datetime)\n",
    "    \n",
    "    return sessions\n",
    "    \n",
    "# \n",
    "\n",
    "debug_print: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## Load across session t_delta CSV, which contains the t_delta for each session:\n",
    "t_delta_csv_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs\\2024-01-18_GL_t_split_df.csv').resolve() # Apogee\n",
    "# t_delta_csv_path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-01-18_GL_t_split_df.csv').resolve() # GL\n",
    "\n",
    "# collected_outputs_directory = '/home/halechr/FastData/collected_outputs/'\n",
    "# collected_outputs_directory = r'C:\\Users\\pho\\Desktop\\collected_outputs'\n",
    "collected_outputs_directory = r'C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs' # APOGEE\n",
    "# collected_outputs_directory = '/home/halechr/cloud/turbo/Data/Output/collected_outputs' # GL\n",
    "\n",
    "## Find the files:\n",
    "csv_files = find_csv_files(collected_outputs_directory)\n",
    "h5_files = find_HDF5_files(collected_outputs_directory)\n",
    "\n",
    "csv_sessions = find_most_recent_files(found_session_export_paths=csv_files)\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "\n",
    "\n",
    "## The CSV containing the session delta time:\n",
    "t_delta_df = pd.read_csv(t_delta_csv_path, index_col=0) # Assuming that your CSV file has an index column\n",
    "t_delta_dict = t_delta_df.to_dict(orient='index')\n",
    "# t_delta_df\n",
    "\n",
    "csv_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Extract each of the separate files from the sessions:\n",
    "def process_csv_file(file: str, session_name: str, curr_session_t_delta: Optional[float], time_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file)\n",
    "    df['session_name'] = session_name \n",
    "    if curr_session_t_delta is not None:\n",
    "        df['delta_aligned_start_t'] = df[time_col] - curr_session_t_delta\n",
    "    return df\n",
    "    \n",
    "final_sessions = {}\n",
    "final_sessions_loaded_laps_dict = {}\n",
    "final_sessions_loaded_ripple_dict = {}\n",
    "final_sessions_loaded_laps_time_bin_dict = {}\n",
    "final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in csv_sessions.items():\n",
    "    # try:\n",
    "        final_sessions[session_str] = {}\n",
    "        for file_type, (an_decoding_time_bin_size_str, a_path, an_export_datetime) in session_dict.items():\n",
    "            final_sessions[session_str][file_type] = a_path\n",
    "            \n",
    "        session_name: str = str(session_str)  # Extract session name from the filename\n",
    "        if debug_print:\n",
    "            print(f'processing session_name: {session_name}')\n",
    "        curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "        if curr_session_t_delta is None:\n",
    "            print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "        # finds each of the four exports:\n",
    "        try:\n",
    "            laps_file = final_sessions[session_str]['laps_marginals_df']\n",
    "            final_sessions_loaded_laps_dict[session_str] = process_csv_file(laps_file, session_name, curr_session_t_delta, 'lap_start_t')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "                    \n",
    "        try:\n",
    "            ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "            final_sessions_loaded_ripple_dict[session_str] = process_csv_file(ripple_file, session_name, curr_session_t_delta, 'ripple_start_t')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "            \n",
    "\n",
    "        try:\n",
    "            laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "            final_sessions_loaded_laps_time_bin_dict[session_str] = process_csv_file(laps_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n",
    "\n",
    "\n",
    "        try:\n",
    "            ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "            final_sessions_loaded_ripple_time_bin_dict[session_str] = process_csv_file(ripple_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "        except BaseException as e:\n",
    "            print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "            pass   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sessions_loaded_laps_time_bin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sessions_loaded_laps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Build across_sessions join dataframes:\n",
    "all_sessions_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# *_time_bin marginals:\n",
    "all_sessions_laps_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "\n",
    "all_sessions_laps_time_bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sessions\n",
    "# {'kdiba_gor01_one_2006-6-08_14-26-15': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_marginals_df).csv')},\n",
    "#  'kdiba_gor01_one_2006-6-09_1-22-43': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(laps_marginals_df).csv')},\n",
    "#  'kdiba_pin01_one_fet11-01_12-58-54': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv')}}\n",
    "\n",
    "\n",
    "\n",
    "all_sessions_laps_df\n",
    "all_sessions_laps_time_bin_df\n",
    "\n",
    "final_output_path = Path(\"../output/\").resolve()\n",
    "\n",
    "# final_output_path.joinpath(\"2024-01-23_AcrossSession_fig_laps.html\").resolve()\n",
    "\n",
    "final_csv_export_path_laps = final_output_path.joinpath(\"2024-01-23_AcrossSession_Laps.csv\").resolve()\n",
    "# all_sessions_laps_df.to_csv(final_csv_export_path_laps)\n",
    "\n",
    "final_csv_export_path_ripple = final_output_path.joinpath(\"2024-01-23_AcrossSession_Ripple.csv\").resolve()\n",
    "# all_sessions_ripple_df.to_csv(final_csv_export_path_ripple)\n",
    "\n",
    "final_csv_export_path_laps, final_csv_export_path_ripple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_across_sessions_results(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    # # Create a bubble chart for laps\n",
    "    # fig_laps = px.scatter(concatenated_laps_df, x='lap_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # # Create a bubble chart for ripples\n",
    "    # fig_ripples = px.scatter(concatenated_ripple_df, x='ripple_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figures to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_laps_name = Path(figures_folder, f\"{session_name}_laps_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_laps_name}\"...')\n",
    "        fig_laps.write_image(fig_laps_name)\n",
    "        fig_ripple_name = Path(figures_folder, f\"{session_name}_ripples_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_ripple_name}\"...')\n",
    "        fig_ripples.write_image(fig_ripple_name)\n",
    "    \n",
    "    # Append both figures to the list\n",
    "    all_figures.append((fig_laps, fig_ripples))\n",
    "    \n",
    "    return all_figures\n",
    "\n",
    "\n",
    "# Plot the time_bin marginals:\n",
    "\n",
    "\n",
    "\n",
    "def plot_across_sessions_results_with_histogram_gpt3(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    # Create a histogram for laps\n",
    "    fig_hist_laps = px.histogram(concatenated_laps_df, x='delta_aligned_start_t', nbins=50, title=f\"Laps - Session: {session_name}\")\n",
    "    \n",
    "    # Assign numerical values to session_name for color\n",
    "    session_name_to_color = {name: i for i, name in enumerate(concatenated_laps_df['session_name'].unique())}\n",
    "\n",
    "    # Create subplots with shared y-axis\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[f\"Laps - Session: {session_name}\", f\"Ripples - Session: {session_name}\"])\n",
    "    \n",
    "    # Add histogram to the left subplot\n",
    "    fig.add_trace(go.Histogram(x=concatenated_laps_df['delta_aligned_start_t'], nbinsx=50, name='Histogram'), row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "    \n",
    "    # Add bubble chart to the right subplot\n",
    "    fig.add_trace(go.Scatter(x=concatenated_laps_df['delta_aligned_start_t'], y=concatenated_laps_df['P_Long'], mode='markers', marker=dict(color=concatenated_laps_df['session_name'].map(session_name_to_color))), row=1, col=2)\n",
    "    fig.update_xaxes(title_text='delta_aligned_start_t', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='P_Long', row=1, col=2)\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figure to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_name = Path(figures_folder, f\"{session_name}_combined_plot{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_name}\"...')\n",
    "        fig.write_image(fig_name)\n",
    "    \n",
    "    # Append the figure to the list\n",
    "    all_figures.append(fig)\n",
    "    \n",
    "    return all_figures\n",
    "\n",
    "\n",
    "def plot_across_sessions_results_with_histogram_new(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Your existing code (not modified)\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create subplot with 2 rows and 1 column\n",
    "    fig_laps = make_subplots(rows=2, cols=1)\n",
    "    # Add scatter plot to first row, first column\n",
    "    fig_laps.add_trace(\n",
    "        go.Scatter(x=concatenated_laps_df['delta_aligned_start_t'], y=concatenated_laps_df['P_Long'], mode='markers', name='Scatter'), \n",
    "        row=1, col=1\n",
    "    )\n",
    "    # add histogram to second row, first column\n",
    "    fig_laps.add_trace(\n",
    "        go.Histogram(x=concatenated_laps_df['delta_aligned_start_t'], name='Histogram'), \n",
    "        row=2, col=1\n",
    "    )\n",
    "    # Same for ripples\n",
    "    fig_ripples = make_subplots(rows=2, cols=1)\n",
    "    fig_ripples.add_trace(\n",
    "        go.Scatter(x=concatenated_ripple_df['delta_aligned_start_t'], y=concatenated_ripple_df['P_Long'], mode='markers', name='Scatter'), \n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig_ripples.add_trace(\n",
    "        go.Histogram(x=concatenated_ripple_df['delta_aligned_start_t'], name='Histogram'), \n",
    "        row=2, col=1\n",
    "    )\n",
    "    # Your existing code continues from here (not modified)\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figures to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_laps_name = Path(figures_folder, f\"{session_name}_laps_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_laps_name}\"...')\n",
    "        fig_laps.write_image(fig_laps_name)\n",
    "        fig_ripple_name = Path(figures_folder, f\"{session_name}_ripples_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_ripple_name}\"...')\n",
    "        fig_ripples.write_image(fig_ripple_name)\n",
    "    \n",
    "    # Append both figures to the list\n",
    "    all_figures.append((fig_laps, fig_ripples))\n",
    "    \n",
    "    return all_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "all_session_figures = plot_across_sessions_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_df, concatenated_ripple_df=all_sessions_ripple_df, save_figures=True)\n",
    "\n",
    "# Show figures for all sessions\n",
    "for fig_laps, fig_ripples in all_session_figures:\n",
    "    fig_laps.show()\n",
    "    fig_ripples.show()\n",
    "\n",
    "    fig_laps.write_html(\"../output/2024-01-23_AcrossSession_fig_laps.html\")\n",
    "    fig_ripples.write_html(\"../output/2024-01-23_AcrossSession_fig_ripples.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_plot_fn = plot_across_sessions_results_with_histogram_gpt3\n",
    "# active_plot_fn = plot_across_sessions_results_with_histogram_new\n",
    "all_time_bin_session_figures = active_plot_fn(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_time_bin_df, concatenated_ripple_df=all_sessions_ripple_time_bin_df, save_figures=False)\n",
    "all_time_bin_session_figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_bin_session_figures[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fig_time_bin_laps, fig_time_bin_ripples in all_time_bin_session_figures:\n",
    "    fig_time_bin_laps.show()\n",
    "    fig_time_bin_ripples.show()\n",
    "    # fig_laps.write_html(\"../output/2024-01-18_AcrossSession_fig_laps.html\")\n",
    "    # fig_ripples.write_html(\"../output/2024-01-18_AcrossSession_fig_ripples.html\")\n",
    "    fig_time_bin_laps.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_laps.html\")\n",
    "    fig_time_bin_ripples.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_ripples.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'Laps'\n",
    "session_spec = 'All Sessions'\n",
    "data_results_df = all_sessions_laps_time_bin_df\n",
    "time_bin_duration_str = \"75 ms\"\n",
    "\n",
    "# get the pre-delta epochs\n",
    "pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "pre_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "plt.show()\n",
    "\n",
    "post_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'Ripples'\n",
    "session_spec = 'All Sessions'\n",
    "data_results_df = all_sessions_ripple_time_bin_df\n",
    "time_bin_duration_str = \"75 ms\"\n",
    "\n",
    "# get the pre-delta epochs\n",
    "pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "pre_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "plt.show()\n",
    "\n",
    "post_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_time_bin_df.hist(column='P_Long')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "from attrs import define, field, Factory\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "@define(slots=False)\n",
    "class TwoCSV:\n",
    "\t\"\"\" simple class wrapper to emulate the object that holds the other dfs \"\"\"\n",
    "\tlaps_all_epoch_bins_marginals_df = field()\n",
    "\tripple_all_epoch_bins_marginals_df = field()\n",
    "\n",
    "save_figure = True\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\tprint(f'final_context: {final_context}')\n",
    "\t# if save_figure:\n",
    "\t# \tfig.save_fig(\n",
    "\t# \t# return owning_pipeline_reference.output_figure(final_context, fig)\n",
    "\t# else:\n",
    "\t# \tpass # do nothing, don't save\n",
    "\t\n",
    "# all_sessions_laps_time_bin_df\n",
    "\n",
    "collector = plot_all_epoch_bins_marginal_predictions(TwoCSV(laps_all_epoch_bins_marginals_df=all_sessions_laps_df, ripple_all_epoch_bins_marginals_df=all_sessions_ripple_df), t_start=None, t_split=0.0, t_end=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=IdentifyingContext(), perform_write_to_file_callback=_perform_write_to_file_callback)\n",
    "collector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tuple(collector.figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import H5FileAggregator\n",
    "\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "h5_sessions\n",
    "\n",
    "final_sessions = {}\n",
    "time_bin_size_sweep_results_files = {}\n",
    "\n",
    "# final_sessions_loaded_laps_dict = {}\n",
    "# final_sessions_loaded_ripple_dict = {}\n",
    "# final_sessions_loaded_laps_time_bin_dict = {}\n",
    "# final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in h5_sessions.items():\n",
    "    final_sessions[session_str] = {}\n",
    "    for file_type, (a_path, an_export_datetime) in session_dict.items():\n",
    "        final_sessions[session_str][file_type] = a_path\n",
    "        \n",
    "    session_name: str = str(session_str)  # Extract session name from the filename\n",
    "    if debug_print:\n",
    "        print(f'processing session_name: {session_name}')\n",
    "    curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "    if curr_session_t_delta is None:\n",
    "        print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "    # finds each of the four exports:\n",
    "    time_bin_size_sweep_results_file = final_sessions[session_str]['time_bin_size_sweep_results']\n",
    "    time_bin_size_sweep_results_files[session_str] = Path(time_bin_size_sweep_results_file).resolve()\n",
    "    # ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "    # laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "    # ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "    \n",
    "\n",
    "\n",
    "session_identifiers = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "# Keys are like: \"/kdiba|gor01|two|2006-6-12_16-53-46\"\n",
    "included_h5_paths = list(time_bin_size_sweep_results_files.values())\n",
    "session_group_keys: List[str] = [(\"/\" + a_ctxt.get_description(separator=\"|\", include_property_names=False)) for a_ctxt in session_identifiers] # 'kdiba/gor01/one/2006-6-08_14-26-15'\n",
    "laps_decoding_accuracy_results_table_keys = [f\"{session_group_key}/laps_decoding_accuracy_results/table\" for session_group_key in session_group_keys]\n",
    "a_loader = H5FileAggregator.init_from_file_lists(file_list=included_h5_paths, table_key_list=laps_decoding_accuracy_results_table_keys)\n",
    "_out_table = a_loader.load_and_consolidate()\n",
    "_out_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_yellow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
