{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Optional, Union, Callable\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Plotting\n",
    "# import pylustrator # customization of figures\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "_bak_rcParams = mpl.rcParams.copy()\n",
    "\n",
    "# matplotlib.use('Qt5Agg')\n",
    "%matplotlib inline\n",
    "# %matplotlib auto\n",
    "\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "# _restore_previous_matplotlib_settings_callback = matplotlib_configuration_update(is_interactive=True, backend='Qt5Agg')\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def find_csv_files(directory: str):\n",
    "    directory_path = Path(directory) # Convert string path to a Path object\n",
    "    return list(directory_path.glob('**/*.csv')) # Return a list of all .csv files in the directory and its subdirectories\n",
    "    \n",
    "\n",
    "def find_HDF5_files(directory: str):\n",
    "    directory_path = Path(directory) # Convert string path to a Path object\n",
    "    return list(directory_path.glob('**/*.h5')) # Return a list of all .h5 files in the directory and its subdirectories\n",
    "\n",
    "\n",
    "# 2024-01-23 - \n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-fet11-01_12-58-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-03_12-3-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_17-46-44_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-11-02_19-28-0_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-58-3_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-10_12-25-50_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_16-40-54_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_22-24-40_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_16-53-46_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-4-09_17-29-30_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_14-26-15_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-09_1-22-43_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-07_16-40-19_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-08_21-16-25_time_bin_size_sweep_results.h5\n",
    "# C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-23_GL-2006-6-12_15-55-31_time_bin_size_sweep_results.h5\n",
    "\n",
    "\n",
    "\n",
    "# found_session_export_paths = [Path(v).resolve() for v in  [\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0645PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\",\n",
    "# \"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\",\n",
    "# ]]\n",
    "\n",
    "def parse_filename(path: Path, debug_print:bool=False) -> Tuple[datetime, str, str]:\n",
    "    \"\"\" \n",
    "    # from the found_session_export_paths, get the most recently exported laps_csv, ripple_csv (by comparing `export_datetime`) for each session (`session_str`)\n",
    "    a_export_filename: str = \"2024-01-12_0420PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\"\n",
    "    export_datetime = \"2024-01-12_0420PM\"\n",
    "    session_str = \"kdiba_pin01_one_fet11-01_12-58-54\"\n",
    "    export_file_type = \"(laps_marginals_df)\" # .csv\n",
    "\n",
    "    # return laps_csv, ripple_csv\n",
    "    laps_csv = Path(\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv\").resolve()\n",
    "    ripple_csv = Path(\"C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv\").resolve()\n",
    "\n",
    "    \"\"\"\n",
    "    filename = path.stem   # Get filename without extension\n",
    "    decoding_time_bin_size_str = None\n",
    "    \n",
    "    pattern = r\"(?P<export_datetime_str>.*_\\d{2}\\d{2}[APMF]{2})-(?P<session_str>.*)-(?P<export_file_type>\\(?.+\\)?)(?:_tbin-(?P<decoding_time_bin_size_str>[^)]+))\"\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match is not None:\n",
    "        # export_datetime_str, session_str, export_file_type = match.groups()\n",
    "        export_datetime_str, session_str, export_file_type, decoding_time_bin_size_str = match.group('export_datetime_str'), match.group('session_str'), match.group('export_file_type'), match.group('decoding_time_bin_size_str')\n",
    "    \n",
    "        # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "        export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d_%I%M%p\")\n",
    "\n",
    "    else:\n",
    "        if debug_print:\n",
    "            print(f'did not match pattern with time.')\n",
    "        # day_date_only_pattern = r\"(.*(?:_\\d{2}\\d{2}[APMF]{2})?)-(.*)-(\\(.+\\))\"\n",
    "        day_date_only_pattern = r\"(\\d{4}-\\d{2}-\\d{2})-(.*)-(\\(?.+\\)?)\" # \n",
    "        day_date_only_match = re.match(day_date_only_pattern, filename) # '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'        \n",
    "        if day_date_only_match is not None:\n",
    "            export_datetime_str, session_str, export_file_type = day_date_only_match.groups()\n",
    "            # print(export_datetime_str, session_str, export_file_type)\n",
    "            # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "            export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d\")\n",
    "        \n",
    "        else:\n",
    "            # Try H5 pattern:\n",
    "            # matches '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'\n",
    "            day_date_with_variant_suffix_pattern = r\"(?P<export_datetime_str>\\d{4}-\\d{2}-\\d{2})_?(?P<variant_suffix>[^-_]*)-(?P<session_str>.+?)_(?P<export_file_type>[A-Za-z_]+)\"\n",
    "            day_date_with_variant_suffix_match = re.match(day_date_with_variant_suffix_pattern, filename) # '2024-01-04-kdiba_gor01_one_2006-6-08_14-26'\n",
    "            if day_date_with_variant_suffix_match is not None:\n",
    "                export_datetime_str, session_str, export_file_type = day_date_with_variant_suffix_match.group('export_datetime_str'), day_date_with_variant_suffix_match.group('session_str'), day_date_with_variant_suffix_match.group('export_file_type')\n",
    "                # parse the datetime from the export_datetime_str and convert it to datetime object\n",
    "                export_datetime = datetime.strptime(export_datetime_str, \"%Y-%m-%d\")\n",
    "        \n",
    "            else:\n",
    "                print(f'ERR: Could not parse filename: \"{filename}\"') # 2024-01-18_GL_t_split_df\n",
    "                return None, None, None # used to return ValueError when it couldn't parse, but we'd rather skip unparsable files\n",
    "\n",
    "        \n",
    "    if export_file_type[0] == '(' and export_file_type[-1] == ')':\n",
    "        # Trim the brackets from the file type if they're present:\n",
    "        export_file_type = export_file_type[1:-1]\n",
    "\n",
    "    return export_datetime, session_str, export_file_type, decoding_time_bin_size_str\n",
    "\n",
    "\n",
    "def find_most_recent_files(found_session_export_paths: List[Path], debug_print: bool = False) -> Dict[str, Dict[str, Tuple[Path, datetime]]]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary representing the most recent files for each session type among a list of provided file paths.\n",
    "\n",
    "    Parameters:\n",
    "    found_session_export_paths (List[Path]): A list of Paths representing files to be checked.\n",
    "    debug_print (bool): A flag to trigger debugging print statements within the function. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Dict[str, Tuple[Path, datetime]]]: A nested dictionary where the main keys represent \n",
    "    different session types. The inner dictionary's keys represent file types and values are the most recent \n",
    "    Path and datetime for this combination of session and file type.\n",
    "    \n",
    "    # now sessions is a dictionary where the key is the session_str and the value is another dictionary.\n",
    "    # This inner dictionary's key is the file type and the value is the most recent path for this combination of session and file type\n",
    "    # Thus, laps_csv and ripple_csv can be obtained from the dictionary for each session\n",
    "\n",
    "    \"\"\"\n",
    "    # Function 'parse_filename' should be defined in the global scope\n",
    "    parsed_paths = [(*parse_filename(p), p) for p in found_session_export_paths if (parse_filename(p)[0] is not None)]\n",
    "    parsed_paths.sort(reverse=True)\n",
    "\n",
    "    if debug_print:\n",
    "        print(f'parsed_paths: {parsed_paths}')\n",
    "\n",
    "    sessions = {}\n",
    "    for export_datetime, session_str, file_type, path, decoding_time_bin_size_str in parsed_paths:\n",
    "        if session_str not in sessions:\n",
    "            sessions[session_str] = {}\n",
    "\n",
    "        if (file_type not in sessions[session_str]) or (sessions[session_str][file_type][-1] < export_datetime):\n",
    "            sessions[session_str][file_type] = (path, decoding_time_bin_size_str, export_datetime)\n",
    "    \n",
    "    return sessions\n",
    "    \n",
    "\n",
    "debug_print: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR: Could not parse filename: \"2024-01-18_GL_t_split_df\"\n",
      "ERR: Could not parse filename: \"2024-01-18_GL_t_split_df\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_start</th>\n",
       "      <th>t_delta</th>\n",
       "      <th>t_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kdiba_gor01_one_2006-6-08_14-26-15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1211.558080</td>\n",
       "      <td>2093.897857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_gor01_one_2006-6-09_1-22-43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1029.316609</td>\n",
       "      <td>1737.196831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_gor01_one_2006-6-12_15-55-31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>656.064809</td>\n",
       "      <td>1122.186487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_gor01_two_2006-6-07_16-40-19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1236.266245</td>\n",
       "      <td>2587.801682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_gor01_two_2006-6-08_21-16-25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>722.653951</td>\n",
       "      <td>1201.083936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_gor01_two_2006-6-09_22-24-40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>911.601160</td>\n",
       "      <td>2573.457162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_gor01_two_2006-6-12_16-53-46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>471.067400</td>\n",
       "      <td>785.451326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_vvp01_one_2006-4-09_17-29-30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>873.624498</td>\n",
       "      <td>1391.655628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_vvp01_one_2006-4-10_12-25-50</th>\n",
       "      <td>0.0</td>\n",
       "      <td>883.897132</td>\n",
       "      <td>1413.399172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_vvp01_two_2006-4-09_16-40-54</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1155.706469</td>\n",
       "      <td>1724.033140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_vvp01_two_2006-4-10_12-58-3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>932.826206</td>\n",
       "      <td>1458.539064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_pin01_one_11-02_17-46-44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1144.228404</td>\n",
       "      <td>1941.433222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_pin01_one_11-02_19-28-0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>787.508042</td>\n",
       "      <td>1177.760564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_pin01_one_11-03_12-3-25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>669.060258</td>\n",
       "      <td>1005.444697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdiba_pin01_one_fet11-01_12-58-54</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2057.225948</td>\n",
       "      <td>3031.727247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    t_start      t_delta        t_end\n",
       "kdiba_gor01_one_2006-6-08_14-26-15      0.0  1211.558080  2093.897857\n",
       "kdiba_gor01_one_2006-6-09_1-22-43       0.0  1029.316609  1737.196831\n",
       "kdiba_gor01_one_2006-6-12_15-55-31      0.0   656.064809  1122.186487\n",
       "kdiba_gor01_two_2006-6-07_16-40-19      0.0  1236.266245  2587.801682\n",
       "kdiba_gor01_two_2006-6-08_21-16-25      0.0   722.653951  1201.083936\n",
       "kdiba_gor01_two_2006-6-09_22-24-40      0.0   911.601160  2573.457162\n",
       "kdiba_gor01_two_2006-6-12_16-53-46      0.0   471.067400   785.451326\n",
       "kdiba_vvp01_one_2006-4-09_17-29-30      0.0   873.624498  1391.655628\n",
       "kdiba_vvp01_one_2006-4-10_12-25-50      0.0   883.897132  1413.399172\n",
       "kdiba_vvp01_two_2006-4-09_16-40-54      0.0  1155.706469  1724.033140\n",
       "kdiba_vvp01_two_2006-4-10_12-58-3       0.0   932.826206  1458.539064\n",
       "kdiba_pin01_one_11-02_17-46-44          0.0  1144.228404  1941.433222\n",
       "kdiba_pin01_one_11-02_19-28-0           0.0   787.508042  1177.760564\n",
       "kdiba_pin01_one_11-03_12-3-25           0.0   669.060258  1005.444697\n",
       "kdiba_pin01_one_fet11-01_12-58-54       0.0  2057.225948  3031.727247"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load across session t_delta CSV, which contains the t_delta for each session:\n",
    "t_delta_csv_path = Path(r'C:\\Users\\pho\\repos\\Spike3DWorkEnv\\Spike3D\\output\\collected_outputs\\2024-01-18_GL_t_split_df.csv').resolve() # Apogee\n",
    "# t_delta_csv_path = Path('/home/halechr/cloud/turbo/Data/Output/collected_outputs/2024-01-18_GL_t_split_df.csv').resolve() # GL\n",
    "\n",
    "# collected_outputs_directory = '/home/halechr/FastData/collected_outputs/'\n",
    "# collected_outputs_directory = r'C:\\Users\\pho\\Desktop\\collected_outputs'\n",
    "collected_outputs_directory = r'C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs' # APOGEE\n",
    "# collected_outputs_directory = '/home/halechr/cloud/turbo/Data/Output/collected_outputs' # GL\n",
    "\n",
    "## Find the files:\n",
    "csv_files = find_csv_files(collected_outputs_directory)\n",
    "h5_files = find_HDF5_files(collected_outputs_directory)\n",
    "\n",
    "csv_sessions = find_most_recent_files(found_session_export_paths=csv_files)\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "\n",
    "\n",
    "## The CSV containing the session delta time:\n",
    "t_delta_df = pd.read_csv(t_delta_csv_path, index_col=0) # Assuming that your CSV file has an index column\n",
    "t_delta_dict = t_delta_df.to_dict(orient='index')\n",
    "t_delta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_files)\n",
    "# print(list(csv_sessions.keys()))\n",
    "# csv_sessions['kdiba_gor01_one_2006-6-08_14-26-15']\n",
    "# {'ripple_time_bin_marginals_df': (WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-24_0745AM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_time_bin_marginals_df).csv'),\n",
    "#   datetime.datetime(2024, 1, 24, 7, 45)),\n",
    "#  'ripple_marginals_df': (WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-24_0745AM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_marginals_df).csv'),\n",
    "#   datetime.datetime(2024, 1, 24, 7, 45)),\n",
    "#  'laps_time_bin_marginals_df': (WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-24_0745AM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_time_bin_marginals_df).csv'),\n",
    "#   datetime.datetime(2024, 1, 24, 7, 45)),\n",
    "#  'laps_marginals_df': (WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-24_0745AM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_marginals_df).csv'),\n",
    "#   datetime.datetime(2024, 1, 24, 7, 45))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session kdiba_pin01_one_fet11-01_12-58-54 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-10_12-58-3 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_pin01_one_11-03_12-3-25 did not fully work. (error [Errno 2] No such file or directory: '0.075'. Skipping.\n",
      "session kdiba_pin01_one_11-02_19-28-0 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_pin01_one_11-02_17-46-44 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_vvp01_two_2006-4-09_16-40-54 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_vvp01_one_2006-4-10_12-25-50 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_vvp01_one_2006-4-09_17-29-30 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-09_22-24-40 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-07_16-40-19 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_gor01_one_2006-6-09_1-22-43 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_gor01_one_2006-6-08_14-26-15 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-12_16-53-46 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_gor01_two_2006-6-08_21-16-25 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "session kdiba_gor01_one_2006-6-12_15-55-31 did not fully work. (error 'ripple_time_bin_marginals_df'. Skipping.\n",
      "WARN: curr_session_t_split is None for session_str = \"kdiba\"\n",
      "session kdiba did not fully work. (error 'laps_marginals_df'. Skipping.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m   \n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m## Build across_sessions join dataframes:\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m all_sessions_laps_df: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_sessions_loaded_laps_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m all_sessions_ripple_df: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mlist\u001b[39m(final_sessions_loaded_ripple_dict\u001b[38;5;241m.\u001b[39mvalues()), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# *_time_bin marginals:\u001b[39;00m\n",
      "File \u001b[1;32mk:\\FastSwap\\AppData\\VSCode\\yellow\\.venv_yellow\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mk:\\FastSwap\\AppData\\VSCode\\yellow\\.venv_yellow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mk:\\FastSwap\\AppData\\VSCode\\yellow\\.venv_yellow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Extract each of the separate files from the sessions:\n",
    "def process_csv_file(file: str, session_name: str, curr_session_t_delta: Optional[float], time_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file)\n",
    "    df['session_name'] = session_name \n",
    "    if curr_session_t_delta is not None:\n",
    "        df['delta_aligned_start_t'] = df[time_col] - curr_session_t_delta\n",
    "    return df\n",
    "    \n",
    "\n",
    "final_sessions = {}\n",
    "final_sessions_loaded_laps_dict = {}\n",
    "final_sessions_loaded_ripple_dict = {}\n",
    "final_sessions_loaded_laps_time_bin_dict = {}\n",
    "final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in csv_sessions.items():\n",
    "    try:\n",
    "        final_sessions[session_str] = {}\n",
    "        for file_type, (a_path, an_export_datetime, an_decoding_time_bin_size_str) in session_dict.items():\n",
    "            final_sessions[session_str][file_type] = a_path\n",
    "            \n",
    "        session_name: str = str(session_str)  # Extract session name from the filename\n",
    "        if debug_print:\n",
    "            print(f'processing session_name: {session_name}')\n",
    "        curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "        if curr_session_t_delta is None:\n",
    "            print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "        # finds each of the four exports:\n",
    "        laps_file = final_sessions[session_str]['laps_marginals_df']\n",
    "        ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "        laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "        ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "\n",
    "        final_sessions_loaded_laps_dict[session_str] = process_csv_file(laps_file, session_name, curr_session_t_delta, 'lap_start_t')\n",
    "        final_sessions_loaded_ripple_dict[session_str] = process_csv_file(ripple_file, session_name, curr_session_t_delta, 'ripple_start_t')\n",
    "        final_sessions_loaded_laps_time_bin_dict[session_str] = process_csv_file(laps_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "        final_sessions_loaded_ripple_time_bin_dict[session_str] = process_csv_file(ripple_time_bin_file, session_name, curr_session_t_delta, 't_bin_center')\n",
    "    except BaseException as e:\n",
    "        print(f'session {session_str} did not fully work. (error {e}. Skipping.') \n",
    "        pass   \n",
    "\n",
    "## Build across_sessions join dataframes:\n",
    "all_sessions_laps_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "# *_time_bin marginals:\n",
    "all_sessions_laps_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_laps_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "all_sessions_ripple_time_bin_df: pd.DataFrame = pd.concat(list(final_sessions_loaded_ripple_time_bin_dict.values()), axis='index', ignore_index=True)\n",
    "\n",
    "\n",
    "all_sessions_laps_time_bin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sessions\n",
    "# {'kdiba_gor01_one_2006-6-08_14-26-15': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-17_0540PM-kdiba_gor01_one_2006-6-08_14-26-15-(laps_marginals_df).csv')},\n",
    "#  'kdiba_gor01_one_2006-6-09_1-22-43': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0838PM-kdiba_gor01_one_2006-6-09_1-22-43-(laps_marginals_df).csv')},\n",
    "#  'kdiba_pin01_one_fet11-01_12-58-54': {'ripple_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(ripple_marginals_df).csv'),\n",
    "#   'laps_marginals_df': WindowsPath('C:/Users/pho/repos/Spike3DWorkEnv/Spike3D/output/collected_outputs/2024-01-12_0828PM-kdiba_pin01_one_fet11-01_12-58-54-(laps_marginals_df).csv')}}\n",
    "\n",
    "\n",
    "\n",
    "all_sessions_laps_df\n",
    "all_sessions_laps_time_bin_df\n",
    "\n",
    "final_output_path = Path(\"../output/\").resolve()\n",
    "\n",
    "# final_output_path.joinpath(\"2024-01-23_AcrossSession_fig_laps.html\").resolve()\n",
    "\n",
    "final_csv_export_path_laps = final_output_path.joinpath(\"2024-01-23_AcrossSession_Laps.csv\").resolve()\n",
    "# all_sessions_laps_df.to_csv(final_csv_export_path_laps)\n",
    "\n",
    "final_csv_export_path_ripple = final_output_path.joinpath(\"2024-01-23_AcrossSession_Ripple.csv\").resolve()\n",
    "# all_sessions_ripple_df.to_csv(final_csv_export_path_ripple)\n",
    "\n",
    "final_csv_export_path_laps, final_csv_export_path_ripple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_across_sessions_results(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    # # Create a bubble chart for laps\n",
    "    # fig_laps = px.scatter(concatenated_laps_df, x='lap_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # # Create a bubble chart for ripples\n",
    "    # fig_ripples = px.scatter(concatenated_ripple_df, x='ripple_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figures to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_laps_name = Path(figures_folder, f\"{session_name}_laps_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_laps_name}\"...')\n",
    "        fig_laps.write_image(fig_laps_name)\n",
    "        fig_ripple_name = Path(figures_folder, f\"{session_name}_ripples_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_ripple_name}\"...')\n",
    "        fig_ripples.write_image(fig_ripple_name)\n",
    "    \n",
    "    # Append both figures to the list\n",
    "    all_figures.append((fig_laps, fig_ripples))\n",
    "    \n",
    "    return all_figures\n",
    "\n",
    "\n",
    "# Plot the time_bin marginals:\n",
    "\n",
    "\n",
    "\n",
    "def plot_across_sessions_results_with_histogram_gpt3(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    # Create a histogram for laps\n",
    "    fig_hist_laps = px.histogram(concatenated_laps_df, x='delta_aligned_start_t', nbins=50, title=f\"Laps - Session: {session_name}\")\n",
    "    \n",
    "    # Assign numerical values to session_name for color\n",
    "    session_name_to_color = {name: i for i, name in enumerate(concatenated_laps_df['session_name'].unique())}\n",
    "\n",
    "    # Create subplots with shared y-axis\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[f\"Laps - Session: {session_name}\", f\"Ripples - Session: {session_name}\"])\n",
    "    \n",
    "    # Add histogram to the left subplot\n",
    "    fig.add_trace(go.Histogram(x=concatenated_laps_df['delta_aligned_start_t'], nbinsx=50, name='Histogram'), row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "    \n",
    "    # Add bubble chart to the right subplot\n",
    "    fig.add_trace(go.Scatter(x=concatenated_laps_df['delta_aligned_start_t'], y=concatenated_laps_df['P_Long'], mode='markers', marker=dict(color=concatenated_laps_df['session_name'].map(session_name_to_color))), row=1, col=2)\n",
    "    fig.update_xaxes(title_text='delta_aligned_start_t', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='P_Long', row=1, col=2)\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figure to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_name = Path(figures_folder, f\"{session_name}_combined_plot{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_name}\"...')\n",
    "        fig.write_image(fig_name)\n",
    "    \n",
    "    # Append the figure to the list\n",
    "    all_figures.append(fig)\n",
    "    \n",
    "    return all_figures\n",
    "\n",
    "\n",
    "def plot_across_sessions_results_with_histogram_new(directory, concatenated_laps_df, concatenated_ripple_df, save_figures=False, figure_save_extension='.png'):\n",
    "    \"\"\" takes the directory containing the .csv pairs that were exported by `export_marginals_df_csv`\n",
    "    Produces and then saves figures out the the f'{directory}/figures/' subfolder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Your existing code (not modified)\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create subplot with 2 rows and 1 column\n",
    "    fig_laps = make_subplots(rows=2, cols=1)\n",
    "    # Add scatter plot to first row, first column\n",
    "    fig_laps.add_trace(\n",
    "        go.Scatter(x=concatenated_laps_df['delta_aligned_start_t'], y=concatenated_laps_df['P_Long'], mode='markers', name='Scatter'), \n",
    "        row=1, col=1\n",
    "    )\n",
    "    # add histogram to second row, first column\n",
    "    fig_laps.add_trace(\n",
    "        go.Histogram(x=concatenated_laps_df['delta_aligned_start_t'], name='Histogram'), \n",
    "        row=2, col=1\n",
    "    )\n",
    "    # Same for ripples\n",
    "    fig_ripples = make_subplots(rows=2, cols=1)\n",
    "    fig_ripples.add_trace(\n",
    "        go.Scatter(x=concatenated_ripple_df['delta_aligned_start_t'], y=concatenated_ripple_df['P_Long'], mode='markers', name='Scatter'), \n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig_ripples.add_trace(\n",
    "        go.Histogram(x=concatenated_ripple_df['delta_aligned_start_t'], name='Histogram'), \n",
    "        row=2, col=1\n",
    "    )\n",
    "    # Your existing code continues from here (not modified)\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory).resolve()\n",
    "    assert directory.exists()\n",
    "    print(f'plot_across_sessions_results(directory: {directory})')\n",
    "    if save_figures:\n",
    "        # Create a 'figures' subfolder if it doesn't exist\n",
    "        figures_folder = Path(directory, 'figures')\n",
    "        figures_folder.mkdir(parents=False, exist_ok=True)\n",
    "        assert figures_folder.exists()\n",
    "        print(f'\\tfigures_folder: {figures_folder}')\n",
    "    \n",
    "    # Create an empty list to store the figures\n",
    "    all_figures = []\n",
    "\n",
    "    ## delta_t aligned:\n",
    "    # Create a bubble chart for laps\n",
    "    fig_laps = px.scatter(concatenated_laps_df, x='delta_aligned_start_t', y='P_Long', title=f\"Laps - Session: {session_name}\", color='session_name')\n",
    "    # Create a bubble chart for ripples\n",
    "    fig_ripples = px.scatter(concatenated_ripple_df, x='delta_aligned_start_t', y='P_Long', title=f\"Ripples - Session: {session_name}\", color='session_name')\n",
    "\n",
    "    if save_figures:\n",
    "        # Save the figures to the 'figures' subfolder\n",
    "        print(f'\\tsaving figures...')\n",
    "        fig_laps_name = Path(figures_folder, f\"{session_name}_laps_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_laps_name}\"...')\n",
    "        fig_laps.write_image(fig_laps_name)\n",
    "        fig_ripple_name = Path(figures_folder, f\"{session_name}_ripples_marginal{figure_save_extension}\").resolve()\n",
    "        print(f'\\tsaving \"{fig_ripple_name}\"...')\n",
    "        fig_ripples.write_image(fig_ripple_name)\n",
    "    \n",
    "    # Append both figures to the list\n",
    "    all_figures.append((fig_laps, fig_ripples))\n",
    "    \n",
    "    return all_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "all_session_figures = plot_across_sessions_results(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_df, concatenated_ripple_df=all_sessions_ripple_df, save_figures=True)\n",
    "\n",
    "# Show figures for all sessions\n",
    "for fig_laps, fig_ripples in all_session_figures:\n",
    "    fig_laps.show()\n",
    "    fig_ripples.show()\n",
    "\n",
    "    fig_laps.write_html(\"../output/2024-01-23_AcrossSession_fig_laps.html\")\n",
    "    fig_ripples.write_html(\"../output/2024-01-23_AcrossSession_fig_ripples.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_plot_fn = plot_across_sessions_results_with_histogram_gpt3\n",
    "# active_plot_fn = plot_across_sessions_results_with_histogram_new\n",
    "all_time_bin_session_figures = active_plot_fn(collected_outputs_directory, concatenated_laps_df=all_sessions_laps_time_bin_df, concatenated_ripple_df=all_sessions_ripple_time_bin_df, save_figures=False)\n",
    "all_time_bin_session_figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_bin_session_figures[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fig_time_bin_laps, fig_time_bin_ripples in all_time_bin_session_figures:\n",
    "    fig_time_bin_laps.show()\n",
    "    fig_time_bin_ripples.show()\n",
    "    # fig_laps.write_html(\"../output/2024-01-18_AcrossSession_fig_laps.html\")\n",
    "    # fig_ripples.write_html(\"../output/2024-01-18_AcrossSession_fig_ripples.html\")\n",
    "    fig_time_bin_laps.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_laps.html\")\n",
    "    fig_time_bin_ripples.write_html(\"../output/2024-01-23_AcrossSession_fig_time_bin_ripples.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'Laps'\n",
    "session_spec = 'All Sessions'\n",
    "data_results_df = all_sessions_laps_time_bin_df\n",
    "time_bin_duration_str = \"75 ms\"\n",
    "\n",
    "# get the pre-delta epochs\n",
    "pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "pre_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "plt.show()\n",
    "\n",
    "post_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'Ripples'\n",
    "session_spec = 'All Sessions'\n",
    "data_results_df = all_sessions_ripple_time_bin_df\n",
    "time_bin_duration_str = \"75 ms\"\n",
    "\n",
    "# get the pre-delta epochs\n",
    "pre_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] <= 0]\n",
    "post_delta_df = data_results_df[data_results_df['delta_aligned_start_t'] > 0]\n",
    "\n",
    "descriptor_str: str = '|'.join([data_type, session_spec, time_bin_duration_str])\n",
    "pre_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - pre-$\\Delta$ time bins')\n",
    "plt.show()\n",
    "\n",
    "post_delta_df.hist(column='P_Long')\n",
    "plt.title(f'{descriptor_str} - post-$\\Delta$ time bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions_laps_time_bin_df.hist(column='P_Long')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Pipeline.Stages.ComputationFunctions.MultiContextComputationFunctions.DirectionalPlacefieldGlobalComputationFunctions import plot_all_epoch_bins_marginal_predictions\n",
    "from attrs import define, field, Factory\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "\n",
    "@define(slots=False)\n",
    "class TwoCSV:\n",
    "\t\"\"\" simple class wrapper to emulate the object that holds the other dfs \"\"\"\n",
    "\tlaps_all_epoch_bins_marginals_df = field()\n",
    "\tripple_all_epoch_bins_marginals_df = field()\n",
    "\n",
    "save_figure = True\n",
    "def _perform_write_to_file_callback(final_context, fig):\n",
    "\tprint(f'final_context: {final_context}')\n",
    "\t# if save_figure:\n",
    "\t# \tfig.save_fig(\n",
    "\t# \t# return owning_pipeline_reference.output_figure(final_context, fig)\n",
    "\t# else:\n",
    "\t# \tpass # do nothing, don't save\n",
    "\t\n",
    "# all_sessions_laps_time_bin_df\n",
    "\n",
    "collector = plot_all_epoch_bins_marginal_predictions(TwoCSV(laps_all_epoch_bins_marginals_df=all_sessions_laps_df, ripple_all_epoch_bins_marginals_df=all_sessions_ripple_df), t_start=None, t_split=0.0, t_end=None,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tactive_context=IdentifyingContext(), perform_write_to_file_callback=_perform_write_to_file_callback)\n",
    "collector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tuple(collector.figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from pyphoplacecellanalysis.SpecificResults.AcrossSessionResults import H5FileAggregator\n",
    "\n",
    "h5_sessions = find_most_recent_files(found_session_export_paths=h5_files)\n",
    "h5_sessions\n",
    "\n",
    "final_sessions = {}\n",
    "time_bin_size_sweep_results_files = {}\n",
    "\n",
    "# final_sessions_loaded_laps_dict = {}\n",
    "# final_sessions_loaded_ripple_dict = {}\n",
    "# final_sessions_loaded_laps_time_bin_dict = {}\n",
    "# final_sessions_loaded_ripple_time_bin_dict = {}\n",
    "\n",
    "for session_str, session_dict in h5_sessions.items():\n",
    "    final_sessions[session_str] = {}\n",
    "    for file_type, (a_path, an_export_datetime) in session_dict.items():\n",
    "        final_sessions[session_str][file_type] = a_path\n",
    "        \n",
    "    session_name: str = str(session_str)  # Extract session name from the filename\n",
    "    if debug_print:\n",
    "        print(f'processing session_name: {session_name}')\n",
    "    curr_session_t_delta: Optional[float] = t_delta_dict.get(session_name, {}).get('t_delta', None)\n",
    "    if curr_session_t_delta is None:\n",
    "        print(f'WARN: curr_session_t_split is None for session_str = \"{session_str}\"')\n",
    "\n",
    "    # finds each of the four exports:\n",
    "    time_bin_size_sweep_results_file = final_sessions[session_str]['time_bin_size_sweep_results']\n",
    "    time_bin_size_sweep_results_files[session_str] = Path(time_bin_size_sweep_results_file).resolve()\n",
    "    # ripple_file = final_sessions[session_str]['ripple_marginals_df']\n",
    "    # laps_time_bin_file = final_sessions[session_str]['laps_time_bin_marginals_df']\n",
    "    # ripple_time_bin_file = final_sessions[session_str]['ripple_time_bin_marginals_df']\n",
    "    \n",
    "\n",
    "\n",
    "session_identifiers = [\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-08_21-16-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-09_22-24-40'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-10_12-25-50'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-09_16-40-54'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='two',session_name='2006-4-10_12-58-3'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_17-46-44'), # prev completed\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-02_19-28-0'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='11-03_12-3-25'),\n",
    "    IdentifyingContext(format_name='kdiba',animal='pin01',exper_name='one',session_name='fet11-01_12-58-54'), # prev completed\n",
    "]\n",
    "\n",
    "# Keys are like: \"/kdiba|gor01|two|2006-6-12_16-53-46\"\n",
    "included_h5_paths = list(time_bin_size_sweep_results_files.values())\n",
    "session_group_keys: List[str] = [(\"/\" + a_ctxt.get_description(separator=\"|\", include_property_names=False)) for a_ctxt in session_identifiers] # 'kdiba/gor01/one/2006-6-08_14-26-15'\n",
    "laps_decoding_accuracy_results_table_keys = [f\"{session_group_key}/laps_decoding_accuracy_results/table\" for session_group_key in session_group_keys]\n",
    "a_loader = H5FileAggregator.init_from_file_lists(file_list=included_h5_paths, table_key_list=laps_decoding_accuracy_results_table_keys)\n",
    "_out_table = a_loader.load_and_consolidate()\n",
    "_out_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_yellow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
