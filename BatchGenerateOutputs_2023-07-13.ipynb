{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056bc66-7629-4ef7-8c87-f28f8fcd9dc8",
   "metadata": {
    "autorun": true,
    "tags": [
     "imports",
     "REQUIRED",
     "ACTIVE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "build_module_logger(module_name=\"Spike3D.pipeline\"):\n",
      "\t Module logger com.PhoHale.Spike3D.pipeline has file logging enabled and will log to EXTERNAL\\TESTING\\Logging\\debug_com.PhoHale.Spike3D.pipeline.log\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# required to enable non-blocking interaction:\n",
    "%gui qt5\n",
    "\n",
    "## Pho's Custom Libraries:\n",
    "from pyphocorehelpers.Filesystem.path_helpers import find_first_extant_path\n",
    "from pyphocorehelpers.function_helpers import function_attributes\n",
    "\n",
    "# pyPhoPlaceCellAnalysis:\n",
    "# NeuroPy (Diba Lab Python Repo) Loading\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder\n",
    "from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.KDibaOldDataSessionFormat import KDibaOldDataSessionFormatRegisteredClass\n",
    "from neuropy.core.session.Formats.Specific.RachelDataSessionFormat import RachelDataSessionFormat\n",
    "from neuropy.core.session.Formats.Specific.HiroDataSessionFormat import HiroDataSessionFormatRegisteredClass\n",
    "\n",
    "## For computation parameters:\n",
    "from neuropy.utils.result_context import IdentifyingContext\n",
    "from neuropy.core.session.Formats.BaseDataSessionFormats import find_local_session_paths\n",
    "\n",
    "# from PendingNotebookCode import _perform_batch_plot, _build_batch_plot_kwargs\n",
    "# from pyphoplacecellanalysis.General.Batch.NonInteractiveProcessing import batch_load_session, batch_extended_computations, SessionBatchProgress, batch_programmatic_figures, batch_extended_programmatic_figures\n",
    "# from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "from pyphoplacecellanalysis.General.Pipeline.Stages.Loading import saveData, loadData\n",
    "import pyphoplacecellanalysis.General.Batch.runBatch\n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import BatchRun, BatchResultAccessor, run_diba_batch, BatchSessionCompletionHandler\n",
    "from pyphoplacecellanalysis.General.Pipeline.NeuropyPipeline import PipelineSavingScheme\n",
    "\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef5938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_global_batch_result_filename='global_batch_result_2023-07-12.pkl'\n",
    "# active_global_batch_result_filename='global_batch_result_2023-07-07.pkl'\n",
    "active_global_batch_result_filename='global_batch_result_2023-07-07_extra.pkl'\n",
    "\n",
    "debug_print=True\n",
    "enable_neptune = False\n",
    "\n",
    "if enable_neptune:\n",
    "    import neptune # for logging progress and results\n",
    "    from neptune.types import File\n",
    "    from pyphoplacecellanalysis.General.Batch.NeptuneAiHelpers import set_environment_variables, neptune_output_figures\n",
    "    neptune_kwargs = {'project':\"commander.pho/PhoDibaLongShort2023\",\n",
    "    'api_token':\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxOGIxODU2My1lZTNhLTQ2ZWMtOTkzNS02ZTRmNzM5YmNjNjIifQ==\"}\n",
    "    set_environment_variables(neptune_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4442b4ad-6384-47ec-a248-185b35b08a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalized_loaded_global_batch_result_pickle_path: W:\\Data\\global_batch_result_2023-07-07_extra.pkl\n",
      "Loading loaded session pickle file results : W:\\Data\\global_batch_result_2023-07-07_extra.pkl... encountered exception 'BatchRun' object is not iterable while printing. Turning into a warning and continuing.\n",
      "done.\n",
      "switching data dir path from \\home\\halechr\\turbo\\Data to W:\\Data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'known_global_data_root_parent_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:150\u001b[0m, in \u001b[0;36mBatchRun.change_global_root_path\u001b[1;34m(self, desired_global_data_root_parent_path)\u001b[0m\n\u001b[0;32m    149\u001b[0m     prev_global_data_root_parent_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_data_root_parent_path \u001b[39m# normally this would work\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     new_session_batch_basedirs \u001b[39m=\u001b[39m convert_filelist_to_new_parent(curr_filelist, original_parent_path\u001b[39m=\u001b[39;49mprev_global_data_root_parent_path, dest_parent_path\u001b[39m=\u001b[39;49mdesired_global_data_root_parent_path)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m     \u001b[39m# The global_batch_run.global_data_root_parent_path is wrong when:`ValueError: '\\\\nfs\\\\turbo\\\\umms-kdiba\\\\Data\\\\KDIBA\\\\gor01\\\\one\\\\2006-6-07_11-26-53' is not in the subpath of '\\\\home\\\\halechr\\\\turbo\\\\Data' OR one path is relative and the other is absolute.`. Try to find the real parent path.\u001b[39;00m\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoCoreHelpers\\src\\pyphocorehelpers\\Filesystem\\path_helpers.py:225\u001b[0m, in \u001b[0;36mconvert_filelist_to_new_parent\u001b[1;34m(filelist_source, original_parent_path, dest_parent_path)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m filelist_source:\n\u001b[1;32m--> 225\u001b[0m     relative_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(path\u001b[39m.\u001b[39;49mrelative_to(original_parent_path))\n\u001b[0;32m    226\u001b[0m     new_path \u001b[39m=\u001b[39m Path(dest_parent_path) \u001b[39m/\u001b[39m relative_path\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\pathlib.py:939\u001b[0m, in \u001b[0;36mPurePath.relative_to\u001b[1;34m(self, *other)\u001b[0m\n\u001b[0;32m    938\u001b[0m     formatted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_parsed_parts(to_drv, to_root, to_parts)\n\u001b[1;32m--> 939\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is not in the subpath of \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    940\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m OR one path is relative and the other is absolute.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m                      \u001b[39m.\u001b[39mformat(\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m), \u001b[39mstr\u001b[39m(formatted)))\n\u001b[0;32m    942\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_from_parsed_parts(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, root \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    943\u001b[0m                                abs_parts[n:])\n",
      "\u001b[1;31mValueError\u001b[0m: '\\\\nfs\\\\turbo\\\\umms-kdiba\\\\Data\\\\KDIBA\\\\gor01\\\\one\\\\2006-6-07_11-26-53' is not in the subpath of '\\\\home\\\\halechr\\\\turbo\\\\Data' OR one path is relative and the other is absolute.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m \trun[\u001b[39m\"\u001b[39m\u001b[39mdataset/latest\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtrack_files(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfile://\u001b[39m\u001b[39m{\u001b[39;00mglobal_batch_result_file_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39m# \"s3://datasets/images\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m# try to load an existing batch result:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m# with set_posix_windows():\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m global_batch_run \u001b[39m=\u001b[39m BatchRun\u001b[39m.\u001b[39;49mtry_init_from_file(global_data_root_parent_path, active_global_batch_result_filename\u001b[39m=\u001b[39;49mactive_global_batch_result_filename,\n\u001b[0;32m     29\u001b[0m \t\t\t\t\t\tskip_root_path_conversion\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, debug_print\u001b[39m=\u001b[39;49mdebug_print) \u001b[39m# on_needs_create_callback_fn=run_diba_batch\u001b[39;00m\n\u001b[0;32m     31\u001b[0m batch_progress_df \u001b[39m=\u001b[39m global_batch_run\u001b[39m.\u001b[39mto_dataframe(expand_context\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, good_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# all\u001b[39;00m\n\u001b[0;32m     32\u001b[0m good_only_batch_progress_df \u001b[39m=\u001b[39m global_batch_run\u001b[39m.\u001b[39mto_dataframe(expand_context\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, good_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:93\u001b[0m, in \u001b[0;36mBatchRun.try_init_from_file\u001b[1;34m(cls, global_data_root_parent_path, active_global_batch_result_filename, skip_root_path_conversion, debug_print)\u001b[0m\n\u001b[0;32m     90\u001b[0m global_batch_run \u001b[39m=\u001b[39m _try_load_global_batch_result()\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m (global_batch_run \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m skip_root_path_conversion):\n\u001b[0;32m     92\u001b[0m     \u001b[39m# One was loaded from file, meaning it has the potential to have the wrong paths. Check.\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     global_batch_run\u001b[39m.\u001b[39;49mchange_global_root_path(global_data_root_parent_path) \u001b[39m# Convert the paths to work on the new system:\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39m## Completely fresh, run the initial (pre-loading) results.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# Build `global_batch_run` pre-loading results (before execution)\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# assert on_needs_create_callback_fn is not None\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     global_batch_run \u001b[39m=\u001b[39m run_diba_batch(global_data_root_parent_path, execute_all\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, extant_batch_run\u001b[39m=\u001b[39mglobal_batch_run, debug_print\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\repos\\Spike3DWorkEnv\\pyPhoPlaceCellAnalysis\\src\\pyphoplacecellanalysis\\General\\Batch\\runBatch.py:153\u001b[0m, in \u001b[0;36mBatchRun.change_global_root_path\u001b[1;34m(self, desired_global_data_root_parent_path)\u001b[0m\n\u001b[0;32m    150\u001b[0m     new_session_batch_basedirs \u001b[39m=\u001b[39m convert_filelist_to_new_parent(curr_filelist, original_parent_path\u001b[39m=\u001b[39mprev_global_data_root_parent_path, dest_parent_path\u001b[39m=\u001b[39mdesired_global_data_root_parent_path)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m     \u001b[39m# The global_batch_run.global_data_root_parent_path is wrong when:`ValueError: '\\\\nfs\\\\turbo\\\\umms-kdiba\\\\Data\\\\KDIBA\\\\gor01\\\\one\\\\2006-6-07_11-26-53' is not in the subpath of '\\\\home\\\\halechr\\\\turbo\\\\Data' OR one path is relative and the other is absolute.`. Try to find the real parent path.\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     prev_global_data_root_parent_path \u001b[39m=\u001b[39m find_matching_parent_path(known_global_data_root_parent_paths, curr_filelist[\u001b[39m0\u001b[39m]) \u001b[39m# TODO: assumes all have the same root, which is a valid assumption so far. ## prev_global_data_root_parent_path should contain the matching path from the list.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[39massert\u001b[39;00m prev_global_data_root_parent_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo matching root parent path could be found!!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m     new_session_batch_basedirs \u001b[39m=\u001b[39m convert_filelist_to_new_parent(curr_filelist, original_parent_path\u001b[39m=\u001b[39mprev_global_data_root_parent_path, dest_parent_path\u001b[39m=\u001b[39mdesired_global_data_root_parent_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'known_global_data_root_parent_paths' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "from pyphoplacecellanalysis.General.Batch.runBatch import main, BatchRun, run_diba_batch, run_specific_batch\n",
    "\n",
    "\"\"\"\n",
    "known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/nfs/turbo/umms-kdiba/Data')]\n",
    "global_data_root_parent_path = find_first_extant_path(known_global_data_root_parent_paths)\n",
    "assert global_data_root_parent_path.exists(), f\"global_data_root_parent_path: {global_data_root_parent_path} does not exist! Is the right computer's config commented out above?\"\n",
    "\n",
    "## NEPTUNE:\n",
    "if enable_neptune:\n",
    "\tproject = neptune.init_project(**neptune_kwargs)\n",
    "\tproject[\"general/global_batch_result_filename\"] = active_global_batch_result_filename\n",
    "\tproject[\"general/global_data_root_parent_path\"] = global_data_root_parent_path.as_posix()\n",
    "\n",
    "## TODO: load the batch result initially:\n",
    "\n",
    "## Build Pickle Path:\n",
    "global_batch_result_file_path = Path(global_data_root_parent_path).joinpath(active_global_batch_result_filename).resolve() # Use Default\n",
    "\n",
    "if enable_neptune:\n",
    "\trun = neptune.init_run() # rember to run.stop()\n",
    "\trun['parameters/global_batch_result_file_path'] = global_batch_result_file_path.as_posix()\n",
    "\t# project[\"general/data_analysis\"].upload(\"data_analysis.ipynb\")\n",
    "\trun[\"dataset/latest\"].track_files(f\"file://{global_batch_result_file_path}\") # \"s3://datasets/images\"\n",
    "\n",
    "# try to load an existing batch result:\n",
    "# with set_posix_windows():\n",
    "global_batch_run = BatchRun.try_init_from_file(global_data_root_parent_path, active_global_batch_result_filename=active_global_batch_result_filename,\n",
    "\t\t\t\t\t\tskip_root_path_conversion=False, debug_print=debug_print) # on_needs_create_callback_fn=run_diba_batch\n",
    "\n",
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "\n",
    "if enable_neptune:\n",
    "\trun[\"dataset/global_batch_run_progress_df\"].upload(File.as_html(batch_progress_df)) # \"path/to/test_preds.csv\"\n",
    "\trun[\"dataset/global_batch_run_good_only_df\"].upload(File.as_html(good_only_batch_progress_df)) # \"path/to/test_preds.csv\"\n",
    "\n",
    "batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_outputs\n",
    "# 'outputs': {'local': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-04_21-20-3/loadedSessPickle.pkl'),\n",
    "#    'global': PosixPath('/nfs/turbo/umms-kdiba/Data/KDIBA/pin01/one/fet11-04_21-20-3/output/global_computation_results.pkl')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85809c31-2e4e-4bbc-a313-5f40fa0c05dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    # display(batch_progress_df)\n",
    "    good_only_batch_progress_df.batch_results.build_all_columns()\n",
    "    display(good_only_batch_progress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e5298-4374-458f-8473-8dbc84b4689c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pyphocorehelpers.Filesystem.path_helpers import set_posix_windows, convert_filelist_to_new_parent, find_matching_parent_path\n",
    "\n",
    "# known_global_data_root_parent_paths = [Path(r'W:\\Data'), Path(r'/media/MAX/Data'), Path(r'/Volumes/MoverNew/data'), Path(r'/home/halechr/turbo/Data'), Path(r'/nfs/turbo/umms-kdiba/Data')]\n",
    "\n",
    "desired_global_data_root_parent_path = global_data_root_parent_path\n",
    "curr_filelist = list(global_batch_run.session_batch_basedirs.values())\n",
    "try:\n",
    "    prev_global_data_root_parent_path = global_batch_run.global_data_root_parent_path # normally this would work\n",
    "    new_session_batch_basedirs = convert_filelist_to_new_parent(curr_filelist, original_parent_path=prev_global_data_root_parent_path, dest_parent_path=desired_global_data_root_parent_path)\n",
    "except ValueError as e:\n",
    "    # The global_batch_run.global_data_root_parent_path is wrong when:`ValueError: '\\\\nfs\\\\turbo\\\\umms-kdiba\\\\Data\\\\KDIBA\\\\gor01\\\\one\\\\2006-6-07_11-26-53' is not in the subpath of '\\\\home\\\\halechr\\\\turbo\\\\Data' OR one path is relative and the other is absolute.`. Try to find the real parent path.\n",
    "    prev_global_data_root_parent_path = find_matching_parent_path(known_global_data_root_parent_paths, curr_filelist[0]) # TODO: assumes all have the same root, which is a valid assumption so far. ## prev_global_data_root_parent_path should contain the matching path from the list.\n",
    "    assert prev_global_data_root_parent_path is not None, f\"No matching root parent path could be found!!\"\n",
    "    new_session_batch_basedirs = convert_filelist_to_new_parent(curr_filelist, original_parent_path=prev_global_data_root_parent_path, dest_parent_path=desired_global_data_root_parent_path)\n",
    "except Exception as e:\n",
    "    ## Unhandled Exception\n",
    "    raise\n",
    "\n",
    "print(f'Switched data dir path from \"{str(prev_global_data_root_parent_path)}\" to \"{str(desired_global_data_root_parent_path)}\"')\n",
    "new_session_batch_basedirs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635a797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[v.resolve().exists() for v in list(good_only_batch_progress_df.basedirs.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a084aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_path_columns = ['basedirs', 'global_computation_result_file', 'loaded_session_pickle_file', 'ripple_result_file']\n",
    "\n",
    "# Convert all potential PosixPath-type columns to regular strings for serialization, so they can be unpickled on Windows:\n",
    "batch_progress_df[potential_filesystem_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4716aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_progress_df = batch_progress_df.batch_results.convert_path_columns_to_str()\n",
    "batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348205cf-4373-4c92-8305-335beafe506a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyphoplacecellanalysis.General.Batch.PhoDiba2023Paper import UserAnnotationsManager\n",
    "\n",
    "user_is_replay_good_annotations = UserAnnotationsManager.get_user_annotations()\n",
    "\n",
    "good_only_batch_progress_df['has_user_replay_annotations'] = [(a_row_context.adding_context_if_missing(display_fn_name='DecodedEpochSlices',epochs='replays',decoder='long_results_obj',user_annotation='selections') in user_is_replay_good_annotations) for a_row_context in good_only_batch_progress_df['context']]\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab6e6b-e289-43ba-b35a-d6c837887026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in good_only_batch_progress_df.iterrows():\n",
    "    if row['is_ready']:\n",
    "        row_context = row['context']\n",
    "        print(f\"context: {row_context}\")\n",
    "        row_context in user_is_replay_good_annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f42b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build `global_batch_run` pre-loading results (before execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a7c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global_batch_result = loadData('global_batch_result.pkl')\n",
    "global_batch_run = run_diba_batch(global_data_root_parent_path, execute_all=False, extant_batch_run=global_batch_run, debug_print=False)\n",
    "# print(f'global_batch_result: {global_batch_run}')\n",
    "global_batch_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab824348",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Run Batch Executions/Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d043e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Skip sessions that aren't already computed. Refuse to do any computations (fail if computations are not done already)\n",
    "# Compute as needed, and if computations were done save the result (caching it for future loading)\n",
    "\n",
    "# Force Recompute:\n",
    "#   - Compute always (this mode would be used after a bug was fixed in the computations and we want to be sure the latest version is used to produce the result) regardless of whether extant results exist. Once computation is complete, overwrite extant files if they exist.\n",
    "#   - Same as above but do not save at all (just use in-memory result for something)\n",
    "#   - Same as first one but save the result with a special filename (a variant) as not to overwrite the original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd73b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I got it doing the bare-minimum loading and computations, so it should be ready to update the laps and constraint the placefields to those. Then we should be able to set up the replays at the same time.\n",
    "# finally, we then finish by computing.\n",
    "# force_reload = True\n",
    "force_reload = False\n",
    "result_handler = BatchSessionCompletionHandler(force_reload_all=force_reload, should_perform_figure_generation_to_file=False, saving_mode=PipelineSavingScheme.SKIP_SAVING, force_global_recompute=False)\n",
    "\n",
    "## Execute with the custom arguments.\n",
    "active_computation_functions_name_includelist=['_perform_baseline_placefield_computation',\n",
    "                                        # '_perform_time_dependent_placefield_computation',\n",
    "                                        '_perform_extended_statistics_computation',\n",
    "                                        '_perform_position_decoding_computation', \n",
    "                                        '_perform_firing_rate_trends_computation',\n",
    "                                        '_perform_pf_find_ratemap_peaks_computation',\n",
    "                                        # '_perform_time_dependent_pf_sequential_surprise_computation'\n",
    "                                        '_perform_two_step_position_decoding_computation',\n",
    "                                        # '_perform_recursive_latent_placefield_decoding'\n",
    "                                    ]\n",
    "# active_computation_functions_name_includelist=['_perform_baseline_placefield_computation']\n",
    "global_batch_run.execute_all(force_reload=force_reload, saving_mode=PipelineSavingScheme.SKIP_SAVING, skip_extended_batch_computations=True, post_run_callback_fn=result_handler.on_complete_success_execution_session,\n",
    "                                                                                        **{'computation_functions_name_includelist': active_computation_functions_name_includelist,\n",
    "                                                                                            'active_session_computation_configs': None,\n",
    "                                                                                            'allow_processing_previously_completed': True}) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 4m 39.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45172b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last completed:\n",
    "r'W:\\Data\\KDIBA\\pin01\\one\\11-03_12-3-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d2321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to file:\n",
    "saveData(global_batch_result_file_path, global_batch_run) # Update the global batch run dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the instantaneous firing rate results dict: (# Dict[IdentifyingContext] = InstantaneousSpikeRateGroupsComputation)\n",
    "inst_fr_output_filename = 'long_short_inst_firing_rate_result.pkl'\n",
    "global_batch_result_inst_fr_file_path = Path(global_data_root_parent_path).joinpath(inst_fr_output_filename).resolve() # Use Default\n",
    "print(f'global_batch_result_inst_fr_file_path: {global_batch_result_inst_fr_file_path}')\n",
    "# Save the all sessions instantaneous firing rate dict to the path:\n",
    "saveData(global_batch_result_inst_fr_file_path, result_handler.across_sessions_instantaneous_fr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a180ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 2023-07-12 10:12: - [ ] New save way after we save out current result and reload\n",
    "result_handler.save_across_sessions_data(global_data_root_parent_path=global_data_root_parent_path, inst_fr_output_filename='across_session_result_long_short_inst_firing_rate.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0e3f6-5c37-42e1-b06d-7a8028cb9512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_sessions = len(result_handler.across_sessions_instantaneous_fr_dict)\n",
    "print(f'num_sessions: {num_sessions}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32781c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Single Session testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_out = global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, computation_functions_name_includelist =['_perform_baseline_placefield_computation'], active_session_computation_configs=None) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "_test_out\n",
    "\n",
    "# global_batch_run.execute_session(session_context=curr_sess_context, force_reload=True, skip_extended_batch_computations=True, **{'computation_functions_name_includelist': ['_perform_baseline_placefield_computation'], 'active_session_computation_configs': None}) # can override `active_session_computation_configs` if we want to set custom ones like only the laps.)\n",
    "\n",
    "# 23.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2bb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "full_good_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is None]\n",
    "bad_dirs = [k for k, v in global_batch_run.session_batch_errors.items() if v is not None]\n",
    "full_good_dirs\n",
    "bad_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f73f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_run.session_batch_status\n",
    "global_batch_run.session_batch_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faf2bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed516134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=False) # all\n",
    "good_only_batch_progress_df = global_batch_run.to_dataframe(expand_context=True, good_only=True)\n",
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf02efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the list of sessions that are completely ready to process:\n",
    "full_good_ready_to_process_sessions = list(good_only_batch_progress_df['context'].to_numpy())\n",
    "full_good_ready_to_process_sessions\n",
    "# Get good sessions for use in the specific session processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ad809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run[\"good_sessions_list\"].extend(full_good_ready_to_process_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636e3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.stop()\n",
    "project.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\",\\n\".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # List definitions\n",
    "\n",
    "# [IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46'),\n",
    "# IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a4bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\ncurr_context = \".join([ctx.get_initialization_code_string() for ctx in full_good_ready_to_process_sessions])) # Line definitions\n",
    "\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-08_14-26-15')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-09_1-22-43')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-12_15-55-31')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='one',session_name='2006-6-13_14-42-6')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-07_16-40-19')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='gor01',exper_name='two',session_name='2006-6-12_16-53-46')\n",
    "# curr_context = IdentifyingContext(format_name='kdiba',animal='vvp01',exper_name='one',session_name='2006-4-09_17-29-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5689d1d-6400-4f87-be0e-a184a1d5bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_only_batch_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82aedf-b024-4f07-b1a9-350730b4db8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "save_time = datetime.now()\n",
    " \n",
    "print(\"save_time =\", save_time)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = save_time.strftime(\"%Y-%m-%d_%I-%M%p\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc6bd-5b34-41d0-b906-b0ad9927b08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get output file paths:\n",
    "completed_pipeline_filename = 'loadedSessPickle.pkl'\n",
    "completed_global_computations_filename = 'outputs/global_computation_results.pkl'\n",
    "\n",
    "full_good_ready_to_process_session_paths = list(good_only_batch_progress_df['basedirs'].to_numpy())\n",
    "session_paths_output_folders = [sess_path.joinpath('outputs').resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "\n",
    "\n",
    "\n",
    "completed_pipeline_file_paths = [sess_path.joinpath(completed_pipeline_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths = [sess_path.joinpath(completed_global_computations_filename).resolve() for sess_path in full_good_ready_to_process_session_paths]\n",
    "completed_global_computations_file_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
